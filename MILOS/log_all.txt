Started [17:15:44 07.01.2023] 301) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1974 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.27
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.27
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.27
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         2             0.10
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.27
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.27
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.27
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.82; perplexity/K = 65.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.57; perplexity/K = 61.15%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  274986.7 e-6; = (1/var)*||X-X_r||^2 =  187045.8 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  87940.9 e-6 = 32.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  278262.8 e-6; = (1/var)*||X-X_r||^2 =  198821.6 e-6 = 71.5 %; (1+beta)*||Z_e-Z_q||^2 =  79441.2 e-6 = 28.5 %)
Min.  Avg. Train Loss across Mini-Batch =  273162.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  273935.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3276.1 e-6; = (1/var)*||X-X_r||^2 val-train = 11775.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8499.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.86; perplexity/K = 65.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.73; perplexity/K = 61.64%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57982.9 e-6; = (1/var)*||X-X_r||^2 =  46475.1 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  11507.8 e-6 = 19.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  65536.9 e-6; = (1/var)*||X-X_r||^2 =  54356.4 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  11180.4 e-6 = 17.1 %)
Min.  Avg. Train Loss across Mini-Batch =  56596.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  64999.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7553.9 e-6; = (1/var)*||X-X_r||^2 val-train = 7881.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -327.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.90; perplexity/K = 62.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.50; perplexity/K = 60.95%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  126474.7 e-6; = (1/var)*||X-X_r||^2 =  109833.3 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  16641.4 e-6 = 13.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  42658.2 e-6; = (1/var)*||X-X_r||^2 =  35434.7 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  7223.5 e-6 = 16.9 %)
Min.  Avg. Train Loss across Mini-Batch =  21505.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25987.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -83816.5 e-6; = (1/var)*||X-X_r||^2 val-train = -74398.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9417.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.71; perplexity/K = 77.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.63; perplexity/K = 73.86%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13260.1 e-6; = (1/var)*||X-X_r||^2 =  9628.8 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  3631.3 e-6 = 27.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  14543.1 e-6; = (1/var)*||X-X_r||^2 =  10987.7 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  3555.4 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  9762.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13169.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1282.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1358.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -75.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.54; perplexity/K = 76.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.89; perplexity/K = 71.54%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5826.3 e-6; = (1/var)*||X-X_r||^2 =  3418.4 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  2407.8 e-6 = 41.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  8099.0 e-6; = (1/var)*||X-X_r||^2 =  5658.8 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  2440.2 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5826.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8099.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2272.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2240.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.66; perplexity/K = 73.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.47; perplexity/K = 73.33%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4005.0 e-6; = (1/var)*||X-X_r||^2 =  2082.3 e-6 = 52.0 %; (1+beta)*||Z_e-Z_q||^2 =  1922.8 e-6 = 48.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  5999.7 e-6; = (1/var)*||X-X_r||^2 =  3979.2 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  2020.6 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4005.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5999.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1994.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1896.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 97.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.23; perplexity/K = 75.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.67; perplexity/K = 70.84%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3498.2 e-6; = (1/var)*||X-X_r||^2 =  2074.2 e-6 = 59.3 %; (1+beta)*||Z_e-Z_q||^2 =  1424.0 e-6 = 40.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  5599.4 e-6; = (1/var)*||X-X_r||^2 =  3994.9 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  1604.6 e-6 = 28.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3062.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5104.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2101.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1920.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 180.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.52; perplexity/K = 73.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.30; perplexity/K = 72.81%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2489.9 e-6; = (1/var)*||X-X_r||^2 =  1274.5 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  1215.4 e-6 = 48.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4046.2 e-6; = (1/var)*||X-X_r||^2 =  2736.9 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  1309.3 e-6 = 32.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2489.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4036.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1556.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1462.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 93.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.83; perplexity/K = 71.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.28; perplexity/K = 69.61%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2373.5 e-6; = (1/var)*||X-X_r||^2 =  1161.6 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  1211.9 e-6 = 51.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3697.7 e-6; = (1/var)*||X-X_r||^2 =  2452.1 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  1245.7 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2299.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3631.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1324.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1290.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 33.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.41; perplexity/K = 66.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.27; perplexity/K = 66.46%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33117.7 e-6; = (1/var)*||X-X_r||^2 =  30045.4 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  3072.2 e-6 = 9.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  18662.6 e-6; = (1/var)*||X-X_r||^2 =  16531.5 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  2131.1 e-6 = 11.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1866.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3073.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -14455.0 e-6; = (1/var)*||X-X_r||^2 val-train = -13513.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -941.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.09; perplexity/K = 72.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.75; perplexity/K = 71.11%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1787.6 e-6; = (1/var)*||X-X_r||^2 =  814.2 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  973.4 e-6 = 54.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2792.5 e-6; = (1/var)*||X-X_r||^2 =  1793.8 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  998.8 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1696.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2792.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1004.9 e-6; = (1/var)*||X-X_r||^2 val-train = 979.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.92; perplexity/K = 71.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.13; perplexity/K = 72.29%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1453.9 e-6; = (1/var)*||X-X_r||^2 =  712.4 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  741.5 e-6 = 51.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2522.0 e-6; = (1/var)*||X-X_r||^2 =  1718.0 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  804.0 e-6 = 31.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1443.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2369.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1068.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1005.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 62.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.93; perplexity/K = 71.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.19; perplexity/K = 72.48%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1386.8 e-6; = (1/var)*||X-X_r||^2 =  706.0 e-6 = 50.9 %; (1+beta)*||Z_e-Z_q||^2 =  680.8 e-6 = 49.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2303.4 e-6; = (1/var)*||X-X_r||^2 =  1571.9 e-6 = 68.2 %; (1+beta)*||Z_e-Z_q||^2 =  731.5 e-6 = 31.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1245.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2109.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   916.6 e-6; = (1/var)*||X-X_r||^2 val-train = 865.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 50.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.13; perplexity/K = 72.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.43; perplexity/K = 70.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1219.3 e-6; = (1/var)*||X-X_r||^2 =  539.9 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  679.3 e-6 = 55.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2035.9 e-6; = (1/var)*||X-X_r||^2 =  1324.6 e-6 = 65.1 %; (1+beta)*||Z_e-Z_q||^2 =  711.3 e-6 = 34.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1189.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1972.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   816.7 e-6; = (1/var)*||X-X_r||^2 val-train = 784.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.69; perplexity/K = 70.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.32; perplexity/K = 72.88%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1116.1 e-6; = (1/var)*||X-X_r||^2 =  509.3 e-6 = 45.6 %; (1+beta)*||Z_e-Z_q||^2 =  606.7 e-6 = 54.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1890.9 e-6; = (1/var)*||X-X_r||^2 =  1261.8 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  629.1 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1054.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1856.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   774.8 e-6; = (1/var)*||X-X_r||^2 val-train = 752.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.12; perplexity/K = 72.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.08; perplexity/K = 72.11%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1157.3 e-6; = (1/var)*||X-X_r||^2 =  648.9 e-6 = 56.1 %; (1+beta)*||Z_e-Z_q||^2 =  508.4 e-6 = 43.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1723.8 e-6; = (1/var)*||X-X_r||^2 =  1161.2 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  562.6 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  976.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1662.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   566.5 e-6; = (1/var)*||X-X_r||^2 val-train = 512.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 54.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.69; perplexity/K = 70.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.68; perplexity/K = 70.87%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1559.2 e-6; = (1/var)*||X-X_r||^2 =  638.5 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  920.7 e-6 = 59.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2287.0 e-6; = (1/var)*||X-X_r||^2 =  1376.2 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  910.7 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  899.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1602.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   727.8 e-6; = (1/var)*||X-X_r||^2 val-train = 737.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.87; perplexity/K = 71.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.23; perplexity/K = 69.46%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  994.9 e-6; = (1/var)*||X-X_r||^2 =  427.5 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  567.4 e-6 = 57.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1680.9 e-6; = (1/var)*||X-X_r||^2 =  1085.8 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  595.1 e-6 = 35.4 %)
Min.  Avg. Train Loss across Mini-Batch =  883.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1529.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   686.0 e-6; = (1/var)*||X-X_r||^2 val-train = 658.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.93; perplexity/K = 68.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.04; perplexity/K = 72.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  924.4 e-6; = (1/var)*||X-X_r||^2 =  399.0 e-6 = 43.2 %; (1+beta)*||Z_e-Z_q||^2 =  525.5 e-6 = 56.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1559.6 e-6; = (1/var)*||X-X_r||^2 =  1011.3 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  548.3 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  791.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1394.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   635.2 e-6; = (1/var)*||X-X_r||^2 val-train = 612.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.13; perplexity/K = 72.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.61; perplexity/K = 70.67%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  787.8 e-6; = (1/var)*||X-X_r||^2 =  392.2 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  395.6 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1412.0 e-6; = (1/var)*||X-X_r||^2 =  980.9 e-6 = 69.5 %; (1+beta)*||Z_e-Z_q||^2 =  431.1 e-6 = 30.5 %)
Min.  Avg. Train Loss across Mini-Batch =  720.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1321.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   624.3 e-6; = (1/var)*||X-X_r||^2 val-train = 588.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35.6 e-6 

----------------------------------------------------------------------------------

Finished [18:20:15 07.01.2023] 301) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = False:
Total training time is = 0:3:30 h/m/s. 

--------------------------------------------------- 

Started [18:20:15 07.01.2023] 302) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1973 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.28
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.28
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.28
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         1             0.05
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.28
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.28
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.28
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.04; perplexity/K = 75.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.64; perplexity/K = 72.77%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  402094.3 e-6; = (1/var)*||X-X_r||^2 =  166605.5 e-6 = 41.4 %; (1+beta)*||Z_e-Z_q||^2 =  235488.8 e-6 = 58.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  384447.3 e-6; = (1/var)*||X-X_r||^2 =  176919.1 e-6 = 46.0 %; (1+beta)*||Z_e-Z_q||^2 =  207528.3 e-6 = 54.0 %)
Min.  Avg. Train Loss across Mini-Batch =  402094.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384447.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17646.9 e-6; = (1/var)*||X-X_r||^2 val-train = 10313.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -27960.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 68.78%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  135227.4 e-6; = (1/var)*||X-X_r||^2 =  44815.4 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  90412.0 e-6 = 66.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  145870.2 e-6; = (1/var)*||X-X_r||^2 =  57317.1 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  88553.1 e-6 = 60.7 %)
Min.  Avg. Train Loss across Mini-Batch =  133133.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142247.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10642.8 e-6; = (1/var)*||X-X_r||^2 val-train = 12501.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1858.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.23; perplexity/K = 70.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.84; perplexity/K = 74.00%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  66126.2 e-6; = (1/var)*||X-X_r||^2 =  16264.3 e-6 = 24.6 %; (1+beta)*||Z_e-Z_q||^2 =  49861.9 e-6 = 75.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  68418.7 e-6; = (1/var)*||X-X_r||^2 =  22009.7 e-6 = 32.2 %; (1+beta)*||Z_e-Z_q||^2 =  46409.0 e-6 = 67.8 %)
Min.  Avg. Train Loss across Mini-Batch =  38851.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49056.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2292.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5745.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3452.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.95; perplexity/K = 74.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.84; perplexity/K = 74.02%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15444.6 e-6; = (1/var)*||X-X_r||^2 =  3739.2 e-6 = 24.2 %; (1+beta)*||Z_e-Z_q||^2 =  11705.5 e-6 = 75.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  22826.4 e-6; = (1/var)*||X-X_r||^2 =  9818.0 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  13008.5 e-6 = 57.0 %)
Min.  Avg. Train Loss across Mini-Batch =  15404.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  22826.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7381.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6078.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1303.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.64; perplexity/K = 72.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.05; perplexity/K = 75.33%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10342.9 e-6; = (1/var)*||X-X_r||^2 =  1891.7 e-6 = 18.3 %; (1+beta)*||Z_e-Z_q||^2 =  8451.2 e-6 = 81.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  16006.5 e-6; = (1/var)*||X-X_r||^2 =  6735.9 e-6 = 42.1 %; (1+beta)*||Z_e-Z_q||^2 =  9270.6 e-6 = 57.9 %)
Min.  Avg. Train Loss across Mini-Batch =  9053.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15353.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5663.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4844.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 819.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.77; perplexity/K = 73.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.42; perplexity/K = 71.36%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7122.0 e-6; = (1/var)*||X-X_r||^2 =  1410.4 e-6 = 19.8 %; (1+beta)*||Z_e-Z_q||^2 =  5711.6 e-6 = 80.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  12407.7 e-6; = (1/var)*||X-X_r||^2 =  6121.8 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  6285.8 e-6 = 50.7 %)
Min.  Avg. Train Loss across Mini-Batch =  6870.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11844.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5285.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4711.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 574.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.85; perplexity/K = 74.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.86; perplexity/K = 67.85%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8044.5 e-6; = (1/var)*||X-X_r||^2 =  1077.8 e-6 = 13.4 %; (1+beta)*||Z_e-Z_q||^2 =  6966.7 e-6 = 86.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  11815.5 e-6; = (1/var)*||X-X_r||^2 =  4420.6 e-6 = 37.4 %; (1+beta)*||Z_e-Z_q||^2 =  7394.9 e-6 = 62.6 %)
Min.  Avg. Train Loss across Mini-Batch =  4020.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9023.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3771.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3342.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 428.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.46; perplexity/K = 71.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.96; perplexity/K = 68.53%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3776.0 e-6; = (1/var)*||X-X_r||^2 =  640.9 e-6 = 17.0 %; (1+beta)*||Z_e-Z_q||^2 =  3135.1 e-6 = 83.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  7750.8 e-6; = (1/var)*||X-X_r||^2 =  4128.9 e-6 = 53.3 %; (1+beta)*||Z_e-Z_q||^2 =  3621.9 e-6 = 46.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3776.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7581.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3974.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3488.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 486.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.55; perplexity/K = 72.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.10; perplexity/K = 69.35%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8452.8 e-6; = (1/var)*||X-X_r||^2 =  884.2 e-6 = 10.5 %; (1+beta)*||Z_e-Z_q||^2 =  7568.5 e-6 = 89.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  12964.8 e-6; = (1/var)*||X-X_r||^2 =  5273.9 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  7690.9 e-6 = 59.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3461.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7242.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4512.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4389.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 122.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.92; perplexity/K = 74.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.44; perplexity/K = 71.50%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11552.0 e-6; = (1/var)*||X-X_r||^2 =  3004.2 e-6 = 26.0 %; (1+beta)*||Z_e-Z_q||^2 =  8547.8 e-6 = 74.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  66942.8 e-6; = (1/var)*||X-X_r||^2 =  17288.0 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  49654.8 e-6 = 74.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2562.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4819.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55390.7 e-6; = (1/var)*||X-X_r||^2 val-train = 14283.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 41107.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.86; perplexity/K = 74.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.80; perplexity/K = 67.50%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3221.4 e-6; = (1/var)*||X-X_r||^2 =  578.1 e-6 = 17.9 %; (1+beta)*||Z_e-Z_q||^2 =  2643.2 e-6 = 82.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8805.9 e-6; = (1/var)*||X-X_r||^2 =  4287.0 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  4518.8 e-6 = 51.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2562.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4819.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5584.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3708.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1875.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.00; perplexity/K = 74.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.47; perplexity/K = 71.71%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11202.8 e-6; = (1/var)*||X-X_r||^2 =  2353.4 e-6 = 21.0 %; (1+beta)*||Z_e-Z_q||^2 =  8849.5 e-6 = 79.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  11264.8 e-6; = (1/var)*||X-X_r||^2 =  3503.0 e-6 = 31.1 %; (1+beta)*||Z_e-Z_q||^2 =  7761.8 e-6 = 68.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2405.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4819.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   62.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1149.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1087.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.82; perplexity/K = 73.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.49; perplexity/K = 71.78%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2739.7 e-6; = (1/var)*||X-X_r||^2 =  333.7 e-6 = 12.2 %; (1+beta)*||Z_e-Z_q||^2 =  2406.0 e-6 = 87.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  7034.3 e-6; = (1/var)*||X-X_r||^2 =  3896.3 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  3138.0 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2405.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4819.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4294.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3562.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 732.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.22; perplexity/K = 70.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.17; perplexity/K = 69.80%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8541.3 e-6; = (1/var)*||X-X_r||^2 =  1939.5 e-6 = 22.7 %; (1+beta)*||Z_e-Z_q||^2 =  6601.8 e-6 = 77.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  313471.7 e-6; = (1/var)*||X-X_r||^2 =  51117.1 e-6 = 16.3 %; (1+beta)*||Z_e-Z_q||^2 =  262354.6 e-6 = 83.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4672.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   304930.4 e-6; = (1/var)*||X-X_r||^2 val-train = 49177.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 255752.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.85; perplexity/K = 74.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.81; perplexity/K = 67.55%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2431.7 e-6; = (1/var)*||X-X_r||^2 =  245.1 e-6 = 10.1 %; (1+beta)*||Z_e-Z_q||^2 =  2186.6 e-6 = 89.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5011.7 e-6; = (1/var)*||X-X_r||^2 =  2515.5 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  2496.2 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4672.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2580.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2270.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 309.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.56; perplexity/K = 72.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.90; perplexity/K = 68.15%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6448.7 e-6; = (1/var)*||X-X_r||^2 =  526.8 e-6 = 8.2 %; (1+beta)*||Z_e-Z_q||^2 =  5921.9 e-6 = 91.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  9526.6 e-6; = (1/var)*||X-X_r||^2 =  3213.7 e-6 = 33.7 %; (1+beta)*||Z_e-Z_q||^2 =  6313.0 e-6 = 66.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4672.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3078.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2686.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 391.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.37; perplexity/K = 71.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.94; perplexity/K = 68.38%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8564.6 e-6; = (1/var)*||X-X_r||^2 =  658.7 e-6 = 7.7 %; (1+beta)*||Z_e-Z_q||^2 =  7905.8 e-6 = 92.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  12392.2 e-6; = (1/var)*||X-X_r||^2 =  3728.6 e-6 = 30.1 %; (1+beta)*||Z_e-Z_q||^2 =  8663.6 e-6 = 69.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4391.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3827.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3069.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 757.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.25; perplexity/K = 76.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.72; perplexity/K = 73.26%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33777.8 e-6; = (1/var)*||X-X_r||^2 =  4713.9 e-6 = 14.0 %; (1+beta)*||Z_e-Z_q||^2 =  29063.9 e-6 = 86.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  26305.2 e-6; = (1/var)*||X-X_r||^2 =  5720.8 e-6 = 21.7 %; (1+beta)*||Z_e-Z_q||^2 =  20584.4 e-6 = 78.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4391.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7472.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1006.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8479.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.01; perplexity/K = 68.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.86; perplexity/K = 67.85%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3648.0 e-6; = (1/var)*||X-X_r||^2 =  260.7 e-6 = 7.1 %; (1+beta)*||Z_e-Z_q||^2 =  3387.4 e-6 = 92.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5821.2 e-6; = (1/var)*||X-X_r||^2 =  2235.3 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  3586.0 e-6 = 61.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1147.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4125.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2173.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1974.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 198.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.03; perplexity/K = 68.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.41; perplexity/K = 71.33%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  62071.1 e-6; = (1/var)*||X-X_r||^2 =  11274.1 e-6 = 18.2 %; (1+beta)*||Z_e-Z_q||^2 =  50797.0 e-6 = 81.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  46695.9 e-6; = (1/var)*||X-X_r||^2 =  14580.0 e-6 = 31.2 %; (1+beta)*||Z_e-Z_q||^2 =  32115.9 e-6 = 68.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1147.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4125.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15375.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3305.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18681.0 e-6 

----------------------------------------------------------------------------------

Finished [19:24:52 07.01.2023] 302) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = False:
Total training time is = 0:3:37 h/m/s. 

--------------------------------------------------- 

Started [19:24:52 07.01.2023] 303) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1972 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.29
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.29
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.29
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.29
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.29
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.29
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.27; perplexity/K = 78.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.46; perplexity/K = 80.81%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  291967.3 e-6; = (1/var)*||X-X_r||^2 =  157158.5 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  134808.9 e-6 = 46.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  279777.7 e-6; = (1/var)*||X-X_r||^2 =  156217.4 e-6 = 55.8 %; (1+beta)*||Z_e-Z_q||^2 =  123560.3 e-6 = 44.2 %)
Min.  Avg. Train Loss across Mini-Batch =  261490.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  276536.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12189.7 e-6; = (1/var)*||X-X_r||^2 val-train = -941.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11248.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.48; perplexity/K = 81.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.30; perplexity/K = 78.70%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  91800.6 e-6; = (1/var)*||X-X_r||^2 =  36860.2 e-6 = 40.2 %; (1+beta)*||Z_e-Z_q||^2 =  54940.3 e-6 = 59.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  114263.4 e-6; = (1/var)*||X-X_r||^2 =  55542.0 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  58721.4 e-6 = 51.4 %)
Min.  Avg. Train Loss across Mini-Batch =  85978.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  96700.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22462.8 e-6; = (1/var)*||X-X_r||^2 val-train = 18681.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3781.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.44; perplexity/K = 80.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.38%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27843.7 e-6; = (1/var)*||X-X_r||^2 =  9115.4 e-6 = 32.7 %; (1+beta)*||Z_e-Z_q||^2 =  18728.3 e-6 = 67.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  40564.1 e-6; = (1/var)*||X-X_r||^2 =  18623.8 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  21940.3 e-6 = 54.1 %)
Min.  Avg. Train Loss across Mini-Batch =  27379.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38894.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12720.4 e-6; = (1/var)*||X-X_r||^2 val-train = 9508.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3212.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.26; perplexity/K = 78.29%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13036.3 e-6; = (1/var)*||X-X_r||^2 =  3658.6 e-6 = 28.1 %; (1+beta)*||Z_e-Z_q||^2 =  9377.8 e-6 = 71.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  22693.0 e-6; = (1/var)*||X-X_r||^2 =  10891.1 e-6 = 48.0 %; (1+beta)*||Z_e-Z_q||^2 =  11801.9 e-6 = 52.0 %)
Min.  Avg. Train Loss across Mini-Batch =  13036.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  22693.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9656.6 e-6; = (1/var)*||X-X_r||^2 val-train = 7232.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2424.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.40; perplexity/K = 80.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.44; perplexity/K = 80.49%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13980.3 e-6; = (1/var)*||X-X_r||^2 =  3150.9 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  10829.3 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  20894.5 e-6; = (1/var)*||X-X_r||^2 =  8710.8 e-6 = 41.7 %; (1+beta)*||Z_e-Z_q||^2 =  12183.7 e-6 = 58.3 %)
Min.  Avg. Train Loss across Mini-Batch =  7511.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16090.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6914.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5559.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1354.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.38; perplexity/K = 79.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.38; perplexity/K = 79.71%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4786.2 e-6; = (1/var)*||X-X_r||^2 =  1493.1 e-6 = 31.2 %; (1+beta)*||Z_e-Z_q||^2 =  3293.1 e-6 = 68.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  12119.8 e-6; = (1/var)*||X-X_r||^2 =  7071.6 e-6 = 58.3 %; (1+beta)*||Z_e-Z_q||^2 =  5048.1 e-6 = 41.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4786.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12119.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7333.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5578.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1755.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.60; perplexity/K = 82.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.58; perplexity/K = 82.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94755.3 e-6; = (1/var)*||X-X_r||^2 =  41931.7 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  52823.6 e-6 = 55.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  91262.3 e-6; = (1/var)*||X-X_r||^2 =  39813.9 e-6 = 43.6 %; (1+beta)*||Z_e-Z_q||^2 =  51448.3 e-6 = 56.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3523.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10707.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3493.0 e-6; = (1/var)*||X-X_r||^2 val-train = -2117.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1375.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.33; perplexity/K = 79.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.48; perplexity/K = 81.02%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3754.2 e-6; = (1/var)*||X-X_r||^2 =  1113.8 e-6 = 29.7 %; (1+beta)*||Z_e-Z_q||^2 =  2640.4 e-6 = 70.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  10464.1 e-6; = (1/var)*||X-X_r||^2 =  6402.0 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  4062.1 e-6 = 38.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3400.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10122.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6709.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5288.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1421.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.46; perplexity/K = 80.73%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7155.2 e-6; = (1/var)*||X-X_r||^2 =  1314.1 e-6 = 18.4 %; (1+beta)*||Z_e-Z_q||^2 =  5841.1 e-6 = 81.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  12202.4 e-6; = (1/var)*||X-X_r||^2 =  5559.0 e-6 = 45.6 %; (1+beta)*||Z_e-Z_q||^2 =  6643.4 e-6 = 54.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2438.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8688.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5047.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4244.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 802.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.40; perplexity/K = 79.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.57; perplexity/K = 82.13%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2966.7 e-6; = (1/var)*||X-X_r||^2 =  839.5 e-6 = 28.3 %; (1+beta)*||Z_e-Z_q||^2 =  2127.2 e-6 = 71.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  9028.4 e-6; = (1/var)*||X-X_r||^2 =  5584.2 e-6 = 61.9 %; (1+beta)*||Z_e-Z_q||^2 =  3444.2 e-6 = 38.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2418.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8293.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6061.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4744.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1317.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.46; perplexity/K = 80.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.45; perplexity/K = 80.64%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3125.0 e-6; = (1/var)*||X-X_r||^2 =  674.8 e-6 = 21.6 %; (1+beta)*||Z_e-Z_q||^2 =  2450.2 e-6 = 78.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  8717.7 e-6; = (1/var)*||X-X_r||^2 =  5081.0 e-6 = 58.3 %; (1+beta)*||Z_e-Z_q||^2 =  3636.7 e-6 = 41.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1853.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7503.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5592.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4406.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1186.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.48; perplexity/K = 80.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.35; perplexity/K = 79.34%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3638.9 e-6; = (1/var)*||X-X_r||^2 =  665.5 e-6 = 18.3 %; (1+beta)*||Z_e-Z_q||^2 =  2973.4 e-6 = 81.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  9398.5 e-6; = (1/var)*||X-X_r||^2 =  5265.7 e-6 = 56.0 %; (1+beta)*||Z_e-Z_q||^2 =  4132.8 e-6 = 44.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1693.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7314.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5759.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4600.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1159.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.45; perplexity/K = 80.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.46; perplexity/K = 80.73%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1553.6 e-6; = (1/var)*||X-X_r||^2 =  442.9 e-6 = 28.5 %; (1+beta)*||Z_e-Z_q||^2 =  1110.8 e-6 = 71.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  6441.5 e-6; = (1/var)*||X-X_r||^2 =  4315.9 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  2125.6 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1553.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6441.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4887.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3873.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1014.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.44; perplexity/K = 80.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.36; perplexity/K = 79.47%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5910.6 e-6; = (1/var)*||X-X_r||^2 =  987.6 e-6 = 16.7 %; (1+beta)*||Z_e-Z_q||^2 =  4923.0 e-6 = 83.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  10953.6 e-6; = (1/var)*||X-X_r||^2 =  4443.9 e-6 = 40.6 %; (1+beta)*||Z_e-Z_q||^2 =  6509.7 e-6 = 59.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1530.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6441.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5043.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3456.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1586.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.44; perplexity/K = 80.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.26; perplexity/K = 78.27%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4414.2 e-6; = (1/var)*||X-X_r||^2 =  655.8 e-6 = 14.9 %; (1+beta)*||Z_e-Z_q||^2 =  3758.4 e-6 = 85.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  10017.7 e-6; = (1/var)*||X-X_r||^2 =  5389.3 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  4628.4 e-6 = 46.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1255.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5603.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4733.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 870.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.48; perplexity/K = 80.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.26; perplexity/K = 78.31%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5380.3 e-6; = (1/var)*||X-X_r||^2 =  717.0 e-6 = 13.3 %; (1+beta)*||Z_e-Z_q||^2 =  4663.3 e-6 = 86.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  11036.8 e-6; = (1/var)*||X-X_r||^2 =  5666.7 e-6 = 51.3 %; (1+beta)*||Z_e-Z_q||^2 =  5370.1 e-6 = 48.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1255.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5656.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4949.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 706.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.39; perplexity/K = 79.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.41; perplexity/K = 80.15%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6993.1 e-6; = (1/var)*||X-X_r||^2 =  903.2 e-6 = 12.9 %; (1+beta)*||Z_e-Z_q||^2 =  6090.0 e-6 = 87.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  12824.6 e-6; = (1/var)*||X-X_r||^2 =  6128.8 e-6 = 47.8 %; (1+beta)*||Z_e-Z_q||^2 =  6695.9 e-6 = 52.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1255.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5831.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5225.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 605.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.55; perplexity/K = 81.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.47; perplexity/K = 80.89%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41770.9 e-6; = (1/var)*||X-X_r||^2 =  6758.5 e-6 = 16.2 %; (1+beta)*||Z_e-Z_q||^2 =  35012.4 e-6 = 83.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  41807.3 e-6; = (1/var)*||X-X_r||^2 =  11590.5 e-6 = 27.7 %; (1+beta)*||Z_e-Z_q||^2 =  30216.8 e-6 = 72.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1243.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4832.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4795.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.43; perplexity/K = 80.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.34%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  846963.8 e-6; = (1/var)*||X-X_r||^2 =  88487.3 e-6 = 10.4 %; (1+beta)*||Z_e-Z_q||^2 =  758476.5 e-6 = 89.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  203183.9 e-6; = (1/var)*||X-X_r||^2 =  32882.4 e-6 = 16.2 %; (1+beta)*||Z_e-Z_q||^2 =  170301.5 e-6 = 83.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1243.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -643779.9 e-6; = (1/var)*||X-X_r||^2 val-train = -55604.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -588175.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.49; perplexity/K = 81.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.25; perplexity/K = 78.16%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1737.0 e-6; = (1/var)*||X-X_r||^2 =  316.2 e-6 = 18.2 %; (1+beta)*||Z_e-Z_q||^2 =  1420.8 e-6 = 81.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  6679.4 e-6; = (1/var)*||X-X_r||^2 =  4415.4 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  2264.0 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1243.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4942.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4099.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 843.2 e-6 

----------------------------------------------------------------------------------

Finished [20:29:33 07.01.2023] 303) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = False:
Total training time is = 0:3:41 h/m/s. 

--------------------------------------------------- 

Started [20:29:33 07.01.2023] 304) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(4, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1972 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.29
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.29
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.29
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.29
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.29
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.29
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.49; perplexity/K = 62.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.28; perplexity/K = 56.93%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  608869.5 e-6; = (1/var)*||X-X_r||^2 =  269952.7 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  338916.8 e-6 = 55.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  534429.9 e-6; = (1/var)*||X-X_r||^2 =  277503.3 e-6 = 51.9 %; (1+beta)*||Z_e-Z_q||^2 =  256926.6 e-6 = 48.1 %)
Min.  Avg. Train Loss across Mini-Batch =  608869.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  534429.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -74439.5 e-6; = (1/var)*||X-X_r||^2 val-train = 7550.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -81990.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.36; perplexity/K = 84.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.26; perplexity/K = 81.38%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  314681.8 e-6; = (1/var)*||X-X_r||^2 =  132070.7 e-6 = 42.0 %; (1+beta)*||Z_e-Z_q||^2 =  182611.2 e-6 = 58.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  348079.4 e-6; = (1/var)*||X-X_r||^2 =  159236.6 e-6 = 45.7 %; (1+beta)*||Z_e-Z_q||^2 =  188842.8 e-6 = 54.3 %)
Min.  Avg. Train Loss across Mini-Batch =  307064.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  322019.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33397.6 e-6; = (1/var)*||X-X_r||^2 val-train = 27165.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6231.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.57; perplexity/K = 89.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.45; perplexity/K = 86.15%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  194413.7 e-6; = (1/var)*||X-X_r||^2 =  73805.6 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  120608.1 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  279193.6 e-6; = (1/var)*||X-X_r||^2 =  136120.2 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  143073.4 e-6 = 51.2 %)
Min.  Avg. Train Loss across Mini-Batch =  175043.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  222425.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   84779.8 e-6; = (1/var)*||X-X_r||^2 val-train = 62314.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22465.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.58; perplexity/K = 89.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.51; perplexity/K = 87.73%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  101417.3 e-6; = (1/var)*||X-X_r||^2 =  38083.0 e-6 = 37.6 %; (1+beta)*||Z_e-Z_q||^2 =  63334.3 e-6 = 62.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  152636.7 e-6; = (1/var)*||X-X_r||^2 =  85591.9 e-6 = 56.1 %; (1+beta)*||Z_e-Z_q||^2 =  67044.8 e-6 = 43.9 %)
Min.  Avg. Train Loss across Mini-Batch =  98858.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151158.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51219.4 e-6; = (1/var)*||X-X_r||^2 val-train = 47508.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3710.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.53; perplexity/K = 88.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 89.01%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57943.9 e-6; = (1/var)*||X-X_r||^2 =  23099.7 e-6 = 39.9 %; (1+beta)*||Z_e-Z_q||^2 =  34844.2 e-6 = 60.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  116090.9 e-6; = (1/var)*||X-X_r||^2 =  74057.0 e-6 = 63.8 %; (1+beta)*||Z_e-Z_q||^2 =  42033.9 e-6 = 36.2 %)
Min.  Avg. Train Loss across Mini-Batch =  57943.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  116090.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58147.0 e-6; = (1/var)*||X-X_r||^2 val-train = 50957.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7189.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.53; perplexity/K = 88.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.47; perplexity/K = 86.69%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55790.2 e-6; = (1/var)*||X-X_r||^2 =  24241.0 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  31549.3 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  139466.8 e-6; = (1/var)*||X-X_r||^2 =  85159.4 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  54307.4 e-6 = 38.9 %)
Min.  Avg. Train Loss across Mini-Batch =  37407.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  98073.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   83676.6 e-6; = (1/var)*||X-X_r||^2 val-train = 60918.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22758.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.57; perplexity/K = 89.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.58; perplexity/K = 89.62%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30574.6 e-6; = (1/var)*||X-X_r||^2 =  13721.5 e-6 = 44.9 %; (1+beta)*||Z_e-Z_q||^2 =  16853.2 e-6 = 55.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  92643.2 e-6; = (1/var)*||X-X_r||^2 =  68895.9 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  23747.3 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  26237.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  86259.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   62068.6 e-6; = (1/var)*||X-X_r||^2 val-train = 55174.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6894.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.55; perplexity/K = 88.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.55; perplexity/K = 88.75%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17186.5 e-6; = (1/var)*||X-X_r||^2 =  8850.7 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  8335.8 e-6 = 48.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  77961.2 e-6; = (1/var)*||X-X_r||^2 =  62958.4 e-6 = 80.8 %; (1+beta)*||Z_e-Z_q||^2 =  15002.8 e-6 = 19.2 %)
Min.  Avg. Train Loss across Mini-Batch =  17186.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  77961.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   60774.7 e-6; = (1/var)*||X-X_r||^2 val-train = 54107.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6667.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.61; perplexity/K = 90.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.50; perplexity/K = 87.55%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16649.4 e-6; = (1/var)*||X-X_r||^2 =  8434.6 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  8214.8 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  79199.9 e-6; = (1/var)*||X-X_r||^2 =  63705.8 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  15494.1 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  13627.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  73584.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   62550.5 e-6; = (1/var)*||X-X_r||^2 val-train = 55271.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7279.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.60; perplexity/K = 89.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.43; perplexity/K = 85.87%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13933.1 e-6; = (1/var)*||X-X_r||^2 =  7038.4 e-6 = 50.5 %; (1+beta)*||Z_e-Z_q||^2 =  6894.8 e-6 = 49.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  74762.8 e-6; = (1/var)*||X-X_r||^2 =  63163.3 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  11599.5 e-6 = 15.5 %)
Min.  Avg. Train Loss across Mini-Batch =  12228.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71616.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   60829.7 e-6; = (1/var)*||X-X_r||^2 val-train = 56124.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4704.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.58; perplexity/K = 89.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.47; perplexity/K = 86.75%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10131.1 e-6; = (1/var)*||X-X_r||^2 =  5550.1 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  4581.0 e-6 = 45.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  68603.8 e-6; = (1/var)*||X-X_r||^2 =  58783.3 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  9820.5 e-6 = 14.3 %)
Min.  Avg. Train Loss across Mini-Batch =  9835.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  68603.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58472.8 e-6; = (1/var)*||X-X_r||^2 val-train = 53233.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5239.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.48; perplexity/K = 87.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.60; perplexity/K = 90.11%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9325.7 e-6; = (1/var)*||X-X_r||^2 =  5118.1 e-6 = 54.9 %; (1+beta)*||Z_e-Z_q||^2 =  4207.6 e-6 = 45.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  67823.2 e-6; = (1/var)*||X-X_r||^2 =  58764.6 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  9058.6 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  7852.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65917.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58497.5 e-6; = (1/var)*||X-X_r||^2 val-train = 53646.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4850.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 88.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.51; perplexity/K = 87.74%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7179.4 e-6; = (1/var)*||X-X_r||^2 =  4396.4 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  2783.0 e-6 = 38.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  65765.9 e-6; = (1/var)*||X-X_r||^2 =  58087.2 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  7678.7 e-6 = 11.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7159.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65765.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58586.6 e-6; = (1/var)*||X-X_r||^2 val-train = 53690.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4895.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.53; perplexity/K = 88.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.54; perplexity/K = 88.55%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12859.6 e-6; = (1/var)*||X-X_r||^2 =  5996.5 e-6 = 46.6 %; (1+beta)*||Z_e-Z_q||^2 =  6863.2 e-6 = 53.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  67083.1 e-6; = (1/var)*||X-X_r||^2 =  57222.8 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  9860.3 e-6 = 14.7 %)
Min.  Avg. Train Loss across Mini-Batch =  6397.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63263.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54223.5 e-6; = (1/var)*||X-X_r||^2 val-train = 51226.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2997.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 89.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 88.88%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9180.3 e-6; = (1/var)*||X-X_r||^2 =  4624.4 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  4555.9 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  65148.2 e-6; = (1/var)*||X-X_r||^2 =  56514.5 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  8633.7 e-6 = 13.3 %)
Min.  Avg. Train Loss across Mini-Batch =  5815.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62438.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55967.9 e-6; = (1/var)*||X-X_r||^2 val-train = 51890.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4077.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.51; perplexity/K = 87.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.60; perplexity/K = 89.96%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5231.6 e-6; = (1/var)*||X-X_r||^2 =  3457.1 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  1774.5 e-6 = 33.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  63581.4 e-6; = (1/var)*||X-X_r||^2 =  58119.3 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  5462.1 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  5186.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62206.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58349.8 e-6; = (1/var)*||X-X_r||^2 val-train = 54662.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3687.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 89.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 89.01%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18409.4 e-6; = (1/var)*||X-X_r||^2 =  8145.9 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  10263.4 e-6 = 55.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  75156.3 e-6; = (1/var)*||X-X_r||^2 =  59919.4 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  15236.9 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  5186.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62206.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56746.9 e-6; = (1/var)*||X-X_r||^2 val-train = 51773.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4973.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.50; perplexity/K = 87.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.48; perplexity/K = 86.94%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13605.7 e-6; = (1/var)*||X-X_r||^2 =  6389.7 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  7216.0 e-6 = 53.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  73674.9 e-6; = (1/var)*||X-X_r||^2 =  60486.3 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  13188.5 e-6 = 17.9 %)
Min.  Avg. Train Loss across Mini-Batch =  4571.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61221.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   60069.2 e-6; = (1/var)*||X-X_r||^2 val-train = 54096.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5972.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.55; perplexity/K = 88.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 89.03%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19941.8 e-6; = (1/var)*||X-X_r||^2 =  8172.5 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  11769.3 e-6 = 59.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  71815.8 e-6; = (1/var)*||X-X_r||^2 =  60839.5 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  10976.3 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  4253.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61080.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51874.0 e-6; = (1/var)*||X-X_r||^2 val-train = 52667.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -793.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.62; perplexity/K = 90.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.48; perplexity/K = 86.91%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12634.1 e-6; = (1/var)*||X-X_r||^2 =  5512.1 e-6 = 43.6 %; (1+beta)*||Z_e-Z_q||^2 =  7122.0 e-6 = 56.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  68218.7 e-6; = (1/var)*||X-X_r||^2 =  58701.1 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  9517.6 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4253.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60450.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55584.6 e-6; = (1/var)*||X-X_r||^2 val-train = 53189.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2395.6 e-6 

----------------------------------------------------------------------------------

Finished [21:34:18 07.01.2023] 304) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = False:
Total training time is = 0:3:45 h/m/s. 

--------------------------------------------------- 

Started [21:34:18 07.01.2023] 305) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1972 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.29
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.29
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.29
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.29
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.29
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.29
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.17; perplexity/K = 58.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.18; perplexity/K = 58.78%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2929480.0 e-6; = (1/var)*||X-X_r||^2 =  574052.1 e-6 = 19.6 %; (1+beta)*||Z_e-Z_q||^2 =  2355428.0 e-6 = 80.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3256545.5 e-6; = (1/var)*||X-X_r||^2 =  599841.6 e-6 = 18.4 %; (1+beta)*||Z_e-Z_q||^2 =  2656703.9 e-6 = 81.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1249623.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1103649.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   327065.5 e-6; = (1/var)*||X-X_r||^2 val-train = 25789.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 301275.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 52.94%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  824146.3 e-6; = (1/var)*||X-X_r||^2 =  794167.5 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  29978.7 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  837246.6 e-6; = (1/var)*||X-X_r||^2 =  803261.7 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  33984.9 e-6 = 4.1 %)
Min.  Avg. Train Loss across Mini-Batch =  802487.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  802921.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13100.3 e-6; = (1/var)*||X-X_r||^2 val-train = 9094.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4006.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.03; perplexity/K = 51.70%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  804192.3 e-6; = (1/var)*||X-X_r||^2 =  799517.5 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  4674.8 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  806329.2 e-6; = (1/var)*||X-X_r||^2 =  801702.9 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  4626.3 e-6 = 0.6 %)
Min.  Avg. Train Loss across Mini-Batch =  797220.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  797818.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2136.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2185.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -48.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.02; perplexity/K = 51.15%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  801293.9 e-6; = (1/var)*||X-X_r||^2 =  800022.1 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1271.9 e-6 = 0.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  808340.5 e-6; = (1/var)*||X-X_r||^2 =  806384.4 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1956.1 e-6 = 0.2 %)
Min.  Avg. Train Loss across Mini-Batch =  794507.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  797818.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7046.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6362.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 684.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.96%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  765196.3 e-6; = (1/var)*||X-X_r||^2 =  741192.4 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  24003.9 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  773763.9 e-6; = (1/var)*||X-X_r||^2 =  739045.2 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  34718.7 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  765188.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8567.6 e-6; = (1/var)*||X-X_r||^2 val-train = -2147.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10714.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 52.94%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  781828.1 e-6; = (1/var)*||X-X_r||^2 =  780598.3 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1229.8 e-6 = 0.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  788154.7 e-6; = (1/var)*||X-X_r||^2 =  786454.1 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1700.5 e-6 = 0.2 %)
Min.  Avg. Train Loss across Mini-Batch =  765188.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6326.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5855.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 470.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.70%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  795022.9 e-6; = (1/var)*||X-X_r||^2 =  769163.0 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  25860.0 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  778130.1 e-6; = (1/var)*||X-X_r||^2 =  767033.3 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  11096.8 e-6 = 1.4 %)
Min.  Avg. Train Loss across Mini-Batch =  760263.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16892.8 e-6; = (1/var)*||X-X_r||^2 val-train = -2129.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14763.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.02; perplexity/K = 51.01%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  791092.4 e-6; = (1/var)*||X-X_r||^2 =  749187.5 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  41904.9 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  775802.2 e-6; = (1/var)*||X-X_r||^2 =  739346.4 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  36455.9 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15290.2 e-6; = (1/var)*||X-X_r||^2 val-train = -9841.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5449.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.03; perplexity/K = 51.57%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  840334.3 e-6; = (1/var)*||X-X_r||^2 =  751919.3 e-6 = 89.5 %; (1+beta)*||Z_e-Z_q||^2 =  88415.0 e-6 = 10.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  775888.0 e-6; = (1/var)*||X-X_r||^2 =  740235.4 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  35652.7 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -64446.3 e-6; = (1/var)*||X-X_r||^2 val-train = -11683.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -52762.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.02; perplexity/K = 51.15%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  800216.5 e-6; = (1/var)*||X-X_r||^2 =  777968.1 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  22248.4 e-6 = 2.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  782370.5 e-6; = (1/var)*||X-X_r||^2 =  773940.5 e-6 = 98.9 %; (1+beta)*||Z_e-Z_q||^2 =  8430.0 e-6 = 1.1 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17846.0 e-6; = (1/var)*||X-X_r||^2 val-train = -4027.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13818.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 53.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.18%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  793762.9 e-6; = (1/var)*||X-X_r||^2 =  791324.6 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2438.2 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  794106.8 e-6; = (1/var)*||X-X_r||^2 =  791276.0 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  2830.8 e-6 = 0.4 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   343.9 e-6; = (1/var)*||X-X_r||^2 val-train = -48.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 392.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.96%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  795610.7 e-6; = (1/var)*||X-X_r||^2 =  786282.4 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  9328.3 e-6 = 1.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  789631.1 e-6; = (1/var)*||X-X_r||^2 =  780599.3 e-6 = 98.9 %; (1+beta)*||Z_e-Z_q||^2 =  9031.8 e-6 = 1.1 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5979.7 e-6; = (1/var)*||X-X_r||^2 val-train = -5683.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -296.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.09; perplexity/K = 54.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.58%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  790882.3 e-6; = (1/var)*||X-X_r||^2 =  787673.1 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3209.2 e-6 = 0.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  785976.9 e-6; = (1/var)*||X-X_r||^2 =  781831.4 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4145.5 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4905.4 e-6; = (1/var)*||X-X_r||^2 val-train = -5841.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 936.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.03; perplexity/K = 51.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 52.94%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  781027.0 e-6; = (1/var)*||X-X_r||^2 =  776581.0 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  4446.0 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  775184.0 e-6; = (1/var)*||X-X_r||^2 =  771273.9 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  3910.1 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5843.0 e-6; = (1/var)*||X-X_r||^2 val-train = -5307.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -535.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 52.82%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  804496.7 e-6; = (1/var)*||X-X_r||^2 =  800111.2 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4385.5 e-6 = 0.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  806195.2 e-6; = (1/var)*||X-X_r||^2 =  798055.9 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  8139.3 e-6 = 1.0 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1698.5 e-6; = (1/var)*||X-X_r||^2 val-train = -2055.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3753.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 52.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.70%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  801565.7 e-6; = (1/var)*||X-X_r||^2 =  795434.9 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  6130.8 e-6 = 0.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  790940.2 e-6; = (1/var)*||X-X_r||^2 =  785451.5 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  5488.7 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10625.5 e-6; = (1/var)*||X-X_r||^2 val-train = -9983.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -642.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.03; perplexity/K = 51.70%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  793610.4 e-6; = (1/var)*||X-X_r||^2 =  780994.9 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  12615.5 e-6 = 1.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  792340.2 e-6; = (1/var)*||X-X_r||^2 =  782963.1 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  9377.1 e-6 = 1.2 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1270.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1968.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3238.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999240.6 e-6; = (1/var)*||X-X_r||^2 =  999240.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963176.4 e-6; = (1/var)*||X-X_r||^2 =  963176.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36064.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36064.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999218.9 e-6; = (1/var)*||X-X_r||^2 =  999218.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962932.7 e-6; = (1/var)*||X-X_r||^2 =  962932.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36286.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36286.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999460.1 e-6; = (1/var)*||X-X_r||^2 =  999460.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962911.1 e-6; = (1/var)*||X-X_r||^2 =  962911.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  758846.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  744393.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36549.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36549.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [22:39:03 07.01.2023] 305) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = False:
Total training time is = 0:3:44 h/m/s. 

--------------------------------------------------- 

Started [22:39:03 07.01.2023] 306) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 80) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7390 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.18
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.18
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.18
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         2             0.03
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.18
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.18
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.18
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.05; perplexity/K = 56.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.26; perplexity/K = 53.94%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  92610.5 e-6; = (1/var)*||X-X_r||^2 =  47325.6 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  45284.9 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  100443.8 e-6; = (1/var)*||X-X_r||^2 =  56420.4 e-6 = 56.2 %; (1+beta)*||Z_e-Z_q||^2 =  44023.4 e-6 = 43.8 %)
Min.  Avg. Train Loss across Mini-Batch =  92610.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  100443.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7833.3 e-6; = (1/var)*||X-X_r||^2 val-train = 9094.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1261.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999632.1 e-6; = (1/var)*||X-X_r||^2 =  999632.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963198.1 e-6; = (1/var)*||X-X_r||^2 =  963198.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36434.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36434.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999544.7 e-6; = (1/var)*||X-X_r||^2 =  999544.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963026.2 e-6; = (1/var)*||X-X_r||^2 =  963026.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36518.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36518.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999456.2 e-6; = (1/var)*||X-X_r||^2 =  999456.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963139.4 e-6; = (1/var)*||X-X_r||^2 =  963139.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36316.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36316.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999330.1 e-6; = (1/var)*||X-X_r||^2 =  999330.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962986.0 e-6; = (1/var)*||X-X_r||^2 =  962986.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36344.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36344.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999472.0 e-6; = (1/var)*||X-X_r||^2 =  999472.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962901.1 e-6; = (1/var)*||X-X_r||^2 =  962901.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36570.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36571.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999241.5 e-6; = (1/var)*||X-X_r||^2 =  999241.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963017.3 e-6; = (1/var)*||X-X_r||^2 =  963017.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36224.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36224.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999296.1 e-6; = (1/var)*||X-X_r||^2 =  999296.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962934.3 e-6; = (1/var)*||X-X_r||^2 =  962934.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36361.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36361.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999294.4 e-6; = (1/var)*||X-X_r||^2 =  999294.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962910.6 e-6; = (1/var)*||X-X_r||^2 =  962910.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36383.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36383.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999320.9 e-6; = (1/var)*||X-X_r||^2 =  999320.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963228.7 e-6; = (1/var)*||X-X_r||^2 =  963228.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36092.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36092.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999302.6 e-6; = (1/var)*||X-X_r||^2 =  999302.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963014.3 e-6; = (1/var)*||X-X_r||^2 =  963014.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36288.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36288.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999255.9 e-6; = (1/var)*||X-X_r||^2 =  999255.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962919.7 e-6; = (1/var)*||X-X_r||^2 =  962919.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36336.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36336.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999260.6 e-6; = (1/var)*||X-X_r||^2 =  999260.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962949.5 e-6; = (1/var)*||X-X_r||^2 =  962949.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36311.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36311.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999150.8 e-6; = (1/var)*||X-X_r||^2 =  999150.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963142.8 e-6; = (1/var)*||X-X_r||^2 =  963142.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36008.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36008.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999235.3 e-6; = (1/var)*||X-X_r||^2 =  999235.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962919.1 e-6; = (1/var)*||X-X_r||^2 =  962919.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36316.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36316.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999172.4 e-6; = (1/var)*||X-X_r||^2 =  999172.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962933.7 e-6; = (1/var)*||X-X_r||^2 =  962933.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36238.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36238.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999293.2 e-6; = (1/var)*||X-X_r||^2 =  999293.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963063.3 e-6; = (1/var)*||X-X_r||^2 =  963063.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36230.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36229.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999317.1 e-6; = (1/var)*||X-X_r||^2 =  999317.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963006.1 e-6; = (1/var)*||X-X_r||^2 =  963006.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36311.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36311.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999241.4 e-6; = (1/var)*||X-X_r||^2 =  999241.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963015.7 e-6; = (1/var)*||X-X_r||^2 =  963015.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36225.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36225.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:15:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999147.0 e-6; = (1/var)*||X-X_r||^2 =  999147.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962934.0 e-6; = (1/var)*||X-X_r||^2 =  962934.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29053.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33321.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36213.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36213.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [00:54:39 08.01.2023] 306) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 80) change_channel_size_across_layers = False:
Total training time is = 0:3:35 h/m/s. 

--------------------------------------------------- 

Started [00:54:39 08.01.2023] 307) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 64) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7389 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.18
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.18
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.18
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         1             0.01
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.18
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.18
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.18
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.09; perplexity/K = 75.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.54; perplexity/K = 72.15%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  190598.7 e-6; = (1/var)*||X-X_r||^2 =  61087.3 e-6 = 32.1 %; (1+beta)*||Z_e-Z_q||^2 =  129511.4 e-6 = 67.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  205495.7 e-6; = (1/var)*||X-X_r||^2 =  77540.8 e-6 = 37.7 %; (1+beta)*||Z_e-Z_q||^2 =  127954.9 e-6 = 62.3 %)
Min.  Avg. Train Loss across Mini-Batch =  190598.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  205495.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14897.0 e-6; = (1/var)*||X-X_r||^2 val-train = 16453.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1556.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.35; perplexity/K = 58.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.93; perplexity/K = 55.82%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  247575.2 e-6; = (1/var)*||X-X_r||^2 =  140233.8 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  107341.3 e-6 = 43.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  285328.3 e-6; = (1/var)*||X-X_r||^2 =  161814.0 e-6 = 56.7 %; (1+beta)*||Z_e-Z_q||^2 =  123514.3 e-6 = 43.3 %)
Min.  Avg. Train Loss across Mini-Batch =  64416.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  76620.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37753.1 e-6; = (1/var)*||X-X_r||^2 val-train = 21580.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16172.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.56; perplexity/K = 72.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.60; perplexity/K = 72.52%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61038.4 e-6; = (1/var)*||X-X_r||^2 =  32971.1 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  28067.3 e-6 = 46.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  81256.1 e-6; = (1/var)*||X-X_r||^2 =  52910.9 e-6 = 65.1 %; (1+beta)*||Z_e-Z_q||^2 =  28345.2 e-6 = 34.9 %)
Min.  Avg. Train Loss across Mini-Batch =  59370.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  76620.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20217.7 e-6; = (1/var)*||X-X_r||^2 val-train = 19939.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 277.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 85.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.62; perplexity/K = 85.12%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29851.8 e-6; = (1/var)*||X-X_r||^2 =  15032.3 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  14819.5 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  43563.6 e-6; = (1/var)*||X-X_r||^2 =  29031.8 e-6 = 66.6 %; (1+beta)*||Z_e-Z_q||^2 =  14531.8 e-6 = 33.4 %)
Min.  Avg. Train Loss across Mini-Batch =  26572.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42138.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13711.8 e-6; = (1/var)*||X-X_r||^2 val-train = 13999.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -287.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.63; perplexity/K = 85.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.50; perplexity/K = 84.35%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33862.1 e-6; = (1/var)*||X-X_r||^2 =  17719.7 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  16142.4 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  38748.7 e-6; = (1/var)*||X-X_r||^2 =  25576.7 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  13172.0 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  17456.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31050.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4886.6 e-6; = (1/var)*||X-X_r||^2 val-train = 7856.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2970.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.75; perplexity/K = 85.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 85.46%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12914.1 e-6; = (1/var)*||X-X_r||^2 =  5461.8 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  7452.3 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  25399.5 e-6; = (1/var)*||X-X_r||^2 =  17492.8 e-6 = 68.9 %; (1+beta)*||Z_e-Z_q||^2 =  7906.7 e-6 = 31.1 %)
Min.  Avg. Train Loss across Mini-Batch =  11869.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25214.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12485.4 e-6; = (1/var)*||X-X_r||^2 val-train = 12031.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 454.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.64; perplexity/K = 85.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.59; perplexity/K = 84.96%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8746.8 e-6; = (1/var)*||X-X_r||^2 =  3835.4 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  4911.4 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  21751.0 e-6; = (1/var)*||X-X_r||^2 =  16128.2 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  5622.9 e-6 = 25.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8731.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  21751.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13004.2 e-6; = (1/var)*||X-X_r||^2 val-train = 12292.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 711.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.75; perplexity/K = 85.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 85.45%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7810.8 e-6; = (1/var)*||X-X_r||^2 =  3601.6 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  4209.2 e-6 = 53.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  19716.2 e-6; = (1/var)*||X-X_r||^2 =  15027.5 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  4688.8 e-6 = 23.8 %)
Min.  Avg. Train Loss across Mini-Batch =  7411.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  19574.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11905.4 e-6; = (1/var)*||X-X_r||^2 val-train = 11425.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 479.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.76; perplexity/K = 86.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.62; perplexity/K = 85.15%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6358.1 e-6; = (1/var)*||X-X_r||^2 =  2534.5 e-6 = 39.9 %; (1+beta)*||Z_e-Z_q||^2 =  3823.7 e-6 = 60.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  17536.0 e-6; = (1/var)*||X-X_r||^2 =  13259.7 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  4276.3 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  6238.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17536.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11177.8 e-6; = (1/var)*||X-X_r||^2 val-train = 10725.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 452.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.70; perplexity/K = 85.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.61; perplexity/K = 85.05%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5810.6 e-6; = (1/var)*||X-X_r||^2 =  2282.5 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  3528.0 e-6 = 60.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  16424.1 e-6; = (1/var)*||X-X_r||^2 =  12467.9 e-6 = 75.9 %; (1+beta)*||Z_e-Z_q||^2 =  3956.3 e-6 = 24.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5505.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10613.6 e-6; = (1/var)*||X-X_r||^2 val-train = 10185.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 428.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.62; perplexity/K = 85.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.77; perplexity/K = 86.07%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11429.1 e-6; = (1/var)*||X-X_r||^2 =  5271.3 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  6157.8 e-6 = 53.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  19931.3 e-6; = (1/var)*||X-X_r||^2 =  14189.7 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  5741.5 e-6 = 28.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4921.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15339.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8502.2 e-6; = (1/var)*||X-X_r||^2 val-train = 8918.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -416.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.78; perplexity/K = 86.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.71; perplexity/K = 85.71%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8911.6 e-6; = (1/var)*||X-X_r||^2 =  4091.3 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  4820.3 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  16763.3 e-6; = (1/var)*||X-X_r||^2 =  12167.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  4596.2 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4291.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14287.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7851.7 e-6; = (1/var)*||X-X_r||^2 val-train = 8075.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -224.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.46; perplexity/K = 90.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.21; perplexity/K = 88.80%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3719.8 e-6; = (1/var)*||X-X_r||^2 =  1635.3 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  2084.4 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  13311.9 e-6; = (1/var)*||X-X_r||^2 =  10943.4 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  2368.5 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3696.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13268.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9592.1 e-6; = (1/var)*||X-X_r||^2 val-train = 9308.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 284.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.23; perplexity/K = 88.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.45; perplexity/K = 90.29%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3807.6 e-6; = (1/var)*||X-X_r||^2 =  1494.1 e-6 = 39.2 %; (1+beta)*||Z_e-Z_q||^2 =  2313.5 e-6 = 60.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  12839.5 e-6; = (1/var)*||X-X_r||^2 =  10270.8 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  2568.7 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3570.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12493.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9031.9 e-6; = (1/var)*||X-X_r||^2 val-train = 8776.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 255.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.54; perplexity/K = 90.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.45; perplexity/K = 90.33%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5110.6 e-6; = (1/var)*||X-X_r||^2 =  2060.3 e-6 = 40.3 %; (1+beta)*||Z_e-Z_q||^2 =  3050.3 e-6 = 59.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  13212.6 e-6; = (1/var)*||X-X_r||^2 =  10113.6 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  3099.0 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3306.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12194.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8102.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8053.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 48.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.13; perplexity/K = 88.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.33; perplexity/K = 89.53%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3282.4 e-6; = (1/var)*||X-X_r||^2 =  1342.6 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  1939.9 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  11783.7 e-6; = (1/var)*||X-X_r||^2 =  9606.4 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  2177.3 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3054.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11568.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8501.3 e-6; = (1/var)*||X-X_r||^2 val-train = 8263.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 237.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.52; perplexity/K = 90.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.36; perplexity/K = 89.75%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3308.4 e-6; = (1/var)*||X-X_r||^2 =  1238.9 e-6 = 37.4 %; (1+beta)*||Z_e-Z_q||^2 =  2069.5 e-6 = 62.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  11742.8 e-6; = (1/var)*||X-X_r||^2 =  9462.8 e-6 = 80.6 %; (1+beta)*||Z_e-Z_q||^2 =  2280.0 e-6 = 19.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2878.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10935.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8434.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8223.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 210.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.39; perplexity/K = 89.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.31; perplexity/K = 89.45%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11392.3 e-6; = (1/var)*||X-X_r||^2 =  5852.5 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  5539.8 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  15799.0 e-6; = (1/var)*||X-X_r||^2 =  11544.5 e-6 = 73.1 %; (1+beta)*||Z_e-Z_q||^2 =  4254.5 e-6 = 26.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2513.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10672.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4406.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5692.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1285.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.57; perplexity/K = 91.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.43; perplexity/K = 90.18%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3171.0 e-6; = (1/var)*||X-X_r||^2 =  1142.9 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  2028.1 e-6 = 64.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  10743.3 e-6; = (1/var)*||X-X_r||^2 =  8613.3 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  2130.0 e-6 = 19.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2513.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10458.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7572.3 e-6; = (1/var)*||X-X_r||^2 val-train = 7470.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 101.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.60; perplexity/K = 91.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.33; perplexity/K = 89.58%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:15:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2486.2 e-6; = (1/var)*||X-X_r||^2 =  958.1 e-6 = 38.5 %; (1+beta)*||Z_e-Z_q||^2 =  1528.1 e-6 = 61.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  10484.3 e-6; = (1/var)*||X-X_r||^2 =  8762.2 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  1722.1 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2292.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9968.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7998.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7804.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 194.0 e-6 

----------------------------------------------------------------------------------

Finished [03:10:15 08.01.2023] 307) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 64) change_channel_size_across_layers = False:
Total training time is = 0:3:36 h/m/s. 

--------------------------------------------------- 

Started [03:10:15 08.01.2023] 308) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 48) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7388 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.19
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.19
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.19
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.19
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.19
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.19
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 75.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.75; perplexity/K = 71.92%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  429359.2 e-6; = (1/var)*||X-X_r||^2 =  114750.4 e-6 = 26.7 %; (1+beta)*||Z_e-Z_q||^2 =  314608.8 e-6 = 73.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  433092.9 e-6; = (1/var)*||X-X_r||^2 =  130714.3 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  302378.7 e-6 = 69.8 %)
Min.  Avg. Train Loss across Mini-Batch =  424927.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  433092.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3733.7 e-6; = (1/var)*||X-X_r||^2 val-train = 15963.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12230.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.79; perplexity/K = 72.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.73; perplexity/K = 71.63%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  129252.2 e-6; = (1/var)*||X-X_r||^2 =  27434.6 e-6 = 21.2 %; (1+beta)*||Z_e-Z_q||^2 =  101817.6 e-6 = 78.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  158399.3 e-6; = (1/var)*||X-X_r||^2 =  55228.5 e-6 = 34.9 %; (1+beta)*||Z_e-Z_q||^2 =  103170.8 e-6 = 65.1 %)
Min.  Avg. Train Loss across Mini-Batch =  112887.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142653.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29147.1 e-6; = (1/var)*||X-X_r||^2 val-train = 27793.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1353.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.74; perplexity/K = 71.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.55; perplexity/K = 69.41%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32324.1 e-6; = (1/var)*||X-X_r||^2 =  4826.1 e-6 = 14.9 %; (1+beta)*||Z_e-Z_q||^2 =  27498.0 e-6 = 85.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  55658.0 e-6; = (1/var)*||X-X_r||^2 =  26342.1 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  29315.9 e-6 = 52.7 %)
Min.  Avg. Train Loss across Mini-Batch =  30448.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55658.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23333.9 e-6; = (1/var)*||X-X_r||^2 val-train = 21516.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1817.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 75.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.68; perplexity/K = 71.01%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19215.2 e-6; = (1/var)*||X-X_r||^2 =  2602.5 e-6 = 13.5 %; (1+beta)*||Z_e-Z_q||^2 =  16612.6 e-6 = 86.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  41629.4 e-6; = (1/var)*||X-X_r||^2 =  22721.8 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  18907.6 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  14717.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36816.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22414.2 e-6; = (1/var)*||X-X_r||^2 val-train = 20119.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2295.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.72; perplexity/K = 71.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.76; perplexity/K = 71.94%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7565.3 e-6; = (1/var)*||X-X_r||^2 =  1190.6 e-6 = 15.7 %; (1+beta)*||Z_e-Z_q||^2 =  6374.7 e-6 = 84.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  28041.4 e-6; = (1/var)*||X-X_r||^2 =  19920.7 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  8120.7 e-6 = 29.0 %)
Min.  Avg. Train Loss across Mini-Batch =  7018.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20476.1 e-6; = (1/var)*||X-X_r||^2 val-train = 18730.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1746.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.60; perplexity/K = 69.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.33; perplexity/K = 66.61%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3244.2 e-6; = (1/var)*||X-X_r||^2 =  717.0 e-6 = 22.1 %; (1+beta)*||Z_e-Z_q||^2 =  2527.2 e-6 = 77.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  22533.4 e-6; = (1/var)*||X-X_r||^2 =  18386.9 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  4146.5 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3244.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  22328.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19289.2 e-6; = (1/var)*||X-X_r||^2 val-train = 17669.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1619.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.91; perplexity/K = 73.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.54; perplexity/K = 69.23%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2200.5 e-6; = (1/var)*||X-X_r||^2 =  516.7 e-6 = 23.5 %; (1+beta)*||Z_e-Z_q||^2 =  1683.8 e-6 = 76.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  20396.8 e-6; = (1/var)*||X-X_r||^2 =  17298.8 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  3097.9 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2200.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20396.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18196.2 e-6; = (1/var)*||X-X_r||^2 val-train = 16782.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1414.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.74; perplexity/K = 71.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.82; perplexity/K = 72.71%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12797.3 e-6; = (1/var)*||X-X_r||^2 =  2411.8 e-6 = 18.8 %; (1+beta)*||Z_e-Z_q||^2 =  10385.5 e-6 = 81.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  29238.1 e-6; = (1/var)*||X-X_r||^2 =  18824.8 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  10413.3 e-6 = 35.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1317.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  18785.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16440.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16413.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.13; perplexity/K = 76.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.56; perplexity/K = 69.51%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1122.0 e-6; = (1/var)*||X-X_r||^2 =  331.6 e-6 = 29.6 %; (1+beta)*||Z_e-Z_q||^2 =  790.4 e-6 = 70.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  17478.9 e-6; = (1/var)*||X-X_r||^2 =  15543.5 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  1935.4 e-6 = 11.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1070.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17478.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16356.8 e-6; = (1/var)*||X-X_r||^2 val-train = 15211.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1145.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.56; perplexity/K = 69.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.81; perplexity/K = 72.66%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20188.7 e-6; = (1/var)*||X-X_r||^2 =  2184.0 e-6 = 10.8 %; (1+beta)*||Z_e-Z_q||^2 =  18004.7 e-6 = 89.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  52554.5 e-6; = (1/var)*||X-X_r||^2 =  23693.8 e-6 = 45.1 %; (1+beta)*||Z_e-Z_q||^2 =  28860.7 e-6 = 54.9 %)
Min.  Avg. Train Loss across Mini-Batch =  743.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17224.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32365.8 e-6; = (1/var)*||X-X_r||^2 val-train = 21509.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10856.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.59; perplexity/K = 69.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.65; perplexity/K = 70.69%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2233.8 e-6; = (1/var)*||X-X_r||^2 =  434.8 e-6 = 19.5 %; (1+beta)*||Z_e-Z_q||^2 =  1798.9 e-6 = 80.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  19151.4 e-6; = (1/var)*||X-X_r||^2 =  16197.6 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  2953.8 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  743.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17224.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16917.7 e-6; = (1/var)*||X-X_r||^2 val-train = 15762.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1154.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.71; perplexity/K = 71.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.45; perplexity/K = 68.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1168.0 e-6; = (1/var)*||X-X_r||^2 =  244.5 e-6 = 20.9 %; (1+beta)*||Z_e-Z_q||^2 =  923.4 e-6 = 79.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  17741.5 e-6; = (1/var)*||X-X_r||^2 =  15851.9 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  1889.6 e-6 = 10.7 %)
Min.  Avg. Train Loss across Mini-Batch =  743.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16278.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16573.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15607.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 966.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.73; perplexity/K = 71.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.37; perplexity/K = 67.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20337.8 e-6; = (1/var)*||X-X_r||^2 =  2335.5 e-6 = 11.5 %; (1+beta)*||Z_e-Z_q||^2 =  18002.4 e-6 = 88.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  35130.3 e-6; = (1/var)*||X-X_r||^2 =  19209.0 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  15921.3 e-6 = 45.3 %)
Min.  Avg. Train Loss across Mini-Batch =  595.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15663.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14792.5 e-6; = (1/var)*||X-X_r||^2 val-train = 16873.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2081.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.84; perplexity/K = 73.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.63; perplexity/K = 70.41%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5171.5 e-6; = (1/var)*||X-X_r||^2 =  923.6 e-6 = 17.9 %; (1+beta)*||Z_e-Z_q||^2 =  4247.9 e-6 = 82.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  20574.6 e-6; = (1/var)*||X-X_r||^2 =  15629.3 e-6 = 76.0 %; (1+beta)*||Z_e-Z_q||^2 =  4945.3 e-6 = 24.0 %)
Min.  Avg. Train Loss across Mini-Batch =  590.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15663.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15403.1 e-6; = (1/var)*||X-X_r||^2 val-train = 14705.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 697.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.81; perplexity/K = 72.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.64; perplexity/K = 70.49%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5443.5 e-6; = (1/var)*||X-X_r||^2 =  1123.9 e-6 = 20.6 %; (1+beta)*||Z_e-Z_q||^2 =  4319.6 e-6 = 79.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  22666.8 e-6; = (1/var)*||X-X_r||^2 =  16886.6 e-6 = 74.5 %; (1+beta)*||Z_e-Z_q||^2 =  5780.2 e-6 = 25.5 %)
Min.  Avg. Train Loss across Mini-Batch =  590.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15663.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17223.3 e-6; = (1/var)*||X-X_r||^2 val-train = 15762.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1460.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.51; perplexity/K = 68.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.46; perplexity/K = 68.21%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1374.5 e-6; = (1/var)*||X-X_r||^2 =  305.1 e-6 = 22.2 %; (1+beta)*||Z_e-Z_q||^2 =  1069.4 e-6 = 77.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  16625.4 e-6; = (1/var)*||X-X_r||^2 =  14990.3 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  1635.1 e-6 = 9.8 %)
Min.  Avg. Train Loss across Mini-Batch =  548.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15569.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15250.9 e-6; = (1/var)*||X-X_r||^2 val-train = 14685.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 565.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.67; perplexity/K = 70.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.49; perplexity/K = 68.65%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1910.4 e-6; = (1/var)*||X-X_r||^2 =  451.0 e-6 = 23.6 %; (1+beta)*||Z_e-Z_q||^2 =  1459.3 e-6 = 76.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  17775.7 e-6; = (1/var)*||X-X_r||^2 =  15595.3 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  2180.5 e-6 = 12.3 %)
Min.  Avg. Train Loss across Mini-Batch =  386.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15279.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15865.4 e-6; = (1/var)*||X-X_r||^2 val-train = 15144.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 721.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.98; perplexity/K = 74.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.66; perplexity/K = 70.79%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  969.1 e-6; = (1/var)*||X-X_r||^2 =  242.4 e-6 = 25.0 %; (1+beta)*||Z_e-Z_q||^2 =  726.7 e-6 = 75.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  15280.0 e-6; = (1/var)*||X-X_r||^2 =  13849.8 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  1430.3 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  386.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15279.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14310.9 e-6; = (1/var)*||X-X_r||^2 val-train = 13607.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 703.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.64; perplexity/K = 70.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.54; perplexity/K = 69.29%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  575.4 e-6; = (1/var)*||X-X_r||^2 =  243.4 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  331.9 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  17815.6 e-6; = (1/var)*||X-X_r||^2 =  16986.1 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  829.5 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  371.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15279.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17240.2 e-6; = (1/var)*||X-X_r||^2 val-train = 16742.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 497.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.53; perplexity/K = 69.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.57; perplexity/K = 69.67%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  374.6 e-6; = (1/var)*||X-X_r||^2 =  134.8 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  239.8 e-6 = 64.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  14373.2 e-6; = (1/var)*||X-X_r||^2 =  13724.7 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  648.5 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  371.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14152.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13998.6 e-6; = (1/var)*||X-X_r||^2 val-train = 13589.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 408.7 e-6 

----------------------------------------------------------------------------------

Finished [05:25:45 08.01.2023] 308) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 48) change_channel_size_across_layers = False:
Total training time is = 0:3:29 h/m/s. 

--------------------------------------------------- 

Started [05:25:45 08.01.2023] 309) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(4, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7388 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.19
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.19
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.19
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.19
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.19
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.19
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.21; perplexity/K = 55.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.24; perplexity/K = 55.91%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405835.7 e-6; = (1/var)*||X-X_r||^2 =  172674.3 e-6 = 42.5 %; (1+beta)*||Z_e-Z_q||^2 =  233161.4 e-6 = 57.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  432607.7 e-6; = (1/var)*||X-X_r||^2 =  193474.1 e-6 = 44.7 %; (1+beta)*||Z_e-Z_q||^2 =  239133.5 e-6 = 55.3 %)
Min.  Avg. Train Loss across Mini-Batch =  405835.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432607.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26771.9 e-6; = (1/var)*||X-X_r||^2 val-train = 20799.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5972.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.29; perplexity/K = 57.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.10; perplexity/K = 52.56%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  156990.0 e-6; = (1/var)*||X-X_r||^2 =  104312.6 e-6 = 66.4 %; (1+beta)*||Z_e-Z_q||^2 =  52677.4 e-6 = 33.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  206672.6 e-6; = (1/var)*||X-X_r||^2 =  148345.1 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  58327.5 e-6 = 28.2 %)
Min.  Avg. Train Loss across Mini-Batch =  156990.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  206672.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   49682.6 e-6; = (1/var)*||X-X_r||^2 val-train = 44032.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5650.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.29; perplexity/K = 57.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.21; perplexity/K = 55.22%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  106664.9 e-6; = (1/var)*||X-X_r||^2 =  92163.7 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  14501.2 e-6 = 13.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  157030.7 e-6; = (1/var)*||X-X_r||^2 =  138876.7 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  18154.0 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  103810.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155657.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50365.8 e-6; = (1/var)*||X-X_r||^2 val-train = 46713.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3652.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.28; perplexity/K = 56.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.26; perplexity/K = 56.52%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93785.6 e-6; = (1/var)*||X-X_r||^2 =  88572.0 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  5213.6 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  148224.0 e-6; = (1/var)*||X-X_r||^2 =  139325.9 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  8898.1 e-6 = 6.0 %)
Min.  Avg. Train Loss across Mini-Batch =  93785.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  146070.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54438.4 e-6; = (1/var)*||X-X_r||^2 val-train = 50753.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3684.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.15; perplexity/K = 53.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.26; perplexity/K = 56.61%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93601.1 e-6; = (1/var)*||X-X_r||^2 =  88626.3 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  4974.7 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  146464.6 e-6; = (1/var)*||X-X_r||^2 =  139751.7 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  6712.9 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  92112.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143846.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52863.6 e-6; = (1/var)*||X-X_r||^2 val-train = 51125.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1738.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.02; perplexity/K = 50.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.10; perplexity/K = 52.40%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90701.2 e-6; = (1/var)*||X-X_r||^2 =  88049.7 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  2651.5 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  142341.9 e-6; = (1/var)*||X-X_r||^2 =  138276.4 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  4065.5 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  90505.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142341.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51640.7 e-6; = (1/var)*||X-X_r||^2 val-train = 50226.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1414.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.49; perplexity/K = 62.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.10; perplexity/K = 52.41%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  92539.5 e-6; = (1/var)*||X-X_r||^2 =  88970.0 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  3569.5 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  142827.6 e-6; = (1/var)*||X-X_r||^2 =  138518.9 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  4308.7 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  89755.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141550.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50288.1 e-6; = (1/var)*||X-X_r||^2 val-train = 49549.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 739.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.25; perplexity/K = 56.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.18; perplexity/K = 54.50%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  96400.4 e-6; = (1/var)*||X-X_r||^2 =  90490.2 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  5910.3 e-6 = 6.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  153249.2 e-6; = (1/var)*||X-X_r||^2 =  144680.3 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  8568.9 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140796.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56848.8 e-6; = (1/var)*||X-X_r||^2 val-train = 54190.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2658.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.18; perplexity/K = 54.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.26; perplexity/K = 56.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99970.9 e-6; = (1/var)*||X-X_r||^2 =  92864.0 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  7107.0 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  157281.9 e-6; = (1/var)*||X-X_r||^2 =  146319.0 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  10962.9 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140796.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57311.0 e-6; = (1/var)*||X-X_r||^2 val-train = 53455.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3856.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.48; perplexity/K = 62.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.05; perplexity/K = 51.13%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  92468.2 e-6; = (1/var)*||X-X_r||^2 =  90061.6 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  2406.6 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  141310.2 e-6; = (1/var)*||X-X_r||^2 =  138839.2 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  2471.0 e-6 = 1.7 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140796.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48842.0 e-6; = (1/var)*||X-X_r||^2 val-train = 48777.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 64.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.20; perplexity/K = 54.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.07; perplexity/K = 51.76%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  98674.8 e-6; = (1/var)*||X-X_r||^2 =  92348.9 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  6325.9 e-6 = 6.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  149975.0 e-6; = (1/var)*||X-X_r||^2 =  143563.4 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  6411.6 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51300.2 e-6; = (1/var)*||X-X_r||^2 val-train = 51214.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 85.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.23; perplexity/K = 55.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.03; perplexity/K = 50.63%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  104608.5 e-6; = (1/var)*||X-X_r||^2 =  96089.2 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  8519.4 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  159319.9 e-6; = (1/var)*||X-X_r||^2 =  150608.2 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  8711.7 e-6 = 5.5 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54711.4 e-6; = (1/var)*||X-X_r||^2 val-train = 54519.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 192.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.23; perplexity/K = 55.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.09; perplexity/K = 52.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  92965.2 e-6; = (1/var)*||X-X_r||^2 =  91092.2 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  1873.1 e-6 = 2.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  148246.1 e-6; = (1/var)*||X-X_r||^2 =  145824.2 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  2422.0 e-6 = 1.6 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55280.9 e-6; = (1/var)*||X-X_r||^2 val-train = 54732.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 548.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.24; perplexity/K = 55.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.19; perplexity/K = 54.77%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  96680.8 e-6; = (1/var)*||X-X_r||^2 =  94336.7 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  2344.1 e-6 = 2.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  152341.4 e-6; = (1/var)*||X-X_r||^2 =  149362.1 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  2979.3 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55660.6 e-6; = (1/var)*||X-X_r||^2 val-train = 55025.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 635.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.10; perplexity/K = 52.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.12; perplexity/K = 52.93%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  103433.3 e-6; = (1/var)*||X-X_r||^2 =  97417.6 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  6015.7 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  156937.8 e-6; = (1/var)*||X-X_r||^2 =  152148.1 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  4789.6 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53504.5 e-6; = (1/var)*||X-X_r||^2 val-train = 54730.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1226.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.18; perplexity/K = 54.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.14; perplexity/K = 53.53%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  98475.1 e-6; = (1/var)*||X-X_r||^2 =  96025.1 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  2450.0 e-6 = 2.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  154172.2 e-6; = (1/var)*||X-X_r||^2 =  149964.9 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  4207.3 e-6 = 2.7 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55697.1 e-6; = (1/var)*||X-X_r||^2 val-train = 53939.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1757.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.24; perplexity/K = 56.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.28; perplexity/K = 56.93%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99463.6 e-6; = (1/var)*||X-X_r||^2 =  96145.2 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  3318.4 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  157151.4 e-6; = (1/var)*||X-X_r||^2 =  154393.2 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  2758.1 e-6 = 1.8 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57687.8 e-6; = (1/var)*||X-X_r||^2 val-train = 58248.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -560.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.27; perplexity/K = 56.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.18; perplexity/K = 54.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  100338.1 e-6; = (1/var)*||X-X_r||^2 =  97457.9 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  2880.2 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  155975.7 e-6; = (1/var)*||X-X_r||^2 =  153333.6 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  2642.1 e-6 = 1.7 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55637.5 e-6; = (1/var)*||X-X_r||^2 val-train = 55875.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -238.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.05; perplexity/K = 51.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.30; perplexity/K = 57.58%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  110248.5 e-6; = (1/var)*||X-X_r||^2 =  101917.7 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  8330.7 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  157454.1 e-6; = (1/var)*||X-X_r||^2 =  153183.7 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  4270.4 e-6 = 2.7 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47205.6 e-6; = (1/var)*||X-X_r||^2 val-train = 51265.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4060.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.11; perplexity/K = 52.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.13; perplexity/K = 53.36%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  109205.7 e-6; = (1/var)*||X-X_r||^2 =  99608.4 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  9597.4 e-6 = 8.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  160901.6 e-6; = (1/var)*||X-X_r||^2 =  152805.1 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  8096.5 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  89270.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51695.9 e-6; = (1/var)*||X-X_r||^2 val-train = 53196.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1500.9 e-6 

----------------------------------------------------------------------------------

Finished [07:41:17 08.01.2023] 309) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = False:
Total training time is = 0:3:32 h/m/s. 

--------------------------------------------------- 

Started [07:41:17 08.01.2023] 310) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7388 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.19
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.19
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.19
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.19
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.19
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.19
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.78; perplexity/K = 89.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.75; perplexity/K = 87.69%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  621226.7 e-6; = (1/var)*||X-X_r||^2 =  329856.9 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  291369.8 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  686660.9 e-6; = (1/var)*||X-X_r||^2 =  351655.9 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  335005.0 e-6 = 48.8 %)
Min.  Avg. Train Loss across Mini-Batch =  621226.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  650663.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   65434.2 e-6; = (1/var)*||X-X_r||^2 val-train = 21799.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 43635.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.74; perplexity/K = 86.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.71; perplexity/K = 85.58%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  479683.3 e-6; = (1/var)*||X-X_r||^2 =  311969.1 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  167714.3 e-6 = 35.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  521568.3 e-6; = (1/var)*||X-X_r||^2 =  347453.1 e-6 = 66.6 %; (1+beta)*||Z_e-Z_q||^2 =  174115.2 e-6 = 33.4 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41884.9 e-6; = (1/var)*||X-X_r||^2 val-train = 35484.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6400.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.64; perplexity/K = 82.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.59; perplexity/K = 79.48%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  545714.0 e-6; = (1/var)*||X-X_r||^2 =  472206.7 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  73507.3 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  571523.0 e-6; = (1/var)*||X-X_r||^2 =  497385.4 e-6 = 87.0 %; (1+beta)*||Z_e-Z_q||^2 =  74137.6 e-6 = 13.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   25809.1 e-6; = (1/var)*||X-X_r||^2 val-train = 25178.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 630.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 57.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 57.56%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  618728.8 e-6; = (1/var)*||X-X_r||^2 =  598850.4 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  19878.4 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  637439.4 e-6; = (1/var)*||X-X_r||^2 =  614815.7 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  22623.7 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18710.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15965.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2745.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.12; perplexity/K = 56.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.14; perplexity/K = 56.88%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  672326.1 e-6; = (1/var)*||X-X_r||^2 =  650032.2 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  22293.9 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  673850.2 e-6; = (1/var)*||X-X_r||^2 =  654893.8 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  18956.3 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1524.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4861.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3337.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 54.08%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  684941.9 e-6; = (1/var)*||X-X_r||^2 =  681073.4 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  3868.5 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  685174.6 e-6; = (1/var)*||X-X_r||^2 =  680613.8 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  4560.8 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   232.7 e-6; = (1/var)*||X-X_r||^2 val-train = -459.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 692.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.09; perplexity/K = 54.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.21%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  691394.3 e-6; = (1/var)*||X-X_r||^2 =  684187.7 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  7206.7 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  688308.5 e-6; = (1/var)*||X-X_r||^2 =  681900.8 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  6407.6 e-6 = 0.9 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3085.9 e-6; = (1/var)*||X-X_r||^2 val-train = -2286.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -799.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.14; perplexity/K = 56.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.14; perplexity/K = 57.07%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  719143.5 e-6; = (1/var)*||X-X_r||^2 =  713739.3 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  5404.2 e-6 = 0.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  721305.1 e-6; = (1/var)*||X-X_r||^2 =  715539.5 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  5765.6 e-6 = 0.8 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2161.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1800.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 361.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999549.2 e-6; = (1/var)*||X-X_r||^2 =  999524.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  25.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962989.7 e-6; = (1/var)*||X-X_r||^2 =  962965.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  24.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36559.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36558.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999320.0 e-6; = (1/var)*||X-X_r||^2 =  999293.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  26.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962948.0 e-6; = (1/var)*||X-X_r||^2 =  962924.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  23.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36372.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36369.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999416.3 e-6; = (1/var)*||X-X_r||^2 =  999393.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  23.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962984.7 e-6; = (1/var)*||X-X_r||^2 =  962964.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  19.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36431.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36428.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999523.5 e-6; = (1/var)*||X-X_r||^2 =  999461.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  62.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963728.7 e-6; = (1/var)*||X-X_r||^2 =  963704.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  24.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35794.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35756.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -37.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999277.9 e-6; = (1/var)*||X-X_r||^2 =  999261.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  16.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962927.9 e-6; = (1/var)*||X-X_r||^2 =  962908.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  19.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36350.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36352.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999346.5 e-6; = (1/var)*||X-X_r||^2 =  999334.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963100.3 e-6; = (1/var)*||X-X_r||^2 =  963089.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36246.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36245.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999393.2 e-6; = (1/var)*||X-X_r||^2 =  999380.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963520.9 e-6; = (1/var)*||X-X_r||^2 =  963512.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  8.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35872.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35868.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999258.1 e-6; = (1/var)*||X-X_r||^2 =  999247.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962948.4 e-6; = (1/var)*||X-X_r||^2 =  962937.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36309.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36309.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999236.9 e-6; = (1/var)*||X-X_r||^2 =  999227.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963135.7 e-6; = (1/var)*||X-X_r||^2 =  963128.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36101.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36098.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999309.6 e-6; = (1/var)*||X-X_r||^2 =  999298.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962953.7 e-6; = (1/var)*||X-X_r||^2 =  962943.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36355.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36354.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999190.2 e-6; = (1/var)*||X-X_r||^2 =  999179.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963020.8 e-6; = (1/var)*||X-X_r||^2 =  963011.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36169.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36167.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999502.3 e-6; = (1/var)*||X-X_r||^2 =  999489.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962942.8 e-6; = (1/var)*||X-X_r||^2 =  962936.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  440804.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  460222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36559.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36553.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6.2 e-6 

----------------------------------------------------------------------------------

Finished [09:56:51 08.01.2023] 310) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = False:
Total training time is = 0:3:33 h/m/s. 

--------------------------------------------------- 

Started [09:56:51 08.01.2023] 311) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 732 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.09
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.37
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.90
4                                  encoder.pre_residual_stack.weight                       147            20.08
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.09
10                                                       VQ.E.weight                         2             0.27
11                             decoder.channel_adjusting_conv.weight                        73             9.97
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.90
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.37
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.09
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.90; perplexity/K = 62.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.13; perplexity/K = 62.90%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  369371.8 e-6; = (1/var)*||X-X_r||^2 =  262291.0 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  107080.8 e-6 = 29.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  357499.2 e-6; = (1/var)*||X-X_r||^2 =  261112.8 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  96386.5 e-6 = 27.0 %)
Min.  Avg. Train Loss across Mini-Batch =  369371.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  357499.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11872.6 e-6; = (1/var)*||X-X_r||^2 val-train = -1178.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10694.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.04; perplexity/K = 53.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.65; perplexity/K = 58.27%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  173311.4 e-6; = (1/var)*||X-X_r||^2 =  133920.8 e-6 = 77.3 %; (1+beta)*||Z_e-Z_q||^2 =  39390.5 e-6 = 22.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  176915.6 e-6; = (1/var)*||X-X_r||^2 =  138537.4 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  38378.3 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  173311.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  176915.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3604.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4616.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1012.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.85; perplexity/K = 62.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.64; perplexity/K = 61.38%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  119462.6 e-6; = (1/var)*||X-X_r||^2 =  94667.1 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  24795.4 e-6 = 20.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  125333.1 e-6; = (1/var)*||X-X_r||^2 =  100740.8 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  24592.3 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  119459.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  125333.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5870.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6073.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -203.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.70; perplexity/K = 61.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.01; perplexity/K = 62.54%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94402.6 e-6; = (1/var)*||X-X_r||^2 =  76222.8 e-6 = 80.7 %; (1+beta)*||Z_e-Z_q||^2 =  18179.9 e-6 = 19.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  101411.1 e-6; = (1/var)*||X-X_r||^2 =  82490.9 e-6 = 81.3 %; (1+beta)*||Z_e-Z_q||^2 =  18920.2 e-6 = 18.7 %)
Min.  Avg. Train Loss across Mini-Batch =  94260.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  100897.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7008.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6268.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 740.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.34; perplexity/K = 63.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.61; perplexity/K = 61.28%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  76700.5 e-6; = (1/var)*||X-X_r||^2 =  61114.5 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  15586.0 e-6 = 20.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  83979.8 e-6; = (1/var)*||X-X_r||^2 =  66876.0 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  17103.8 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  76671.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  83355.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7279.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5761.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1517.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.62; perplexity/K = 61.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.88; perplexity/K = 62.13%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  64848.7 e-6; = (1/var)*||X-X_r||^2 =  52111.9 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  12736.7 e-6 = 19.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  71972.0 e-6; = (1/var)*||X-X_r||^2 =  57354.2 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  14617.8 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  64848.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71918.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7123.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5242.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1881.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.26; perplexity/K = 60.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.12; perplexity/K = 65.99%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59100.5 e-6; = (1/var)*||X-X_r||^2 =  48500.6 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  10599.8 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  66163.9 e-6; = (1/var)*||X-X_r||^2 =  53630.6 e-6 = 81.1 %; (1+beta)*||Z_e-Z_q||^2 =  12533.3 e-6 = 18.9 %)
Min.  Avg. Train Loss across Mini-Batch =  58803.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65467.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7063.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5130.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1933.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.63; perplexity/K = 64.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.68; perplexity/K = 61.51%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  54539.4 e-6; = (1/var)*||X-X_r||^2 =  45649.7 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  8889.7 e-6 = 16.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  61954.4 e-6; = (1/var)*||X-X_r||^2 =  51191.6 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  10762.8 e-6 = 17.4 %)
Min.  Avg. Train Loss across Mini-Batch =  54539.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61425.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7415.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5541.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1873.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.61; perplexity/K = 64.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.13; perplexity/K = 59.77%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52858.0 e-6; = (1/var)*||X-X_r||^2 =  44559.3 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  8298.6 e-6 = 15.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  59190.4 e-6; = (1/var)*||X-X_r||^2 =  49221.9 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  9968.6 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  52403.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  59190.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6332.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4662.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1669.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.36; perplexity/K = 60.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.64; perplexity/K = 58.25%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50698.8 e-6; = (1/var)*||X-X_r||^2 =  43104.2 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  7594.6 e-6 = 15.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  57436.2 e-6; = (1/var)*||X-X_r||^2 =  48224.6 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  9211.6 e-6 = 16.0 %)
Min.  Avg. Train Loss across Mini-Batch =  50501.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56975.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6737.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5120.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1616.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.64; perplexity/K = 64.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.35; perplexity/K = 63.58%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49253.5 e-6; = (1/var)*||X-X_r||^2 =  41895.2 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  7358.3 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  55939.9 e-6; = (1/var)*||X-X_r||^2 =  47007.9 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  8931.9 e-6 = 16.0 %)
Min.  Avg. Train Loss across Mini-Batch =  49253.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55939.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6686.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5112.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1573.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.10; perplexity/K = 62.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.96; perplexity/K = 62.39%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47732.4 e-6; = (1/var)*||X-X_r||^2 =  41124.9 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  6607.5 e-6 = 13.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  54317.6 e-6; = (1/var)*||X-X_r||^2 =  45978.5 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  8339.1 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  47684.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  54317.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6585.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4853.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1731.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.42; perplexity/K = 60.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.31; perplexity/K = 60.36%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46862.7 e-6; = (1/var)*||X-X_r||^2 =  40369.3 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  6493.4 e-6 = 13.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  53685.6 e-6; = (1/var)*||X-X_r||^2 =  45287.0 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  8398.7 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  46742.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  53358.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6822.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4917.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1905.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.22; perplexity/K = 63.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.62; perplexity/K = 64.44%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47245.6 e-6; = (1/var)*||X-X_r||^2 =  40232.1 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  7013.5 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  52321.0 e-6; = (1/var)*||X-X_r||^2 =  44412.2 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  7908.7 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  45967.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52263.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5075.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4180.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 895.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.58; perplexity/K = 67.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.49; perplexity/K = 64.04%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  56975.3 e-6; = (1/var)*||X-X_r||^2 =  45515.3 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  11460.0 e-6 = 20.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  58307.7 e-6; = (1/var)*||X-X_r||^2 =  47456.1 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  10851.6 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  45289.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51098.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1332.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1940.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -608.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.75; perplexity/K = 67.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.39; perplexity/K = 66.84%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50030.1 e-6; = (1/var)*||X-X_r||^2 =  42753.6 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  7276.4 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  51834.0 e-6; = (1/var)*||X-X_r||^2 =  44891.2 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  6942.9 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  44682.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50090.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1804.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2137.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -333.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.11; perplexity/K = 62.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.71; perplexity/K = 67.84%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44449.6 e-6; = (1/var)*||X-X_r||^2 =  38531.7 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  5918.0 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  50509.8 e-6; = (1/var)*||X-X_r||^2 =  43233.4 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  7276.4 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  44080.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49539.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6060.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4701.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1358.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.33; perplexity/K = 66.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.41; perplexity/K = 60.67%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46849.6 e-6; = (1/var)*||X-X_r||^2 =  40002.7 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  6846.9 e-6 = 14.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  51671.6 e-6; = (1/var)*||X-X_r||^2 =  43976.3 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  7695.4 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  43419.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49142.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4822.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3973.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 848.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.92; perplexity/K = 62.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.11; perplexity/K = 65.96%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  70758.4 e-6; = (1/var)*||X-X_r||^2 =  58788.8 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  11969.6 e-6 = 16.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  65222.7 e-6; = (1/var)*||X-X_r||^2 =  52683.2 e-6 = 80.8 %; (1+beta)*||Z_e-Z_q||^2 =  12539.6 e-6 = 19.2 %)
Min.  Avg. Train Loss across Mini-Batch =  43407.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49058.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5535.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6105.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 569.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.18; perplexity/K = 63.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.18; perplexity/K = 66.18%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43541.6 e-6; = (1/var)*||X-X_r||^2 =  37212.3 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  6329.4 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  48956.9 e-6; = (1/var)*||X-X_r||^2 =  41614.0 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  7342.9 e-6 = 15.0 %)
Min.  Avg. Train Loss across Mini-Batch =  43041.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48311.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5415.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4401.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1013.6 e-6 

----------------------------------------------------------------------------------

Finished [10:45:12 08.01.2023] 311) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:
Total training time is = 0:3:21 h/m/s. 

--------------------------------------------------- 

Started [10:45:12 08.01.2023] 312) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 731 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.09
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.38
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.92
4                                  encoder.pre_residual_stack.weight                       147            20.11
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.09
10                                                       VQ.E.weight                         1             0.14
11                             decoder.channel_adjusting_conv.weight                        73             9.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.92
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.38
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.09
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.94; perplexity/K = 62.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.54; perplexity/K = 65.87%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  604849.2 e-6; = (1/var)*||X-X_r||^2 =  513675.8 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  91173.3 e-6 = 15.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  591994.7 e-6; = (1/var)*||X-X_r||^2 =  501045.6 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  90949.1 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  604849.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  586919.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12854.5 e-6; = (1/var)*||X-X_r||^2 val-train = -12630.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -224.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.76; perplexity/K = 67.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.43; perplexity/K = 65.17%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  180162.2 e-6; = (1/var)*||X-X_r||^2 =  150612.2 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  29549.9 e-6 = 16.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  186960.8 e-6; = (1/var)*||X-X_r||^2 =  157416.3 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  29544.5 e-6 = 15.8 %)
Min.  Avg. Train Loss across Mini-Batch =  180162.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186402.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6798.6 e-6; = (1/var)*||X-X_r||^2 val-train = 6804.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.92; perplexity/K = 68.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.87; perplexity/K = 67.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  128285.9 e-6; = (1/var)*||X-X_r||^2 =  100640.0 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  27645.9 e-6 = 21.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  134119.8 e-6; = (1/var)*||X-X_r||^2 =  106638.8 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  27481.0 e-6 = 20.5 %)
Min.  Avg. Train Loss across Mini-Batch =  128285.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  134119.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5833.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5998.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -165.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.85; perplexity/K = 67.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.61; perplexity/K = 66.28%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99629.6 e-6; = (1/var)*||X-X_r||^2 =  77109.6 e-6 = 77.4 %; (1+beta)*||Z_e-Z_q||^2 =  22520.0 e-6 = 22.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  108357.0 e-6; = (1/var)*||X-X_r||^2 =  84839.2 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  23517.7 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  99462.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  108357.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8727.4 e-6; = (1/var)*||X-X_r||^2 val-train = 7729.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 997.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.25; perplexity/K = 70.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.43; perplexity/K = 71.43%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  79147.4 e-6; = (1/var)*||X-X_r||^2 =  62317.0 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  16830.4 e-6 = 21.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  89213.5 e-6; = (1/var)*||X-X_r||^2 =  70649.7 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  18563.8 e-6 = 20.8 %)
Min.  Avg. Train Loss across Mini-Batch =  79044.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  88011.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10066.1 e-6; = (1/var)*||X-X_r||^2 val-train = 8332.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1733.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.55; perplexity/K = 78.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 78.16%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  65287.6 e-6; = (1/var)*||X-X_r||^2 =  52557.0 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  12730.5 e-6 = 19.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  74766.0 e-6; = (1/var)*||X-X_r||^2 =  60576.7 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  14189.3 e-6 = 19.0 %)
Min.  Avg. Train Loss across Mini-Batch =  65287.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74766.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9478.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8019.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1458.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.21; perplexity/K = 82.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.33; perplexity/K = 77.04%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57554.5 e-6; = (1/var)*||X-X_r||^2 =  47278.0 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  10276.5 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  66815.6 e-6; = (1/var)*||X-X_r||^2 =  54441.5 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  12374.1 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  57554.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  66520.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9261.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7163.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2097.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.67; perplexity/K = 79.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.78; perplexity/K = 79.90%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53189.5 e-6; = (1/var)*||X-X_r||^2 =  44700.1 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  8489.4 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  63224.1 e-6; = (1/var)*||X-X_r||^2 =  52636.4 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  10587.6 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  53189.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62080.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10034.5 e-6; = (1/var)*||X-X_r||^2 val-train = 7936.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2098.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.27; perplexity/K = 76.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 77.64%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50081.5 e-6; = (1/var)*||X-X_r||^2 =  42686.4 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  7395.1 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  58978.3 e-6; = (1/var)*||X-X_r||^2 =  49630.0 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  9348.3 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  50081.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  58325.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8896.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6943.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1953.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.84; perplexity/K = 80.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.38; perplexity/K = 77.35%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55003.9 e-6; = (1/var)*||X-X_r||^2 =  44487.4 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  10516.6 e-6 = 19.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  59398.1 e-6; = (1/var)*||X-X_r||^2 =  48573.6 e-6 = 81.8 %; (1+beta)*||Z_e-Z_q||^2 =  10824.5 e-6 = 18.2 %)
Min.  Avg. Train Loss across Mini-Batch =  47530.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56237.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4394.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4086.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 307.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 80.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.19; perplexity/K = 76.17%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47224.9 e-6; = (1/var)*||X-X_r||^2 =  39959.0 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  7266.0 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  54414.2 e-6; = (1/var)*||X-X_r||^2 =  45770.5 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  8643.7 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  46108.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  54251.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7189.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5811.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1377.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 78.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 77.02%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45799.3 e-6; = (1/var)*||X-X_r||^2 =  38994.5 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  6804.8 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  52990.3 e-6; = (1/var)*||X-X_r||^2 =  44857.8 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  8132.6 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  44516.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52528.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7191.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5863.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1327.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.53; perplexity/K = 78.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.55; perplexity/K = 78.44%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42939.7 e-6; = (1/var)*||X-X_r||^2 =  37117.6 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  5822.2 e-6 = 13.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  51352.9 e-6; = (1/var)*||X-X_r||^2 =  43865.8 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  7487.0 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  42813.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50927.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8413.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6748.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1664.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 78.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.34; perplexity/K = 83.38%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41851.9 e-6; = (1/var)*||X-X_r||^2 =  36725.2 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  5126.7 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  49664.0 e-6; = (1/var)*||X-X_r||^2 =  43017.1 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  6646.8 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  41381.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49516.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7812.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6291.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1520.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.55; perplexity/K = 78.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.07; perplexity/K = 75.46%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68720.9 e-6; = (1/var)*||X-X_r||^2 =  50898.2 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  17822.7 e-6 = 25.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  60438.6 e-6; = (1/var)*||X-X_r||^2 =  47377.1 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  13061.5 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  40446.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48323.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8282.3 e-6; = (1/var)*||X-X_r||^2 val-train = -3521.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4761.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 77.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 78.57%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39400.9 e-6; = (1/var)*||X-X_r||^2 =  34377.3 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  5023.6 e-6 = 12.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  47282.3 e-6; = (1/var)*||X-X_r||^2 =  40706.7 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  6575.6 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  39392.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47282.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7881.4 e-6; = (1/var)*||X-X_r||^2 val-train = 6329.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1552.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.72; perplexity/K = 79.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.00; perplexity/K = 75.02%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42429.3 e-6; = (1/var)*||X-X_r||^2 =  34476.3 e-6 = 81.3 %; (1+beta)*||Z_e-Z_q||^2 =  7953.0 e-6 = 18.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  48420.5 e-6; = (1/var)*||X-X_r||^2 =  39875.6 e-6 = 82.4 %; (1+beta)*||Z_e-Z_q||^2 =  8545.0 e-6 = 17.6 %)
Min.  Avg. Train Loss across Mini-Batch =  37841.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46004.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5991.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5399.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 591.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.80; perplexity/K = 79.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.64; perplexity/K = 78.97%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36014.3 e-6; = (1/var)*||X-X_r||^2 =  31298.0 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  4716.3 e-6 = 13.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  45783.6 e-6; = (1/var)*||X-X_r||^2 =  38784.4 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  6999.2 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  35611.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44003.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9769.3 e-6; = (1/var)*||X-X_r||^2 val-train = 7486.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2282.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.91; perplexity/K = 80.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.66; perplexity/K = 79.15%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38961.8 e-6; = (1/var)*||X-X_r||^2 =  30472.6 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  8489.2 e-6 = 21.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  45447.3 e-6; = (1/var)*||X-X_r||^2 =  36415.2 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  9032.1 e-6 = 19.9 %)
Min.  Avg. Train Loss across Mini-Batch =  33635.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42033.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6485.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5942.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 542.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.49; perplexity/K = 78.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.34; perplexity/K = 77.12%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32127.4 e-6; = (1/var)*||X-X_r||^2 =  27333.1 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  4794.3 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  40401.2 e-6; = (1/var)*||X-X_r||^2 =  33776.4 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  6624.8 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  32127.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  39933.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8273.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6443.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1830.5 e-6 

----------------------------------------------------------------------------------

Finished [11:33:45 08.01.2023] 312) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:
Total training time is = 0:3:32 h/m/s. 

--------------------------------------------------- 

Started [11:33:45 08.01.2023] 313) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 730 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.10
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.38
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.95
4                                  encoder.pre_residual_stack.weight                       147            20.14
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.10
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73            10.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.95
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.38
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.10
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.62; perplexity/K = 45.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.55; perplexity/K = 44.39%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  535274.8 e-6; = (1/var)*||X-X_r||^2 =  271354.7 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  263920.1 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  568484.8 e-6; = (1/var)*||X-X_r||^2 =  276871.9 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  291612.9 e-6 = 51.3 %)
Min.  Avg. Train Loss across Mini-Batch =  535274.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  519331.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33210.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5517.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27692.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.02; perplexity/K = 62.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.68; perplexity/K = 58.53%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  311405.9 e-6; = (1/var)*||X-X_r||^2 =  153649.4 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  157756.5 e-6 = 50.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  342479.7 e-6; = (1/var)*||X-X_r||^2 =  162995.1 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  179484.6 e-6 = 52.4 %)
Min.  Avg. Train Loss across Mini-Batch =  311405.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  321632.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31073.8 e-6; = (1/var)*||X-X_r||^2 val-train = 9345.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21728.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.48; perplexity/K = 68.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.33; perplexity/K = 66.68%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  175233.7 e-6; = (1/var)*||X-X_r||^2 =  98181.8 e-6 = 56.0 %; (1+beta)*||Z_e-Z_q||^2 =  77051.8 e-6 = 44.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  186369.0 e-6; = (1/var)*||X-X_r||^2 =  107928.4 e-6 = 57.9 %; (1+beta)*||Z_e-Z_q||^2 =  78440.6 e-6 = 42.1 %)
Min.  Avg. Train Loss across Mini-Batch =  171588.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  184942.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11135.3 e-6; = (1/var)*||X-X_r||^2 val-train = 9746.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1388.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.28; perplexity/K = 66.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.14; perplexity/K = 64.25%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  162611.6 e-6; = (1/var)*||X-X_r||^2 =  81412.3 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  81199.3 e-6 = 49.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  353835.6 e-6; = (1/var)*||X-X_r||^2 =  155042.2 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  198793.5 e-6 = 56.2 %)
Min.  Avg. Train Loss across Mini-Batch =  120887.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  137977.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   191224.0 e-6; = (1/var)*||X-X_r||^2 val-train = 73629.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 117594.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.52; perplexity/K = 69.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.44; perplexity/K = 68.05%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  98051.3 e-6; = (1/var)*||X-X_r||^2 =  55855.1 e-6 = 57.0 %; (1+beta)*||Z_e-Z_q||^2 =  42196.2 e-6 = 43.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  119487.2 e-6; = (1/var)*||X-X_r||^2 =  69994.7 e-6 = 58.6 %; (1+beta)*||Z_e-Z_q||^2 =  49492.5 e-6 = 41.4 %)
Min.  Avg. Train Loss across Mini-Batch =  91480.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  111818.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21435.9 e-6; = (1/var)*||X-X_r||^2 val-train = 14139.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7296.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.49; perplexity/K = 68.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.47; perplexity/K = 68.34%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  77378.1 e-6; = (1/var)*||X-X_r||^2 =  48350.3 e-6 = 62.5 %; (1+beta)*||Z_e-Z_q||^2 =  29027.8 e-6 = 37.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  98340.4 e-6; = (1/var)*||X-X_r||^2 =  62926.7 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  35413.8 e-6 = 36.0 %)
Min.  Avg. Train Loss across Mini-Batch =  77304.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  97132.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20962.3 e-6; = (1/var)*||X-X_r||^2 val-train = 14576.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6385.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.37; perplexity/K = 67.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.46; perplexity/K = 68.19%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67969.2 e-6; = (1/var)*||X-X_r||^2 =  44674.9 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  23294.2 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  88911.6 e-6; = (1/var)*||X-X_r||^2 =  59764.7 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  29146.9 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  67657.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  87776.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20942.4 e-6; = (1/var)*||X-X_r||^2 val-train = 15089.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5852.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.38; perplexity/K = 67.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.40; perplexity/K = 67.54%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60123.9 e-6; = (1/var)*||X-X_r||^2 =  38877.4 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  21246.6 e-6 = 35.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  79700.4 e-6; = (1/var)*||X-X_r||^2 =  53496.1 e-6 = 67.1 %; (1+beta)*||Z_e-Z_q||^2 =  26204.3 e-6 = 32.9 %)
Min.  Avg. Train Loss across Mini-Batch =  59089.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78968.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19576.5 e-6; = (1/var)*||X-X_r||^2 val-train = 14618.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4957.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.40; perplexity/K = 67.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.25; perplexity/K = 65.60%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61847.1 e-6; = (1/var)*||X-X_r||^2 =  37102.1 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  24745.0 e-6 = 40.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  78312.3 e-6; = (1/var)*||X-X_r||^2 =  50339.9 e-6 = 64.3 %; (1+beta)*||Z_e-Z_q||^2 =  27972.4 e-6 = 35.7 %)
Min.  Avg. Train Loss across Mini-Batch =  54776.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  73560.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16465.3 e-6; = (1/var)*||X-X_r||^2 val-train = 13237.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3227.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.43; perplexity/K = 67.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.36; perplexity/K = 66.99%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  51017.2 e-6; = (1/var)*||X-X_r||^2 =  34158.4 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  16858.8 e-6 = 33.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  70135.1 e-6; = (1/var)*||X-X_r||^2 =  48237.2 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  21897.9 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  51017.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  70135.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19117.9 e-6; = (1/var)*||X-X_r||^2 val-train = 14078.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5039.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.53; perplexity/K = 69.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.32; perplexity/K = 66.54%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48814.7 e-6; = (1/var)*||X-X_r||^2 =  32621.7 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  16193.0 e-6 = 33.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  66442.2 e-6; = (1/var)*||X-X_r||^2 =  45861.3 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  20580.9 e-6 = 31.0 %)
Min.  Avg. Train Loss across Mini-Batch =  48814.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  66442.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17627.5 e-6; = (1/var)*||X-X_r||^2 val-train = 13239.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4387.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.38; perplexity/K = 67.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.28; perplexity/K = 66.06%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53707.0 e-6; = (1/var)*||X-X_r||^2 =  31196.2 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  22510.7 e-6 = 41.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  69665.4 e-6; = (1/var)*||X-X_r||^2 =  43398.2 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  26267.2 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  44639.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62814.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15958.4 e-6; = (1/var)*||X-X_r||^2 val-train = 12201.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3756.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.57; perplexity/K = 69.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.49; perplexity/K = 68.58%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41710.1 e-6; = (1/var)*||X-X_r||^2 =  28877.8 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  12832.4 e-6 = 30.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  58476.9 e-6; = (1/var)*||X-X_r||^2 =  40947.4 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  17529.4 e-6 = 30.0 %)
Min.  Avg. Train Loss across Mini-Batch =  41710.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  58461.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16766.8 e-6; = (1/var)*||X-X_r||^2 val-train = 12069.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4697.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.60; perplexity/K = 69.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.25; perplexity/K = 65.58%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39847.6 e-6; = (1/var)*||X-X_r||^2 =  27361.3 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  12486.2 e-6 = 31.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  58739.2 e-6; = (1/var)*||X-X_r||^2 =  41864.2 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  16874.9 e-6 = 28.7 %)
Min.  Avg. Train Loss across Mini-Batch =  39453.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  57799.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18891.6 e-6; = (1/var)*||X-X_r||^2 val-train = 14502.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4388.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.38; perplexity/K = 67.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.31; perplexity/K = 66.37%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38140.0 e-6; = (1/var)*||X-X_r||^2 =  22800.8 e-6 = 59.8 %; (1+beta)*||Z_e-Z_q||^2 =  15339.2 e-6 = 40.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  56217.9 e-6; = (1/var)*||X-X_r||^2 =  36439.5 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  19778.4 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  34218.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52471.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18077.9 e-6; = (1/var)*||X-X_r||^2 val-train = 13638.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4439.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.56; perplexity/K = 69.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.63; perplexity/K = 70.36%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35239.0 e-6; = (1/var)*||X-X_r||^2 =  21594.6 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  13644.4 e-6 = 38.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  52387.0 e-6; = (1/var)*||X-X_r||^2 =  34392.6 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  17994.3 e-6 = 34.3 %)
Min.  Avg. Train Loss across Mini-Batch =  31443.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49443.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17148.0 e-6; = (1/var)*||X-X_r||^2 val-train = 12798.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4349.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.60; perplexity/K = 70.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.44; perplexity/K = 67.97%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33297.5 e-6; = (1/var)*||X-X_r||^2 =  21205.0 e-6 = 63.7 %; (1+beta)*||Z_e-Z_q||^2 =  12092.5 e-6 = 36.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  50650.5 e-6; = (1/var)*||X-X_r||^2 =  35009.2 e-6 = 69.1 %; (1+beta)*||Z_e-Z_q||^2 =  15641.3 e-6 = 30.9 %)
Min.  Avg. Train Loss across Mini-Batch =  29179.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46723.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17353.1 e-6; = (1/var)*||X-X_r||^2 val-train = 13804.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3548.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.52; perplexity/K = 69.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.46; perplexity/K = 68.31%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31700.3 e-6; = (1/var)*||X-X_r||^2 =  17340.1 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  14360.2 e-6 = 45.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  49683.6 e-6; = (1/var)*||X-X_r||^2 =  32221.9 e-6 = 64.9 %; (1+beta)*||Z_e-Z_q||^2 =  17461.8 e-6 = 35.1 %)
Min.  Avg. Train Loss across Mini-Batch =  24566.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42555.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17983.3 e-6; = (1/var)*||X-X_r||^2 val-train = 14881.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3101.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.44; perplexity/K = 68.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.38; perplexity/K = 67.24%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31522.8 e-6; = (1/var)*||X-X_r||^2 =  18170.9 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  13351.9 e-6 = 42.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  49550.5 e-6; = (1/var)*||X-X_r||^2 =  33454.5 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  16096.0 e-6 = 32.5 %)
Min.  Avg. Train Loss across Mini-Batch =  24566.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42555.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18027.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15283.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2744.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.37; perplexity/K = 67.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.42; perplexity/K = 67.80%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37089.8 e-6; = (1/var)*||X-X_r||^2 =  18609.1 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  18480.7 e-6 = 49.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  54630.7 e-6; = (1/var)*||X-X_r||^2 =  33208.8 e-6 = 60.8 %; (1+beta)*||Z_e-Z_q||^2 =  21421.9 e-6 = 39.2 %)
Min.  Avg. Train Loss across Mini-Batch =  24566.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42555.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17540.9 e-6; = (1/var)*||X-X_r||^2 val-train = 14599.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2941.2 e-6 

----------------------------------------------------------------------------------

Finished [12:22:00 08.01.2023] 313) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:
Total training time is = 0:3:15 h/m/s. 

--------------------------------------------------- 

Started [12:22:00 08.01.2023] 314) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(4, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 730 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.10
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.38
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.95
4                                  encoder.pre_residual_stack.weight                       147            20.14
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.10
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73            10.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.95
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.38
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.10
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.88; perplexity/K = 46.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.80; perplexity/K = 45.05%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1008461.5 e-6; = (1/var)*||X-X_r||^2 =  311033.7 e-6 = 30.8 %; (1+beta)*||Z_e-Z_q||^2 =  697427.8 e-6 = 69.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  983070.9 e-6; = (1/var)*||X-X_r||^2 =  309111.9 e-6 = 31.4 %; (1+beta)*||Z_e-Z_q||^2 =  673959.0 e-6 = 68.6 %)
Min.  Avg. Train Loss across Mini-Batch =  979895.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  888822.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25390.6 e-6; = (1/var)*||X-X_r||^2 val-train = -1921.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -23468.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.94; perplexity/K = 48.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.76; perplexity/K = 43.93%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1166344.7 e-6; = (1/var)*||X-X_r||^2 =  245167.3 e-6 = 21.0 %; (1+beta)*||Z_e-Z_q||^2 =  921177.4 e-6 = 79.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1182564.0 e-6; = (1/var)*||X-X_r||^2 =  265069.9 e-6 = 22.4 %; (1+beta)*||Z_e-Z_q||^2 =  917494.2 e-6 = 77.6 %)
Min.  Avg. Train Loss across Mini-Batch =  979895.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  888822.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16219.3 e-6; = (1/var)*||X-X_r||^2 val-train = 19902.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3683.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.89; perplexity/K = 47.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.94; perplexity/K = 48.50%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  628436.8 e-6; = (1/var)*||X-X_r||^2 =  222606.9 e-6 = 35.4 %; (1+beta)*||Z_e-Z_q||^2 =  405829.8 e-6 = 64.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  655106.2 e-6; = (1/var)*||X-X_r||^2 =  247968.9 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  407137.3 e-6 = 62.1 %)
Min.  Avg. Train Loss across Mini-Batch =  628436.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  655106.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26669.4 e-6; = (1/var)*||X-X_r||^2 val-train = 25361.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1307.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 47.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.86; perplexity/K = 46.56%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  407625.6 e-6; = (1/var)*||X-X_r||^2 =  213764.5 e-6 = 52.4 %; (1+beta)*||Z_e-Z_q||^2 =  193861.1 e-6 = 47.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  448461.9 e-6; = (1/var)*||X-X_r||^2 =  238290.0 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  210171.9 e-6 = 46.9 %)
Min.  Avg. Train Loss across Mini-Batch =  407625.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  423719.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40836.4 e-6; = (1/var)*||X-X_r||^2 val-train = 24525.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16310.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 48.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 47.81%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  301867.9 e-6; = (1/var)*||X-X_r||^2 =  208404.3 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  93463.6 e-6 = 31.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  343494.4 e-6; = (1/var)*||X-X_r||^2 =  234555.7 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  108938.7 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  301867.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  337529.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41626.5 e-6; = (1/var)*||X-X_r||^2 val-train = 26151.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15475.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.89; perplexity/K = 47.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.81; perplexity/K = 45.36%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  263088.8 e-6; = (1/var)*||X-X_r||^2 =  205142.1 e-6 = 78.0 %; (1+beta)*||Z_e-Z_q||^2 =  57946.6 e-6 = 22.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  296060.2 e-6; = (1/var)*||X-X_r||^2 =  231559.5 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  64500.7 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  263088.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  295649.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32971.5 e-6; = (1/var)*||X-X_r||^2 val-train = 26417.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6554.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.84; perplexity/K = 45.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.69; perplexity/K = 42.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  247715.3 e-6; = (1/var)*||X-X_r||^2 =  202808.6 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  44906.6 e-6 = 18.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  285223.5 e-6; = (1/var)*||X-X_r||^2 =  232534.3 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  52689.2 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  244841.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  278251.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37508.3 e-6; = (1/var)*||X-X_r||^2 val-train = 29725.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7782.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.77; perplexity/K = 44.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.76; perplexity/K = 43.91%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  235829.5 e-6; = (1/var)*||X-X_r||^2 =  200006.6 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  35822.9 e-6 = 15.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  267122.2 e-6; = (1/var)*||X-X_r||^2 =  229260.8 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  37861.4 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  234641.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  266060.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31292.7 e-6; = (1/var)*||X-X_r||^2 val-train = 29254.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2038.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 47.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.96; perplexity/K = 49.11%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  235400.7 e-6; = (1/var)*||X-X_r||^2 =  195421.0 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  39979.7 e-6 = 17.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  268330.7 e-6; = (1/var)*||X-X_r||^2 =  223599.4 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  44731.4 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  228188.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  261715.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32930.0 e-6; = (1/var)*||X-X_r||^2 val-train = 28178.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4751.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.77; perplexity/K = 44.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.95; perplexity/K = 48.87%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  223372.4 e-6; = (1/var)*||X-X_r||^2 =  191212.8 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  32159.6 e-6 = 14.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  253989.1 e-6; = (1/var)*||X-X_r||^2 =  218355.6 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  35633.5 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  217839.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  252416.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30616.6 e-6; = (1/var)*||X-X_r||^2 val-train = 27142.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3473.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.88; perplexity/K = 47.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.96; perplexity/K = 49.04%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  224435.4 e-6; = (1/var)*||X-X_r||^2 =  190100.1 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  34335.4 e-6 = 15.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  252675.3 e-6; = (1/var)*||X-X_r||^2 =  218495.2 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  34180.1 e-6 = 13.5 %)
Min.  Avg. Train Loss across Mini-Batch =  211886.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  245869.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28239.9 e-6; = (1/var)*||X-X_r||^2 val-train = 28395.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -155.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.83; perplexity/K = 45.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.87; perplexity/K = 46.80%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  207779.5 e-6; = (1/var)*||X-X_r||^2 =  184960.3 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  22819.2 e-6 = 11.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  247957.1 e-6; = (1/var)*||X-X_r||^2 =  218672.0 e-6 = 88.2 %; (1+beta)*||Z_e-Z_q||^2 =  29285.1 e-6 = 11.8 %)
Min.  Avg. Train Loss across Mini-Batch =  207536.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  241572.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40177.7 e-6; = (1/var)*||X-X_r||^2 val-train = 33711.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6465.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 49.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.89; perplexity/K = 47.16%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  204143.0 e-6; = (1/var)*||X-X_r||^2 =  183250.5 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  20892.5 e-6 = 10.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  242710.3 e-6; = (1/var)*||X-X_r||^2 =  217035.9 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  25674.4 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  202389.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  239274.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38567.3 e-6; = (1/var)*||X-X_r||^2 val-train = 33785.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4781.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.11; perplexity/K = 52.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 47.70%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  199848.1 e-6; = (1/var)*||X-X_r||^2 =  181199.4 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  18648.7 e-6 = 9.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  237126.7 e-6; = (1/var)*||X-X_r||^2 =  213644.3 e-6 = 90.1 %; (1+beta)*||Z_e-Z_q||^2 =  23482.4 e-6 = 9.9 %)
Min.  Avg. Train Loss across Mini-Batch =  199848.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  235877.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37278.6 e-6; = (1/var)*||X-X_r||^2 val-train = 32444.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4833.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.02; perplexity/K = 50.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.90; perplexity/K = 47.55%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  197863.9 e-6; = (1/var)*||X-X_r||^2 =  180198.0 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  17665.9 e-6 = 8.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  237498.4 e-6; = (1/var)*||X-X_r||^2 =  216982.6 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  20515.8 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  196199.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  234129.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39634.5 e-6; = (1/var)*||X-X_r||^2 val-train = 36784.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2849.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.06; perplexity/K = 51.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.94; perplexity/K = 48.48%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  196455.3 e-6; = (1/var)*||X-X_r||^2 =  179349.6 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  17105.7 e-6 = 8.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  235095.0 e-6; = (1/var)*||X-X_r||^2 =  214727.7 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  20367.3 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  196199.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  231007.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38639.7 e-6; = (1/var)*||X-X_r||^2 val-train = 35378.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3261.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 47.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 48.19%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  203527.0 e-6; = (1/var)*||X-X_r||^2 =  181186.3 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  22340.7 e-6 = 11.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  237810.7 e-6; = (1/var)*||X-X_r||^2 =  213673.8 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  24136.9 e-6 = 10.1 %)
Min.  Avg. Train Loss across Mini-Batch =  192034.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  228768.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34283.7 e-6; = (1/var)*||X-X_r||^2 val-train = 32487.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1796.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.95; perplexity/K = 48.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 49.80%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  198630.7 e-6; = (1/var)*||X-X_r||^2 =  179124.2 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  19506.5 e-6 = 9.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  231613.5 e-6; = (1/var)*||X-X_r||^2 =  211286.7 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  20326.7 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  189023.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  226851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32982.7 e-6; = (1/var)*||X-X_r||^2 val-train = 32162.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 820.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.80; perplexity/K = 45.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.74; perplexity/K = 43.61%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  196811.1 e-6; = (1/var)*||X-X_r||^2 =  178206.9 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  18604.2 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  232397.4 e-6; = (1/var)*||X-X_r||^2 =  210736.3 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  21661.1 e-6 = 9.3 %)
Min.  Avg. Train Loss across Mini-Batch =  189023.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  226851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35586.3 e-6; = (1/var)*||X-X_r||^2 val-train = 32529.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3056.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.92; perplexity/K = 48.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.88; perplexity/K = 47.01%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  199442.3 e-6; = (1/var)*||X-X_r||^2 =  176642.3 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  22800.0 e-6 = 11.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  236289.9 e-6; = (1/var)*||X-X_r||^2 =  209211.2 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  27078.7 e-6 = 11.5 %)
Min.  Avg. Train Loss across Mini-Batch =  189023.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  226851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36847.6 e-6; = (1/var)*||X-X_r||^2 val-train = 32568.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4278.7 e-6 

----------------------------------------------------------------------------------

Finished [13:10:11 08.01.2023] 314) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = True:
Total training time is = 0:3:10 h/m/s. 

--------------------------------------------------- 

Started [13:10:11 08.01.2023] 315) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 730 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.10
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.38
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.95
4                                  encoder.pre_residual_stack.weight                       147            20.14
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.10
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73            10.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.95
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.38
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.10
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.51; perplexity/K = 75.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.50; perplexity/K = 75.05%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3805160.8 e-6; = (1/var)*||X-X_r||^2 =  681558.1 e-6 = 17.9 %; (1+beta)*||Z_e-Z_q||^2 =  3123602.7 e-6 = 82.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3925845.5 e-6; = (1/var)*||X-X_r||^2 =  691522.1 e-6 = 17.6 %; (1+beta)*||Z_e-Z_q||^2 =  3234323.4 e-6 = 82.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1141164.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1055958.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   120684.7 e-6; = (1/var)*||X-X_r||^2 val-train = 9964.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 110720.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1540992.1 e-6; = (1/var)*||X-X_r||^2 =  637481.0 e-6 = 41.4 %; (1+beta)*||Z_e-Z_q||^2 =  903511.1 e-6 = 58.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1448087.0 e-6; = (1/var)*||X-X_r||^2 =  630832.0 e-6 = 43.6 %; (1+beta)*||Z_e-Z_q||^2 =  817255.0 e-6 = 56.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1141164.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1055958.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -92905.1 e-6; = (1/var)*||X-X_r||^2 val-train = -6649.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -86256.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1184816.2 e-6; = (1/var)*||X-X_r||^2 =  615512.0 e-6 = 51.9 %; (1+beta)*||Z_e-Z_q||^2 =  569304.2 e-6 = 48.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1188721.7 e-6; = (1/var)*||X-X_r||^2 =  612792.3 e-6 = 51.6 %; (1+beta)*||Z_e-Z_q||^2 =  575929.4 e-6 = 48.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1141164.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1055958.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3905.5 e-6; = (1/var)*||X-X_r||^2 val-train = -2719.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6625.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  917647.2 e-6; = (1/var)*||X-X_r||^2 =  618022.5 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  299624.8 e-6 = 32.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  971843.2 e-6; = (1/var)*||X-X_r||^2 =  645667.7 e-6 = 66.4 %; (1+beta)*||Z_e-Z_q||^2 =  326175.5 e-6 = 33.6 %)
Min.  Avg. Train Loss across Mini-Batch =  909062.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  883514.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54196.0 e-6; = (1/var)*||X-X_r||^2 val-train = 27645.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26550.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  685423.4 e-6; = (1/var)*||X-X_r||^2 =  594849.4 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  90574.0 e-6 = 13.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  725210.0 e-6; = (1/var)*||X-X_r||^2 =  623217.6 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  101992.3 e-6 = 14.1 %)
Min.  Avg. Train Loss across Mini-Batch =  680680.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  708515.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39786.5 e-6; = (1/var)*||X-X_r||^2 val-train = 28368.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11418.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  628965.7 e-6; = (1/var)*||X-X_r||^2 =  574863.5 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  54102.2 e-6 = 8.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  662290.9 e-6; = (1/var)*||X-X_r||^2 =  606737.0 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  55553.9 e-6 = 8.4 %)
Min.  Avg. Train Loss across Mini-Batch =  623358.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  658731.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33325.2 e-6; = (1/var)*||X-X_r||^2 val-train = 31873.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1451.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  623069.9 e-6; = (1/var)*||X-X_r||^2 =  572723.1 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  50346.8 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  661199.5 e-6; = (1/var)*||X-X_r||^2 =  605464.2 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  55735.3 e-6 = 8.4 %)
Min.  Avg. Train Loss across Mini-Batch =  612516.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  642988.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38129.6 e-6; = (1/var)*||X-X_r||^2 val-train = 32741.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5388.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  609797.9 e-6; = (1/var)*||X-X_r||^2 =  577246.5 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  32551.4 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  643446.2 e-6; = (1/var)*||X-X_r||^2 =  606189.9 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  37256.2 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  604418.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  642338.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33648.3 e-6; = (1/var)*||X-X_r||^2 val-train = 28943.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4704.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  599119.2 e-6; = (1/var)*||X-X_r||^2 =  572724.4 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  26394.7 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  657511.1 e-6; = (1/var)*||X-X_r||^2 =  620664.0 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  36847.1 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  596247.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  639131.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58391.9 e-6; = (1/var)*||X-X_r||^2 val-train = 47939.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10452.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  599692.9 e-6; = (1/var)*||X-X_r||^2 =  574303.9 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  25389.1 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  647062.4 e-6; = (1/var)*||X-X_r||^2 =  617789.2 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  29273.2 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  596247.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  638316.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47369.5 e-6; = (1/var)*||X-X_r||^2 val-train = 43485.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3884.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  601297.8 e-6; = (1/var)*||X-X_r||^2 =  577570.9 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  23726.9 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  640258.8 e-6; = (1/var)*||X-X_r||^2 =  613837.5 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  26421.3 e-6 = 4.1 %)
Min.  Avg. Train Loss across Mini-Batch =  596247.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  634363.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38961.0 e-6; = (1/var)*||X-X_r||^2 val-train = 36266.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2694.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  601105.6 e-6; = (1/var)*||X-X_r||^2 =  582311.2 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  18794.5 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  648659.2 e-6; = (1/var)*||X-X_r||^2 =  622781.4 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  25877.7 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  596247.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  634363.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47553.5 e-6; = (1/var)*||X-X_r||^2 val-train = 40470.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7083.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  602579.9 e-6; = (1/var)*||X-X_r||^2 =  580405.0 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  22174.9 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  646509.2 e-6; = (1/var)*||X-X_r||^2 =  617769.5 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  28739.8 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  595772.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43929.4 e-6; = (1/var)*||X-X_r||^2 val-train = 37364.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6564.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  600108.5 e-6; = (1/var)*||X-X_r||^2 =  579247.9 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  20860.5 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  652485.8 e-6; = (1/var)*||X-X_r||^2 =  618971.9 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  33514.0 e-6 = 5.1 %)
Min.  Avg. Train Loss across Mini-Batch =  593553.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52377.4 e-6; = (1/var)*||X-X_r||^2 val-train = 39723.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12653.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  599220.3 e-6; = (1/var)*||X-X_r||^2 =  579420.3 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  19800.1 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  643804.9 e-6; = (1/var)*||X-X_r||^2 =  617019.6 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  26785.3 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  593129.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   44584.6 e-6; = (1/var)*||X-X_r||^2 val-train = 37599.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6985.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.98%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  598713.3 e-6; = (1/var)*||X-X_r||^2 =  578889.5 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  19823.8 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  642148.3 e-6; = (1/var)*||X-X_r||^2 =  614531.4 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  27617.0 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  593129.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43435.0 e-6; = (1/var)*||X-X_r||^2 val-train = 35641.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7793.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  601174.2 e-6; = (1/var)*||X-X_r||^2 =  578653.8 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  22520.4 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  654709.9 e-6; = (1/var)*||X-X_r||^2 =  620364.7 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  34345.2 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  593129.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53535.7 e-6; = (1/var)*||X-X_r||^2 val-train = 41710.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11824.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  601967.0 e-6; = (1/var)*||X-X_r||^2 =  580079.8 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  21887.2 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  646498.7 e-6; = (1/var)*||X-X_r||^2 =  615891.3 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  30607.4 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  591082.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   44531.7 e-6; = (1/var)*||X-X_r||^2 val-train = 35811.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8720.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 100.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  669662.1 e-6; = (1/var)*||X-X_r||^2 =  600818.6 e-6 = 89.7 %; (1+beta)*||Z_e-Z_q||^2 =  68843.4 e-6 = 10.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  681392.1 e-6; = (1/var)*||X-X_r||^2 =  607029.6 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  74362.5 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  591082.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11730.0 e-6; = (1/var)*||X-X_r||^2 val-train = 6211.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5519.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.99%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  604991.0 e-6; = (1/var)*||X-X_r||^2 =  574590.5 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  30400.6 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  638964.6 e-6; = (1/var)*||X-X_r||^2 =  598397.1 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  40567.4 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  591082.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  632274.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33973.5 e-6; = (1/var)*||X-X_r||^2 val-train = 23806.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10166.9 e-6 

----------------------------------------------------------------------------------

Finished [13:58:35 08.01.2023] 315) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:3:23 h/m/s. 

--------------------------------------------------- 

Started [13:58:35 08.01.2023] 316) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 80) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2454 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.30
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.35
4                                  encoder.pre_residual_stack.weight                       589            24.00
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         2             0.08
11                             decoder.channel_adjusting_conv.weight                       147             5.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.35
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.14; perplexity/K = 53.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.93; perplexity/K = 56.04%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  237074.6 e-6; = (1/var)*||X-X_r||^2 =  161179.4 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  75895.2 e-6 = 32.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  245964.5 e-6; = (1/var)*||X-X_r||^2 =  167994.7 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  77969.8 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  237074.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  244384.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8889.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6815.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2074.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.64; perplexity/K = 64.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.07; perplexity/K = 65.83%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93748.5 e-6; = (1/var)*||X-X_r||^2 =  65083.7 e-6 = 69.4 %; (1+beta)*||Z_e-Z_q||^2 =  28664.8 e-6 = 30.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  101541.6 e-6; = (1/var)*||X-X_r||^2 =  72474.6 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  29067.0 e-6 = 28.6 %)
Min.  Avg. Train Loss across Mini-Batch =  93748.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  101541.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7793.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7390.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 402.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.46; perplexity/K = 63.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.47; perplexity/K = 67.08%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49084.6 e-6; = (1/var)*||X-X_r||^2 =  33462.9 e-6 = 68.2 %; (1+beta)*||Z_e-Z_q||^2 =  15621.7 e-6 = 31.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  53627.5 e-6; = (1/var)*||X-X_r||^2 =  37536.3 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  16091.2 e-6 = 30.0 %)
Min.  Avg. Train Loss across Mini-Batch =  49084.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  53627.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4542.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4073.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 469.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.45; perplexity/K = 67.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.53; perplexity/K = 64.15%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23023.0 e-6; = (1/var)*||X-X_r||^2 =  13931.7 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  9091.3 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  26295.5 e-6; = (1/var)*||X-X_r||^2 =  17003.5 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  9292.1 e-6 = 35.3 %)
Min.  Avg. Train Loss across Mini-Batch =  23023.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26295.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3272.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3071.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 200.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.77; perplexity/K = 61.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.34; perplexity/K = 63.57%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11088.3 e-6; = (1/var)*||X-X_r||^2 =  4578.4 e-6 = 41.3 %; (1+beta)*||Z_e-Z_q||^2 =  6509.9 e-6 = 58.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  13327.8 e-6; = (1/var)*||X-X_r||^2 =  6602.3 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  6725.5 e-6 = 50.5 %)
Min.  Avg. Train Loss across Mini-Batch =  11088.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13327.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2239.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2023.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 215.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.19; perplexity/K = 66.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.02; perplexity/K = 62.57%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17874.7 e-6; = (1/var)*||X-X_r||^2 =  5518.4 e-6 = 30.9 %; (1+beta)*||Z_e-Z_q||^2 =  12356.4 e-6 = 69.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  18787.6 e-6; = (1/var)*||X-X_r||^2 =  7040.4 e-6 = 37.5 %; (1+beta)*||Z_e-Z_q||^2 =  11747.1 e-6 = 62.5 %)
Min.  Avg. Train Loss across Mini-Batch =  5598.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7367.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   912.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1522.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -609.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.40; perplexity/K = 60.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.08; perplexity/K = 56.50%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5063.1 e-6; = (1/var)*||X-X_r||^2 =  1309.3 e-6 = 25.9 %; (1+beta)*||Z_e-Z_q||^2 =  3753.8 e-6 = 74.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  6971.0 e-6; = (1/var)*||X-X_r||^2 =  3100.3 e-6 = 44.5 %; (1+beta)*||Z_e-Z_q||^2 =  3870.7 e-6 = 55.5 %)
Min.  Avg. Train Loss across Mini-Batch =  5063.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6971.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1907.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1790.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 116.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.12; perplexity/K = 59.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.16; perplexity/K = 59.89%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8504.0 e-6; = (1/var)*||X-X_r||^2 =  1916.4 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  6587.6 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  9241.2 e-6; = (1/var)*||X-X_r||^2 =  3116.3 e-6 = 33.7 %; (1+beta)*||Z_e-Z_q||^2 =  6124.9 e-6 = 66.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3459.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4887.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   737.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1199.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -462.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.94; perplexity/K = 62.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.70; perplexity/K = 58.45%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8958.2 e-6; = (1/var)*||X-X_r||^2 =  1823.7 e-6 = 20.4 %; (1+beta)*||Z_e-Z_q||^2 =  7134.5 e-6 = 79.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  9416.0 e-6; = (1/var)*||X-X_r||^2 =  2991.0 e-6 = 31.8 %; (1+beta)*||Z_e-Z_q||^2 =  6425.0 e-6 = 68.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3145.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4508.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   457.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1167.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -709.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.93; perplexity/K = 65.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.04; perplexity/K = 59.50%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7831.0 e-6; = (1/var)*||X-X_r||^2 =  1502.3 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  6328.7 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  8136.2 e-6; = (1/var)*||X-X_r||^2 =  2326.7 e-6 = 28.6 %; (1+beta)*||Z_e-Z_q||^2 =  5809.4 e-6 = 71.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2843.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3931.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   305.1 e-6; = (1/var)*||X-X_r||^2 val-train = 824.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -519.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.69; perplexity/K = 58.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.15; perplexity/K = 56.72%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4326.9 e-6; = (1/var)*||X-X_r||^2 =  699.6 e-6 = 16.2 %; (1+beta)*||Z_e-Z_q||^2 =  3627.3 e-6 = 83.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  5235.5 e-6; = (1/var)*||X-X_r||^2 =  1701.0 e-6 = 32.5 %; (1+beta)*||Z_e-Z_q||^2 =  3534.5 e-6 = 67.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2843.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3846.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   908.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1001.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -92.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.79; perplexity/K = 58.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.52; perplexity/K = 57.87%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3282.6 e-6; = (1/var)*||X-X_r||^2 =  526.2 e-6 = 16.0 %; (1+beta)*||Z_e-Z_q||^2 =  2756.5 e-6 = 84.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4314.8 e-6; = (1/var)*||X-X_r||^2 =  1492.0 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  2822.8 e-6 = 65.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2049.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2942.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1032.2 e-6; = (1/var)*||X-X_r||^2 val-train = 965.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 66.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.27; perplexity/K = 57.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.22; perplexity/K = 60.06%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7075.7 e-6; = (1/var)*||X-X_r||^2 =  968.0 e-6 = 13.7 %; (1+beta)*||Z_e-Z_q||^2 =  6107.7 e-6 = 86.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  8110.1 e-6; = (1/var)*||X-X_r||^2 =  2684.3 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  5425.8 e-6 = 66.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2006.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2942.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1034.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1716.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -681.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.16; perplexity/K = 59.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.94; perplexity/K = 59.19%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1640.7 e-6; = (1/var)*||X-X_r||^2 =  270.7 e-6 = 16.5 %; (1+beta)*||Z_e-Z_q||^2 =  1370.0 e-6 = 83.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2849.1 e-6; = (1/var)*||X-X_r||^2 =  1351.8 e-6 = 47.4 %; (1+beta)*||Z_e-Z_q||^2 =  1497.2 e-6 = 52.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1575.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2796.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1208.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1081.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 127.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.42; perplexity/K = 57.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.22; perplexity/K = 60.07%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3719.6 e-6; = (1/var)*||X-X_r||^2 =  467.7 e-6 = 12.6 %; (1+beta)*||Z_e-Z_q||^2 =  3251.9 e-6 = 87.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  5222.0 e-6; = (1/var)*||X-X_r||^2 =  1959.9 e-6 = 37.5 %; (1+beta)*||Z_e-Z_q||^2 =  3262.1 e-6 = 62.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1259.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2341.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1502.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1492.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.82; perplexity/K = 61.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.68; perplexity/K = 58.36%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2628.8 e-6; = (1/var)*||X-X_r||^2 =  347.7 e-6 = 13.2 %; (1+beta)*||Z_e-Z_q||^2 =  2281.1 e-6 = 86.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4004.7 e-6; = (1/var)*||X-X_r||^2 =  1684.5 e-6 = 42.1 %; (1+beta)*||Z_e-Z_q||^2 =  2320.2 e-6 = 57.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1168.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2043.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1375.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1336.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.12; perplexity/K = 56.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.57; perplexity/K = 54.91%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  980.1 e-6; = (1/var)*||X-X_r||^2 =  185.0 e-6 = 18.9 %; (1+beta)*||Z_e-Z_q||^2 =  795.0 e-6 = 81.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2117.8 e-6; = (1/var)*||X-X_r||^2 =  1216.1 e-6 = 57.4 %; (1+beta)*||Z_e-Z_q||^2 =  901.7 e-6 = 42.6 %)
Min.  Avg. Train Loss across Mini-Batch =  980.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1975.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1137.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1031.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 106.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.87; perplexity/K = 55.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.53; perplexity/K = 54.79%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1046.5 e-6; = (1/var)*||X-X_r||^2 =  173.2 e-6 = 16.6 %; (1+beta)*||Z_e-Z_q||^2 =  873.3 e-6 = 83.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2044.5 e-6; = (1/var)*||X-X_r||^2 =  1091.4 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  953.1 e-6 = 46.6 %)
Min.  Avg. Train Loss across Mini-Batch =  782.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1765.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   998.0 e-6; = (1/var)*||X-X_r||^2 val-train = 918.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 79.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.29; perplexity/K = 57.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.90; perplexity/K = 55.94%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  865.9 e-6; = (1/var)*||X-X_r||^2 =  165.3 e-6 = 19.1 %; (1+beta)*||Z_e-Z_q||^2 =  700.5 e-6 = 80.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1698.9 e-6; = (1/var)*||X-X_r||^2 =  910.6 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  788.3 e-6 = 46.4 %)
Min.  Avg. Train Loss across Mini-Batch =  761.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1610.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   833.1 e-6; = (1/var)*||X-X_r||^2 val-train = 745.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 87.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.32; perplexity/K = 57.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.72; perplexity/K = 55.37%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1002.9 e-6; = (1/var)*||X-X_r||^2 =  162.0 e-6 = 16.2 %; (1+beta)*||Z_e-Z_q||^2 =  840.9 e-6 = 83.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1758.3 e-6; = (1/var)*||X-X_r||^2 =  857.6 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  900.7 e-6 = 51.2 %)
Min.  Avg. Train Loss across Mini-Batch =  583.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   755.5 e-6; = (1/var)*||X-X_r||^2 val-train = 695.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 59.8 e-6 

----------------------------------------------------------------------------------

Finished [14:48:09 08.01.2023] 316) Finished running for K = 32 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 80) change_channel_size_across_layers = True:
Total training time is = 0:3:33 h/m/s. 

--------------------------------------------------- 

Started [14:48:09 08.01.2023] 317) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 64) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2453 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.30
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.36
4                                  encoder.pre_residual_stack.weight                       589            24.01
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         1             0.04
11                             decoder.channel_adjusting_conv.weight                       147             5.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.36
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.96; perplexity/K = 55.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.50; perplexity/K = 53.11%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  240804.0 e-6; = (1/var)*||X-X_r||^2 =  155147.6 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  85656.4 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  242426.7 e-6; = (1/var)*||X-X_r||^2 =  158825.8 e-6 = 65.5 %; (1+beta)*||Z_e-Z_q||^2 =  83601.0 e-6 = 34.5 %)
Min.  Avg. Train Loss across Mini-Batch =  240804.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  242426.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1622.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3678.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2055.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.13; perplexity/K = 69.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.01; perplexity/K = 68.84%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  91575.5 e-6; = (1/var)*||X-X_r||^2 =  56951.1 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  34624.4 e-6 = 37.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  98618.7 e-6; = (1/var)*||X-X_r||^2 =  64257.2 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  34361.5 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  91575.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  98387.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7043.2 e-6; = (1/var)*||X-X_r||^2 val-train = 7306.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -262.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.45; perplexity/K = 77.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.38; perplexity/K = 77.36%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40960.6 e-6; = (1/var)*||X-X_r||^2 =  27158.7 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  13801.9 e-6 = 33.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  47311.3 e-6; = (1/var)*||X-X_r||^2 =  31686.2 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  15625.0 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  40422.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45653.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6350.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4527.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1823.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.48; perplexity/K = 77.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.50; perplexity/K = 78.13%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19627.4 e-6; = (1/var)*||X-X_r||^2 =  6831.4 e-6 = 34.8 %; (1+beta)*||Z_e-Z_q||^2 =  12796.0 e-6 = 65.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  20987.2 e-6; = (1/var)*||X-X_r||^2 =  8876.3 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  12110.9 e-6 = 57.7 %)
Min.  Avg. Train Loss across Mini-Batch =  16512.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20987.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1359.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2044.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -685.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.00; perplexity/K = 75.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.20; perplexity/K = 76.22%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11034.8 e-6; = (1/var)*||X-X_r||^2 =  2986.1 e-6 = 27.1 %; (1+beta)*||Z_e-Z_q||^2 =  8048.7 e-6 = 72.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  12765.1 e-6; = (1/var)*||X-X_r||^2 =  4628.4 e-6 = 36.3 %; (1+beta)*||Z_e-Z_q||^2 =  8136.7 e-6 = 63.7 %)
Min.  Avg. Train Loss across Mini-Batch =  8061.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10513.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1730.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1642.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 88.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.96; perplexity/K = 81.00%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17306.2 e-6; = (1/var)*||X-X_r||^2 =  3918.7 e-6 = 22.6 %; (1+beta)*||Z_e-Z_q||^2 =  13387.5 e-6 = 77.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  19757.0 e-6; = (1/var)*||X-X_r||^2 =  5776.9 e-6 = 29.2 %; (1+beta)*||Z_e-Z_q||^2 =  13980.1 e-6 = 70.8 %)
Min.  Avg. Train Loss across Mini-Batch =  6419.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8708.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2450.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1858.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 592.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.12; perplexity/K = 81.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.58; perplexity/K = 78.66%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11868.6 e-6; = (1/var)*||X-X_r||^2 =  2479.9 e-6 = 20.9 %; (1+beta)*||Z_e-Z_q||^2 =  9388.7 e-6 = 79.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  11809.8 e-6; = (1/var)*||X-X_r||^2 =  3307.0 e-6 = 28.0 %; (1+beta)*||Z_e-Z_q||^2 =  8502.8 e-6 = 72.0 %)
Min.  Avg. Train Loss across Mini-Batch =  6419.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8341.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -58.8 e-6; = (1/var)*||X-X_r||^2 val-train = 827.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -885.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.59; perplexity/K = 84.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.76; perplexity/K = 79.76%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3453.0 e-6; = (1/var)*||X-X_r||^2 =  739.7 e-6 = 21.4 %; (1+beta)*||Z_e-Z_q||^2 =  2713.3 e-6 = 78.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5149.7 e-6; = (1/var)*||X-X_r||^2 =  2199.1 e-6 = 42.7 %; (1+beta)*||Z_e-Z_q||^2 =  2950.6 e-6 = 57.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2912.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4536.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1696.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1459.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 237.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.35; perplexity/K = 83.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.21; perplexity/K = 82.57%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13172.7 e-6; = (1/var)*||X-X_r||^2 =  2734.8 e-6 = 20.8 %; (1+beta)*||Z_e-Z_q||^2 =  10437.9 e-6 = 79.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  16263.2 e-6; = (1/var)*||X-X_r||^2 =  4117.5 e-6 = 25.3 %; (1+beta)*||Z_e-Z_q||^2 =  12145.7 e-6 = 74.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2394.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3540.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3090.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1382.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1707.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.46; perplexity/K = 77.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.28; perplexity/K = 76.75%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2592.7 e-6; = (1/var)*||X-X_r||^2 =  494.2 e-6 = 19.1 %; (1+beta)*||Z_e-Z_q||^2 =  2098.5 e-6 = 80.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3583.1 e-6; = (1/var)*||X-X_r||^2 =  1344.9 e-6 = 37.5 %; (1+beta)*||Z_e-Z_q||^2 =  2238.2 e-6 = 62.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2394.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3540.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   990.4 e-6; = (1/var)*||X-X_r||^2 val-train = 850.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 139.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.02; perplexity/K = 81.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 78.16%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7091.3 e-6; = (1/var)*||X-X_r||^2 =  1125.6 e-6 = 15.9 %; (1+beta)*||Z_e-Z_q||^2 =  5965.7 e-6 = 84.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  7847.0 e-6; = (1/var)*||X-X_r||^2 =  2483.2 e-6 = 31.6 %; (1+beta)*||Z_e-Z_q||^2 =  5363.8 e-6 = 68.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1874.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2942.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   755.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1357.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -601.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.43; perplexity/K = 77.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.12%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2179.2 e-6; = (1/var)*||X-X_r||^2 =  318.9 e-6 = 14.6 %; (1+beta)*||Z_e-Z_q||^2 =  1860.3 e-6 = 85.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3286.2 e-6; = (1/var)*||X-X_r||^2 =  1261.1 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  2025.1 e-6 = 61.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1539.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2709.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1107.0 e-6; = (1/var)*||X-X_r||^2 val-train = 942.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 164.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.58; perplexity/K = 78.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.82%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2317.6 e-6; = (1/var)*||X-X_r||^2 =  294.5 e-6 = 12.7 %; (1+beta)*||Z_e-Z_q||^2 =  2023.2 e-6 = 87.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3016.6 e-6; = (1/var)*||X-X_r||^2 =  851.2 e-6 = 28.2 %; (1+beta)*||Z_e-Z_q||^2 =  2165.3 e-6 = 71.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1539.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2709.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   698.9 e-6; = (1/var)*||X-X_r||^2 val-train = 556.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 142.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.63; perplexity/K = 85.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 81.72%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8794.9 e-6; = (1/var)*||X-X_r||^2 =  4210.3 e-6 = 47.9 %; (1+beta)*||Z_e-Z_q||^2 =  4584.6 e-6 = 52.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  13918.6 e-6; = (1/var)*||X-X_r||^2 =  6831.7 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  7086.9 e-6 = 50.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1435.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2415.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5123.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2621.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2502.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.25; perplexity/K = 76.55%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1098.3 e-6; = (1/var)*||X-X_r||^2 =  178.7 e-6 = 16.3 %; (1+beta)*||Z_e-Z_q||^2 =  919.6 e-6 = 83.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2082.2 e-6; = (1/var)*||X-X_r||^2 =  963.5 e-6 = 46.3 %; (1+beta)*||Z_e-Z_q||^2 =  1118.7 e-6 = 53.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1098.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2037.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   983.9 e-6; = (1/var)*||X-X_r||^2 val-train = 784.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 199.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.20; perplexity/K = 76.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.89; perplexity/K = 74.29%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1923.9 e-6; = (1/var)*||X-X_r||^2 =  215.9 e-6 = 11.2 %; (1+beta)*||Z_e-Z_q||^2 =  1708.0 e-6 = 88.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2824.0 e-6; = (1/var)*||X-X_r||^2 =  1015.5 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  1808.5 e-6 = 64.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1027.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2037.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   900.1 e-6; = (1/var)*||X-X_r||^2 val-train = 799.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 100.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.81; perplexity/K = 73.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.11%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1550.9 e-6; = (1/var)*||X-X_r||^2 =  178.1 e-6 = 11.5 %; (1+beta)*||Z_e-Z_q||^2 =  1372.8 e-6 = 88.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2555.6 e-6; = (1/var)*||X-X_r||^2 =  1050.3 e-6 = 41.1 %; (1+beta)*||Z_e-Z_q||^2 =  1505.3 e-6 = 58.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1027.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2022.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1004.7 e-6; = (1/var)*||X-X_r||^2 val-train = 872.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 132.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 76.88%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2515.9 e-6; = (1/var)*||X-X_r||^2 =  332.7 e-6 = 13.2 %; (1+beta)*||Z_e-Z_q||^2 =  2183.2 e-6 = 86.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3339.5 e-6; = (1/var)*||X-X_r||^2 =  1200.8 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  2138.7 e-6 = 64.0 %)
Min.  Avg. Train Loss across Mini-Batch =  847.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1982.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   823.6 e-6; = (1/var)*||X-X_r||^2 val-train = 868.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -44.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.59; perplexity/K = 84.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.29; perplexity/K = 83.05%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8560.1 e-6; = (1/var)*||X-X_r||^2 =  1058.5 e-6 = 12.4 %; (1+beta)*||Z_e-Z_q||^2 =  7501.6 e-6 = 87.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  9060.7 e-6; = (1/var)*||X-X_r||^2 =  2223.0 e-6 = 24.5 %; (1+beta)*||Z_e-Z_q||^2 =  6837.7 e-6 = 75.5 %)
Min.  Avg. Train Loss across Mini-Batch =  599.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1436.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   500.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1164.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -663.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.09; perplexity/K = 75.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 78.21%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  879.3 e-6; = (1/var)*||X-X_r||^2 =  120.8 e-6 = 13.7 %; (1+beta)*||Z_e-Z_q||^2 =  758.6 e-6 = 86.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2110.4 e-6; = (1/var)*||X-X_r||^2 =  1221.8 e-6 = 57.9 %; (1+beta)*||Z_e-Z_q||^2 =  888.6 e-6 = 42.1 %)
Min.  Avg. Train Loss across Mini-Batch =  599.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1436.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1231.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1101.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 130.0 e-6 

----------------------------------------------------------------------------------

Finished [15:37:49 08.01.2023] 317) Finished running for K = 16 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 64) change_channel_size_across_layers = True:
Total training time is = 0:3:40 h/m/s. 

--------------------------------------------------- 

Started [15:37:49 08.01.2023] 318) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 48) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2452 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.31
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.37
4                                  encoder.pre_residual_stack.weight                       589            24.02
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             6.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.37
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.31
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.42; perplexity/K = 55.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.63; perplexity/K = 57.89%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  683819.7 e-6; = (1/var)*||X-X_r||^2 =  260616.2 e-6 = 38.1 %; (1+beta)*||Z_e-Z_q||^2 =  423203.5 e-6 = 61.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  588826.9 e-6; = (1/var)*||X-X_r||^2 =  254595.6 e-6 = 43.2 %; (1+beta)*||Z_e-Z_q||^2 =  334231.4 e-6 = 56.8 %)
Min.  Avg. Train Loss across Mini-Batch =  466159.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426966.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -94992.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6020.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -88972.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.63; perplexity/K = 70.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.83; perplexity/K = 72.90%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  172323.9 e-6; = (1/var)*||X-X_r||^2 =  88151.2 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  84172.7 e-6 = 48.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  182101.1 e-6; = (1/var)*||X-X_r||^2 =  101199.0 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  80902.1 e-6 = 44.4 %)
Min.  Avg. Train Loss across Mini-Batch =  167969.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  182101.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9777.1 e-6; = (1/var)*||X-X_r||^2 val-train = 13047.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3270.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.91; perplexity/K = 73.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.71; perplexity/K = 71.39%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99990.0 e-6; = (1/var)*||X-X_r||^2 =  42960.7 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  57029.3 e-6 = 57.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  112500.9 e-6; = (1/var)*||X-X_r||^2 =  53193.0 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  59307.9 e-6 = 52.7 %)
Min.  Avg. Train Loss across Mini-Batch =  88909.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  106789.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12510.9 e-6; = (1/var)*||X-X_r||^2 val-train = 10232.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2278.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.78; perplexity/K = 72.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.12; perplexity/K = 76.55%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  83027.5 e-6; = (1/var)*||X-X_r||^2 =  30544.8 e-6 = 36.8 %; (1+beta)*||Z_e-Z_q||^2 =  52482.7 e-6 = 63.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  88830.7 e-6; = (1/var)*||X-X_r||^2 =  36144.4 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  52686.3 e-6 = 59.3 %)
Min.  Avg. Train Loss across Mini-Batch =  45137.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62422.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5803.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5599.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 203.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.88; perplexity/K = 73.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.73; perplexity/K = 71.62%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24061.6 e-6; = (1/var)*||X-X_r||^2 =  5373.9 e-6 = 22.3 %; (1+beta)*||Z_e-Z_q||^2 =  18687.7 e-6 = 77.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  37726.5 e-6; = (1/var)*||X-X_r||^2 =  14954.4 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  22772.1 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  21645.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37472.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13664.9 e-6; = (1/var)*||X-X_r||^2 val-train = 9580.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4084.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.10; perplexity/K = 76.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.84; perplexity/K = 73.05%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11599.3 e-6; = (1/var)*||X-X_r||^2 =  2974.9 e-6 = 25.6 %; (1+beta)*||Z_e-Z_q||^2 =  8624.4 e-6 = 74.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  23752.9 e-6; = (1/var)*||X-X_r||^2 =  11665.2 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  12087.6 e-6 = 50.9 %)
Min.  Avg. Train Loss across Mini-Batch =  10945.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  23625.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12153.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8690.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3463.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.10; perplexity/K = 76.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.78; perplexity/K = 72.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11727.8 e-6; = (1/var)*||X-X_r||^2 =  1963.3 e-6 = 16.7 %; (1+beta)*||Z_e-Z_q||^2 =  9764.5 e-6 = 83.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  22870.5 e-6; = (1/var)*||X-X_r||^2 =  9933.2 e-6 = 43.4 %; (1+beta)*||Z_e-Z_q||^2 =  12937.3 e-6 = 56.6 %)
Min.  Avg. Train Loss across Mini-Batch =  9585.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  21414.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11142.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7969.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3172.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.81; perplexity/K = 72.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.95; perplexity/K = 74.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  107234.3 e-6; = (1/var)*||X-X_r||^2 =  28750.7 e-6 = 26.8 %; (1+beta)*||Z_e-Z_q||^2 =  78483.6 e-6 = 73.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  86888.9 e-6; = (1/var)*||X-X_r||^2 =  23307.0 e-6 = 26.8 %; (1+beta)*||Z_e-Z_q||^2 =  63581.9 e-6 = 73.2 %)
Min.  Avg. Train Loss across Mini-Batch =  5950.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16501.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20345.4 e-6; = (1/var)*||X-X_r||^2 val-train = -5443.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14901.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.68; perplexity/K = 71.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.87; perplexity/K = 73.41%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9021.2 e-6; = (1/var)*||X-X_r||^2 =  1871.1 e-6 = 20.7 %; (1+beta)*||Z_e-Z_q||^2 =  7150.1 e-6 = 79.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  18435.7 e-6; = (1/var)*||X-X_r||^2 =  10438.1 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  7997.6 e-6 = 43.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4717.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13779.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9414.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8567.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 847.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.00; perplexity/K = 75.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.78; perplexity/K = 72.22%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3404.0 e-6; = (1/var)*||X-X_r||^2 =  727.5 e-6 = 21.4 %; (1+beta)*||Z_e-Z_q||^2 =  2676.6 e-6 = 78.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  13096.8 e-6; = (1/var)*||X-X_r||^2 =  8570.8 e-6 = 65.4 %; (1+beta)*||Z_e-Z_q||^2 =  4526.0 e-6 = 34.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3099.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12380.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9692.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7843.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1849.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.87; perplexity/K = 73.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.82; perplexity/K = 72.77%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7956.4 e-6; = (1/var)*||X-X_r||^2 =  1235.1 e-6 = 15.5 %; (1+beta)*||Z_e-Z_q||^2 =  6721.4 e-6 = 84.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  20868.7 e-6; = (1/var)*||X-X_r||^2 =  10543.2 e-6 = 50.5 %; (1+beta)*||Z_e-Z_q||^2 =  10325.4 e-6 = 49.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2748.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9676.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12912.3 e-6; = (1/var)*||X-X_r||^2 val-train = 9308.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3604.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.77; perplexity/K = 72.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.05; perplexity/K = 75.68%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7419.4 e-6; = (1/var)*||X-X_r||^2 =  1330.4 e-6 = 17.9 %; (1+beta)*||Z_e-Z_q||^2 =  6089.0 e-6 = 82.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  18906.1 e-6; = (1/var)*||X-X_r||^2 =  9494.8 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  9411.2 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2748.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9676.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11486.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8164.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3322.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.96; perplexity/K = 74.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.95; perplexity/K = 74.33%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11201.0 e-6; = (1/var)*||X-X_r||^2 =  1355.1 e-6 = 12.1 %; (1+beta)*||Z_e-Z_q||^2 =  9845.9 e-6 = 87.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  16679.0 e-6; = (1/var)*||X-X_r||^2 =  7468.8 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  9210.2 e-6 = 55.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2748.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9676.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5477.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6113.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -635.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.04; perplexity/K = 75.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.77; perplexity/K = 72.11%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13988.1 e-6; = (1/var)*||X-X_r||^2 =  2520.3 e-6 = 18.0 %; (1+beta)*||Z_e-Z_q||^2 =  11467.8 e-6 = 82.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  20703.5 e-6; = (1/var)*||X-X_r||^2 =  10254.4 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  10449.1 e-6 = 50.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2748.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7862.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6715.4 e-6; = (1/var)*||X-X_r||^2 val-train = 7734.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1018.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.13; perplexity/K = 76.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.94; perplexity/K = 74.29%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36940.2 e-6; = (1/var)*||X-X_r||^2 =  8793.5 e-6 = 23.8 %; (1+beta)*||Z_e-Z_q||^2 =  28146.7 e-6 = 76.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  36952.7 e-6; = (1/var)*||X-X_r||^2 =  13674.6 e-6 = 37.0 %; (1+beta)*||Z_e-Z_q||^2 =  23278.1 e-6 = 63.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1503.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7862.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4881.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4868.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 75.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.00; perplexity/K = 75.04%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7425.4 e-6; = (1/var)*||X-X_r||^2 =  870.6 e-6 = 11.7 %; (1+beta)*||Z_e-Z_q||^2 =  6554.9 e-6 = 88.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  13587.2 e-6; = (1/var)*||X-X_r||^2 =  6598.3 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  6988.9 e-6 = 51.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1300.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7862.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6161.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5727.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 434.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.21; perplexity/K = 77.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.25; perplexity/K = 78.09%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10241.9 e-6; = (1/var)*||X-X_r||^2 =  1652.0 e-6 = 16.1 %; (1+beta)*||Z_e-Z_q||^2 =  8589.9 e-6 = 83.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  16523.3 e-6; = (1/var)*||X-X_r||^2 =  7479.8 e-6 = 45.3 %; (1+beta)*||Z_e-Z_q||^2 =  9043.6 e-6 = 54.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1244.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7696.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6281.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5827.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 453.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.79; perplexity/K = 72.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.20; perplexity/K = 77.55%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3344.1 e-6; = (1/var)*||X-X_r||^2 =  421.9 e-6 = 12.6 %; (1+beta)*||Z_e-Z_q||^2 =  2922.2 e-6 = 87.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  11069.4 e-6; = (1/var)*||X-X_r||^2 =  7170.5 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  3898.9 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1223.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7696.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7725.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6748.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 976.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.82; perplexity/K = 72.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.12; perplexity/K = 76.49%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32954.0 e-6; = (1/var)*||X-X_r||^2 =  6224.8 e-6 = 18.9 %; (1+beta)*||Z_e-Z_q||^2 =  26729.2 e-6 = 81.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  29639.5 e-6; = (1/var)*||X-X_r||^2 =  9344.9 e-6 = 31.5 %; (1+beta)*||Z_e-Z_q||^2 =  20294.6 e-6 = 68.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1223.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7696.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3314.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3120.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6434.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.96; perplexity/K = 74.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.86; perplexity/K = 73.29%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5210.1 e-6; = (1/var)*||X-X_r||^2 =  695.0 e-6 = 13.3 %; (1+beta)*||Z_e-Z_q||^2 =  4515.1 e-6 = 86.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  16208.5 e-6; = (1/var)*||X-X_r||^2 =  7615.3 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  8593.3 e-6 = 53.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1223.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7696.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10998.4 e-6; = (1/var)*||X-X_r||^2 val-train = 6920.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4078.2 e-6 

----------------------------------------------------------------------------------

Finished [16:27:22 08.01.2023] 318) Finished running for K = 8 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 48) change_channel_size_across_layers = True:
Total training time is = 0:3:33 h/m/s. 

--------------------------------------------------- 

Started [16:27:22 08.01.2023] 319) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(4, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2452 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.31
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.37
4                                  encoder.pre_residual_stack.weight                       589            24.02
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             6.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.37
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.31
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 48.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.97; perplexity/K = 49.20%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4943519.2 e-6; = (1/var)*||X-X_r||^2 =  302698.2 e-6 = 6.1 %; (1+beta)*||Z_e-Z_q||^2 =  4640821.0 e-6 = 93.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4175106.1 e-6; = (1/var)*||X-X_r||^2 =  301789.1 e-6 = 7.2 %; (1+beta)*||Z_e-Z_q||^2 =  3873316.9 e-6 = 92.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1026684.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  942050.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -768413.2 e-6; = (1/var)*||X-X_r||^2 val-train = -909.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -767504.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.60; perplexity/K = 64.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.51; perplexity/K = 62.79%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  471908.3 e-6; = (1/var)*||X-X_r||^2 =  173322.1 e-6 = 36.7 %; (1+beta)*||Z_e-Z_q||^2 =  298586.1 e-6 = 63.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  501573.1 e-6; = (1/var)*||X-X_r||^2 =  189354.7 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  312218.4 e-6 = 62.2 %)
Min.  Avg. Train Loss across Mini-Batch =  456220.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  466359.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29664.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16032.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13632.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.59; perplexity/K = 64.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.52; perplexity/K = 62.90%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  265943.6 e-6; = (1/var)*||X-X_r||^2 =  131194.6 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  134749.0 e-6 = 50.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  312367.1 e-6; = (1/var)*||X-X_r||^2 =  161001.3 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  151365.8 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  252787.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  294505.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46423.5 e-6; = (1/var)*||X-X_r||^2 val-train = 29806.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16616.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.78%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  170945.1 e-6; = (1/var)*||X-X_r||^2 =  108130.5 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  62814.7 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  221708.1 e-6; = (1/var)*||X-X_r||^2 =  152299.8 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  69408.2 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  169677.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  219206.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50762.9 e-6; = (1/var)*||X-X_r||^2 val-train = 44169.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6593.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.58; perplexity/K = 64.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.56; perplexity/K = 64.12%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  123718.6 e-6; = (1/var)*||X-X_r||^2 =  98383.9 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  25334.8 e-6 = 20.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  176569.4 e-6; = (1/var)*||X-X_r||^2 =  144967.0 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  31602.4 e-6 = 17.9 %)
Min.  Avg. Train Loss across Mini-Batch =  122949.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  173882.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52850.8 e-6; = (1/var)*||X-X_r||^2 val-train = 46583.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6267.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.57; perplexity/K = 64.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.45; perplexity/K = 61.37%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  121015.4 e-6; = (1/var)*||X-X_r||^2 =  95562.1 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  25453.3 e-6 = 21.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  171126.7 e-6; = (1/var)*||X-X_r||^2 =  143048.9 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  28077.8 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  111224.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  163562.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50111.3 e-6; = (1/var)*||X-X_r||^2 val-train = 47486.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2624.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.60; perplexity/K = 64.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.58; perplexity/K = 64.57%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  105751.0 e-6; = (1/var)*||X-X_r||^2 =  92745.6 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  13005.4 e-6 = 12.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  159423.6 e-6; = (1/var)*||X-X_r||^2 =  142334.3 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  17089.2 e-6 = 10.7 %)
Min.  Avg. Train Loss across Mini-Batch =  104583.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157837.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53672.5 e-6; = (1/var)*||X-X_r||^2 val-train = 49588.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4083.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.52; perplexity/K = 63.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.58; perplexity/K = 64.54%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102017.5 e-6; = (1/var)*||X-X_r||^2 =  90903.5 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  11114.0 e-6 = 10.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  156593.7 e-6; = (1/var)*||X-X_r||^2 =  142043.5 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  14550.2 e-6 = 9.3 %)
Min.  Avg. Train Loss across Mini-Batch =  101784.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  153945.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54576.2 e-6; = (1/var)*||X-X_r||^2 val-train = 51140.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3436.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.57; perplexity/K = 64.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.60; perplexity/K = 64.91%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  108934.4 e-6; = (1/var)*||X-X_r||^2 =  91312.2 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  17622.2 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  162634.7 e-6; = (1/var)*||X-X_r||^2 =  142839.8 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  19794.9 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  98809.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  150815.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53700.3 e-6; = (1/var)*||X-X_r||^2 val-train = 51527.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2172.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.58; perplexity/K = 64.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.53; perplexity/K = 63.29%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  96984.1 e-6; = (1/var)*||X-X_r||^2 =  88950.1 e-6 = 91.7 %; (1+beta)*||Z_e-Z_q||^2 =  8033.9 e-6 = 8.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  151211.9 e-6; = (1/var)*||X-X_r||^2 =  140440.0 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  10771.9 e-6 = 7.1 %)
Min.  Avg. Train Loss across Mini-Batch =  95832.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147935.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54227.9 e-6; = (1/var)*||X-X_r||^2 val-train = 51489.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2738.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.58; perplexity/K = 64.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.72%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  95127.7 e-6; = (1/var)*||X-X_r||^2 =  87994.2 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  7133.5 e-6 = 7.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  148552.6 e-6; = (1/var)*||X-X_r||^2 =  138997.1 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  9555.6 e-6 = 6.4 %)
Min.  Avg. Train Loss across Mini-Batch =  94567.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147935.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53425.0 e-6; = (1/var)*||X-X_r||^2 val-train = 51002.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2422.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.60; perplexity/K = 64.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.48; perplexity/K = 61.95%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102194.0 e-6; = (1/var)*||X-X_r||^2 =  88556.9 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  13637.2 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  154687.1 e-6; = (1/var)*||X-X_r||^2 =  139337.0 e-6 = 90.1 %; (1+beta)*||Z_e-Z_q||^2 =  15350.0 e-6 = 9.9 %)
Min.  Avg. Train Loss across Mini-Batch =  92605.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147476.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52493.0 e-6; = (1/var)*||X-X_r||^2 val-train = 50780.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1712.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.65; perplexity/K = 66.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.53; perplexity/K = 63.32%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94047.6 e-6; = (1/var)*||X-X_r||^2 =  86846.0 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  7201.6 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  150257.0 e-6; = (1/var)*||X-X_r||^2 =  139193.2 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  11063.7 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  91563.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  146481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56209.4 e-6; = (1/var)*||X-X_r||^2 val-train = 52347.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3862.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.67; perplexity/K = 66.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.50; perplexity/K = 62.40%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102977.7 e-6; = (1/var)*||X-X_r||^2 =  89434.3 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  13543.4 e-6 = 13.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  151263.7 e-6; = (1/var)*||X-X_r||^2 =  137738.2 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  13525.6 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  89853.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  144479.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48286.1 e-6; = (1/var)*||X-X_r||^2 val-train = 48303.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.66; perplexity/K = 66.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.81%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90521.2 e-6; = (1/var)*||X-X_r||^2 =  85323.0 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  5198.2 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  146905.1 e-6; = (1/var)*||X-X_r||^2 =  138939.6 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  7965.4 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  88902.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143318.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56383.9 e-6; = (1/var)*||X-X_r||^2 val-train = 53616.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2767.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.57; perplexity/K = 64.30%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90638.4 e-6; = (1/var)*||X-X_r||^2 =  85500.8 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  5137.6 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  143121.6 e-6; = (1/var)*||X-X_r||^2 =  136210.7 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  6910.9 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  88549.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142430.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52483.2 e-6; = (1/var)*||X-X_r||^2 val-train = 50709.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1773.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.62; perplexity/K = 65.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.66; perplexity/K = 66.47%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99118.2 e-6; = (1/var)*||X-X_r||^2 =  87847.4 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  11270.8 e-6 = 11.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  155908.1 e-6; = (1/var)*||X-X_r||^2 =  139971.3 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  15936.8 e-6 = 10.2 %)
Min.  Avg. Train Loss across Mini-Batch =  87998.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141033.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56789.9 e-6; = (1/var)*||X-X_r||^2 val-train = 52124.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4666.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.61; perplexity/K = 65.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.63%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  95963.9 e-6; = (1/var)*||X-X_r||^2 =  85993.2 e-6 = 89.6 %; (1+beta)*||Z_e-Z_q||^2 =  9970.7 e-6 = 10.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  149243.5 e-6; = (1/var)*||X-X_r||^2 =  137500.9 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  11742.6 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  86711.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141033.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53279.5 e-6; = (1/var)*||X-X_r||^2 val-train = 51507.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1771.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.73; perplexity/K = 68.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.53; perplexity/K = 63.26%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  113307.4 e-6; = (1/var)*||X-X_r||^2 =  90414.5 e-6 = 79.8 %; (1+beta)*||Z_e-Z_q||^2 =  22892.8 e-6 = 20.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  161516.9 e-6; = (1/var)*||X-X_r||^2 =  141056.7 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  20460.2 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  86711.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141033.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48209.6 e-6; = (1/var)*||X-X_r||^2 val-train = 50642.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2432.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.55; perplexity/K = 63.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.56; perplexity/K = 64.02%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  88059.9 e-6; = (1/var)*||X-X_r||^2 =  83906.1 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  4153.7 e-6 = 4.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  143437.6 e-6; = (1/var)*||X-X_r||^2 =  137213.8 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  6223.8 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  86689.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141033.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55377.7 e-6; = (1/var)*||X-X_r||^2 val-train = 53307.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2070.0 e-6 

----------------------------------------------------------------------------------

Finished [17:17:06 08.01.2023] 319) Finished running for K = 4 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = True:
Total training time is = 0:3:43 h/m/s. 

--------------------------------------------------- 

Started [17:17:06 08.01.2023] 320) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2452 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.31
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.37
4                                  encoder.pre_residual_stack.weight                       589            24.02
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             6.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.37
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.31
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.78; perplexity/K = 88.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.77; perplexity/K = 88.25%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1177759.7 e-6; = (1/var)*||X-X_r||^2 =  391838.2 e-6 = 33.3 %; (1+beta)*||Z_e-Z_q||^2 =  785921.5 e-6 = 66.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1079288.8 e-6; = (1/var)*||X-X_r||^2 =  386109.1 e-6 = 35.8 %; (1+beta)*||Z_e-Z_q||^2 =  693179.7 e-6 = 64.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1121885.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1030989.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -98470.9 e-6; = (1/var)*||X-X_r||^2 val-train = -5729.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -92741.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.79; perplexity/K = 89.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.77; perplexity/K = 88.71%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  771801.4 e-6; = (1/var)*||X-X_r||^2 =  315318.4 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  456482.9 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  806388.1 e-6; = (1/var)*||X-X_r||^2 =  336941.2 e-6 = 41.8 %; (1+beta)*||Z_e-Z_q||^2 =  469446.9 e-6 = 58.2 %)
Min.  Avg. Train Loss across Mini-Batch =  750254.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  779568.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34586.7 e-6; = (1/var)*||X-X_r||^2 val-train = 21622.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12963.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.76; perplexity/K = 88.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.77; perplexity/K = 88.43%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  651581.3 e-6; = (1/var)*||X-X_r||^2 =  312237.4 e-6 = 47.9 %; (1+beta)*||Z_e-Z_q||^2 =  339343.9 e-6 = 52.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  808858.0 e-6; = (1/var)*||X-X_r||^2 =  352227.0 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  456630.9 e-6 = 56.5 %)
Min.  Avg. Train Loss across Mini-Batch =  647638.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  725275.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157276.6 e-6; = (1/var)*||X-X_r||^2 val-train = 39989.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 117287.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.70; perplexity/K = 84.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.72; perplexity/K = 86.24%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  590713.8 e-6; = (1/var)*||X-X_r||^2 =  362897.9 e-6 = 61.4 %; (1+beta)*||Z_e-Z_q||^2 =  227815.8 e-6 = 38.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  620244.0 e-6; = (1/var)*||X-X_r||^2 =  414630.7 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  205613.3 e-6 = 33.2 %)
Min.  Avg. Train Loss across Mini-Batch =  555017.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  599287.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29530.2 e-6; = (1/var)*||X-X_r||^2 val-train = 51732.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22202.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.67; perplexity/K = 83.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.70; perplexity/K = 85.17%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  492202.9 e-6; = (1/var)*||X-X_r||^2 =  382170.2 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  110032.7 e-6 = 22.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  551185.4 e-6; = (1/var)*||X-X_r||^2 =  440838.1 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  110347.4 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  488457.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  544049.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58982.5 e-6; = (1/var)*||X-X_r||^2 val-train = 58667.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 314.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.70; perplexity/K = 84.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.70; perplexity/K = 84.81%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  470105.8 e-6; = (1/var)*||X-X_r||^2 =  389224.9 e-6 = 82.8 %; (1+beta)*||Z_e-Z_q||^2 =  80880.9 e-6 = 17.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  526771.3 e-6; = (1/var)*||X-X_r||^2 =  447440.2 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  79331.2 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  458255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  517526.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56665.5 e-6; = (1/var)*||X-X_r||^2 val-train = 58215.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1549.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.31; perplexity/K = 65.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.35; perplexity/K = 67.41%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  477238.4 e-6; = (1/var)*||X-X_r||^2 =  415852.7 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  61385.7 e-6 = 12.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  523653.0 e-6; = (1/var)*||X-X_r||^2 =  463838.1 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  59814.9 e-6 = 11.4 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46414.6 e-6; = (1/var)*||X-X_r||^2 val-train = 47985.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1570.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.35; perplexity/K = 67.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.22; perplexity/K = 61.12%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  543036.8 e-6; = (1/var)*||X-X_r||^2 =  494559.5 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  48477.3 e-6 = 8.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  592950.4 e-6; = (1/var)*||X-X_r||^2 =  526073.5 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  66877.0 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   49913.6 e-6; = (1/var)*||X-X_r||^2 val-train = 31514.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18399.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.19; perplexity/K = 59.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 61.98%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  614810.1 e-6; = (1/var)*||X-X_r||^2 =  504819.9 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  109990.1 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  623438.1 e-6; = (1/var)*||X-X_r||^2 =  532142.8 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  91295.3 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8628.0 e-6; = (1/var)*||X-X_r||^2 val-train = 27322.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18694.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.25; perplexity/K = 62.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.18; perplexity/K = 58.87%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  523782.8 e-6; = (1/var)*||X-X_r||^2 =  500901.3 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  22881.5 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  602014.7 e-6; = (1/var)*||X-X_r||^2 =  547142.9 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  54871.8 e-6 = 9.1 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   78231.8 e-6; = (1/var)*||X-X_r||^2 val-train = 46241.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 31990.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.23; perplexity/K = 61.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.22; perplexity/K = 60.77%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  545323.9 e-6; = (1/var)*||X-X_r||^2 =  511098.2 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  34225.7 e-6 = 6.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  604792.3 e-6; = (1/var)*||X-X_r||^2 =  554387.1 e-6 = 91.7 %; (1+beta)*||Z_e-Z_q||^2 =  50405.2 e-6 = 8.3 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   59468.4 e-6; = (1/var)*||X-X_r||^2 val-train = 43288.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16179.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.30; perplexity/K = 64.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.23; perplexity/K = 61.73%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  532275.2 e-6; = (1/var)*||X-X_r||^2 =  500800.3 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  31475.0 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  581945.1 e-6; = (1/var)*||X-X_r||^2 =  549526.6 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  32418.6 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   49669.9 e-6; = (1/var)*||X-X_r||^2 val-train = 48726.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 943.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.11; perplexity/K = 55.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.16; perplexity/K = 57.84%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  598475.9 e-6; = (1/var)*||X-X_r||^2 =  572523.3 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  25952.5 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  639629.2 e-6; = (1/var)*||X-X_r||^2 =  618953.5 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  20675.8 e-6 = 3.2 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41153.3 e-6; = (1/var)*||X-X_r||^2 val-train = 46430.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5276.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.16; perplexity/K = 57.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.17; perplexity/K = 58.32%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  601757.3 e-6; = (1/var)*||X-X_r||^2 =  576630.2 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  25127.1 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  662349.3 e-6; = (1/var)*||X-X_r||^2 =  614185.2 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  48164.1 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   60592.0 e-6; = (1/var)*||X-X_r||^2 val-train = 37555.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23037.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.18; perplexity/K = 58.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.21; perplexity/K = 60.50%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  591117.0 e-6; = (1/var)*||X-X_r||^2 =  577314.5 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  13802.5 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  638725.3 e-6; = (1/var)*||X-X_r||^2 =  621388.3 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  17337.1 e-6 = 2.7 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47608.3 e-6; = (1/var)*||X-X_r||^2 val-train = 44073.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3534.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.14; perplexity/K = 56.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.12; perplexity/K = 55.78%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  585543.4 e-6; = (1/var)*||X-X_r||^2 =  570938.6 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  14604.8 e-6 = 2.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  637388.3 e-6; = (1/var)*||X-X_r||^2 =  621753.6 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  15634.7 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51844.9 e-6; = (1/var)*||X-X_r||^2 val-train = 50815.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1029.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 60.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 60.24%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  586745.5 e-6; = (1/var)*||X-X_r||^2 =  547490.5 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  39255.0 e-6 = 6.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  627744.3 e-6; = (1/var)*||X-X_r||^2 =  587975.4 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  39768.8 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40998.7 e-6; = (1/var)*||X-X_r||^2 val-train = 40484.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 513.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.19; perplexity/K = 59.52%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  585778.6 e-6; = (1/var)*||X-X_r||^2 =  565998.2 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  19780.4 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  632078.7 e-6; = (1/var)*||X-X_r||^2 =  608097.3 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  23981.4 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46300.1 e-6; = (1/var)*||X-X_r||^2 val-train = 42099.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4201.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 60.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.22; perplexity/K = 60.94%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  586762.1 e-6; = (1/var)*||X-X_r||^2 =  552511.2 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  34250.9 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  623125.7 e-6; = (1/var)*||X-X_r||^2 =  586276.3 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  36849.4 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36363.5 e-6; = (1/var)*||X-X_r||^2 val-train = 33765.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2598.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.22; perplexity/K = 60.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 59.88%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  571826.5 e-6; = (1/var)*||X-X_r||^2 =  553394.9 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  18431.6 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  611713.1 e-6; = (1/var)*||X-X_r||^2 =  592551.5 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  19161.6 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  444255.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39886.6 e-6; = (1/var)*||X-X_r||^2 val-train = 39156.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 730.0 e-6 

----------------------------------------------------------------------------------

Finished [18:06:47 08.01.2023] 320) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:3:40 h/m/s. 

--------------------------------------------------- 

Started [19:12:11 08.01.2023] 305) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 730 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.10
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.38
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.95
4                                  encoder.pre_residual_stack.weight                       147            20.14
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
9                              encoder.channel_adjusting_conv.weight                         8             1.10
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73            10.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.93
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.93
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.95
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.38
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.10
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.13; perplexity/K = 56.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.13; perplexity/K = 56.28%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6676713.7 e-6; = (1/var)*||X-X_r||^2 =  655899.0 e-6 = 9.8 %; (1+beta)*||Z_e-Z_q||^2 =  6020814.8 e-6 = 90.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  7565038.1 e-6; = (1/var)*||X-X_r||^2 =  635754.3 e-6 = 8.4 %; (1+beta)*||Z_e-Z_q||^2 =  6929283.7 e-6 = 91.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   888324.4 e-6; = (1/var)*||X-X_r||^2 val-train = -20144.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 908468.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.09; perplexity/K = 54.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 54.08%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10538229.6 e-6; = (1/var)*||X-X_r||^2 =  678921.2 e-6 = 6.4 %; (1+beta)*||Z_e-Z_q||^2 =  9859308.4 e-6 = 93.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  13124363.5 e-6; = (1/var)*||X-X_r||^2 =  657149.8 e-6 = 5.0 %; (1+beta)*||Z_e-Z_q||^2 =  12467213.6 e-6 = 95.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2586133.9 e-6; = (1/var)*||X-X_r||^2 val-train = -21771.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2607905.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.18%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12903087.9 e-6; = (1/var)*||X-X_r||^2 =  709487.0 e-6 = 5.5 %; (1+beta)*||Z_e-Z_q||^2 =  12193600.8 e-6 = 94.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  11930818.1 e-6; = (1/var)*||X-X_r||^2 =  690884.7 e-6 = 5.8 %; (1+beta)*||Z_e-Z_q||^2 =  11239933.5 e-6 = 94.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -972269.7 e-6; = (1/var)*||X-X_r||^2 val-train = -18602.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -953667.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.09; perplexity/K = 54.73%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14192642.6 e-6; = (1/var)*||X-X_r||^2 =  701728.1 e-6 = 4.9 %; (1+beta)*||Z_e-Z_q||^2 =  13490914.6 e-6 = 95.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  17436174.9 e-6; = (1/var)*||X-X_r||^2 =  692366.3 e-6 = 4.0 %; (1+beta)*||Z_e-Z_q||^2 =  16743808.5 e-6 = 96.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3243532.3 e-6; = (1/var)*||X-X_r||^2 val-train = -9361.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3252894.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.52%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17271827.4 e-6; = (1/var)*||X-X_r||^2 =  736373.0 e-6 = 4.3 %; (1+beta)*||Z_e-Z_q||^2 =  16535454.5 e-6 = 95.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  15937798.9 e-6; = (1/var)*||X-X_r||^2 =  723285.7 e-6 = 4.5 %; (1+beta)*||Z_e-Z_q||^2 =  15214513.2 e-6 = 95.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1334028.6 e-6; = (1/var)*||X-X_r||^2 val-train = -13087.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1320941.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 52.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 51.83%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12011688.0 e-6; = (1/var)*||X-X_r||^2 =  757499.0 e-6 = 6.3 %; (1+beta)*||Z_e-Z_q||^2 =  11254188.9 e-6 = 93.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  11191225.9 e-6; = (1/var)*||X-X_r||^2 =  747639.1 e-6 = 6.7 %; (1+beta)*||Z_e-Z_q||^2 =  10443587.0 e-6 = 93.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -820462.1 e-6; = (1/var)*||X-X_r||^2 val-train = -9860.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -810601.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.03; perplexity/K = 51.57%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3713506.1 e-6; = (1/var)*||X-X_r||^2 =  762753.6 e-6 = 20.5 %; (1+beta)*||Z_e-Z_q||^2 =  2950752.5 e-6 = 79.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3839378.4 e-6; = (1/var)*||X-X_r||^2 =  760271.6 e-6 = 19.8 %; (1+beta)*||Z_e-Z_q||^2 =  3079106.8 e-6 = 80.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   125872.3 e-6; = (1/var)*||X-X_r||^2 val-train = -2482.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 128354.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1130780.0 e-6; = (1/var)*||X-X_r||^2 =  884779.1 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  246000.9 e-6 = 21.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1068915.5 e-6; = (1/var)*||X-X_r||^2 =  864162.6 e-6 = 80.8 %; (1+beta)*||Z_e-Z_q||^2 =  204752.9 e-6 = 19.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1071216.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1025867.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -61864.5 e-6; = (1/var)*||X-X_r||^2 val-train = -20616.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -41248.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.71%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  926578.5 e-6; = (1/var)*||X-X_r||^2 =  905965.3 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  20613.2 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  911266.8 e-6; = (1/var)*||X-X_r||^2 =  892290.1 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  18976.6 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  922831.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908494.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15311.7 e-6; = (1/var)*||X-X_r||^2 val-train = -13675.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1636.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  916053.1 e-6; = (1/var)*||X-X_r||^2 =  906033.2 e-6 = 98.9 %; (1+beta)*||Z_e-Z_q||^2 =  10019.9 e-6 = 1.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  902163.4 e-6; = (1/var)*||X-X_r||^2 =  893039.4 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  9124.1 e-6 = 1.0 %)
Min.  Avg. Train Loss across Mini-Batch =  914736.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898517.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13889.7 e-6; = (1/var)*||X-X_r||^2 val-train = -12993.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -895.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.02; perplexity/K = 51.15%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  912865.0 e-6; = (1/var)*||X-X_r||^2 =  906752.9 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6112.1 e-6 = 0.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  901420.3 e-6; = (1/var)*||X-X_r||^2 =  895127.2 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6293.1 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  912387.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898517.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11444.7 e-6; = (1/var)*||X-X_r||^2 val-train = -11625.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 181.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.21%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  912060.0 e-6; = (1/var)*||X-X_r||^2 =  906692.3 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  5367.7 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  900015.1 e-6; = (1/var)*||X-X_r||^2 =  894786.4 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  5228.8 e-6 = 0.6 %)
Min.  Avg. Train Loss across Mini-Batch =  911712.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898517.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12044.8 e-6; = (1/var)*||X-X_r||^2 val-train = -11905.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -138.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  911078.8 e-6; = (1/var)*||X-X_r||^2 =  906933.7 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4145.2 e-6 = 0.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  899629.6 e-6; = (1/var)*||X-X_r||^2 =  895334.4 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4295.2 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  909328.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897480.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11449.2 e-6; = (1/var)*||X-X_r||^2 val-train = -11599.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 150.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909678.8 e-6; = (1/var)*||X-X_r||^2 =  906724.1 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2954.7 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  898497.9 e-6; = (1/var)*||X-X_r||^2 =  895258.3 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3239.6 e-6 = 0.4 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11180.9 e-6; = (1/var)*||X-X_r||^2 val-train = -11465.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 284.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.02; perplexity/K = 50.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909220.5 e-6; = (1/var)*||X-X_r||^2 =  906671.8 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2548.7 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  897865.4 e-6; = (1/var)*||X-X_r||^2 =  895023.0 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2842.4 e-6 = 0.3 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11355.1 e-6; = (1/var)*||X-X_r||^2 val-train = -11648.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 293.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909971.5 e-6; = (1/var)*||X-X_r||^2 =  906808.2 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  3163.4 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  898749.4 e-6; = (1/var)*||X-X_r||^2 =  894986.0 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3763.3 e-6 = 0.4 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11222.2 e-6; = (1/var)*||X-X_r||^2 val-train = -11822.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 600.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.21%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909807.7 e-6; = (1/var)*||X-X_r||^2 =  906705.9 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  3101.8 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  899158.4 e-6; = (1/var)*||X-X_r||^2 =  894966.3 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4192.1 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10649.3 e-6; = (1/var)*||X-X_r||^2 val-train = -11739.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1090.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909915.6 e-6; = (1/var)*||X-X_r||^2 =  906668.2 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3247.3 e-6 = 0.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  899075.5 e-6; = (1/var)*||X-X_r||^2 =  895355.7 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3719.8 e-6 = 0.4 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10840.1 e-6; = (1/var)*||X-X_r||^2 val-train = -11312.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 472.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  911605.3 e-6; = (1/var)*||X-X_r||^2 =  908325.6 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3279.8 e-6 = 0.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  905876.1 e-6; = (1/var)*||X-X_r||^2 =  902004.8 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3871.2 e-6 = 0.4 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5729.2 e-6; = (1/var)*||X-X_r||^2 val-train = -6320.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 591.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 50.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.21%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  910302.4 e-6; = (1/var)*||X-X_r||^2 =  908352.5 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1949.9 e-6 = 0.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  904848.6 e-6; = (1/var)*||X-X_r||^2 =  902144.3 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2704.3 e-6 = 0.3 %)
Min.  Avg. Train Loss across Mini-Batch =  909030.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897343.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5453.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6208.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 754.5 e-6 

----------------------------------------------------------------------------------

Finished [20:00:04 08.01.2023] 305) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:3:52 h/m/s. 

--------------------------------------------------- 


Started [20:45:36 08.01.2023] 310) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2452 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.31
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.34
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.37
4                                  encoder.pre_residual_stack.weight                       589            24.02
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             6.00
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.37
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.31
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 99.85%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  990900.5 e-6; = (1/var)*||X-X_r||^2 =  362306.4 e-6 = 36.6 %; (1+beta)*||Z_e-Z_q||^2 =  628594.1 e-6 = 63.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  920777.2 e-6; = (1/var)*||X-X_r||^2 =  364819.0 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  555958.2 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  941475.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  920777.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -70123.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2512.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -72636.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.17%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  618410.9 e-6; = (1/var)*||X-X_r||^2 =  281078.2 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  337332.8 e-6 = 54.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  675046.5 e-6; = (1/var)*||X-X_r||^2 =  303491.1 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  371555.4 e-6 = 55.0 %)
Min.  Avg. Train Loss across Mini-Batch =  604876.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  625075.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56635.5 e-6; = (1/var)*||X-X_r||^2 val-train = 22413.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34222.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.45%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  434496.4 e-6; = (1/var)*||X-X_r||^2 =  249694.4 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  184802.1 e-6 = 42.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  477797.6 e-6; = (1/var)*||X-X_r||^2 =  286554.7 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  191242.9 e-6 = 40.0 %)
Min.  Avg. Train Loss across Mini-Batch =  429943.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  477797.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43301.1 e-6; = (1/var)*||X-X_r||^2 val-train = 36860.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6440.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.58%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391225.8 e-6; = (1/var)*||X-X_r||^2 =  255521.8 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  135704.0 e-6 = 34.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  479342.3 e-6; = (1/var)*||X-X_r||^2 =  316246.7 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  163095.6 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  352609.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  418631.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   88116.5 e-6; = (1/var)*||X-X_r||^2 val-train = 60724.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27391.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.33%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  345183.1 e-6; = (1/var)*||X-X_r||^2 =  241443.9 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  103739.1 e-6 = 30.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  416511.4 e-6; = (1/var)*||X-X_r||^2 =  308623.9 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  107887.5 e-6 = 25.9 %)
Min.  Avg. Train Loss across Mini-Batch =  327186.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  402840.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   71328.3 e-6; = (1/var)*||X-X_r||^2 val-train = 67180.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4148.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 98.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.53%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  339544.8 e-6; = (1/var)*||X-X_r||^2 =  236461.5 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  103083.3 e-6 = 30.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  418268.4 e-6; = (1/var)*||X-X_r||^2 =  298065.9 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  120202.5 e-6 = 28.7 %)
Min.  Avg. Train Loss across Mini-Batch =  318339.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  393245.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   78723.6 e-6; = (1/var)*||X-X_r||^2 val-train = 61604.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17119.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.31%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344541.6 e-6; = (1/var)*||X-X_r||^2 =  231487.8 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  113053.9 e-6 = 32.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  400605.1 e-6; = (1/var)*||X-X_r||^2 =  297476.1 e-6 = 74.3 %; (1+beta)*||Z_e-Z_q||^2 =  103129.0 e-6 = 25.7 %)
Min.  Avg. Train Loss across Mini-Batch =  309664.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  390628.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56063.4 e-6; = (1/var)*||X-X_r||^2 val-train = 65988.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9924.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.38%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  329700.8 e-6; = (1/var)*||X-X_r||^2 =  227317.2 e-6 = 68.9 %; (1+beta)*||Z_e-Z_q||^2 =  102383.5 e-6 = 31.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  409236.7 e-6; = (1/var)*||X-X_r||^2 =  304906.8 e-6 = 74.5 %; (1+beta)*||Z_e-Z_q||^2 =  104329.9 e-6 = 25.5 %)
Min.  Avg. Train Loss across Mini-Batch =  309058.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   79535.9 e-6; = (1/var)*||X-X_r||^2 val-train = 77589.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1946.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.62%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  375856.2 e-6; = (1/var)*||X-X_r||^2 =  252299.1 e-6 = 67.1 %; (1+beta)*||Z_e-Z_q||^2 =  123557.1 e-6 = 32.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  462172.3 e-6; = (1/var)*||X-X_r||^2 =  324669.6 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  137502.7 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   86316.1 e-6; = (1/var)*||X-X_r||^2 val-train = 72370.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13945.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.16%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  354671.7 e-6; = (1/var)*||X-X_r||^2 =  238177.8 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  116493.9 e-6 = 32.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  423496.9 e-6; = (1/var)*||X-X_r||^2 =  318869.2 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  104627.7 e-6 = 24.7 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   68825.2 e-6; = (1/var)*||X-X_r||^2 val-train = 80691.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11866.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 98.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.69%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  336909.2 e-6; = (1/var)*||X-X_r||^2 =  236724.0 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  100185.2 e-6 = 29.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  426766.6 e-6; = (1/var)*||X-X_r||^2 =  322094.6 e-6 = 75.5 %; (1+beta)*||Z_e-Z_q||^2 =  104672.1 e-6 = 24.5 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   89857.4 e-6; = (1/var)*||X-X_r||^2 val-train = 85370.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4486.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.41%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  360984.2 e-6; = (1/var)*||X-X_r||^2 =  251473.8 e-6 = 69.7 %; (1+beta)*||Z_e-Z_q||^2 =  109510.4 e-6 = 30.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  477009.0 e-6; = (1/var)*||X-X_r||^2 =  341949.9 e-6 = 71.7 %; (1+beta)*||Z_e-Z_q||^2 =  135059.1 e-6 = 28.3 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   116024.7 e-6; = (1/var)*||X-X_r||^2 val-train = 90476.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25548.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.58%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  332580.9 e-6; = (1/var)*||X-X_r||^2 =  242758.6 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  89822.3 e-6 = 27.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  443368.3 e-6; = (1/var)*||X-X_r||^2 =  350170.7 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  93197.6 e-6 = 21.0 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   110787.4 e-6; = (1/var)*||X-X_r||^2 val-train = 107412.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3375.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.27%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  334418.5 e-6; = (1/var)*||X-X_r||^2 =  243021.3 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  91397.1 e-6 = 27.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  450211.3 e-6; = (1/var)*||X-X_r||^2 =  352188.2 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  98023.1 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   115792.8 e-6; = (1/var)*||X-X_r||^2 val-train = 109166.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6626.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 98.96%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  338349.6 e-6; = (1/var)*||X-X_r||^2 =  244678.9 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  93670.7 e-6 = 27.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  460446.1 e-6; = (1/var)*||X-X_r||^2 =  367083.3 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  93362.9 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   122096.6 e-6; = (1/var)*||X-X_r||^2 val-train = 122404.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -307.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.48%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  349896.8 e-6; = (1/var)*||X-X_r||^2 =  251386.6 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  98510.2 e-6 = 28.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  471329.6 e-6; = (1/var)*||X-X_r||^2 =  366095.4 e-6 = 77.7 %; (1+beta)*||Z_e-Z_q||^2 =  105234.2 e-6 = 22.3 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   121432.8 e-6; = (1/var)*||X-X_r||^2 val-train = 114708.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6724.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.52%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  341608.1 e-6; = (1/var)*||X-X_r||^2 =  252613.8 e-6 = 73.9 %; (1+beta)*||Z_e-Z_q||^2 =  88994.4 e-6 = 26.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  449869.6 e-6; = (1/var)*||X-X_r||^2 =  360029.9 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  89839.6 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   108261.4 e-6; = (1/var)*||X-X_r||^2 val-train = 107416.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 845.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 98.83%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  339347.6 e-6; = (1/var)*||X-X_r||^2 =  251168.2 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  88179.4 e-6 = 26.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  448544.8 e-6; = (1/var)*||X-X_r||^2 =  367772.8 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  80771.9 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   109197.2 e-6; = (1/var)*||X-X_r||^2 val-train = 116604.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7407.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 99.04%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  359453.0 e-6; = (1/var)*||X-X_r||^2 =  255726.5 e-6 = 71.1 %; (1+beta)*||Z_e-Z_q||^2 =  103726.5 e-6 = 28.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  451275.0 e-6; = (1/var)*||X-X_r||^2 =  356578.7 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  94696.3 e-6 = 21.0 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   91822.1 e-6; = (1/var)*||X-X_r||^2 val-train = 100852.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9030.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.98; perplexity/K = 98.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.99; perplexity/K = 99.44%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344225.4 e-6; = (1/var)*||X-X_r||^2 =  247940.7 e-6 = 72.0 %; (1+beta)*||Z_e-Z_q||^2 =  96284.7 e-6 = 28.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  454810.5 e-6; = (1/var)*||X-X_r||^2 =  361483.9 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  93326.7 e-6 = 20.5 %)
Min.  Avg. Train Loss across Mini-Batch =  305509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382309.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   110585.2 e-6; = (1/var)*||X-X_r||^2 val-train = 113543.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2958.0 e-6 

----------------------------------------------------------------------------------

Finished [21:34:54 08.01.2023] 310) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:3:18 h/m/s. 

--------------------------------------------------- 

Started [21:34:54 08.01.2023] 315) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1972 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.29
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.29
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.29
4                                  encoder.pre_residual_stack.weight                       147             7.45
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.41
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        73             3.70
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.29
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.29
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.29
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.28; perplexity/K = 64.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.32; perplexity/K = 65.85%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1519895.2 e-6; = (1/var)*||X-X_r||^2 =  525848.5 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  994046.6 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1614666.3 e-6; = (1/var)*||X-X_r||^2 =  497748.5 e-6 = 30.8 %; (1+beta)*||Z_e-Z_q||^2 =  1116917.8 e-6 = 69.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1307639.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1172487.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   94771.2 e-6; = (1/var)*||X-X_r||^2 val-train = -28100.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 122871.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 54.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 55.05%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  741790.8 e-6; = (1/var)*||X-X_r||^2 =  693874.5 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  47916.4 e-6 = 6.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  737053.5 e-6; = (1/var)*||X-X_r||^2 =  685436.0 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  51617.5 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  732125.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4737.3 e-6; = (1/var)*||X-X_r||^2 val-train = -8438.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3701.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.29%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  755767.7 e-6; = (1/var)*||X-X_r||^2 =  739010.2 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  16757.5 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  753500.3 e-6; = (1/var)*||X-X_r||^2 =  726156.4 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  27344.0 e-6 = 3.6 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2267.4 e-6; = (1/var)*||X-X_r||^2 val-train = -12853.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10586.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 53.06%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  758689.7 e-6; = (1/var)*||X-X_r||^2 =  743576.5 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  15113.2 e-6 = 2.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  746812.2 e-6; = (1/var)*||X-X_r||^2 =  734566.5 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  12245.7 e-6 = 1.6 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11877.5 e-6; = (1/var)*||X-X_r||^2 val-train = -9009.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2867.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.04; perplexity/K = 52.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.34%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  758569.0 e-6; = (1/var)*||X-X_r||^2 =  755128.7 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  3440.3 e-6 = 0.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  758106.5 e-6; = (1/var)*||X-X_r||^2 =  753533.3 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  4573.2 e-6 = 0.6 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -462.5 e-6; = (1/var)*||X-X_r||^2 val-train = -1595.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1132.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999448.8 e-6; = (1/var)*||X-X_r||^2 =  999386.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  62.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962937.9 e-6; = (1/var)*||X-X_r||^2 =  962876.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  61.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36510.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36510.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999355.5 e-6; = (1/var)*||X-X_r||^2 =  999333.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  22.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962885.9 e-6; = (1/var)*||X-X_r||^2 =  962863.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  22.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36469.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36469.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999284.2 e-6; = (1/var)*||X-X_r||^2 =  999250.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  33.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962991.5 e-6; = (1/var)*||X-X_r||^2 =  962951.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  39.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36292.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36298.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999242.9 e-6; = (1/var)*||X-X_r||^2 =  999223.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  19.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963149.2 e-6; = (1/var)*||X-X_r||^2 =  963133.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  15.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36093.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36089.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999401.6 e-6; = (1/var)*||X-X_r||^2 =  999384.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  17.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963074.8 e-6; = (1/var)*||X-X_r||^2 =  963061.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  13.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36326.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36323.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999245.8 e-6; = (1/var)*||X-X_r||^2 =  999236.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962964.2 e-6; = (1/var)*||X-X_r||^2 =  962959.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36281.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36277.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999217.5 e-6; = (1/var)*||X-X_r||^2 =  999210.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962939.0 e-6; = (1/var)*||X-X_r||^2 =  962932.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36278.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36277.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999297.7 e-6; = (1/var)*||X-X_r||^2 =  999291.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963309.7 e-6; = (1/var)*||X-X_r||^2 =  963306.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35987.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35985.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999231.8 e-6; = (1/var)*||X-X_r||^2 =  999225.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963256.9 e-6; = (1/var)*||X-X_r||^2 =  963251.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35974.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35974.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999261.9 e-6; = (1/var)*||X-X_r||^2 =  999254.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963150.8 e-6; = (1/var)*||X-X_r||^2 =  963146.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36111.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36108.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999286.0 e-6; = (1/var)*||X-X_r||^2 =  999276.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962887.0 e-6; = (1/var)*||X-X_r||^2 =  962879.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36399.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36396.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999247.8 e-6; = (1/var)*||X-X_r||^2 =  999236.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962985.7 e-6; = (1/var)*||X-X_r||^2 =  962977.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  8.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36262.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36259.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999368.5 e-6; = (1/var)*||X-X_r||^2 =  999359.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  8.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962901.7 e-6; = (1/var)*||X-X_r||^2 =  962894.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36466.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36465.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999278.9 e-6; = (1/var)*||X-X_r||^2 =  999265.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  13.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962949.1 e-6; = (1/var)*||X-X_r||^2 =  962936.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36329.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36329.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999209.8 e-6; = (1/var)*||X-X_r||^2 =  999189.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  20.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963057.7 e-6; = (1/var)*||X-X_r||^2 =  963042.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  15.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  733587.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723027.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36152.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36147.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.8 e-6 

----------------------------------------------------------------------------------

Finished [22:39:50 08.01.2023] 315) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = False:
Total training time is = 0:3:55 h/m/s. 

--------------------------------------------------- 

Started [22:39:50 08.01.2023] 320) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7388 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.19
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.19
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.19
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.19
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.19
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.19
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.68; perplexity/K = 83.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.70; perplexity/K = 85.12%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  609055.8 e-6; = (1/var)*||X-X_r||^2 =  327970.8 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  281084.9 e-6 = 46.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  570505.5 e-6; = (1/var)*||X-X_r||^2 =  352218.3 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  218287.2 e-6 = 38.3 %)
Min.  Avg. Train Loss across Mini-Batch =  588987.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  570505.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -38550.3 e-6; = (1/var)*||X-X_r||^2 val-train = 24247.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -62797.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.69; perplexity/K = 84.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.66; perplexity/K = 82.99%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  566785.6 e-6; = (1/var)*||X-X_r||^2 =  472495.4 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  94290.2 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  602503.9 e-6; = (1/var)*||X-X_r||^2 =  505706.8 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  96797.1 e-6 = 16.1 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35718.3 e-6; = (1/var)*||X-X_r||^2 val-train = 33211.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2506.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 53.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.13; perplexity/K = 56.58%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  659161.6 e-6; = (1/var)*||X-X_r||^2 =  619305.2 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  39856.5 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  673434.2 e-6; = (1/var)*||X-X_r||^2 =  638204.3 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  35229.8 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14272.5 e-6; = (1/var)*||X-X_r||^2 val-train = 18899.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4626.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 52.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 53.97%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  703628.5 e-6; = (1/var)*||X-X_r||^2 =  690742.1 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  12886.4 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  710337.3 e-6; = (1/var)*||X-X_r||^2 =  696205.2 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  14132.0 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6708.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5463.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1245.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 54.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.07; perplexity/K = 53.29%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  773655.1 e-6; = (1/var)*||X-X_r||^2 =  730125.1 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  43530.0 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  742408.9 e-6; = (1/var)*||X-X_r||^2 =  705246.9 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  37162.0 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31246.1 e-6; = (1/var)*||X-X_r||^2 val-train = -24878.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6368.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999501.3 e-6; = (1/var)*||X-X_r||^2 =  999462.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  39.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962959.3 e-6; = (1/var)*||X-X_r||^2 =  962942.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  17.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36542.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36519.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999480.0 e-6; = (1/var)*||X-X_r||^2 =  999460.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  19.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962925.2 e-6; = (1/var)*||X-X_r||^2 =  962904.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  20.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36554.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36555.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999386.2 e-6; = (1/var)*||X-X_r||^2 =  999356.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  29.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963129.7 e-6; = (1/var)*||X-X_r||^2 =  963101.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  28.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36256.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36255.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999450.4 e-6; = (1/var)*||X-X_r||^2 =  999414.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  36.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963380.2 e-6; = (1/var)*||X-X_r||^2 =  963342.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  38.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36070.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36072.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999493.7 e-6; = (1/var)*||X-X_r||^2 =  999466.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  27.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963234.1 e-6; = (1/var)*||X-X_r||^2 =  963209.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  24.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36259.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36256.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999483.0 e-6; = (1/var)*||X-X_r||^2 =  999466.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  16.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963091.4 e-6; = (1/var)*||X-X_r||^2 =  963078.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  13.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36391.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36388.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999385.2 e-6; = (1/var)*||X-X_r||^2 =  999368.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  17.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962874.9 e-6; = (1/var)*||X-X_r||^2 =  962860.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  14.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36510.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36508.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999249.4 e-6; = (1/var)*||X-X_r||^2 =  999235.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  14.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962931.4 e-6; = (1/var)*||X-X_r||^2 =  962919.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36318.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36315.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999392.7 e-6; = (1/var)*||X-X_r||^2 =  999382.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962932.8 e-6; = (1/var)*||X-X_r||^2 =  962922.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36459.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36460.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999242.5 e-6; = (1/var)*||X-X_r||^2 =  999233.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962970.8 e-6; = (1/var)*||X-X_r||^2 =  962963.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36271.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36269.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999235.6 e-6; = (1/var)*||X-X_r||^2 =  999225.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962912.3 e-6; = (1/var)*||X-X_r||^2 =  962902.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36323.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36322.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999373.6 e-6; = (1/var)*||X-X_r||^2 =  999361.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962910.4 e-6; = (1/var)*||X-X_r||^2 =  962892.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  17.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36463.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36469.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999216.3 e-6; = (1/var)*||X-X_r||^2 =  999204.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963184.0 e-6; = (1/var)*||X-X_r||^2 =  963176.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36032.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36027.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:7:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999223.0 e-6; = (1/var)*||X-X_r||^2 =  999211.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963241.0 e-6; = (1/var)*||X-X_r||^2 =  963231.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35982.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35980.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 50.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999288.2 e-6; = (1/var)*||X-X_r||^2 =  999275.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962931.6 e-6; = (1/var)*||X-X_r||^2 =  962920.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500540.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  528942.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36356.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36354.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.1 e-6 

----------------------------------------------------------------------------------

Finished [00:55:04 09.01.2023] 320) Finished running for K = 2 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 16) change_channel_size_across_layers = False:
Total training time is = 0:3:14 h/m/s. 

--------------------------------------------------- 


################################# from here upward   run_id <  400 #################################
################################# from here downward run_id => 400 #################################

Started [02:30:37 09.01.2023] 401) Finished running for K = 64 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 691 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.16
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.63
3                           encoder.sequential_convs.conv2d_4.weight                       131            18.96
4                                  encoder.pre_residual_stack.weight                       147            21.27
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.21
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.21
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
9                              encoder.channel_adjusting_conv.weight                         4             0.58
10                                                       VQ.E.weight                         2             0.29
11                             decoder.channel_adjusting_conv.weight                        36             5.21
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.21
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.21
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.96
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.63
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.16
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.51; perplexity/K = 30.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.27; perplexity/K = 30.11%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  313716.6 e-6; = (1/var)*||X-X_r||^2 =  265502.1 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  48214.5 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  316231.6 e-6; = (1/var)*||X-X_r||^2 =  270550.0 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  45681.7 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  313716.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  316231.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2515.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5047.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2532.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.97; perplexity/K = 51.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.33; perplexity/K = 48.95%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  149298.5 e-6; = (1/var)*||X-X_r||^2 =  137888.5 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  11410.0 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  151625.2 e-6; = (1/var)*||X-X_r||^2 =  140631.8 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  10993.3 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  149298.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151625.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2326.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2743.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -416.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.21; perplexity/K = 56.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.89; perplexity/K = 54.51%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94413.5 e-6; = (1/var)*||X-X_r||^2 =  86039.0 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  8374.5 e-6 = 8.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  99718.0 e-6; = (1/var)*||X-X_r||^2 =  91943.7 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  7774.3 e-6 = 7.8 %)
Min.  Avg. Train Loss across Mini-Batch =  94237.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  98713.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5304.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5904.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -600.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.51; perplexity/K = 60.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.61; perplexity/K = 63.45%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  70586.8 e-6; = (1/var)*||X-X_r||^2 =  64411.4 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  6175.3 e-6 = 8.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  74953.2 e-6; = (1/var)*||X-X_r||^2 =  68780.2 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  6173.1 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  70167.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74953.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4366.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4368.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.01; perplexity/K = 65.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.49; perplexity/K = 61.70%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57616.4 e-6; = (1/var)*||X-X_r||^2 =  53030.0 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  4586.3 e-6 = 8.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  62586.8 e-6; = (1/var)*||X-X_r||^2 =  57452.7 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  5134.1 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  55962.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61094.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4970.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4422.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 547.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.77; perplexity/K = 68.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.58; perplexity/K = 66.53%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48550.5 e-6; = (1/var)*||X-X_r||^2 =  45121.4 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  3429.1 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  52683.2 e-6; = (1/var)*||X-X_r||^2 =  49012.9 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  3670.3 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  48550.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52683.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4132.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3891.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 241.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.71; perplexity/K = 65.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.76; perplexity/K = 65.25%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  54937.0 e-6; = (1/var)*||X-X_r||^2 =  49641.3 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  5295.7 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  53538.4 e-6; = (1/var)*||X-X_r||^2 =  49144.6 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  4393.8 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  44327.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48127.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1398.5 e-6; = (1/var)*||X-X_r||^2 val-train = -496.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -901.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.95; perplexity/K = 68.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.65; perplexity/K = 66.64%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41412.9 e-6; = (1/var)*||X-X_r||^2 =  39001.4 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  2411.6 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  44907.7 e-6; = (1/var)*||X-X_r||^2 =  42201.0 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  2706.7 e-6 = 6.0 %)
Min.  Avg. Train Loss across Mini-Batch =  41211.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44756.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3494.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3199.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 295.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.21; perplexity/K = 70.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.10; perplexity/K = 70.47%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39459.7 e-6; = (1/var)*||X-X_r||^2 =  37244.2 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  2215.5 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  42392.5 e-6; = (1/var)*||X-X_r||^2 =  39977.2 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  2415.2 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  39459.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42392.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2932.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2733.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 199.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.86; perplexity/K = 74.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.74; perplexity/K = 74.60%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38124.2 e-6; = (1/var)*||X-X_r||^2 =  36149.8 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  1974.4 e-6 = 5.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  40644.0 e-6; = (1/var)*||X-X_r||^2 =  38493.8 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  2150.2 e-6 = 5.3 %)
Min.  Avg. Train Loss across Mini-Batch =  37186.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40285.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2519.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2344.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 175.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.20; perplexity/K = 72.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.62; perplexity/K = 72.85%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36468.8 e-6; = (1/var)*||X-X_r||^2 =  34809.0 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  1659.8 e-6 = 4.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  39345.0 e-6; = (1/var)*||X-X_r||^2 =  37425.9 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  1919.1 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  36317.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38949.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2876.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2616.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 259.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.43; perplexity/K = 74.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.05; perplexity/K = 73.52%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35496.7 e-6; = (1/var)*||X-X_r||^2 =  33970.5 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  1526.2 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  38274.3 e-6; = (1/var)*||X-X_r||^2 =  36484.7 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  1789.6 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  35220.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38069.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2777.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2514.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 263.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.13; perplexity/K = 73.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.68; perplexity/K = 72.94%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35400.0 e-6; = (1/var)*||X-X_r||^2 =  33898.3 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  1501.7 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  38746.3 e-6; = (1/var)*||X-X_r||^2 =  36739.3 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  2007.0 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  34470.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37202.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3346.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2841.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 505.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.14; perplexity/K = 72.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.15; perplexity/K = 72.12%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34076.5 e-6; = (1/var)*||X-X_r||^2 =  32760.6 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  1315.8 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  37029.3 e-6; = (1/var)*||X-X_r||^2 =  35387.0 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  1642.3 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  33964.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36714.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2952.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2626.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 326.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.85; perplexity/K = 70.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.22; perplexity/K = 69.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34336.7 e-6; = (1/var)*||X-X_r||^2 =  32684.9 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  1651.9 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  36483.7 e-6; = (1/var)*||X-X_r||^2 =  34750.7 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  1733.0 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  33579.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36387.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2147.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2065.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 81.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.96; perplexity/K = 68.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.28; perplexity/K = 70.75%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37380.7 e-6; = (1/var)*||X-X_r||^2 =  35253.2 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  2127.4 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  38242.5 e-6; = (1/var)*||X-X_r||^2 =  36162.7 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  2079.8 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  33241.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35838.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   861.8 e-6; = (1/var)*||X-X_r||^2 val-train = 909.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -47.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.69; perplexity/K = 69.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.66; perplexity/K = 69.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33193.3 e-6; = (1/var)*||X-X_r||^2 =  31764.5 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  1428.8 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  35604.5 e-6; = (1/var)*||X-X_r||^2 =  34038.7 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  1565.8 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  33151.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35466.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2411.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2274.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 137.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.83; perplexity/K = 66.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.67; perplexity/K = 68.23%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32682.6 e-6; = (1/var)*||X-X_r||^2 =  31408.0 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  1274.6 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  35115.9 e-6; = (1/var)*||X-X_r||^2 =  33618.0 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  1497.9 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  32682.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35115.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2433.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2210.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 223.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.12; perplexity/K = 70.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.27; perplexity/K = 69.17%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32452.6 e-6; = (1/var)*||X-X_r||^2 =  31143.2 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  1309.4 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  34751.4 e-6; = (1/var)*||X-X_r||^2 =  33235.6 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  1515.8 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  32381.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34751.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2298.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2092.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 206.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.95; perplexity/K = 68.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.12; perplexity/K = 67.37%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39141.3 e-6; = (1/var)*||X-X_r||^2 =  36710.6 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  2430.8 e-6 = 6.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  37302.4 e-6; = (1/var)*||X-X_r||^2 =  35287.6 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  2014.8 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  32062.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34469.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1839.0 e-6; = (1/var)*||X-X_r||^2 val-train = -1423.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -416.0 e-6 

----------------------------------------------------------------------------------

Finished [03:18:35 09.01.2023] 401) Finished running for K = 64 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:
Total training time is = 0:3:58 h/m/s. 

--------------------------------------------------- 

Started [03:18:35 09.01.2023] 402) Finished running for K = 64 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 670 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.19
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.78
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.55
4                                  encoder.pre_residual_stack.weight                       147            21.94
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.37
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.37
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
9                              encoder.channel_adjusting_conv.weight                         2             0.30
10                                                       VQ.E.weight                         1             0.15
11                             decoder.channel_adjusting_conv.weight                        18             2.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.37
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.37
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.55
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.78
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.19
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.03; perplexity/K = 35.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.65; perplexity/K = 38.52%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  495697.9 e-6; = (1/var)*||X-X_r||^2 =  457627.5 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  38070.3 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  481757.5 e-6; = (1/var)*||X-X_r||^2 =  442472.6 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  39284.9 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  495697.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  481757.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13940.4 e-6; = (1/var)*||X-X_r||^2 val-train = -15155.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1214.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.89; perplexity/K = 46.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.75; perplexity/K = 46.48%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  143004.5 e-6; = (1/var)*||X-X_r||^2 =  116352.4 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  26652.0 e-6 = 18.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  148744.4 e-6; = (1/var)*||X-X_r||^2 =  122176.5 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  26567.9 e-6 = 17.9 %)
Min.  Avg. Train Loss across Mini-Batch =  143004.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148744.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5740.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5824.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -84.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.60; perplexity/K = 54.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.68; perplexity/K = 52.63%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  97874.0 e-6; = (1/var)*||X-X_r||^2 =  82156.2 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  15717.8 e-6 = 16.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  100598.7 e-6; = (1/var)*||X-X_r||^2 =  85148.2 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  15450.5 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  94707.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  99267.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2724.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2992.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -267.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.41; perplexity/K = 52.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.94; perplexity/K = 49.91%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72236.5 e-6; = (1/var)*||X-X_r||^2 =  60385.8 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  11850.7 e-6 = 16.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  77831.5 e-6; = (1/var)*||X-X_r||^2 =  65548.8 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  12282.6 e-6 = 15.8 %)
Min.  Avg. Train Loss across Mini-Batch =  72236.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  77577.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5595.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5163.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 432.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.56; perplexity/K = 49.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.91; perplexity/K = 52.99%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  100066.4 e-6; = (1/var)*||X-X_r||^2 =  79166.7 e-6 = 79.1 %; (1+beta)*||Z_e-Z_q||^2 =  20899.7 e-6 = 20.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  74502.8 e-6; = (1/var)*||X-X_r||^2 =  61400.6 e-6 = 82.4 %; (1+beta)*||Z_e-Z_q||^2 =  13102.3 e-6 = 17.6 %)
Min.  Avg. Train Loss across Mini-Batch =  60657.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  64964.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25563.6 e-6; = (1/var)*||X-X_r||^2 val-train = -17766.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7797.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.83; perplexity/K = 51.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.58; perplexity/K = 50.91%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52152.7 e-6; = (1/var)*||X-X_r||^2 =  44277.3 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  7875.4 e-6 = 15.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  56848.6 e-6; = (1/var)*||X-X_r||^2 =  48345.2 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  8503.4 e-6 = 15.0 %)
Min.  Avg. Train Loss across Mini-Batch =  51675.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56598.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4695.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4067.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 628.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.43; perplexity/K = 50.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.91; perplexity/K = 51.42%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47448.1 e-6; = (1/var)*||X-X_r||^2 =  40650.3 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  6797.8 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  51519.6 e-6; = (1/var)*||X-X_r||^2 =  44171.2 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  7348.4 e-6 = 14.3 %)
Min.  Avg. Train Loss across Mini-Batch =  46971.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51519.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4071.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3520.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 550.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.86; perplexity/K = 54.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.14; perplexity/K = 53.34%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40996.6 e-6; = (1/var)*||X-X_r||^2 =  35156.7 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  5839.9 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  45667.1 e-6; = (1/var)*||X-X_r||^2 =  39119.6 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  6547.5 e-6 = 14.3 %)
Min.  Avg. Train Loss across Mini-Batch =  40996.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45579.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4670.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3962.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 707.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.06; perplexity/K = 53.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.29; perplexity/K = 52.01%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37151.8 e-6; = (1/var)*||X-X_r||^2 =  31368.6 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  5783.2 e-6 = 15.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  40862.8 e-6; = (1/var)*||X-X_r||^2 =  34664.4 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  6198.4 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  34896.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  39289.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3711.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3295.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 415.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.96; perplexity/K = 49.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.26; perplexity/K = 50.40%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31753.2 e-6; = (1/var)*||X-X_r||^2 =  27224.2 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  4529.1 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  36256.9 e-6; = (1/var)*||X-X_r||^2 =  30880.6 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  5376.3 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  31751.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36138.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4503.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3656.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 847.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.87; perplexity/K = 51.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.07; perplexity/K = 51.67%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29619.9 e-6; = (1/var)*||X-X_r||^2 =  25305.1 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  4314.8 e-6 = 14.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  33941.1 e-6; = (1/var)*||X-X_r||^2 =  28992.6 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  4948.5 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  29549.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33808.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4321.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3687.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 633.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.98; perplexity/K = 54.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.03; perplexity/K = 51.62%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27782.7 e-6; = (1/var)*||X-X_r||^2 =  24357.5 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  3425.2 e-6 = 12.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  32300.5 e-6; = (1/var)*||X-X_r||^2 =  28089.5 e-6 = 87.0 %; (1+beta)*||Z_e-Z_q||^2 =  4211.0 e-6 = 13.0 %)
Min.  Avg. Train Loss across Mini-Batch =  27698.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32202.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4517.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3732.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 785.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.37; perplexity/K = 53.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.39; perplexity/K = 55.30%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28676.9 e-6; = (1/var)*||X-X_r||^2 =  24433.3 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  4243.6 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  32458.1 e-6; = (1/var)*||X-X_r||^2 =  27818.1 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  4640.1 e-6 = 14.3 %)
Min.  Avg. Train Loss across Mini-Batch =  27018.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31569.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3781.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3384.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 396.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.42; perplexity/K = 56.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.63; perplexity/K = 55.67%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26633.2 e-6; = (1/var)*||X-X_r||^2 =  23674.3 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  2958.9 e-6 = 11.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  29782.5 e-6; = (1/var)*||X-X_r||^2 =  26296.1 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  3486.4 e-6 = 11.7 %)
Min.  Avg. Train Loss across Mini-Batch =  25830.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29782.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3149.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2621.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 527.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.14; perplexity/K = 58.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.23; perplexity/K = 55.05%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25638.0 e-6; = (1/var)*||X-X_r||^2 =  22555.1 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  3083.0 e-6 = 12.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  29315.1 e-6; = (1/var)*||X-X_r||^2 =  25729.6 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  3585.5 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  25202.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29061.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3677.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3174.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 502.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.88; perplexity/K = 54.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.83; perplexity/K = 55.98%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25141.5 e-6; = (1/var)*||X-X_r||^2 =  22258.2 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  2883.4 e-6 = 11.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  28915.2 e-6; = (1/var)*||X-X_r||^2 =  25532.4 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  3382.8 e-6 = 11.7 %)
Min.  Avg. Train Loss across Mini-Batch =  24952.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28580.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3773.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3274.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 499.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.35; perplexity/K = 55.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.56; perplexity/K = 57.12%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24346.2 e-6; = (1/var)*||X-X_r||^2 =  21769.3 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  2576.9 e-6 = 10.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  27723.2 e-6; = (1/var)*||X-X_r||^2 =  24696.0 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  3027.2 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  24247.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  27618.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3377.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2926.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 450.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.75; perplexity/K = 58.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.56; perplexity/K = 57.13%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23379.1 e-6; = (1/var)*||X-X_r||^2 =  21383.2 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  1995.8 e-6 = 8.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  26977.3 e-6; = (1/var)*||X-X_r||^2 =  24423.2 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  2554.1 e-6 = 9.5 %)
Min.  Avg. Train Loss across Mini-Batch =  23260.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26751.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3598.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3040.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 558.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.01; perplexity/K = 62.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.40; perplexity/K = 60.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22939.3 e-6; = (1/var)*||X-X_r||^2 =  21020.1 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  1919.2 e-6 = 8.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  26840.5 e-6; = (1/var)*||X-X_r||^2 =  24361.0 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  2479.4 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  22916.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26572.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3901.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3341.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 560.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.63; perplexity/K = 63.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.33; perplexity/K = 61.46%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22089.2 e-6; = (1/var)*||X-X_r||^2 =  20182.2 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  1907.0 e-6 = 8.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  25486.2 e-6; = (1/var)*||X-X_r||^2 =  23092.6 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  2393.6 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  22044.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25376.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3397.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2910.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 486.6 e-6 

----------------------------------------------------------------------------------

Finished [04:06:40 09.01.2023] 402) Finished running for K = 64 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:
Total training time is = 0:3:5 h/m/s. 

--------------------------------------------------- 

Started [04:06:40 09.01.2023] 403) Finished running for K = 64 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 659 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.21
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.86
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.88
4                                  encoder.pre_residual_stack.weight                       147            22.31
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
9                              encoder.channel_adjusting_conv.weight                         1             0.15
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                         9             1.37
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.88
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.86
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.13; perplexity/K = 47.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.96; perplexity/K = 46.81%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  483879.0 e-6; = (1/var)*||X-X_r||^2 =  456867.8 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  27011.2 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  470090.2 e-6; = (1/var)*||X-X_r||^2 =  444838.5 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  25251.6 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  483879.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  470090.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13788.8 e-6; = (1/var)*||X-X_r||^2 val-train = -12029.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1759.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.32; perplexity/K = 52.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.75; perplexity/K = 51.17%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  150353.4 e-6; = (1/var)*||X-X_r||^2 =  124733.3 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  25620.0 e-6 = 17.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  156828.6 e-6; = (1/var)*||X-X_r||^2 =  130330.2 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  26498.4 e-6 = 16.9 %)
Min.  Avg. Train Loss across Mini-Batch =  150353.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154273.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6475.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5596.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 878.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.98; perplexity/K = 53.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.64; perplexity/K = 47.87%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  105987.9 e-6; = (1/var)*||X-X_r||^2 =  88834.7 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  17153.2 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  111451.5 e-6; = (1/var)*||X-X_r||^2 =  93539.4 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  17912.0 e-6 = 16.1 %)
Min.  Avg. Train Loss across Mini-Batch =  105987.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  111451.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5463.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4704.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 758.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.95; perplexity/K = 48.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.93; perplexity/K = 53.02%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  85420.0 e-6; = (1/var)*||X-X_r||^2 =  71495.2 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  13924.8 e-6 = 16.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  92059.3 e-6; = (1/var)*||X-X_r||^2 =  76954.8 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  15104.5 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  85420.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  91914.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6639.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5459.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1179.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.42; perplexity/K = 50.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.72; perplexity/K = 52.69%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72579.1 e-6; = (1/var)*||X-X_r||^2 =  60519.8 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  12059.3 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  80226.3 e-6; = (1/var)*||X-X_r||^2 =  66916.0 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  13310.4 e-6 = 16.6 %)
Min.  Avg. Train Loss across Mini-Batch =  72501.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78982.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7647.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6396.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1251.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.80; perplexity/K = 54.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.20; perplexity/K = 50.31%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  64612.8 e-6; = (1/var)*||X-X_r||^2 =  53576.0 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  11036.8 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  71814.1 e-6; = (1/var)*||X-X_r||^2 =  59240.4 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  12573.7 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  63877.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71084.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7201.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5664.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1536.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.83; perplexity/K = 52.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.21; perplexity/K = 55.01%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57393.9 e-6; = (1/var)*||X-X_r||^2 =  47747.4 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  9646.5 e-6 = 16.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  64930.2 e-6; = (1/var)*||X-X_r||^2 =  53732.0 e-6 = 82.8 %; (1+beta)*||Z_e-Z_q||^2 =  11198.2 e-6 = 17.2 %)
Min.  Avg. Train Loss across Mini-Batch =  57199.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  64874.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7536.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5984.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1551.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.70; perplexity/K = 52.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.52; perplexity/K = 52.37%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53832.4 e-6; = (1/var)*||X-X_r||^2 =  45091.4 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  8741.1 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  61678.9 e-6; = (1/var)*||X-X_r||^2 =  51105.2 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  10573.7 e-6 = 17.1 %)
Min.  Avg. Train Loss across Mini-Batch =  53366.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60914.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7846.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6013.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1832.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.04; perplexity/K = 51.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.46; perplexity/K = 52.28%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50939.7 e-6; = (1/var)*||X-X_r||^2 =  42956.6 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  7983.0 e-6 = 15.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  60137.3 e-6; = (1/var)*||X-X_r||^2 =  50271.0 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  9866.3 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  50315.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  58150.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9197.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7314.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1883.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.42; perplexity/K = 55.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.23; perplexity/K = 50.36%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47668.3 e-6; = (1/var)*||X-X_r||^2 =  40506.1 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  7162.2 e-6 = 15.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  55293.5 e-6; = (1/var)*||X-X_r||^2 =  46396.9 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  8896.5 e-6 = 16.1 %)
Min.  Avg. Train Loss across Mini-Batch =  47310.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55194.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7625.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5890.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1734.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.21; perplexity/K = 53.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.91; perplexity/K = 52.99%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45952.0 e-6; = (1/var)*||X-X_r||^2 =  39284.7 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  6667.3 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  54059.0 e-6; = (1/var)*||X-X_r||^2 =  45490.2 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  8568.8 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  45603.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52984.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8107.0 e-6; = (1/var)*||X-X_r||^2 val-train = 6205.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1901.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.18; perplexity/K = 51.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.21; perplexity/K = 53.45%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43766.4 e-6; = (1/var)*||X-X_r||^2 =  37605.7 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  6160.7 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  52630.7 e-6; = (1/var)*||X-X_r||^2 =  44155.0 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  8475.7 e-6 = 16.1 %)
Min.  Avg. Train Loss across Mini-Batch =  43766.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51550.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8864.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6549.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2315.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.34; perplexity/K = 52.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.38; perplexity/K = 53.72%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41898.9 e-6; = (1/var)*||X-X_r||^2 =  36232.3 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  5666.6 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  50458.6 e-6; = (1/var)*||X-X_r||^2 =  43086.0 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  7372.6 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  41729.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49475.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8559.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6853.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1706.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.96; perplexity/K = 54.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.33; perplexity/K = 53.64%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41236.7 e-6; = (1/var)*||X-X_r||^2 =  35713.9 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  5522.7 e-6 = 13.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  49078.2 e-6; = (1/var)*||X-X_r||^2 =  41651.7 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  7426.6 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  40661.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48138.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7841.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5937.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1903.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.77; perplexity/K = 51.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.38; perplexity/K = 53.72%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40756.5 e-6; = (1/var)*||X-X_r||^2 =  35396.2 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  5360.3 e-6 = 13.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  50737.0 e-6; = (1/var)*||X-X_r||^2 =  43054.9 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  7682.1 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  40223.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47900.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9980.4 e-6; = (1/var)*||X-X_r||^2 val-train = 7658.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2321.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.87; perplexity/K = 52.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.80; perplexity/K = 55.93%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40845.1 e-6; = (1/var)*||X-X_r||^2 =  35342.3 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  5502.8 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  47352.7 e-6; = (1/var)*||X-X_r||^2 =  40500.4 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  6852.3 e-6 = 14.5 %)
Min.  Avg. Train Loss across Mini-Batch =  39485.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47005.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6507.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5158.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1349.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.56; perplexity/K = 55.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.42; perplexity/K = 53.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39371.8 e-6; = (1/var)*||X-X_r||^2 =  34371.9 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  4999.8 e-6 = 12.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  47137.8 e-6; = (1/var)*||X-X_r||^2 =  40357.1 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  6780.6 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  38741.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46418.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7766.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5985.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1780.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.31; perplexity/K = 53.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.72; perplexity/K = 54.24%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38959.2 e-6; = (1/var)*||X-X_r||^2 =  34069.6 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  4889.5 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  46852.7 e-6; = (1/var)*||X-X_r||^2 =  40347.1 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  6505.6 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  38228.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45999.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7893.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6277.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1616.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.43; perplexity/K = 53.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.86; perplexity/K = 51.34%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38701.9 e-6; = (1/var)*||X-X_r||^2 =  33818.7 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  4883.2 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  47429.1 e-6; = (1/var)*||X-X_r||^2 =  40368.3 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  7060.8 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  38179.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45786.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8727.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6549.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2177.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.70; perplexity/K = 52.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.30; perplexity/K = 50.47%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38166.8 e-6; = (1/var)*||X-X_r||^2 =  33521.7 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  4645.1 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  46011.5 e-6; = (1/var)*||X-X_r||^2 =  39722.5 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  6289.0 e-6 = 13.7 %)
Min.  Avg. Train Loss across Mini-Batch =  37525.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45341.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7844.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6200.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1643.8 e-6 

----------------------------------------------------------------------------------

Finished [04:54:27 09.01.2023] 403) Finished running for K = 64 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:
Total training time is = 0:3:46 h/m/s. 

--------------------------------------------------- 

Started [04:54:27 09.01.2023] 404) Finished running for K = 64 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 695 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.29
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.15
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.60
4                           encoder.sequential_convs.conv2d_5.weight                       131            18.85
5                                  encoder.pre_residual_stack.weight                       147            21.15
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.18
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.18
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
10                             encoder.channel_adjusting_conv.weight                         4             0.58
11                                                       VQ.E.weight                         2             0.29
12                             decoder.channel_adjusting_conv.weight                        36             5.18
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.18
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.18
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.85
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.60
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.15
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.29
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000001.5 e-6; = (1/var)*||X-X_r||^2 =  1000001.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963445.6 e-6; = (1/var)*||X-X_r||^2 =  963445.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963430.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36555.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36555.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000021.5 e-6; = (1/var)*||X-X_r||^2 =  1000021.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963491.2 e-6; = (1/var)*||X-X_r||^2 =  963491.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963423.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36530.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36530.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000022.0 e-6; = (1/var)*||X-X_r||^2 =  1000022.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963453.2 e-6; = (1/var)*||X-X_r||^2 =  963453.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963422.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36568.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36568.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000019.7 e-6; = (1/var)*||X-X_r||^2 =  1000019.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963430.0 e-6; = (1/var)*||X-X_r||^2 =  963430.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963422.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36589.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36589.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000012.6 e-6; = (1/var)*||X-X_r||^2 =  1000012.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963448.4 e-6; = (1/var)*||X-X_r||^2 =  963448.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963420.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36564.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36564.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000008.5 e-6; = (1/var)*||X-X_r||^2 =  1000008.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963460.7 e-6; = (1/var)*||X-X_r||^2 =  963460.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963420.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36547.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36547.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000016.0 e-6; = (1/var)*||X-X_r||^2 =  1000016.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963440.9 e-6; = (1/var)*||X-X_r||^2 =  963440.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963420.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36575.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36575.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000013.9 e-6; = (1/var)*||X-X_r||^2 =  1000013.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963464.4 e-6; = (1/var)*||X-X_r||^2 =  963464.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36549.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36549.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000016.3 e-6; = (1/var)*||X-X_r||^2 =  1000016.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963466.0 e-6; = (1/var)*||X-X_r||^2 =  963466.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36550.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36550.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.3 e-6; = (1/var)*||X-X_r||^2 =  1000014.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963485.8 e-6; = (1/var)*||X-X_r||^2 =  963485.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000000.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36528.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36528.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.7 e-6; = (1/var)*||X-X_r||^2 =  1000014.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963423.9 e-6; = (1/var)*||X-X_r||^2 =  963423.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36590.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36590.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000017.3 e-6; = (1/var)*||X-X_r||^2 =  1000017.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963428.4 e-6; = (1/var)*||X-X_r||^2 =  963428.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36588.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36588.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000015.0 e-6; = (1/var)*||X-X_r||^2 =  1000015.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963490.5 e-6; = (1/var)*||X-X_r||^2 =  963490.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36524.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36524.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.0 e-6; = (1/var)*||X-X_r||^2 =  1000014.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963436.4 e-6; = (1/var)*||X-X_r||^2 =  963436.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36577.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36577.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.2 e-6; = (1/var)*||X-X_r||^2 =  1000014.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963439.1 e-6; = (1/var)*||X-X_r||^2 =  963439.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36575.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36575.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.9 e-6; = (1/var)*||X-X_r||^2 =  1000014.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963455.2 e-6; = (1/var)*||X-X_r||^2 =  963455.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36559.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36559.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000010.1 e-6; = (1/var)*||X-X_r||^2 =  1000010.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963443.6 e-6; = (1/var)*||X-X_r||^2 =  963443.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36566.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36566.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000015.4 e-6; = (1/var)*||X-X_r||^2 =  1000015.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963457.5 e-6; = (1/var)*||X-X_r||^2 =  963457.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36558.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36558.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000011.1 e-6; = (1/var)*||X-X_r||^2 =  1000011.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963433.1 e-6; = (1/var)*||X-X_r||^2 =  963433.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36578.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36578.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.83; perplexity/K = 4.42%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000014.1 e-6; = (1/var)*||X-X_r||^2 =  1000014.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963436.7 e-6; = (1/var)*||X-X_r||^2 =  963436.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999993.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963419.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36577.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36577.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [05:45:06 09.01.2023] 404) Finished running for K = 64 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:
Total training time is = 0:1:39 h/m/s. 

--------------------------------------------------- 

Started [05:45:06 09.01.2023] 405) Finished running for K = 64 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 674 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.19
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.75
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.44
5                                  encoder.pre_residual_stack.weight                       147            21.81
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.34
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.34
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
10                             encoder.channel_adjusting_conv.weight                         2             0.30
11                                                       VQ.E.weight                         1             0.15
12                             decoder.channel_adjusting_conv.weight                        18             2.67
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.34
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.34
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.44
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.75
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.19
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.81; perplexity/K = 13.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.08; perplexity/K = 12.62%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  714553.9 e-6; = (1/var)*||X-X_r||^2 =  607115.9 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  107438.0 e-6 = 15.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  642152.2 e-6; = (1/var)*||X-X_r||^2 =  591001.4 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  51150.7 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  668250.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  622383.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -72401.7 e-6; = (1/var)*||X-X_r||^2 val-train = -16114.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -56287.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.38; perplexity/K = 25.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.68; perplexity/K = 24.50%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  545208.8 e-6; = (1/var)*||X-X_r||^2 =  516376.4 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  28832.4 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  533759.2 e-6; = (1/var)*||X-X_r||^2 =  505844.2 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  27915.0 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  545208.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  530015.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11449.6 e-6; = (1/var)*||X-X_r||^2 val-train = -10532.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -917.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.92; perplexity/K = 29.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.72; perplexity/K = 27.69%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  507639.1 e-6; = (1/var)*||X-X_r||^2 =  478217.9 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  29421.3 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  506717.4 e-6; = (1/var)*||X-X_r||^2 =  476744.3 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  29973.1 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  505415.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  504701.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -921.7 e-6; = (1/var)*||X-X_r||^2 val-train = -1473.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 551.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.71; perplexity/K = 33.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.84; perplexity/K = 32.56%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  471116.7 e-6; = (1/var)*||X-X_r||^2 =  444376.3 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  26740.3 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  473230.8 e-6; = (1/var)*||X-X_r||^2 =  443399.0 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  29831.8 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  470982.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  468025.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2114.2 e-6; = (1/var)*||X-X_r||^2 val-train = -977.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3091.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.42; perplexity/K = 31.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.31; perplexity/K = 33.30%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  446341.1 e-6; = (1/var)*||X-X_r||^2 =  421949.1 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  24392.1 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  449417.9 e-6; = (1/var)*||X-X_r||^2 =  423441.2 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  25976.7 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  444099.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  446370.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3076.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1492.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1584.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.26; perplexity/K = 31.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.26; perplexity/K = 33.22%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  432439.3 e-6; = (1/var)*||X-X_r||^2 =  409350.8 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  23088.5 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  439466.4 e-6; = (1/var)*||X-X_r||^2 =  413901.2 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  25565.2 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  431422.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  436847.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7027.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4550.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2476.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.59; perplexity/K = 32.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.19; perplexity/K = 33.11%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  428778.0 e-6; = (1/var)*||X-X_r||^2 =  404807.7 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  23970.3 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  433190.7 e-6; = (1/var)*||X-X_r||^2 =  407611.3 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  25579.4 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  422359.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431520.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4412.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2803.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1609.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.93; perplexity/K = 34.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.29; perplexity/K = 33.26%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  412987.4 e-6; = (1/var)*||X-X_r||^2 =  393815.4 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  19172.0 e-6 = 4.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  430885.0 e-6; = (1/var)*||X-X_r||^2 =  405800.4 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  25084.6 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  412224.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425697.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17897.7 e-6; = (1/var)*||X-X_r||^2 val-train = 11985.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5912.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.99; perplexity/K = 34.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.58; perplexity/K = 33.71%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  408534.4 e-6; = (1/var)*||X-X_r||^2 =  388749.1 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  19785.4 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  426533.5 e-6; = (1/var)*||X-X_r||^2 =  400697.3 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  25836.2 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  406820.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  422414.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17999.0 e-6; = (1/var)*||X-X_r||^2 val-train = 11948.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6050.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.91; perplexity/K = 32.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.82; perplexity/K = 32.53%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  403327.8 e-6; = (1/var)*||X-X_r||^2 =  384339.1 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  18988.7 e-6 = 4.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  422096.4 e-6; = (1/var)*||X-X_r||^2 =  399314.7 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22781.7 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  400438.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  418677.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18768.7 e-6; = (1/var)*||X-X_r||^2 val-train = 14975.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3793.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.83; perplexity/K = 34.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.67; perplexity/K = 33.85%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  395974.0 e-6; = (1/var)*||X-X_r||^2 =  379360.2 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  16613.8 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  417557.8 e-6; = (1/var)*||X-X_r||^2 =  395989.9 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  21567.9 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  395974.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416815.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21583.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16629.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4954.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.58; perplexity/K = 33.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.47; perplexity/K = 35.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399521.1 e-6; = (1/var)*||X-X_r||^2 =  380386.8 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  19134.4 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  420214.7 e-6; = (1/var)*||X-X_r||^2 =  397602.2 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22612.4 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  391937.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415137.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20693.6 e-6; = (1/var)*||X-X_r||^2 val-train = 17215.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3478.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.39; perplexity/K = 34.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.53; perplexity/K = 35.21%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393216.6 e-6; = (1/var)*||X-X_r||^2 =  376357.1 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  16859.5 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  419534.3 e-6; = (1/var)*||X-X_r||^2 =  394960.9 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  24573.4 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  388884.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  411878.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26317.7 e-6; = (1/var)*||X-X_r||^2 val-train = 18603.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7713.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.32; perplexity/K = 34.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.46; perplexity/K = 35.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387465.4 e-6; = (1/var)*||X-X_r||^2 =  373614.9 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  13850.5 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  424358.6 e-6; = (1/var)*||X-X_r||^2 =  400716.5 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  23642.2 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  385449.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  410437.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36893.2 e-6; = (1/var)*||X-X_r||^2 val-train = 27101.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9791.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.32; perplexity/K = 36.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.34; perplexity/K = 34.91%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  386973.5 e-6; = (1/var)*||X-X_r||^2 =  373378.6 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  13594.9 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  412226.0 e-6; = (1/var)*||X-X_r||^2 =  392128.3 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  20097.7 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  382629.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  408430.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   25252.5 e-6; = (1/var)*||X-X_r||^2 val-train = 18749.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6502.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.02; perplexity/K = 34.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.16; perplexity/K = 36.19%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  381375.0 e-6; = (1/var)*||X-X_r||^2 =  369768.1 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11606.8 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  409351.9 e-6; = (1/var)*||X-X_r||^2 =  391863.0 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  17488.9 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  379124.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  407331.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27977.0 e-6; = (1/var)*||X-X_r||^2 val-train = 22094.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5882.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.19; perplexity/K = 34.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.39; perplexity/K = 34.98%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  378094.5 e-6; = (1/var)*||X-X_r||^2 =  367789.2 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  10305.3 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  407754.2 e-6; = (1/var)*||X-X_r||^2 =  391452.8 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16301.5 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  377303.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  406958.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29659.7 e-6; = (1/var)*||X-X_r||^2 val-train = 23663.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5996.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.59; perplexity/K = 35.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.82; perplexity/K = 35.66%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  396284.5 e-6; = (1/var)*||X-X_r||^2 =  378772.1 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  17512.5 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  424847.6 e-6; = (1/var)*||X-X_r||^2 =  403259.5 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  21588.2 e-6 = 5.1 %)
Min.  Avg. Train Loss across Mini-Batch =  377261.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  405743.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28563.1 e-6; = (1/var)*||X-X_r||^2 val-train = 24487.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4075.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.29; perplexity/K = 36.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.30; perplexity/K = 34.84%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  376624.4 e-6; = (1/var)*||X-X_r||^2 =  366474.5 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  10149.8 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  411376.3 e-6; = (1/var)*||X-X_r||^2 =  393195.9 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  18180.4 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  374975.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  405743.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34751.9 e-6; = (1/var)*||X-X_r||^2 val-train = 26721.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8030.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.18; perplexity/K = 36.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.90; perplexity/K = 35.79%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382642.4 e-6; = (1/var)*||X-X_r||^2 =  369581.1 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  13061.3 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  423479.4 e-6; = (1/var)*||X-X_r||^2 =  397844.6 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  25634.8 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  373075.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  403803.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40837.1 e-6; = (1/var)*||X-X_r||^2 val-train = 28263.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12573.5 e-6 

----------------------------------------------------------------------------------

Finished [06:35:53 09.01.2023] 405) Finished running for K = 64 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:
Total training time is = 0:1:46 h/m/s. 

--------------------------------------------------- 

Started [06:35:53 09.01.2023] 406) Finished running for K = 64 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 663 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.21
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.83
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.76
5                                  encoder.pre_residual_stack.weight                       147            22.17
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
10                             encoder.channel_adjusting_conv.weight                         1             0.15
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                         9             1.36
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.76
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.83
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.74; perplexity/K = 66.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.38; perplexity/K = 69.35%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  614971.7 e-6; = (1/var)*||X-X_r||^2 =  571898.6 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  43073.1 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  583385.1 e-6; = (1/var)*||X-X_r||^2 =  557734.3 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  25650.8 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  614971.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  583385.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31586.6 e-6; = (1/var)*||X-X_r||^2 val-train = -14164.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17422.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.45; perplexity/K = 67.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.03; perplexity/K = 68.80%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  547468.9 e-6; = (1/var)*||X-X_r||^2 =  506793.2 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  40675.7 e-6 = 7.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  537073.2 e-6; = (1/var)*||X-X_r||^2 =  501831.8 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  35241.4 e-6 = 6.6 %)
Min.  Avg. Train Loss across Mini-Batch =  538495.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  530113.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10395.8 e-6; = (1/var)*||X-X_r||^2 val-train = -4961.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5434.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.62; perplexity/K = 66.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.47; perplexity/K = 77.30%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  499165.1 e-6; = (1/var)*||X-X_r||^2 =  462440.8 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  36724.2 e-6 = 7.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  497402.1 e-6; = (1/var)*||X-X_r||^2 =  459719.7 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  37682.4 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  499165.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  495133.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1762.9 e-6; = (1/var)*||X-X_r||^2 val-train = -2721.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 958.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.96; perplexity/K = 74.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.07; perplexity/K = 79.80%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  470358.5 e-6; = (1/var)*||X-X_r||^2 =  431952.0 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  38406.5 e-6 = 8.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  473546.8 e-6; = (1/var)*||X-X_r||^2 =  431519.0 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  42027.8 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  470358.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  473546.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3188.3 e-6; = (1/var)*||X-X_r||^2 val-train = -433.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3621.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.01; perplexity/K = 79.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.17; perplexity/K = 75.27%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  445261.9 e-6; = (1/var)*||X-X_r||^2 =  411697.3 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  33564.7 e-6 = 7.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  457165.4 e-6; = (1/var)*||X-X_r||^2 =  418575.4 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  38590.0 e-6 = 8.4 %)
Min.  Avg. Train Loss across Mini-Batch =  445261.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  455572.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11903.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6878.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5025.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.52; perplexity/K = 82.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.99; perplexity/K = 84.36%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  430893.0 e-6; = (1/var)*||X-X_r||^2 =  401173.2 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  29719.8 e-6 = 6.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  447873.7 e-6; = (1/var)*||X-X_r||^2 =  411001.6 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  36872.2 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  430510.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  446280.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16980.8 e-6; = (1/var)*||X-X_r||^2 val-train = 9828.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7152.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.29; perplexity/K = 81.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.74; perplexity/K = 82.40%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  421587.9 e-6; = (1/var)*||X-X_r||^2 =  395038.6 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  26549.4 e-6 = 6.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  441419.0 e-6; = (1/var)*||X-X_r||^2 =  407407.1 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  34011.9 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  419386.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  440335.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19831.1 e-6; = (1/var)*||X-X_r||^2 val-train = 12368.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7462.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.38; perplexity/K = 78.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.79; perplexity/K = 80.93%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  422119.7 e-6; = (1/var)*||X-X_r||^2 =  393795.4 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  28324.4 e-6 = 6.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  439450.4 e-6; = (1/var)*||X-X_r||^2 =  404801.7 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  34648.7 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  412185.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  436085.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17330.7 e-6; = (1/var)*||X-X_r||^2 val-train = 11006.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6324.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.68; perplexity/K = 82.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.36; perplexity/K = 83.37%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  407487.7 e-6; = (1/var)*||X-X_r||^2 =  386035.5 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  21452.1 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  435026.7 e-6; = (1/var)*||X-X_r||^2 =  404379.1 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30647.7 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  404786.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432757.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27539.1 e-6; = (1/var)*||X-X_r||^2 val-train = 18343.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9195.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.69; perplexity/K = 82.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.64; perplexity/K = 80.68%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  400627.3 e-6; = (1/var)*||X-X_r||^2 =  381740.5 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  18886.8 e-6 = 4.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  432694.8 e-6; = (1/var)*||X-X_r||^2 =  403777.2 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  28917.7 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  399810.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  430054.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32067.6 e-6; = (1/var)*||X-X_r||^2 val-train = 22036.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10030.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.20; perplexity/K = 84.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.99; perplexity/K = 78.11%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  402310.8 e-6; = (1/var)*||X-X_r||^2 =  382319.2 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  19991.6 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  431925.3 e-6; = (1/var)*||X-X_r||^2 =  403385.1 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  28540.2 e-6 = 6.6 %)
Min.  Avg. Train Loss across Mini-Batch =  395975.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  428090.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29614.6 e-6; = (1/var)*||X-X_r||^2 val-train = 21066.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8548.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.85; perplexity/K = 79.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.50; perplexity/K = 80.47%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  392443.3 e-6; = (1/var)*||X-X_r||^2 =  376602.4 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  15840.9 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  429019.2 e-6; = (1/var)*||X-X_r||^2 =  403861.6 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  25157.6 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  391872.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425755.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36576.0 e-6; = (1/var)*||X-X_r||^2 val-train = 27259.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9316.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.05; perplexity/K = 84.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.11; perplexity/K = 76.73%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391037.7 e-6; = (1/var)*||X-X_r||^2 =  375639.6 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15398.2 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  430155.0 e-6; = (1/var)*||X-X_r||^2 =  404595.2 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  25559.8 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  389546.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425147.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39117.3 e-6; = (1/var)*||X-X_r||^2 val-train = 28955.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10161.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.25; perplexity/K = 83.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.48; perplexity/K = 82.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388396.2 e-6; = (1/var)*||X-X_r||^2 =  373769.8 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14626.4 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  427947.4 e-6; = (1/var)*||X-X_r||^2 =  404269.3 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  23678.0 e-6 = 5.5 %)
Min.  Avg. Train Loss across Mini-Batch =  387026.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424383.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39551.2 e-6; = (1/var)*||X-X_r||^2 val-train = 30499.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9051.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.82; perplexity/K = 87.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.19; perplexity/K = 83.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  385591.2 e-6; = (1/var)*||X-X_r||^2 =  370919.6 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14671.6 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  425240.4 e-6; = (1/var)*||X-X_r||^2 =  401571.6 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  23668.8 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  383823.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  420230.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39649.2 e-6; = (1/var)*||X-X_r||^2 val-train = 30652.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8997.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.98; perplexity/K = 84.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.36; perplexity/K = 81.82%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  384053.0 e-6; = (1/var)*||X-X_r||^2 =  369597.9 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14455.2 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  421915.2 e-6; = (1/var)*||X-X_r||^2 =  399002.2 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22913.0 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  381730.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  419487.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37862.2 e-6; = (1/var)*||X-X_r||^2 val-train = 29404.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8457.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.71; perplexity/K = 79.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.42; perplexity/K = 78.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382753.4 e-6; = (1/var)*||X-X_r||^2 =  368146.0 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14607.4 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  422949.1 e-6; = (1/var)*||X-X_r||^2 =  399898.1 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  23051.0 e-6 = 5.5 %)
Min.  Avg. Train Loss across Mini-Batch =  378723.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  418651.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40195.7 e-6; = (1/var)*||X-X_r||^2 val-train = 31752.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8443.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.66; perplexity/K = 83.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.02; perplexity/K = 82.84%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  383541.6 e-6; = (1/var)*||X-X_r||^2 =  368601.7 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  14939.9 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  422522.2 e-6; = (1/var)*||X-X_r||^2 =  400344.2 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  22178.0 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  377395.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416965.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38980.6 e-6; = (1/var)*||X-X_r||^2 val-train = 31742.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7238.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.43; perplexity/K = 85.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.04; perplexity/K = 78.18%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  376568.9 e-6; = (1/var)*||X-X_r||^2 =  364549.1 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  12019.8 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  417012.6 e-6; = (1/var)*||X-X_r||^2 =  396943.4 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  20069.2 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  375556.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415795.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40443.7 e-6; = (1/var)*||X-X_r||^2 val-train = 32394.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8049.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.61; perplexity/K = 82.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.00; perplexity/K = 79.68%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  374343.4 e-6; = (1/var)*||X-X_r||^2 =  363394.2 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  10949.2 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  415623.0 e-6; = (1/var)*||X-X_r||^2 =  396473.8 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  19149.2 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  374343.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  414417.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41279.6 e-6; = (1/var)*||X-X_r||^2 val-train = 33079.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8200.0 e-6 

----------------------------------------------------------------------------------

Finished [07:26:36 09.01.2023] 406) Finished running for K = 64 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:
Total training time is = 0:1:43 h/m/s. 

--------------------------------------------------- 

Started [07:26:36 09.01.2023] 407) Finished running for K = 32 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 690 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.16
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.64
3                           encoder.sequential_convs.conv2d_4.weight                       131            18.99
4                                  encoder.pre_residual_stack.weight                       147            21.30
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
9                              encoder.channel_adjusting_conv.weight                         4             0.58
10                                                       VQ.E.weight                         1             0.14
11                             decoder.channel_adjusting_conv.weight                        36             5.22
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.99
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.64
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.16
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.27; perplexity/K = 57.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.05; perplexity/K = 56.41%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  360070.7 e-6; = (1/var)*||X-X_r||^2 =  276178.4 e-6 = 76.7 %; (1+beta)*||Z_e-Z_q||^2 =  83892.3 e-6 = 23.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  354160.4 e-6; = (1/var)*||X-X_r||^2 =  277793.8 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  76366.6 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  360070.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  354085.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5910.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1615.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7525.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.65; perplexity/K = 64.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.33; perplexity/K = 63.53%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  161809.2 e-6; = (1/var)*||X-X_r||^2 =  130633.9 e-6 = 80.7 %; (1+beta)*||Z_e-Z_q||^2 =  31175.3 e-6 = 19.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  165875.7 e-6; = (1/var)*||X-X_r||^2 =  135911.4 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  29964.3 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  161809.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  165781.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4066.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5277.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1211.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.16; perplexity/K = 66.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.48; perplexity/K = 67.13%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  107214.4 e-6; = (1/var)*||X-X_r||^2 =  83116.6 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  24097.8 e-6 = 22.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  115681.1 e-6; = (1/var)*||X-X_r||^2 =  90630.6 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  25050.5 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  107071.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  113460.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8466.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7514.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 952.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.39; perplexity/K = 63.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.15; perplexity/K = 59.84%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  82673.0 e-6; = (1/var)*||X-X_r||^2 =  62990.4 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  19682.6 e-6 = 23.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  89275.2 e-6; = (1/var)*||X-X_r||^2 =  69002.1 e-6 = 77.3 %; (1+beta)*||Z_e-Z_q||^2 =  20273.0 e-6 = 22.7 %)
Min.  Avg. Train Loss across Mini-Batch =  80627.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  87157.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6602.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6011.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 590.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.26; perplexity/K = 69.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.53; perplexity/K = 64.17%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  65110.4 e-6; = (1/var)*||X-X_r||^2 =  51476.0 e-6 = 79.1 %; (1+beta)*||Z_e-Z_q||^2 =  13634.3 e-6 = 20.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  73723.2 e-6; = (1/var)*||X-X_r||^2 =  58129.7 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  15593.5 e-6 = 21.2 %)
Min.  Avg. Train Loss across Mini-Batch =  65110.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72890.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8612.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6653.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1959.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.83; perplexity/K = 68.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.28; perplexity/K = 63.37%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55905.2 e-6; = (1/var)*||X-X_r||^2 =  44698.0 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  11207.2 e-6 = 20.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  63221.6 e-6; = (1/var)*||X-X_r||^2 =  50692.2 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  12529.5 e-6 = 19.8 %)
Min.  Avg. Train Loss across Mini-Batch =  55905.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63221.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7316.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5994.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1322.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.45; perplexity/K = 63.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.94; perplexity/K = 68.57%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50934.9 e-6; = (1/var)*||X-X_r||^2 =  41360.6 e-6 = 81.2 %; (1+beta)*||Z_e-Z_q||^2 =  9574.3 e-6 = 18.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  59066.4 e-6; = (1/var)*||X-X_r||^2 =  48169.5 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  10896.9 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  50787.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  58105.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8131.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6808.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1322.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.25; perplexity/K = 66.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.68; perplexity/K = 64.63%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48102.7 e-6; = (1/var)*||X-X_r||^2 =  39425.0 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  8677.7 e-6 = 18.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  55256.0 e-6; = (1/var)*||X-X_r||^2 =  45427.3 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  9828.7 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  47402.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55101.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7153.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6002.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1151.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.01; perplexity/K = 65.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.87; perplexity/K = 68.34%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44856.1 e-6; = (1/var)*||X-X_r||^2 =  37283.4 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  7572.7 e-6 = 16.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  53061.1 e-6; = (1/var)*||X-X_r||^2 =  43741.6 e-6 = 82.4 %; (1+beta)*||Z_e-Z_q||^2 =  9319.5 e-6 = 17.6 %)
Min.  Avg. Train Loss across Mini-Batch =  44502.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51817.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8205.0 e-6; = (1/var)*||X-X_r||^2 val-train = 6458.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1746.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.98; perplexity/K = 65.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.19; perplexity/K = 63.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44035.2 e-6; = (1/var)*||X-X_r||^2 =  35882.9 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  8152.3 e-6 = 18.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  50713.7 e-6; = (1/var)*||X-X_r||^2 =  41604.9 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  9108.9 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  42651.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49955.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6678.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5722.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 956.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.88; perplexity/K = 68.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.49; perplexity/K = 67.16%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41353.6 e-6; = (1/var)*||X-X_r||^2 =  34975.3 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  6378.2 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  49051.3 e-6; = (1/var)*||X-X_r||^2 =  41293.9 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  7757.4 e-6 = 15.8 %)
Min.  Avg. Train Loss across Mini-Batch =  40866.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48286.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7697.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6318.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1379.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.47; perplexity/K = 67.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.65; perplexity/K = 67.65%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40148.1 e-6; = (1/var)*||X-X_r||^2 =  34156.3 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  5991.9 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  47168.0 e-6; = (1/var)*||X-X_r||^2 =  39828.1 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  7339.8 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  39755.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46901.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7019.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5671.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1348.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.25; perplexity/K = 66.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.07; perplexity/K = 65.85%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39520.4 e-6; = (1/var)*||X-X_r||^2 =  33206.9 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  6313.5 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  46296.1 e-6; = (1/var)*||X-X_r||^2 =  38729.8 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  7566.3 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  38804.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45709.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6775.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5522.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1252.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.52; perplexity/K = 70.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.71; perplexity/K = 64.72%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38282.6 e-6; = (1/var)*||X-X_r||^2 =  32990.5 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  5292.1 e-6 = 13.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  45293.9 e-6; = (1/var)*||X-X_r||^2 =  38549.5 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  6744.4 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  37800.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44623.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7011.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5559.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1452.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.45; perplexity/K = 63.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.88; perplexity/K = 65.24%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36792.0 e-6; = (1/var)*||X-X_r||^2 =  31904.9 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  4887.2 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  44372.0 e-6; = (1/var)*||X-X_r||^2 =  37929.1 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  6442.9 e-6 = 14.5 %)
Min.  Avg. Train Loss across Mini-Batch =  36682.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43594.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7580.0 e-6; = (1/var)*||X-X_r||^2 val-train = 6024.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1555.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.85; perplexity/K = 65.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.94; perplexity/K = 65.45%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36364.8 e-6; = (1/var)*||X-X_r||^2 =  31450.1 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  4914.7 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  43441.3 e-6; = (1/var)*||X-X_r||^2 =  37288.6 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  6152.7 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  36364.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43209.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7076.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5838.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1238.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.24; perplexity/K = 66.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.32; perplexity/K = 66.64%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36981.0 e-6; = (1/var)*||X-X_r||^2 =  31833.6 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  5147.3 e-6 = 13.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  42628.3 e-6; = (1/var)*||X-X_r||^2 =  36548.3 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  6080.0 e-6 = 14.3 %)
Min.  Avg. Train Loss across Mini-Batch =  35406.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42063.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5647.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4714.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 932.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.31; perplexity/K = 66.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.31; perplexity/K = 66.59%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35300.8 e-6; = (1/var)*||X-X_r||^2 =  30349.5 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  4951.3 e-6 = 14.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  41853.8 e-6; = (1/var)*||X-X_r||^2 =  35768.3 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  6085.5 e-6 = 14.5 %)
Min.  Avg. Train Loss across Mini-Batch =  35010.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41645.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6553.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5418.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1134.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.45; perplexity/K = 63.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.89; perplexity/K = 65.28%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34628.4 e-6; = (1/var)*||X-X_r||^2 =  30110.8 e-6 = 87.0 %; (1+beta)*||Z_e-Z_q||^2 =  4517.6 e-6 = 13.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  41147.3 e-6; = (1/var)*||X-X_r||^2 =  35442.6 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  5704.7 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  34322.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41052.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6518.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5331.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1187.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.12; perplexity/K = 66.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.20; perplexity/K = 66.24%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34951.0 e-6; = (1/var)*||X-X_r||^2 =  29674.5 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  5276.5 e-6 = 15.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  42016.4 e-6; = (1/var)*||X-X_r||^2 =  35773.6 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  6242.8 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  33942.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40709.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7065.4 e-6; = (1/var)*||X-X_r||^2 val-train = 6099.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 966.4 e-6 

----------------------------------------------------------------------------------

Finished [08:14:51 09.01.2023] 407) Finished running for K = 32 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:
Total training time is = 0:3:15 h/m/s. 

--------------------------------------------------- 

Started [08:14:51 09.01.2023] 408) Finished running for K = 32 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 669 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.20
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.78
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.58
4                                  encoder.pre_residual_stack.weight                       147            21.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
9                              encoder.channel_adjusting_conv.weight                         2             0.30
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        18             2.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.58
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.78
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.20
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.77; perplexity/K = 36.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 34.37%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  551964.0 e-6; = (1/var)*||X-X_r||^2 =  498009.5 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  53954.4 e-6 = 9.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  530952.6 e-6; = (1/var)*||X-X_r||^2 =  487460.1 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  43492.5 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  551964.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  530952.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21011.3 e-6; = (1/var)*||X-X_r||^2 val-train = -10549.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10461.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 38.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.62; perplexity/K = 36.33%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  427136.4 e-6; = (1/var)*||X-X_r||^2 =  405538.9 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  21597.5 e-6 = 5.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  411498.4 e-6; = (1/var)*||X-X_r||^2 =  392963.4 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  18535.0 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  417826.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  409408.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15638.1 e-6; = (1/var)*||X-X_r||^2 val-train = -12575.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3062.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.29; perplexity/K = 50.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.29; perplexity/K = 47.78%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382105.1 e-6; = (1/var)*||X-X_r||^2 =  371449.8 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  10655.3 e-6 = 2.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  374729.1 e-6; = (1/var)*||X-X_r||^2 =  364223.8 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  10505.3 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  382105.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  374729.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7376.1 e-6; = (1/var)*||X-X_r||^2 val-train = -7226.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -150.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.75; perplexity/K = 52.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.51; perplexity/K = 48.48%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  368467.5 e-6; = (1/var)*||X-X_r||^2 =  360033.3 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8434.1 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  361043.4 e-6; = (1/var)*||X-X_r||^2 =  351854.0 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  9189.5 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  368066.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  360563.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7424.1 e-6; = (1/var)*||X-X_r||^2 val-train = -8179.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 755.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.44; perplexity/K = 54.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.10; perplexity/K = 53.44%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  359424.9 e-6; = (1/var)*||X-X_r||^2 =  353427.9 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  5997.1 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  350955.3 e-6; = (1/var)*||X-X_r||^2 =  344827.4 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6127.9 e-6 = 1.7 %)
Min.  Avg. Train Loss across Mini-Batch =  359424.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  350955.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8469.6 e-6; = (1/var)*||X-X_r||^2 val-train = -8600.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 130.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.43; perplexity/K = 51.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.31; perplexity/K = 54.11%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  353691.8 e-6; = (1/var)*||X-X_r||^2 =  348868.5 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  4823.3 e-6 = 1.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  346667.1 e-6; = (1/var)*||X-X_r||^2 =  341345.6 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5321.5 e-6 = 1.5 %)
Min.  Avg. Train Loss across Mini-Batch =  353375.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  345292.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7024.7 e-6; = (1/var)*||X-X_r||^2 val-train = -7523.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 498.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.67; perplexity/K = 48.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.55; perplexity/K = 48.59%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  368284.8 e-6; = (1/var)*||X-X_r||^2 =  360111.6 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  8173.1 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  355069.9 e-6; = (1/var)*||X-X_r||^2 =  347559.4 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7510.5 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  349724.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  341627.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13214.9 e-6; = (1/var)*||X-X_r||^2 val-train = -12552.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -662.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.02; perplexity/K = 53.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.22; perplexity/K = 53.80%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347249.1 e-6; = (1/var)*||X-X_r||^2 =  344164.5 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3084.6 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  338712.1 e-6; = (1/var)*||X-X_r||^2 =  335179.5 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3532.6 e-6 = 1.0 %)
Min.  Avg. Train Loss across Mini-Batch =  347248.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  338619.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8537.0 e-6; = (1/var)*||X-X_r||^2 val-train = -8985.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 448.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.19; perplexity/K = 53.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.27; perplexity/K = 53.96%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346436.2 e-6; = (1/var)*||X-X_r||^2 =  343629.5 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  2806.6 e-6 = 0.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  338301.1 e-6; = (1/var)*||X-X_r||^2 =  335066.5 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3234.7 e-6 = 1.0 %)
Min.  Avg. Train Loss across Mini-Batch =  345262.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  336600.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8135.0 e-6; = (1/var)*||X-X_r||^2 val-train = -8563.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 428.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.75; perplexity/K = 52.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.23; perplexity/K = 53.83%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  343624.1 e-6; = (1/var)*||X-X_r||^2 =  341319.3 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  2304.8 e-6 = 0.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  335029.1 e-6; = (1/var)*||X-X_r||^2 =  332147.1 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  2882.0 e-6 = 0.9 %)
Min.  Avg. Train Loss across Mini-Batch =  343624.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  334920.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8595.0 e-6; = (1/var)*||X-X_r||^2 val-train = -9172.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 577.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.93; perplexity/K = 52.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.93; perplexity/K = 52.91%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  343023.3 e-6; = (1/var)*||X-X_r||^2 =  340980.3 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  2043.0 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  334335.5 e-6; = (1/var)*||X-X_r||^2 =  331755.9 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  2579.6 e-6 = 0.8 %)
Min.  Avg. Train Loss across Mini-Batch =  342727.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  333995.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8687.8 e-6; = (1/var)*||X-X_r||^2 val-train = -9224.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 536.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.64; perplexity/K = 55.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.86; perplexity/K = 52.69%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  342340.2 e-6; = (1/var)*||X-X_r||^2 =  340498.3 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  1841.9 e-6 = 0.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  333868.0 e-6; = (1/var)*||X-X_r||^2 =  331606.5 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  2261.5 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  341982.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  333216.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8472.2 e-6; = (1/var)*||X-X_r||^2 val-train = -8891.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 419.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.05; perplexity/K = 56.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.10; perplexity/K = 56.56%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  338307.0 e-6; = (1/var)*||X-X_r||^2 =  335210.0 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3097.0 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  325906.9 e-6; = (1/var)*||X-X_r||^2 =  321042.1 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  4864.8 e-6 = 1.5 %)
Min.  Avg. Train Loss across Mini-Batch =  338307.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  325906.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12400.2 e-6; = (1/var)*||X-X_r||^2 val-train = -14167.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1767.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.46; perplexity/K = 57.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.31; perplexity/K = 57.21%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  204687.8 e-6; = (1/var)*||X-X_r||^2 =  200082.5 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  4605.3 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  199232.6 e-6; = (1/var)*||X-X_r||^2 =  194269.8 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  4962.8 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  204654.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  199232.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5455.2 e-6; = (1/var)*||X-X_r||^2 val-train = -5812.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 357.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.59; perplexity/K = 58.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.18; perplexity/K = 56.81%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  199854.5 e-6; = (1/var)*||X-X_r||^2 =  195963.3 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  3891.2 e-6 = 1.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  195439.0 e-6; = (1/var)*||X-X_r||^2 =  190644.3 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  4794.7 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  199854.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  194567.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4415.4 e-6; = (1/var)*||X-X_r||^2 val-train = -5318.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 903.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.74; perplexity/K = 58.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.65; perplexity/K = 61.41%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  197369.9 e-6; = (1/var)*||X-X_r||^2 =  193769.1 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  3600.9 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  191662.3 e-6; = (1/var)*||X-X_r||^2 =  187598.8 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  4063.5 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  197043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  191588.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5707.6 e-6; = (1/var)*||X-X_r||^2 val-train = -6170.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 462.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.29; perplexity/K = 66.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.03; perplexity/K = 65.72%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  194326.5 e-6; = (1/var)*||X-X_r||^2 =  190826.4 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  3500.1 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  188910.1 e-6; = (1/var)*||X-X_r||^2 =  184749.6 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  4160.4 e-6 = 2.2 %)
Min.  Avg. Train Loss across Mini-Batch =  194092.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  188325.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5416.4 e-6; = (1/var)*||X-X_r||^2 val-train = -6076.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 660.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.71; perplexity/K = 67.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.59; perplexity/K = 64.35%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  191864.5 e-6; = (1/var)*||X-X_r||^2 =  188439.6 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  3424.9 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  185471.0 e-6; = (1/var)*||X-X_r||^2 =  181741.9 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  3729.2 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  191521.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  185471.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6393.4 e-6; = (1/var)*||X-X_r||^2 val-train = -6697.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 304.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.42; perplexity/K = 66.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.03; perplexity/K = 65.71%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  187938.8 e-6; = (1/var)*||X-X_r||^2 =  184860.5 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  3078.3 e-6 = 1.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  182491.9 e-6; = (1/var)*||X-X_r||^2 =  179014.9 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  3477.0 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  187938.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  182491.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5446.9 e-6; = (1/var)*||X-X_r||^2 val-train = -5845.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 398.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.85; perplexity/K = 68.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.82; perplexity/K = 68.19%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  183713.9 e-6; = (1/var)*||X-X_r||^2 =  180529.1 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  3184.8 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  177924.9 e-6; = (1/var)*||X-X_r||^2 =  174340.3 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  3584.6 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  183713.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  177924.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5789.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6188.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 399.8 e-6 

----------------------------------------------------------------------------------

Finished [09:02:48 09.01.2023] 408) Finished running for K = 32 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:
Total training time is = 0:3:56 h/m/s. 

--------------------------------------------------- 

Started [09:02:48 09.01.2023] 409) Finished running for K = 32 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 659 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.21
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.86
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.88
4                                  encoder.pre_residual_stack.weight                       147            22.31
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
9                              encoder.channel_adjusting_conv.weight                         1             0.15
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                         9             1.37
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.88
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.86
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.16; perplexity/K = 53.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.23; perplexity/K = 50.73%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  370952.2 e-6; = (1/var)*||X-X_r||^2 =  251075.4 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  119876.8 e-6 = 32.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  364071.0 e-6; = (1/var)*||X-X_r||^2 =  251033.5 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  113037.5 e-6 = 31.0 %)
Min.  Avg. Train Loss across Mini-Batch =  370867.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  361737.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6881.2 e-6; = (1/var)*||X-X_r||^2 val-train = -41.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6839.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.57; perplexity/K = 54.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.90; perplexity/K = 55.95%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  174178.2 e-6; = (1/var)*||X-X_r||^2 =  132547.6 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  41630.6 e-6 = 23.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  179027.8 e-6; = (1/var)*||X-X_r||^2 =  136943.1 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  42084.8 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  174178.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  178934.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4849.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4395.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 454.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.89; perplexity/K = 55.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.19; perplexity/K = 56.84%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  121022.7 e-6; = (1/var)*||X-X_r||^2 =  93704.1 e-6 = 77.4 %; (1+beta)*||Z_e-Z_q||^2 =  27318.6 e-6 = 22.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  125429.8 e-6; = (1/var)*||X-X_r||^2 =  98771.1 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  26658.6 e-6 = 21.3 %)
Min.  Avg. Train Loss across Mini-Batch =  119027.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  125223.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4407.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5067.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -659.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.79; perplexity/K = 58.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.38; perplexity/K = 57.43%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90656.6 e-6; = (1/var)*||X-X_r||^2 =  74564.3 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  16092.2 e-6 = 17.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  98744.7 e-6; = (1/var)*||X-X_r||^2 =  80916.3 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  17828.4 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  90656.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  98093.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8088.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6352.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1736.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.83; perplexity/K = 61.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.99; perplexity/K = 59.35%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  76579.4 e-6; = (1/var)*||X-X_r||^2 =  64624.2 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  11955.2 e-6 = 15.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  83829.6 e-6; = (1/var)*||X-X_r||^2 =  70720.7 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  13108.8 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  76579.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  83829.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7250.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6096.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1153.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.33; perplexity/K = 69.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.88; perplexity/K = 65.26%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  66666.4 e-6; = (1/var)*||X-X_r||^2 =  57152.4 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  9514.1 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  75335.3 e-6; = (1/var)*||X-X_r||^2 =  64344.3 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  10991.0 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  66666.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74514.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8668.9 e-6; = (1/var)*||X-X_r||^2 val-train = 7192.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1476.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.63; perplexity/K = 67.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.02; perplexity/K = 65.69%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60796.3 e-6; = (1/var)*||X-X_r||^2 =  51087.6 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  9708.7 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  67885.4 e-6; = (1/var)*||X-X_r||^2 =  57236.0 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  10649.4 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  59811.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67655.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7089.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6148.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 940.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.75; perplexity/K = 67.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.04; perplexity/K = 65.74%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55293.6 e-6; = (1/var)*||X-X_r||^2 =  47397.5 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  7896.1 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  63569.4 e-6; = (1/var)*||X-X_r||^2 =  54314.0 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  9255.4 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  54876.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63016.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8275.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6916.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1359.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.92; perplexity/K = 65.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.58; perplexity/K = 64.33%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50783.8 e-6; = (1/var)*||X-X_r||^2 =  43569.5 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  7214.3 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  59754.4 e-6; = (1/var)*||X-X_r||^2 =  50591.0 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  9163.5 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  50783.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  59092.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8970.6 e-6; = (1/var)*||X-X_r||^2 val-train = 7021.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1949.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.16; perplexity/K = 66.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.54; perplexity/K = 67.30%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47812.2 e-6; = (1/var)*||X-X_r||^2 =  41087.4 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  6724.8 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  56144.4 e-6; = (1/var)*||X-X_r||^2 =  47929.0 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  8215.4 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  47705.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56144.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8332.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6841.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1490.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.81; perplexity/K = 68.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.12; perplexity/K = 65.99%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44556.3 e-6; = (1/var)*||X-X_r||^2 =  38289.7 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  6266.7 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  53400.8 e-6; = (1/var)*||X-X_r||^2 =  45428.6 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  7972.2 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  44556.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52700.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8844.5 e-6; = (1/var)*||X-X_r||^2 val-train = 7139.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1705.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.59; perplexity/K = 67.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.20; perplexity/K = 66.24%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42575.7 e-6; = (1/var)*||X-X_r||^2 =  36328.9 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  6246.8 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  50951.0 e-6; = (1/var)*||X-X_r||^2 =  43128.7 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  7822.2 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  42200.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50338.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8375.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6799.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1575.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.39; perplexity/K = 66.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.68; perplexity/K = 64.63%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39906.4 e-6; = (1/var)*||X-X_r||^2 =  34128.6 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  5777.8 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  48921.1 e-6; = (1/var)*||X-X_r||^2 =  41486.4 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  7434.7 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  39784.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48561.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9014.6 e-6; = (1/var)*||X-X_r||^2 val-train = 7357.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1656.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.73; perplexity/K = 67.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.73; perplexity/K = 64.80%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38333.0 e-6; = (1/var)*||X-X_r||^2 =  32875.6 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  5457.3 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  47764.0 e-6; = (1/var)*||X-X_r||^2 =  40630.5 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  7133.6 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  38224.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46421.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9431.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7754.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1676.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.09; perplexity/K = 65.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.35; perplexity/K = 66.72%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37307.7 e-6; = (1/var)*||X-X_r||^2 =  32047.9 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  5259.8 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  45631.3 e-6; = (1/var)*||X-X_r||^2 =  38941.1 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  6690.1 e-6 = 14.7 %)
Min.  Avg. Train Loss across Mini-Batch =  36892.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45421.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8323.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6893.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1430.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.94; perplexity/K = 65.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.69; perplexity/K = 67.77%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36332.6 e-6; = (1/var)*||X-X_r||^2 =  31283.9 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  5048.7 e-6 = 13.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  46468.1 e-6; = (1/var)*||X-X_r||^2 =  39691.7 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  6776.4 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  35981.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44191.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10135.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8407.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1727.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.13; perplexity/K = 69.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.52; perplexity/K = 67.24%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37909.4 e-6; = (1/var)*||X-X_r||^2 =  31659.9 e-6 = 83.5 %; (1+beta)*||Z_e-Z_q||^2 =  6249.5 e-6 = 16.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  44264.7 e-6; = (1/var)*||X-X_r||^2 =  37092.7 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  7172.0 e-6 = 16.2 %)
Min.  Avg. Train Loss across Mini-Batch =  35023.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43049.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6355.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5432.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 922.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.14; perplexity/K = 66.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.52; perplexity/K = 64.12%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34845.8 e-6; = (1/var)*||X-X_r||^2 =  30136.4 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  4709.4 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  49155.9 e-6; = (1/var)*||X-X_r||^2 =  41627.2 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  7528.7 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  34205.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42784.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14310.1 e-6; = (1/var)*||X-X_r||^2 val-train = 11490.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2819.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.23; perplexity/K = 66.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.73; perplexity/K = 64.80%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50022.7 e-6; = (1/var)*||X-X_r||^2 =  39889.3 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  10133.4 e-6 = 20.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  48850.3 e-6; = (1/var)*||X-X_r||^2 =  40138.6 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  8711.7 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  33876.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42784.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1172.4 e-6; = (1/var)*||X-X_r||^2 val-train = 249.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1421.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.42; perplexity/K = 66.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.12; perplexity/K = 66.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33621.8 e-6; = (1/var)*||X-X_r||^2 =  29336.1 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  4285.7 e-6 = 12.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  44376.7 e-6; = (1/var)*||X-X_r||^2 =  38385.7 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  5991.0 e-6 = 13.5 %)
Min.  Avg. Train Loss across Mini-Batch =  33214.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42087.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10754.9 e-6; = (1/var)*||X-X_r||^2 val-train = 9049.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1705.3 e-6 

----------------------------------------------------------------------------------

Finished [09:50:40 09.01.2023] 409) Finished running for K = 32 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 80) change_channel_size_across_layers = True:
Total training time is = 0:3:52 h/m/s. 

--------------------------------------------------- 

Started [09:50:40 09.01.2023] 410) Finished running for K = 32 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 694 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.29
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.15
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.61
4                           encoder.sequential_convs.conv2d_5.weight                       131            18.88
5                                  encoder.pre_residual_stack.weight                       147            21.18
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
10                             encoder.channel_adjusting_conv.weight                         4             0.58
11                                                       VQ.E.weight                         1             0.14
12                             decoder.channel_adjusting_conv.weight                        36             5.19
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.88
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.61
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.15
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.29
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 3.12%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000866.0 e-6; = (1/var)*||X-X_r||^2 =  1000859.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963692.8 e-6; = (1/var)*||X-X_r||^2 =  963686.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000866.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963361.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -37173.2 e-6; = (1/var)*||X-X_r||^2 val-train = -37173.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.22; perplexity/K = 56.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.14; perplexity/K = 59.80%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  847667.8 e-6; = (1/var)*||X-X_r||^2 =  598420.7 e-6 = 70.6 %; (1+beta)*||Z_e-Z_q||^2 =  249247.2 e-6 = 29.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  752693.5 e-6; = (1/var)*||X-X_r||^2 =  577945.7 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  174747.8 e-6 = 23.2 %)
Min.  Avg. Train Loss across Mini-Batch =  780608.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  687445.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -94974.4 e-6; = (1/var)*||X-X_r||^2 val-train = -20475.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -74499.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.84; perplexity/K = 65.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.96; perplexity/K = 65.50%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  574517.2 e-6; = (1/var)*||X-X_r||^2 =  510150.5 e-6 = 88.8 %; (1+beta)*||Z_e-Z_q||^2 =  64366.7 e-6 = 11.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  551641.5 e-6; = (1/var)*||X-X_r||^2 =  504221.1 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  47420.5 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  562556.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  547239.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22875.7 e-6; = (1/var)*||X-X_r||^2 val-train = -5929.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -16946.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.44; perplexity/K = 67.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.44; perplexity/K = 67.00%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  506388.8 e-6; = (1/var)*||X-X_r||^2 =  460601.0 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  45787.9 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  502513.7 e-6; = (1/var)*||X-X_r||^2 =  458725.2 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  43788.5 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  505510.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  502513.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3875.1 e-6; = (1/var)*||X-X_r||^2 val-train = -1875.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1999.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.41; perplexity/K = 73.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.90; perplexity/K = 77.81%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  476502.6 e-6; = (1/var)*||X-X_r||^2 =  431302.0 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  45200.6 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  489349.9 e-6; = (1/var)*||X-X_r||^2 =  438441.7 e-6 = 89.6 %; (1+beta)*||Z_e-Z_q||^2 =  50908.1 e-6 = 10.4 %)
Min.  Avg. Train Loss across Mini-Batch =  474509.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  482099.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12847.3 e-6; = (1/var)*||X-X_r||^2 val-train = 7139.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5707.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.61; perplexity/K = 76.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.51; perplexity/K = 79.73%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  458312.8 e-6; = (1/var)*||X-X_r||^2 =  415002.6 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  43310.2 e-6 = 9.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  470925.4 e-6; = (1/var)*||X-X_r||^2 =  423956.1 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  46969.3 e-6 = 10.0 %)
Min.  Avg. Train Loss across Mini-Batch =  453157.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  466838.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12612.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8953.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3659.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.00; perplexity/K = 74.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.57; perplexity/K = 64.29%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  439434.0 e-6; = (1/var)*||X-X_r||^2 =  400897.9 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  38536.1 e-6 = 8.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  458883.8 e-6; = (1/var)*||X-X_r||^2 =  417679.6 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  41204.2 e-6 = 9.0 %)
Min.  Avg. Train Loss across Mini-Batch =  435868.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  457231.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19449.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16781.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2668.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.60; perplexity/K = 80.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.10; perplexity/K = 75.31%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  428982.7 e-6; = (1/var)*||X-X_r||^2 =  392445.7 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  36537.1 e-6 = 8.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  458203.1 e-6; = (1/var)*||X-X_r||^2 =  417518.7 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  40684.4 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  425978.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  454341.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29220.3 e-6; = (1/var)*||X-X_r||^2 val-train = 25073.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4147.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.49; perplexity/K = 70.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.85; perplexity/K = 71.40%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  417098.5 e-6; = (1/var)*||X-X_r||^2 =  384357.7 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  32740.8 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  457742.5 e-6; = (1/var)*||X-X_r||^2 =  419313.9 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  38428.6 e-6 = 8.4 %)
Min.  Avg. Train Loss across Mini-Batch =  417098.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  452411.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40644.0 e-6; = (1/var)*||X-X_r||^2 val-train = 34956.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5687.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.29; perplexity/K = 79.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.02; perplexity/K = 71.93%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  413059.6 e-6; = (1/var)*||X-X_r||^2 =  381280.2 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  31779.4 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  453906.8 e-6; = (1/var)*||X-X_r||^2 =  416499.3 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  37407.5 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  410301.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  450734.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40847.2 e-6; = (1/var)*||X-X_r||^2 val-train = 35219.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5628.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.00; perplexity/K = 68.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.51; perplexity/K = 76.58%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405625.9 e-6; = (1/var)*||X-X_r||^2 =  376761.8 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  28864.1 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  446428.8 e-6; = (1/var)*||X-X_r||^2 =  411032.6 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  35396.2 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  404817.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  445527.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40802.9 e-6; = (1/var)*||X-X_r||^2 val-train = 34270.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6532.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.29; perplexity/K = 75.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.99; perplexity/K = 74.96%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399241.9 e-6; = (1/var)*||X-X_r||^2 =  373023.3 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  26218.6 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  449720.4 e-6; = (1/var)*||X-X_r||^2 =  416381.1 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  33339.3 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  399241.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  443935.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50478.5 e-6; = (1/var)*||X-X_r||^2 val-train = 43357.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7120.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.01; perplexity/K = 78.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.62; perplexity/K = 73.83%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399964.6 e-6; = (1/var)*||X-X_r||^2 =  373477.6 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  26486.9 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  446385.5 e-6; = (1/var)*||X-X_r||^2 =  413143.0 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  33242.6 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  395497.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  441453.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46421.0 e-6; = (1/var)*||X-X_r||^2 val-train = 39665.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6755.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.34; perplexity/K = 72.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.97; perplexity/K = 74.91%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  395363.9 e-6; = (1/var)*||X-X_r||^2 =  371247.3 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  24116.6 e-6 = 6.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  444621.8 e-6; = (1/var)*||X-X_r||^2 =  413697.2 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30924.6 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  391759.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  438658.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   49258.0 e-6; = (1/var)*||X-X_r||^2 val-train = 42449.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6808.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.89; perplexity/K = 77.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.98; perplexity/K = 74.92%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391625.3 e-6; = (1/var)*||X-X_r||^2 =  369278.6 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  22346.7 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  445655.9 e-6; = (1/var)*||X-X_r||^2 =  416401.6 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  29254.3 e-6 = 6.6 %)
Min.  Avg. Train Loss across Mini-Batch =  389099.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  438658.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54030.7 e-6; = (1/var)*||X-X_r||^2 val-train = 47123.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6907.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.40; perplexity/K = 76.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.36; perplexity/K = 73.01%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  385660.4 e-6; = (1/var)*||X-X_r||^2 =  365919.4 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  19741.0 e-6 = 5.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  443503.8 e-6; = (1/var)*||X-X_r||^2 =  416225.1 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  27278.7 e-6 = 6.2 %)
Min.  Avg. Train Loss across Mini-Batch =  385660.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  437991.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57843.4 e-6; = (1/var)*||X-X_r||^2 val-train = 50305.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7537.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.01; perplexity/K = 78.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.49; perplexity/K = 76.53%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  385744.4 e-6; = (1/var)*||X-X_r||^2 =  365294.0 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  20450.4 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  436150.3 e-6; = (1/var)*||X-X_r||^2 =  408685.2 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  27465.1 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  384398.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  435421.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50405.9 e-6; = (1/var)*||X-X_r||^2 val-train = 43391.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7014.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.14; perplexity/K = 72.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.67; perplexity/K = 70.84%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382885.1 e-6; = (1/var)*||X-X_r||^2 =  364395.2 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  18490.0 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  440633.5 e-6; = (1/var)*||X-X_r||^2 =  414330.9 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  26302.6 e-6 = 6.0 %)
Min.  Avg. Train Loss across Mini-Batch =  381102.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  434207.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57748.4 e-6; = (1/var)*||X-X_r||^2 val-train = 49935.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7812.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.16; perplexity/K = 75.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.15; perplexity/K = 72.33%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  381927.1 e-6; = (1/var)*||X-X_r||^2 =  364228.1 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  17699.0 e-6 = 4.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  440878.3 e-6; = (1/var)*||X-X_r||^2 =  415844.9 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  25033.3 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  379468.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  434207.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58951.2 e-6; = (1/var)*||X-X_r||^2 val-train = 51616.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7334.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.72; perplexity/K = 74.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.85; perplexity/K = 77.65%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390832.2 e-6; = (1/var)*||X-X_r||^2 =  370100.5 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  20731.7 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  467739.7 e-6; = (1/var)*||X-X_r||^2 =  436598.6 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  31141.1 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  377110.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431162.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   76907.5 e-6; = (1/var)*||X-X_r||^2 val-train = 66498.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10409.4 e-6 

----------------------------------------------------------------------------------

Finished [10:41:34 09.01.2023] 410) Finished running for K = 32 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:
Total training time is = 0:1:53 h/m/s. 

--------------------------------------------------- 

Started [10:41:34 09.01.2023] 411) Finished running for K = 32 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 673 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.19
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.75
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.47
5                                  encoder.pre_residual_stack.weight                       147            21.84
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.35
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.35
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
10                             encoder.channel_adjusting_conv.weight                         2             0.30
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                        18             2.67
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.35
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.35
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.47
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.75
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.19
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.44; perplexity/K = 26.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.64; perplexity/K = 27.01%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1118106.7 e-6; = (1/var)*||X-X_r||^2 =  661583.9 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  456522.8 e-6 = 40.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  696138.0 e-6; = (1/var)*||X-X_r||^2 =  626756.8 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  69381.2 e-6 = 10.0 %)
Min.  Avg. Train Loss across Mini-Batch =  844431.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  648886.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -421968.7 e-6; = (1/var)*||X-X_r||^2 val-train = -34827.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -387141.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.65; perplexity/K = 67.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.90; perplexity/K = 65.31%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  554073.4 e-6; = (1/var)*||X-X_r||^2 =  521947.1 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  32126.3 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  543215.3 e-6; = (1/var)*||X-X_r||^2 =  508858.3 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  34357.0 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  552233.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  533627.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10858.1 e-6; = (1/var)*||X-X_r||^2 val-train = -13088.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2230.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.20; perplexity/K = 72.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.35; perplexity/K = 76.10%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  502149.0 e-6; = (1/var)*||X-X_r||^2 =  467169.4 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  34979.6 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  493384.9 e-6; = (1/var)*||X-X_r||^2 =  457702.6 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  35682.3 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  497226.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  489488.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8764.0 e-6; = (1/var)*||X-X_r||^2 val-train = -9466.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 702.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.10; perplexity/K = 78.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.12; perplexity/K = 72.24%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  479559.8 e-6; = (1/var)*||X-X_r||^2 =  437187.2 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  42372.6 e-6 = 8.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  478367.7 e-6; = (1/var)*||X-X_r||^2 =  434518.6 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  43849.1 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  479559.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  475523.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1192.2 e-6; = (1/var)*||X-X_r||^2 val-train = -2668.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1476.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.19; perplexity/K = 78.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.48; perplexity/K = 70.25%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  468356.3 e-6; = (1/var)*||X-X_r||^2 =  427504.8 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  40851.5 e-6 = 8.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  484294.9 e-6; = (1/var)*||X-X_r||^2 =  432888.6 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  51406.3 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  458080.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  462065.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15938.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5383.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10554.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.05; perplexity/K = 81.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.67; perplexity/K = 80.22%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  445209.7 e-6; = (1/var)*||X-X_r||^2 =  411589.8 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  33619.9 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  456226.8 e-6; = (1/var)*||X-X_r||^2 =  415645.7 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  40581.1 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  441944.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  450909.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11017.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4055.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6961.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.75; perplexity/K = 77.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.66; perplexity/K = 77.07%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  433039.2 e-6; = (1/var)*||X-X_r||^2 =  404498.1 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  28541.1 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  446302.7 e-6; = (1/var)*||X-X_r||^2 =  411719.4 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  34583.3 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  431750.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  443473.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13263.5 e-6; = (1/var)*||X-X_r||^2 val-train = 7221.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6042.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.90; perplexity/K = 74.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.32; perplexity/K = 79.11%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  424440.7 e-6; = (1/var)*||X-X_r||^2 =  399161.6 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  25279.1 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  439894.9 e-6; = (1/var)*||X-X_r||^2 =  407655.3 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  32239.6 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  423023.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  438215.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15454.2 e-6; = (1/var)*||X-X_r||^2 val-train = 8493.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6960.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.39; perplexity/K = 79.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.87; perplexity/K = 77.71%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  417984.0 e-6; = (1/var)*||X-X_r||^2 =  395234.0 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22750.0 e-6 = 5.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  435737.9 e-6; = (1/var)*||X-X_r||^2 =  405956.1 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  29781.8 e-6 = 6.8 %)
Min.  Avg. Train Loss across Mini-Batch =  416853.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  434580.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17753.8 e-6; = (1/var)*||X-X_r||^2 val-train = 10722.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7031.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.54; perplexity/K = 76.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.35; perplexity/K = 79.20%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  411896.3 e-6; = (1/var)*||X-X_r||^2 =  392174.3 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  19722.0 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  437674.5 e-6; = (1/var)*||X-X_r||^2 =  407127.4 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30547.1 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  411322.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432000.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   25778.1 e-6; = (1/var)*||X-X_r||^2 val-train = 14953.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10825.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.63; perplexity/K = 80.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.95; perplexity/K = 77.97%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410620.5 e-6; = (1/var)*||X-X_r||^2 =  390891.7 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  19728.9 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  434101.1 e-6; = (1/var)*||X-X_r||^2 =  406854.2 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  27246.9 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  407357.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431222.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23480.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15962.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7518.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.31; perplexity/K = 75.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.13; perplexity/K = 78.53%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  406876.6 e-6; = (1/var)*||X-X_r||^2 =  389021.8 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  17854.7 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  431878.0 e-6; = (1/var)*||X-X_r||^2 =  405505.0 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  26373.0 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  404934.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  429595.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   25001.4 e-6; = (1/var)*||X-X_r||^2 val-train = 16483.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8518.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.36; perplexity/K = 76.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.40; perplexity/K = 76.24%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  403493.1 e-6; = (1/var)*||X-X_r||^2 =  386854.0 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  16639.1 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  430608.2 e-6; = (1/var)*||X-X_r||^2 =  405875.2 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  24733.1 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  402217.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  429157.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27115.1 e-6; = (1/var)*||X-X_r||^2 val-train = 19021.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8094.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.62; perplexity/K = 76.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.49; perplexity/K = 76.55%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  400952.6 e-6; = (1/var)*||X-X_r||^2 =  385191.6 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15761.0 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  434465.4 e-6; = (1/var)*||X-X_r||^2 =  406900.8 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  27564.6 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  399215.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426823.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33512.8 e-6; = (1/var)*||X-X_r||^2 val-train = 21709.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11803.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.11; perplexity/K = 72.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.48; perplexity/K = 73.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399054.3 e-6; = (1/var)*||X-X_r||^2 =  384218.8 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14835.5 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  428781.2 e-6; = (1/var)*||X-X_r||^2 =  405579.7 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  23201.4 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  397844.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425540.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29726.8 e-6; = (1/var)*||X-X_r||^2 val-train = 21360.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8365.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.75; perplexity/K = 77.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.60; perplexity/K = 79.99%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  398131.4 e-6; = (1/var)*||X-X_r||^2 =  383357.3 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14774.1 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  427872.2 e-6; = (1/var)*||X-X_r||^2 =  405206.1 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  22666.0 e-6 = 5.3 %)
Min.  Avg. Train Loss across Mini-Batch =  396455.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425540.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29740.8 e-6; = (1/var)*||X-X_r||^2 val-train = 21848.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7892.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.40; perplexity/K = 76.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.05; perplexity/K = 78.29%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  394645.5 e-6; = (1/var)*||X-X_r||^2 =  381189.7 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  13455.8 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  426300.3 e-6; = (1/var)*||X-X_r||^2 =  404872.4 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  21427.9 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  394374.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424185.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31654.8 e-6; = (1/var)*||X-X_r||^2 val-train = 23682.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7972.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.15; perplexity/K = 81.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.78; perplexity/K = 77.43%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  396071.7 e-6; = (1/var)*||X-X_r||^2 =  381591.0 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14480.6 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  427972.8 e-6; = (1/var)*||X-X_r||^2 =  405255.1 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  22717.7 e-6 = 5.3 %)
Min.  Avg. Train Loss across Mini-Batch =  394374.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424185.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31901.2 e-6; = (1/var)*||X-X_r||^2 val-train = 23664.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8237.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.99; perplexity/K = 81.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.62; perplexity/K = 76.93%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  396209.6 e-6; = (1/var)*||X-X_r||^2 =  381385.6 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14824.0 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  428199.6 e-6; = (1/var)*||X-X_r||^2 =  405863.3 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  22336.3 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  393312.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424126.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31990.1 e-6; = (1/var)*||X-X_r||^2 val-train = 24477.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7512.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.55; perplexity/K = 79.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.80; perplexity/K = 80.62%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391990.5 e-6; = (1/var)*||X-X_r||^2 =  379002.4 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  12988.1 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  426498.1 e-6; = (1/var)*||X-X_r||^2 =  406003.7 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  20494.3 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  391990.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424126.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34507.6 e-6; = (1/var)*||X-X_r||^2 val-train = 27001.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7506.3 e-6 

----------------------------------------------------------------------------------

Finished [11:32:32 09.01.2023] 411) Finished running for K = 32 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:
Total training time is = 0:1:57 h/m/s. 

--------------------------------------------------- 

Started [11:32:32 09.01.2023] 412) Finished running for K = 32 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(32, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 663 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.21
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.83
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.76
5                                  encoder.pre_residual_stack.weight                       147            22.17
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
10                             encoder.channel_adjusting_conv.weight                         1             0.15
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                         9             1.36
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.76
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.83
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.06; perplexity/K = 72.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.62; perplexity/K = 70.69%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  696055.3 e-6; = (1/var)*||X-X_r||^2 =  587234.3 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  108821.0 e-6 = 15.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  609423.2 e-6; = (1/var)*||X-X_r||^2 =  572825.8 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  36597.4 e-6 = 6.0 %)
Min.  Avg. Train Loss across Mini-Batch =  676928.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  595649.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -86632.1 e-6; = (1/var)*||X-X_r||^2 val-train = -14408.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -72223.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.32; perplexity/K = 88.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.19; perplexity/K = 84.96%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  546109.1 e-6; = (1/var)*||X-X_r||^2 =  519905.6 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  26203.5 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  530482.2 e-6; = (1/var)*||X-X_r||^2 =  510414.5 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  20067.7 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  539843.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  524188.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15626.9 e-6; = (1/var)*||X-X_r||^2 val-train = -9491.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6135.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.63; perplexity/K = 86.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.83; perplexity/K = 86.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  479357.2 e-6; = (1/var)*||X-X_r||^2 =  463641.0 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  15716.2 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  469656.6 e-6; = (1/var)*||X-X_r||^2 =  457047.5 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  12609.1 e-6 = 2.7 %)
Min.  Avg. Train Loss across Mini-Batch =  477455.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  467083.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9700.6 e-6; = (1/var)*||X-X_r||^2 val-train = -6593.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3107.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.98; perplexity/K = 87.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.57; perplexity/K = 86.16%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  446835.1 e-6; = (1/var)*||X-X_r||^2 =  432585.4 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  14249.7 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  442320.9 e-6; = (1/var)*||X-X_r||^2 =  427671.1 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  14649.8 e-6 = 3.3 %)
Min.  Avg. Train Loss across Mini-Batch =  446331.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  442078.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4514.2 e-6; = (1/var)*||X-X_r||^2 val-train = -4914.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 400.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.83; perplexity/K = 86.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.27; perplexity/K = 88.35%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  433286.1 e-6; = (1/var)*||X-X_r||^2 =  418376.5 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  14909.6 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  430949.5 e-6; = (1/var)*||X-X_r||^2 =  416151.1 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  14798.5 e-6 = 3.4 %)
Min.  Avg. Train Loss across Mini-Batch =  431468.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  428946.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2336.6 e-6; = (1/var)*||X-X_r||^2 val-train = -2225.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -111.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.65; perplexity/K = 86.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.03; perplexity/K = 87.60%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  423499.9 e-6; = (1/var)*||X-X_r||^2 =  408496.4 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  15003.4 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  424721.2 e-6; = (1/var)*||X-X_r||^2 =  407847.4 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16873.8 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  423218.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  423727.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1221.4 e-6; = (1/var)*||X-X_r||^2 val-train = -649.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1870.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.31; perplexity/K = 85.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.80; perplexity/K = 86.87%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  418736.7 e-6; = (1/var)*||X-X_r||^2 =  403120.5 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  15616.2 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  424401.6 e-6; = (1/var)*||X-X_r||^2 =  404995.5 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  19406.1 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  418040.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  419789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5664.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1875.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3789.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.10; perplexity/K = 84.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.88; perplexity/K = 87.12%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  414271.2 e-6; = (1/var)*||X-X_r||^2 =  398291.9 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15979.3 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  417654.2 e-6; = (1/var)*||X-X_r||^2 =  400167.8 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17486.3 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  412743.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416344.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3382.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1875.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1507.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.56; perplexity/K = 86.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.19; perplexity/K = 88.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  409313.6 e-6; = (1/var)*||X-X_r||^2 =  394186.3 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  15127.3 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  414969.1 e-6; = (1/var)*||X-X_r||^2 =  397060.0 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  17909.1 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  408788.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  412959.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5655.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2873.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2781.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.42; perplexity/K = 85.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.71; perplexity/K = 86.60%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405749.0 e-6; = (1/var)*||X-X_r||^2 =  390948.5 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  14800.5 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  413192.5 e-6; = (1/var)*||X-X_r||^2 =  395771.1 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17421.4 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  405749.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  412249.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7443.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4822.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2620.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.18; perplexity/K = 88.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.40; perplexity/K = 88.75%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  406044.6 e-6; = (1/var)*||X-X_r||^2 =  390601.1 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15443.5 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  414057.0 e-6; = (1/var)*||X-X_r||^2 =  396848.9 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17208.1 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  402297.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  411212.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8012.4 e-6; = (1/var)*||X-X_r||^2 val-train = 6247.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1764.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.18; perplexity/K = 88.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.13; perplexity/K = 87.90%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  402038.0 e-6; = (1/var)*||X-X_r||^2 =  387527.3 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  14510.7 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  411577.1 e-6; = (1/var)*||X-X_r||^2 =  394508.8 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  17068.4 e-6 = 4.1 %)
Min.  Avg. Train Loss across Mini-Batch =  399023.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  409518.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9539.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6981.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2557.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.35; perplexity/K = 88.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.91; perplexity/K = 87.21%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  397355.9 e-6; = (1/var)*||X-X_r||^2 =  384248.6 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  13107.3 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  408089.0 e-6; = (1/var)*||X-X_r||^2 =  391731.1 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16357.9 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  396658.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  407629.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10733.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7482.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3250.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.69; perplexity/K = 89.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.58; perplexity/K = 89.31%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  394156.9 e-6; = (1/var)*||X-X_r||^2 =  381742.6 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  12414.4 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  405710.8 e-6; = (1/var)*||X-X_r||^2 =  389739.9 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15971.0 e-6 = 3.9 %)
Min.  Avg. Train Loss across Mini-Batch =  394156.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  405660.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11553.9 e-6; = (1/var)*||X-X_r||^2 val-train = 7997.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3556.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.02; perplexity/K = 84.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.37; perplexity/K = 85.53%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  392915.1 e-6; = (1/var)*||X-X_r||^2 =  380780.8 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  12134.3 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  406994.5 e-6; = (1/var)*||X-X_r||^2 =  391377.7 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15616.8 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  392735.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  405464.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14079.4 e-6; = (1/var)*||X-X_r||^2 val-train = 10596.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3482.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.80; perplexity/K = 86.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.96; perplexity/K = 84.23%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393895.4 e-6; = (1/var)*||X-X_r||^2 =  381522.8 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  12372.5 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  408005.8 e-6; = (1/var)*||X-X_r||^2 =  391867.7 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16138.1 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  389935.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  403803.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14110.5 e-6; = (1/var)*||X-X_r||^2 val-train = 10344.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3765.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.54; perplexity/K = 89.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.93; perplexity/K = 87.27%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390238.4 e-6; = (1/var)*||X-X_r||^2 =  378924.5 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11313.9 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  407347.3 e-6; = (1/var)*||X-X_r||^2 =  390548.8 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  16798.5 e-6 = 4.1 %)
Min.  Avg. Train Loss across Mini-Batch =  389489.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  402396.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17108.9 e-6; = (1/var)*||X-X_r||^2 val-train = 11624.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5484.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.37; perplexity/K = 85.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.45; perplexity/K = 85.79%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390298.0 e-6; = (1/var)*||X-X_r||^2 =  378957.9 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11340.0 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  405024.4 e-6; = (1/var)*||X-X_r||^2 =  389685.3 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15339.0 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  387718.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  402374.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14726.4 e-6; = (1/var)*||X-X_r||^2 val-train = 10727.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3999.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.32; perplexity/K = 85.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.99; perplexity/K = 87.47%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387364.5 e-6; = (1/var)*||X-X_r||^2 =  376902.2 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  10462.3 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  402211.7 e-6; = (1/var)*||X-X_r||^2 =  387968.3 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  14243.3 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  386286.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  400571.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14847.2 e-6; = (1/var)*||X-X_r||^2 val-train = 11066.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3781.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.95; perplexity/K = 87.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.94; perplexity/K = 87.33%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387573.7 e-6; = (1/var)*||X-X_r||^2 =  377166.1 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  10407.6 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  406839.3 e-6; = (1/var)*||X-X_r||^2 =  390747.9 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16091.4 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  384954.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  399766.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19265.5 e-6; = (1/var)*||X-X_r||^2 val-train = 13581.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5683.7 e-6 

----------------------------------------------------------------------------------

Finished [12:23:32 09.01.2023] 412) Finished running for K = 32 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 20) change_channel_size_across_layers = True:
Total training time is = 0:1:59 h/m/s. 

--------------------------------------------------- 

Started [12:23:32 09.01.2023] 413) Finished running for K = 16 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 689 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.16
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.64
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.01
4                                  encoder.pre_residual_stack.weight                       147            21.34
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
9                              encoder.channel_adjusting_conv.weight                         4             0.58
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        36             5.22
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.01
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.64
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.16
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.46; perplexity/K = 52.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.80; perplexity/K = 48.74%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  592700.3 e-6; = (1/var)*||X-X_r||^2 =  510978.6 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  81721.8 e-6 = 13.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  575416.5 e-6; = (1/var)*||X-X_r||^2 =  498686.8 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  76729.8 e-6 = 13.3 %)
Min.  Avg. Train Loss across Mini-Batch =  589342.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  570120.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17283.8 e-6; = (1/var)*||X-X_r||^2 val-train = -12291.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4992.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.08; perplexity/K = 62.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.29; perplexity/K = 58.08%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  201682.8 e-6; = (1/var)*||X-X_r||^2 =  154351.1 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  47331.7 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  203205.3 e-6; = (1/var)*||X-X_r||^2 =  158955.8 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  44249.6 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  201682.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  203205.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1522.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4604.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3082.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.39; perplexity/K = 77.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.83; perplexity/K = 80.18%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  123738.4 e-6; = (1/var)*||X-X_r||^2 =  100115.0 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  23623.4 e-6 = 19.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  126323.6 e-6; = (1/var)*||X-X_r||^2 =  103805.6 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  22518.0 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  122641.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  126302.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2585.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3690.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1105.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.86; perplexity/K = 80.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.10; perplexity/K = 81.89%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  95031.8 e-6; = (1/var)*||X-X_r||^2 =  73718.4 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  21313.4 e-6 = 22.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  99664.1 e-6; = (1/var)*||X-X_r||^2 =  78999.1 e-6 = 79.3 %; (1+beta)*||Z_e-Z_q||^2 =  20664.9 e-6 = 20.7 %)
Min.  Avg. Train Loss across Mini-Batch =  95031.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  99664.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4632.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5280.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -648.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.48; perplexity/K = 77.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.62; perplexity/K = 78.90%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  79654.8 e-6; = (1/var)*||X-X_r||^2 =  60978.4 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  18676.4 e-6 = 23.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  84750.4 e-6; = (1/var)*||X-X_r||^2 =  65929.0 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  18821.4 e-6 = 22.2 %)
Min.  Avg. Train Loss across Mini-Batch =  78776.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  84663.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5095.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4950.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 145.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 78.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.87; perplexity/K = 80.42%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  66331.7 e-6; = (1/var)*||X-X_r||^2 =  50742.5 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  15589.2 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  72920.8 e-6; = (1/var)*||X-X_r||^2 =  56393.1 e-6 = 77.3 %; (1+beta)*||Z_e-Z_q||^2 =  16527.7 e-6 = 22.7 %)
Min.  Avg. Train Loss across Mini-Batch =  66331.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71534.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6589.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5650.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 938.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.02; perplexity/K = 81.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.52; perplexity/K = 78.26%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57042.0 e-6; = (1/var)*||X-X_r||^2 =  42417.5 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  14624.5 e-6 = 25.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  63483.5 e-6; = (1/var)*||X-X_r||^2 =  47658.9 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  15824.6 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  56447.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62347.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6441.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5241.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1200.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.68; perplexity/K = 79.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.79%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47068.1 e-6; = (1/var)*||X-X_r||^2 =  35412.7 e-6 = 75.2 %; (1+beta)*||Z_e-Z_q||^2 =  11655.4 e-6 = 24.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  53553.1 e-6; = (1/var)*||X-X_r||^2 =  40568.1 e-6 = 75.8 %; (1+beta)*||Z_e-Z_q||^2 =  12985.0 e-6 = 24.2 %)
Min.  Avg. Train Loss across Mini-Batch =  47068.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  53285.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6485.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5155.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1329.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.39; perplexity/K = 83.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.65; perplexity/K = 79.08%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41417.9 e-6; = (1/var)*||X-X_r||^2 =  31335.1 e-6 = 75.7 %; (1+beta)*||Z_e-Z_q||^2 =  10082.8 e-6 = 24.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  47568.1 e-6; = (1/var)*||X-X_r||^2 =  36369.4 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  11198.7 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  41417.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47204.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6150.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5034.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1115.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.92; perplexity/K = 80.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.59; perplexity/K = 78.66%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39530.4 e-6; = (1/var)*||X-X_r||^2 =  29170.1 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  10360.4 e-6 = 26.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  44893.1 e-6; = (1/var)*||X-X_r||^2 =  33583.0 e-6 = 74.8 %; (1+beta)*||Z_e-Z_q||^2 =  11310.1 e-6 = 25.2 %)
Min.  Avg. Train Loss across Mini-Batch =  37714.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43607.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5362.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4413.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 949.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.72; perplexity/K = 79.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.93; perplexity/K = 80.79%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38875.9 e-6; = (1/var)*||X-X_r||^2 =  28194.2 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  10681.7 e-6 = 27.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  42921.7 e-6; = (1/var)*||X-X_r||^2 =  31935.1 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  10986.7 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  35280.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40593.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4045.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3740.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 305.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.01; perplexity/K = 81.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.31; perplexity/K = 76.94%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33583.0 e-6; = (1/var)*||X-X_r||^2 =  26326.4 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  7256.5 e-6 = 21.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  38979.5 e-6; = (1/var)*||X-X_r||^2 =  30775.4 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  8204.1 e-6 = 21.0 %)
Min.  Avg. Train Loss across Mini-Batch =  33157.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38794.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5396.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4449.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 947.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.94; perplexity/K = 80.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.98; perplexity/K = 81.14%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38903.2 e-6; = (1/var)*||X-X_r||^2 =  27813.9 e-6 = 71.5 %; (1+beta)*||Z_e-Z_q||^2 =  11089.3 e-6 = 28.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  42257.3 e-6; = (1/var)*||X-X_r||^2 =  31081.5 e-6 = 73.6 %; (1+beta)*||Z_e-Z_q||^2 =  11175.8 e-6 = 26.4 %)
Min.  Avg. Train Loss across Mini-Batch =  31986.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37379.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3354.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3267.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 86.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.81; perplexity/K = 80.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 81.69%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36323.4 e-6; = (1/var)*||X-X_r||^2 =  26421.6 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  9901.8 e-6 = 27.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  40089.1 e-6; = (1/var)*||X-X_r||^2 =  29925.2 e-6 = 74.6 %; (1+beta)*||Z_e-Z_q||^2 =  10163.9 e-6 = 25.4 %)
Min.  Avg. Train Loss across Mini-Batch =  31359.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36519.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3765.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3503.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 262.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.66; perplexity/K = 79.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.64; perplexity/K = 79.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46679.9 e-6; = (1/var)*||X-X_r||^2 =  31499.1 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  15180.8 e-6 = 32.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  48394.0 e-6; = (1/var)*||X-X_r||^2 =  33964.6 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  14429.4 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  30767.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35863.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1714.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2465.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -751.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.81; perplexity/K = 80.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 76.61%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33531.2 e-6; = (1/var)*||X-X_r||^2 =  24708.3 e-6 = 73.7 %; (1+beta)*||Z_e-Z_q||^2 =  8823.0 e-6 = 26.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  37856.5 e-6; = (1/var)*||X-X_r||^2 =  28653.8 e-6 = 75.7 %; (1+beta)*||Z_e-Z_q||^2 =  9202.6 e-6 = 24.3 %)
Min.  Avg. Train Loss across Mini-Batch =  30377.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35499.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4325.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3945.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 379.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.49; perplexity/K = 78.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.88; perplexity/K = 80.49%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29690.8 e-6; = (1/var)*||X-X_r||^2 =  23668.6 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  6022.2 e-6 = 20.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  34529.8 e-6; = (1/var)*||X-X_r||^2 =  27588.0 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  6941.8 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  29690.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34428.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4839.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3919.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 919.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.35; perplexity/K = 77.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.59; perplexity/K = 78.70%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27480.6 e-6; = (1/var)*||X-X_r||^2 =  22045.4 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  5435.3 e-6 = 19.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  32715.2 e-6; = (1/var)*||X-X_r||^2 =  26427.0 e-6 = 80.8 %; (1+beta)*||Z_e-Z_q||^2 =  6288.1 e-6 = 19.2 %)
Min.  Avg. Train Loss across Mini-Batch =  27480.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32328.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5234.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4381.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 852.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 78.18%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31767.4 e-6; = (1/var)*||X-X_r||^2 =  22122.6 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  9644.8 e-6 = 30.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  35108.3 e-6; = (1/var)*||X-X_r||^2 =  25517.2 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  9591.2 e-6 = 27.3 %)
Min.  Avg. Train Loss across Mini-Batch =  25988.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30689.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3341.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3394.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -53.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.47; perplexity/K = 77.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.27; perplexity/K = 76.68%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24830.6 e-6; = (1/var)*||X-X_r||^2 =  19498.9 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  5331.7 e-6 = 21.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  29701.6 e-6; = (1/var)*||X-X_r||^2 =  23192.5 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  6509.1 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  24722.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29222.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4871.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3693.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1177.4 e-6 

----------------------------------------------------------------------------------

Finished [13:11:36 09.01.2023] 413) Finished running for K = 16 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:
Total training time is = 0:3:4 h/m/s. 

--------------------------------------------------- 

Started [13:11:36 09.01.2023] 414) Finished running for K = 16 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 669 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.20
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.78
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.58
4                                  encoder.pre_residual_stack.weight                       147            21.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
9                              encoder.channel_adjusting_conv.weight                         2             0.30
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        18             2.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.58
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.78
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.20
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.26; perplexity/K = 57.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.77; perplexity/K = 54.81%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  507937.3 e-6; = (1/var)*||X-X_r||^2 =  473441.8 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  34495.5 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  487205.4 e-6; = (1/var)*||X-X_r||^2 =  463011.5 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  24194.0 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  505973.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  486365.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20731.9 e-6; = (1/var)*||X-X_r||^2 val-train = -10430.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10301.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.46; perplexity/K = 59.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.12; perplexity/K = 63.26%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  421862.5 e-6; = (1/var)*||X-X_r||^2 =  399680.4 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  22182.1 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  415372.4 e-6; = (1/var)*||X-X_r||^2 =  393843.6 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  21528.8 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  421862.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415329.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6490.1 e-6; = (1/var)*||X-X_r||^2 val-train = -5836.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -653.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.63; perplexity/K = 66.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.88; perplexity/K = 68.03%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  389975.7 e-6; = (1/var)*||X-X_r||^2 =  374582.9 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15392.8 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  383776.1 e-6; = (1/var)*||X-X_r||^2 =  367520.4 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  16255.7 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  389139.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383184.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6199.6 e-6; = (1/var)*||X-X_r||^2 val-train = -7062.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 862.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.59; perplexity/K = 78.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 74.17%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  373803.0 e-6; = (1/var)*||X-X_r||^2 =  362709.4 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11093.6 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  368256.7 e-6; = (1/var)*||X-X_r||^2 =  356222.7 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  12033.9 e-6 = 3.3 %)
Min.  Avg. Train Loss across Mini-Batch =  373664.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  367729.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5546.3 e-6; = (1/var)*||X-X_r||^2 val-train = -6486.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 940.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.73; perplexity/K = 79.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.95; perplexity/K = 74.68%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  364979.4 e-6; = (1/var)*||X-X_r||^2 =  356554.2 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8425.2 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  359129.9 e-6; = (1/var)*||X-X_r||^2 =  349689.2 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  9440.7 e-6 = 2.6 %)
Min.  Avg. Train Loss across Mini-Batch =  364933.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  359129.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5849.5 e-6; = (1/var)*||X-X_r||^2 val-train = -6865.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1015.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.57; perplexity/K = 72.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.03; perplexity/K = 75.18%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  360510.7 e-6; = (1/var)*||X-X_r||^2 =  353076.4 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7434.3 e-6 = 2.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  354883.4 e-6; = (1/var)*||X-X_r||^2 =  346438.9 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  8444.5 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  360510.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  354883.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5627.2 e-6; = (1/var)*||X-X_r||^2 val-train = -6637.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1010.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 74.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.37; perplexity/K = 71.08%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  357108.6 e-6; = (1/var)*||X-X_r||^2 =  350988.9 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6119.7 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  351569.3 e-6; = (1/var)*||X-X_r||^2 =  344149.5 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7419.9 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  356918.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  351438.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5539.2 e-6; = (1/var)*||X-X_r||^2 val-train = -6839.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1300.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 74.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 74.16%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  354305.8 e-6; = (1/var)*||X-X_r||^2 =  349014.8 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5291.0 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  350499.6 e-6; = (1/var)*||X-X_r||^2 =  343675.4 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  6824.3 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  354305.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  348965.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3806.2 e-6; = (1/var)*||X-X_r||^2 val-train = -5339.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1533.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.69; perplexity/K = 73.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.60; perplexity/K = 72.51%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  353187.7 e-6; = (1/var)*||X-X_r||^2 =  347995.2 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5192.5 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  348338.6 e-6; = (1/var)*||X-X_r||^2 =  341856.8 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  6481.8 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  352735.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  346942.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4849.1 e-6; = (1/var)*||X-X_r||^2 val-train = -6138.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1289.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.79; perplexity/K = 73.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.91; perplexity/K = 74.42%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  351580.5 e-6; = (1/var)*||X-X_r||^2 =  346944.3 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  4636.2 e-6 = 1.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  347002.3 e-6; = (1/var)*||X-X_r||^2 =  340896.9 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  6105.4 e-6 = 1.8 %)
Min.  Avg. Train Loss across Mini-Batch =  351239.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  346297.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4578.2 e-6; = (1/var)*||X-X_r||^2 val-train = -6047.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1469.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.91; perplexity/K = 74.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.94; perplexity/K = 74.65%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350098.4 e-6; = (1/var)*||X-X_r||^2 =  345900.5 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  4197.9 e-6 = 1.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  345029.3 e-6; = (1/var)*||X-X_r||^2 =  339590.5 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  5438.8 e-6 = 1.6 %)
Min.  Avg. Train Loss across Mini-Batch =  350098.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  344327.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5069.1 e-6; = (1/var)*||X-X_r||^2 val-train = -6310.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1241.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.19; perplexity/K = 76.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.67; perplexity/K = 72.96%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  349680.9 e-6; = (1/var)*||X-X_r||^2 =  345590.6 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  4090.2 e-6 = 1.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  344485.2 e-6; = (1/var)*||X-X_r||^2 =  339124.7 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  5360.5 e-6 = 1.6 %)
Min.  Avg. Train Loss across Mini-Batch =  349011.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  343648.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5195.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6465.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1270.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.48; perplexity/K = 71.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 76.85%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350332.7 e-6; = (1/var)*||X-X_r||^2 =  346011.1 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  4321.5 e-6 = 1.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  345425.2 e-6; = (1/var)*||X-X_r||^2 =  339309.6 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  6115.6 e-6 = 1.8 %)
Min.  Avg. Train Loss across Mini-Batch =  348264.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  342603.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4907.5 e-6; = (1/var)*||X-X_r||^2 val-train = -6701.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1794.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.75; perplexity/K = 73.43%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347779.7 e-6; = (1/var)*||X-X_r||^2 =  344151.1 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3628.6 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  342556.7 e-6; = (1/var)*||X-X_r||^2 =  337620.5 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  4936.2 e-6 = 1.4 %)
Min.  Avg. Train Loss across Mini-Batch =  347682.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  342023.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5223.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6530.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1307.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.55; perplexity/K = 72.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.72; perplexity/K = 73.24%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347755.5 e-6; = (1/var)*||X-X_r||^2 =  344140.3 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3615.2 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  341988.1 e-6; = (1/var)*||X-X_r||^2 =  337322.4 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  4665.7 e-6 = 1.4 %)
Min.  Avg. Train Loss across Mini-Batch =  347139.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  341550.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5767.4 e-6; = (1/var)*||X-X_r||^2 val-train = -6817.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1050.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 75.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.69; perplexity/K = 73.07%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346727.7 e-6; = (1/var)*||X-X_r||^2 =  343391.7 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3336.0 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  341194.8 e-6; = (1/var)*||X-X_r||^2 =  336522.0 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  4672.8 e-6 = 1.4 %)
Min.  Avg. Train Loss across Mini-Batch =  346685.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  340569.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5533.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6869.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1336.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.10%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346586.7 e-6; = (1/var)*||X-X_r||^2 =  343261.9 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3324.7 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  341244.2 e-6; = (1/var)*||X-X_r||^2 =  336744.2 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  4500.0 e-6 = 1.3 %)
Min.  Avg. Train Loss across Mini-Batch =  346450.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  340522.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5342.5 e-6; = (1/var)*||X-X_r||^2 val-train = -6517.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1175.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 76.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.99; perplexity/K = 74.92%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346094.0 e-6; = (1/var)*||X-X_r||^2 =  342923.9 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3170.1 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  340596.1 e-6; = (1/var)*||X-X_r||^2 =  336233.3 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  4362.8 e-6 = 1.3 %)
Min.  Avg. Train Loss across Mini-Batch =  345938.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  340092.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5498.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6690.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1192.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.22; perplexity/K = 76.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.90; perplexity/K = 74.37%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  345904.1 e-6; = (1/var)*||X-X_r||^2 =  342637.1 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3267.0 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  340027.1 e-6; = (1/var)*||X-X_r||^2 =  335708.2 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  4318.9 e-6 = 1.3 %)
Min.  Avg. Train Loss across Mini-Batch =  345776.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  339836.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5876.9 e-6; = (1/var)*||X-X_r||^2 val-train = -6928.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1051.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.77; perplexity/K = 73.55%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346740.6 e-6; = (1/var)*||X-X_r||^2 =  343273.0 e-6 = 99.0 %; (1+beta)*||Z_e-Z_q||^2 =  3467.6 e-6 = 1.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  342084.2 e-6; = (1/var)*||X-X_r||^2 =  337034.9 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5049.4 e-6 = 1.5 %)
Min.  Avg. Train Loss across Mini-Batch =  345534.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  339640.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4656.3 e-6; = (1/var)*||X-X_r||^2 val-train = -6238.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1581.7 e-6 

----------------------------------------------------------------------------------

Finished [13:59:30 09.01.2023] 414) Finished running for K = 16 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:
Total training time is = 0:3:53 h/m/s. 

--------------------------------------------------- 

Started [13:59:30 09.01.2023] 415) Finished running for K = 16 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 659 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.21
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.86
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.88
4                                  encoder.pre_residual_stack.weight                       147            22.31
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
9                              encoder.channel_adjusting_conv.weight                         1             0.15
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                         9             1.37
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.88
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.86
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.46; perplexity/K = 52.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.42; perplexity/K = 46.40%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  626514.3 e-6; = (1/var)*||X-X_r||^2 =  499033.1 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  127481.1 e-6 = 20.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  603862.2 e-6; = (1/var)*||X-X_r||^2 =  486541.5 e-6 = 80.6 %; (1+beta)*||Z_e-Z_q||^2 =  117320.7 e-6 = 19.4 %)
Min.  Avg. Train Loss across Mini-Batch =  626514.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  603862.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22652.0 e-6; = (1/var)*||X-X_r||^2 val-train = -12491.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10160.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.40; perplexity/K = 71.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.12%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  175321.6 e-6; = (1/var)*||X-X_r||^2 =  145258.9 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  30062.8 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  176755.2 e-6; = (1/var)*||X-X_r||^2 =  149318.1 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  27437.0 e-6 = 15.5 %)
Min.  Avg. Train Loss across Mini-Batch =  175321.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  176755.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1433.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4059.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2625.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.31; perplexity/K = 76.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 75.72%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  131562.6 e-6; = (1/var)*||X-X_r||^2 =  103658.5 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  27904.2 e-6 = 21.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  151831.7 e-6; = (1/var)*||X-X_r||^2 =  120781.1 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  31050.6 e-6 = 20.5 %)
Min.  Avg. Train Loss across Mini-Batch =  125430.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  132020.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20269.1 e-6; = (1/var)*||X-X_r||^2 val-train = 17122.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3146.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.43; perplexity/K = 77.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 76.61%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  107931.7 e-6; = (1/var)*||X-X_r||^2 =  85251.8 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  22679.9 e-6 = 21.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  117523.3 e-6; = (1/var)*||X-X_r||^2 =  93571.2 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  23952.1 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  107036.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  115788.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9591.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8319.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1272.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.66; perplexity/K = 79.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 76.01%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  96432.6 e-6; = (1/var)*||X-X_r||^2 =  74763.2 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  21669.5 e-6 = 22.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  105325.6 e-6; = (1/var)*||X-X_r||^2 =  82281.8 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  23043.8 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  96180.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  105325.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8892.9 e-6; = (1/var)*||X-X_r||^2 val-train = 7518.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1374.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 76.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.94; perplexity/K = 74.63%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  84151.7 e-6; = (1/var)*||X-X_r||^2 =  65877.9 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  18273.8 e-6 = 21.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  95420.8 e-6; = (1/var)*||X-X_r||^2 =  74601.2 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  20819.6 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  84151.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  95420.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11269.1 e-6; = (1/var)*||X-X_r||^2 val-train = 8723.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2545.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 76.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.04; perplexity/K = 75.24%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  77680.7 e-6; = (1/var)*||X-X_r||^2 =  61099.4 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  16581.2 e-6 = 21.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  90612.5 e-6; = (1/var)*||X-X_r||^2 =  71472.8 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  19139.7 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  76619.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  88106.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12931.8 e-6; = (1/var)*||X-X_r||^2 val-train = 10373.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2558.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.49; perplexity/K = 78.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.03; perplexity/K = 75.18%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  70502.5 e-6; = (1/var)*||X-X_r||^2 =  55539.3 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  14963.2 e-6 = 21.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  82299.7 e-6; = (1/var)*||X-X_r||^2 =  64531.4 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  17768.3 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  70094.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  82199.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11797.2 e-6; = (1/var)*||X-X_r||^2 val-train = 8992.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2805.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.03; perplexity/K = 75.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.34; perplexity/K = 77.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  66505.3 e-6; = (1/var)*||X-X_r||^2 =  52444.0 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  14061.3 e-6 = 21.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  78005.8 e-6; = (1/var)*||X-X_r||^2 =  60913.5 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  17092.2 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  65682.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  77576.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11500.5 e-6; = (1/var)*||X-X_r||^2 val-train = 8469.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3031.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.17; perplexity/K = 76.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.00; perplexity/K = 74.98%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61204.7 e-6; = (1/var)*||X-X_r||^2 =  48555.0 e-6 = 79.3 %; (1+beta)*||Z_e-Z_q||^2 =  12649.7 e-6 = 20.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  73114.8 e-6; = (1/var)*||X-X_r||^2 =  57517.0 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  15597.8 e-6 = 21.3 %)
Min.  Avg. Train Loss across Mini-Batch =  61199.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72863.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11910.1 e-6; = (1/var)*||X-X_r||^2 val-train = 8962.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2948.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.56; perplexity/K = 78.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.96; perplexity/K = 74.73%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57603.9 e-6; = (1/var)*||X-X_r||^2 =  46339.7 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  11264.2 e-6 = 19.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  70344.9 e-6; = (1/var)*||X-X_r||^2 =  56061.3 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  14283.6 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  57423.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  69953.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12741.0 e-6; = (1/var)*||X-X_r||^2 val-train = 9721.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3019.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 76.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 76.86%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57932.3 e-6; = (1/var)*||X-X_r||^2 =  45249.3 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  12683.0 e-6 = 21.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  69400.0 e-6; = (1/var)*||X-X_r||^2 =  54720.5 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  14679.5 e-6 = 21.2 %)
Min.  Avg. Train Loss across Mini-Batch =  54339.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67292.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11467.7 e-6; = (1/var)*||X-X_r||^2 val-train = 9471.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1996.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.29; perplexity/K = 76.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.22; perplexity/K = 76.37%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  51957.9 e-6; = (1/var)*||X-X_r||^2 =  41185.9 e-6 = 79.3 %; (1+beta)*||Z_e-Z_q||^2 =  10772.0 e-6 = 20.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  62411.0 e-6; = (1/var)*||X-X_r||^2 =  48956.4 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  13454.6 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  49891.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62411.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10453.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7770.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2682.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.93; perplexity/K = 74.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.13; perplexity/K = 75.82%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44974.5 e-6; = (1/var)*||X-X_r||^2 =  34613.8 e-6 = 77.0 %; (1+beta)*||Z_e-Z_q||^2 =  10360.7 e-6 = 23.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  55782.2 e-6; = (1/var)*||X-X_r||^2 =  42823.1 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  12959.1 e-6 = 23.2 %)
Min.  Avg. Train Loss across Mini-Batch =  43854.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55782.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10807.7 e-6; = (1/var)*||X-X_r||^2 val-train = 8209.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2598.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.48; perplexity/K = 77.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.38; perplexity/K = 77.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39703.6 e-6; = (1/var)*||X-X_r||^2 =  31874.1 e-6 = 80.3 %; (1+beta)*||Z_e-Z_q||^2 =  7829.5 e-6 = 19.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  53828.5 e-6; = (1/var)*||X-X_r||^2 =  42859.3 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  10969.2 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  39703.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51833.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14124.9 e-6; = (1/var)*||X-X_r||^2 val-train = 10985.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3139.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.40; perplexity/K = 77.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.17; perplexity/K = 76.04%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37860.8 e-6; = (1/var)*||X-X_r||^2 =  30825.6 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  7035.2 e-6 = 18.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  50129.1 e-6; = (1/var)*||X-X_r||^2 =  40173.9 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  9955.2 e-6 = 19.9 %)
Min.  Avg. Train Loss across Mini-Batch =  37860.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49482.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12268.4 e-6; = (1/var)*||X-X_r||^2 val-train = 9348.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2920.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.03; perplexity/K = 75.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.35; perplexity/K = 77.18%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32648.4 e-6; = (1/var)*||X-X_r||^2 =  24576.3 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  8072.1 e-6 = 24.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  43589.6 e-6; = (1/var)*||X-X_r||^2 =  32832.5 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  10757.1 e-6 = 24.7 %)
Min.  Avg. Train Loss across Mini-Batch =  32648.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43252.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10941.2 e-6; = (1/var)*||X-X_r||^2 val-train = 8256.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2685.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.63; perplexity/K = 78.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 76.60%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26556.8 e-6; = (1/var)*||X-X_r||^2 =  19515.6 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  7041.2 e-6 = 26.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  38061.2 e-6; = (1/var)*||X-X_r||^2 =  28171.8 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  9889.5 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  26345.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37833.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11504.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8656.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2848.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 78.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.13; perplexity/K = 75.81%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23487.3 e-6; = (1/var)*||X-X_r||^2 =  17198.2 e-6 = 73.2 %; (1+beta)*||Z_e-Z_q||^2 =  6289.1 e-6 = 26.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  34384.1 e-6; = (1/var)*||X-X_r||^2 =  25253.6 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  9130.4 e-6 = 26.6 %)
Min.  Avg. Train Loss across Mini-Batch =  23073.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33881.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10896.8 e-6; = (1/var)*||X-X_r||^2 val-train = 8055.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2841.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.91; perplexity/K = 80.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.11%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21835.6 e-6; = (1/var)*||X-X_r||^2 =  16350.1 e-6 = 74.9 %; (1+beta)*||Z_e-Z_q||^2 =  5485.4 e-6 = 25.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  31976.7 e-6; = (1/var)*||X-X_r||^2 =  23981.5 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  7995.2 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  21543.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31941.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10141.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7631.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2509.7 e-6 

----------------------------------------------------------------------------------

Finished [14:47:36 09.01.2023] 415) Finished running for K = 16 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 64) change_channel_size_across_layers = True:
Total training time is = 0:3:6 h/m/s. 

--------------------------------------------------- 

Started [14:47:36 09.01.2023] 416) Finished running for K = 16 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 693 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.29
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.15
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.62
4                           encoder.sequential_convs.conv2d_5.weight                       131            18.90
5                                  encoder.pre_residual_stack.weight                       147            21.21
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
10                             encoder.channel_adjusting_conv.weight                         4             0.58
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                        36             5.19
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.90
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.62
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.15
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.29
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.63; perplexity/K = 60.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.52; perplexity/K = 53.26%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  758485.8 e-6; = (1/var)*||X-X_r||^2 =  600864.5 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  157621.4 e-6 = 20.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  630962.5 e-6; = (1/var)*||X-X_r||^2 =  582549.4 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  48413.1 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  671546.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  604218.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -127523.3 e-6; = (1/var)*||X-X_r||^2 val-train = -18315.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -109208.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.08; perplexity/K = 63.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.41; perplexity/K = 65.04%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  579683.2 e-6; = (1/var)*||X-X_r||^2 =  518501.8 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  61181.5 e-6 = 10.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  558955.8 e-6; = (1/var)*||X-X_r||^2 =  503991.0 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  54964.8 e-6 = 9.8 %)
Min.  Avg. Train Loss across Mini-Batch =  570602.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  552527.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20727.4 e-6; = (1/var)*||X-X_r||^2 val-train = -14510.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6216.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 68.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.06; perplexity/K = 62.90%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  530043.4 e-6; = (1/var)*||X-X_r||^2 =  472991.6 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  57051.8 e-6 = 10.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  525039.3 e-6; = (1/var)*||X-X_r||^2 =  471812.9 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  53226.4 e-6 = 10.1 %)
Min.  Avg. Train Loss across Mini-Batch =  527562.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  518346.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5004.1 e-6; = (1/var)*||X-X_r||^2 val-train = -1178.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3825.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.27; perplexity/K = 64.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.74; perplexity/K = 67.13%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  499789.6 e-6; = (1/var)*||X-X_r||^2 =  441068.4 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  58721.2 e-6 = 11.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  510960.0 e-6; = (1/var)*||X-X_r||^2 =  449368.3 e-6 = 87.9 %; (1+beta)*||Z_e-Z_q||^2 =  61591.8 e-6 = 12.1 %)
Min.  Avg. Train Loss across Mini-Batch =  499789.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  507929.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11170.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8299.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2870.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.08; perplexity/K = 69.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.81; perplexity/K = 67.55%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  480013.6 e-6; = (1/var)*||X-X_r||^2 =  424815.9 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  55197.7 e-6 = 11.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  497558.8 e-6; = (1/var)*||X-X_r||^2 =  436975.0 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  60583.7 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  474409.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  492735.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17545.2 e-6; = (1/var)*||X-X_r||^2 val-train = 12159.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5386.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.51; perplexity/K = 71.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.96; perplexity/K = 68.50%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  456187.4 e-6; = (1/var)*||X-X_r||^2 =  412861.6 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  43325.9 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  490638.4 e-6; = (1/var)*||X-X_r||^2 =  433850.6 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  56787.8 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  454866.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  481633.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34451.0 e-6; = (1/var)*||X-X_r||^2 val-train = 20989.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13462.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.71; perplexity/K = 73.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.41; perplexity/K = 65.06%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  449488.3 e-6; = (1/var)*||X-X_r||^2 =  409870.1 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  39618.2 e-6 = 8.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  485697.3 e-6; = (1/var)*||X-X_r||^2 =  432823.7 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  52873.7 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  445635.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  477785.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36209.1 e-6; = (1/var)*||X-X_r||^2 val-train = 22953.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13255.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.30; perplexity/K = 70.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.96; perplexity/K = 68.48%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  439840.3 e-6; = (1/var)*||X-X_r||^2 =  405478.0 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  34362.3 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  478977.8 e-6; = (1/var)*||X-X_r||^2 =  433738.3 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  45239.5 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  437853.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  474523.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39137.5 e-6; = (1/var)*||X-X_r||^2 val-train = 28260.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10877.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.38; perplexity/K = 71.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.12; perplexity/K = 69.51%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  432937.8 e-6; = (1/var)*||X-X_r||^2 =  402273.6 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  30664.2 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  473812.5 e-6; = (1/var)*||X-X_r||^2 =  431505.4 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  42307.1 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  431631.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  471534.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40874.7 e-6; = (1/var)*||X-X_r||^2 val-train = 29231.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11642.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.95; perplexity/K = 68.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.60; perplexity/K = 66.27%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  430617.3 e-6; = (1/var)*||X-X_r||^2 =  400462.7 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30154.5 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  471925.6 e-6; = (1/var)*||X-X_r||^2 =  431972.4 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  39953.2 e-6 = 8.5 %)
Min.  Avg. Train Loss across Mini-Batch =  428836.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  469772.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41308.3 e-6; = (1/var)*||X-X_r||^2 val-train = 31509.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9798.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.69; perplexity/K = 73.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.21; perplexity/K = 63.82%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  425289.7 e-6; = (1/var)*||X-X_r||^2 =  397463.7 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  27826.0 e-6 = 6.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  468416.2 e-6; = (1/var)*||X-X_r||^2 =  429828.9 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  38587.3 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  423736.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  467032.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43126.6 e-6; = (1/var)*||X-X_r||^2 val-train = 32365.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10761.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.04; perplexity/K = 68.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.70; perplexity/K = 66.86%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  430254.3 e-6; = (1/var)*||X-X_r||^2 =  399436.1 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  30818.2 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  478464.4 e-6; = (1/var)*||X-X_r||^2 =  437724.4 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  40740.0 e-6 = 8.5 %)
Min.  Avg. Train Loss across Mini-Batch =  419947.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465833.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48210.1 e-6; = (1/var)*||X-X_r||^2 val-train = 38288.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9921.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.05; perplexity/K = 69.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.81; perplexity/K = 67.53%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  420345.0 e-6; = (1/var)*||X-X_r||^2 =  394766.9 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  25578.2 e-6 = 6.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  473839.6 e-6; = (1/var)*||X-X_r||^2 =  437814.5 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  36025.1 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  417904.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465833.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53494.6 e-6; = (1/var)*||X-X_r||^2 val-train = 43047.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10447.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.84; perplexity/K = 67.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.02; perplexity/K = 68.88%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  416656.2 e-6; = (1/var)*||X-X_r||^2 =  392561.2 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  24095.0 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  474011.1 e-6; = (1/var)*||X-X_r||^2 =  439544.9 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  34466.2 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  415527.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465833.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57354.9 e-6; = (1/var)*||X-X_r||^2 val-train = 46983.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10371.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.42; perplexity/K = 71.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.06; perplexity/K = 69.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  413936.9 e-6; = (1/var)*||X-X_r||^2 =  392032.4 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  21904.5 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  472653.0 e-6; = (1/var)*||X-X_r||^2 =  440076.5 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  32576.5 e-6 = 6.9 %)
Min.  Avg. Train Loss across Mini-Batch =  413936.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465833.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58716.1 e-6; = (1/var)*||X-X_r||^2 val-train = 48044.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10672.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.62; perplexity/K = 66.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.19; perplexity/K = 63.70%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  414400.1 e-6; = (1/var)*||X-X_r||^2 =  391705.8 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  22694.2 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  473379.5 e-6; = (1/var)*||X-X_r||^2 =  439423.9 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  33955.6 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  411333.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465833.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58979.4 e-6; = (1/var)*||X-X_r||^2 val-train = 47718.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11261.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.39; perplexity/K = 64.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.62; perplexity/K = 66.35%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410194.1 e-6; = (1/var)*||X-X_r||^2 =  389947.9 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  20246.2 e-6 = 4.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  469833.7 e-6; = (1/var)*||X-X_r||^2 =  437918.1 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  31915.6 e-6 = 6.8 %)
Min.  Avg. Train Loss across Mini-Batch =  409528.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465699.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   59639.6 e-6; = (1/var)*||X-X_r||^2 val-train = 47970.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11669.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.67; perplexity/K = 66.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.05; perplexity/K = 69.05%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410613.8 e-6; = (1/var)*||X-X_r||^2 =  389883.9 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  20729.8 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  469189.3 e-6; = (1/var)*||X-X_r||^2 =  438085.7 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  31103.6 e-6 = 6.6 %)
Min.  Avg. Train Loss across Mini-Batch =  408222.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  464999.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58575.6 e-6; = (1/var)*||X-X_r||^2 val-train = 48201.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10373.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 68.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.70; perplexity/K = 66.88%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410706.5 e-6; = (1/var)*||X-X_r||^2 =  390159.6 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  20546.9 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  466370.9 e-6; = (1/var)*||X-X_r||^2 =  436175.0 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  30195.9 e-6 = 6.5 %)
Min.  Avg. Train Loss across Mini-Batch =  406185.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  461222.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55664.4 e-6; = (1/var)*||X-X_r||^2 val-train = 46015.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9649.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.84; perplexity/K = 67.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.10; perplexity/K = 69.39%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  407875.8 e-6; = (1/var)*||X-X_r||^2 =  388556.3 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  19319.5 e-6 = 4.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  466620.6 e-6; = (1/var)*||X-X_r||^2 =  435276.2 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  31344.4 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  406059.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  461222.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58744.8 e-6; = (1/var)*||X-X_r||^2 val-train = 46719.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12024.9 e-6 

----------------------------------------------------------------------------------

Finished [15:38:07 09.01.2023] 416) Finished running for K = 16 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:1:30 h/m/s. 

--------------------------------------------------- 

Started [15:38:07 09.01.2023] 417) Finished running for K = 16 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 673 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.19
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.75
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.47
5                                  encoder.pre_residual_stack.weight                       147            21.84
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.35
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.35
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
10                             encoder.channel_adjusting_conv.weight                         2             0.30
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                        18             2.67
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.35
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.59
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.35
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.59
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.47
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.75
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.19
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.51; perplexity/K = 34.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.60; perplexity/K = 28.77%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2579166.1 e-6; = (1/var)*||X-X_r||^2 =  818151.2 e-6 = 31.7 %; (1+beta)*||Z_e-Z_q||^2 =  1761014.9 e-6 = 68.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1866754.4 e-6; = (1/var)*||X-X_r||^2 =  770955.4 e-6 = 41.3 %; (1+beta)*||Z_e-Z_q||^2 =  1095799.0 e-6 = 58.7 %)
Min.  Avg. Train Loss across Mini-Batch =  964151.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  906744.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -712411.7 e-6; = (1/var)*||X-X_r||^2 val-train = -47195.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -665215.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.88; perplexity/K = 49.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.15; perplexity/K = 50.96%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  611618.7 e-6; = (1/var)*||X-X_r||^2 =  555764.5 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  55854.2 e-6 = 9.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  615727.8 e-6; = (1/var)*||X-X_r||^2 =  556652.8 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  59075.0 e-6 = 9.6 %)
Min.  Avg. Train Loss across Mini-Batch =  597858.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  567556.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4109.0 e-6; = (1/var)*||X-X_r||^2 val-train = 888.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3220.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.89; perplexity/K = 68.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.49; perplexity/K = 65.58%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  548536.2 e-6; = (1/var)*||X-X_r||^2 =  501382.8 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  47153.4 e-6 = 8.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  531990.2 e-6; = (1/var)*||X-X_r||^2 =  485544.9 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  46445.3 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  531285.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  522172.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16546.0 e-6; = (1/var)*||X-X_r||^2 val-train = -15837.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -708.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.59; perplexity/K = 66.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.26; perplexity/K = 70.38%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  507620.9 e-6; = (1/var)*||X-X_r||^2 =  456583.1 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  51037.9 e-6 = 10.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  512485.4 e-6; = (1/var)*||X-X_r||^2 =  460228.0 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  52257.4 e-6 = 10.2 %)
Min.  Avg. Train Loss across Mini-Batch =  506617.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  508447.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4864.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3644.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1219.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.27; perplexity/K = 64.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.80; perplexity/K = 67.47%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  490949.6 e-6; = (1/var)*||X-X_r||^2 =  436481.2 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  54468.5 e-6 = 11.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  504647.5 e-6; = (1/var)*||X-X_r||^2 =  448234.4 e-6 = 88.8 %; (1+beta)*||Z_e-Z_q||^2 =  56413.1 e-6 = 11.2 %)
Min.  Avg. Train Loss across Mini-Batch =  489642.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  501108.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13697.9 e-6; = (1/var)*||X-X_r||^2 val-train = 11753.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1944.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.78; perplexity/K = 73.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.50; perplexity/K = 65.65%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  475003.8 e-6; = (1/var)*||X-X_r||^2 =  421531.1 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  53472.7 e-6 = 11.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  496423.5 e-6; = (1/var)*||X-X_r||^2 =  437227.3 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  59196.2 e-6 = 11.9 %)
Min.  Avg. Train Loss across Mini-Batch =  475003.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  496066.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21419.8 e-6; = (1/var)*||X-X_r||^2 val-train = 15696.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5723.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.90; perplexity/K = 68.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.62; perplexity/K = 72.61%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  462099.9 e-6; = (1/var)*||X-X_r||^2 =  413424.8 e-6 = 89.5 %; (1+beta)*||Z_e-Z_q||^2 =  48675.2 e-6 = 10.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  485328.7 e-6; = (1/var)*||X-X_r||^2 =  429123.1 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  56205.7 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  462099.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  485328.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23228.8 e-6; = (1/var)*||X-X_r||^2 val-train = 15698.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7530.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.89; perplexity/K = 68.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.16; perplexity/K = 63.51%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  452158.5 e-6; = (1/var)*||X-X_r||^2 =  408093.1 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  44065.4 e-6 = 9.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  484815.3 e-6; = (1/var)*||X-X_r||^2 =  433216.3 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  51599.0 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  452158.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  477910.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32656.8 e-6; = (1/var)*||X-X_r||^2 val-train = 25123.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7533.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.86; perplexity/K = 67.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.47; perplexity/K = 65.45%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  445329.2 e-6; = (1/var)*||X-X_r||^2 =  403811.2 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  41518.0 e-6 = 9.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  476030.9 e-6; = (1/var)*||X-X_r||^2 =  426965.1 e-6 = 89.7 %; (1+beta)*||Z_e-Z_q||^2 =  49065.7 e-6 = 10.3 %)
Min.  Avg. Train Loss across Mini-Batch =  443955.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  474955.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30701.7 e-6; = (1/var)*||X-X_r||^2 val-train = 23153.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7547.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.04; perplexity/K = 69.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.17; perplexity/K = 63.56%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  438576.3 e-6; = (1/var)*||X-X_r||^2 =  401082.4 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  37493.9 e-6 = 8.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  471481.8 e-6; = (1/var)*||X-X_r||^2 =  426676.9 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  44804.9 e-6 = 9.5 %)
Min.  Avg. Train Loss across Mini-Batch =  437309.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  471481.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32905.4 e-6; = (1/var)*||X-X_r||^2 val-train = 25594.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7311.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.71; perplexity/K = 66.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.78; perplexity/K = 67.38%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  437057.4 e-6; = (1/var)*||X-X_r||^2 =  400142.5 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  36915.0 e-6 = 8.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  483337.3 e-6; = (1/var)*||X-X_r||^2 =  437247.8 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  46089.5 e-6 = 9.5 %)
Min.  Avg. Train Loss across Mini-Batch =  432257.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  471372.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46279.8 e-6; = (1/var)*||X-X_r||^2 val-train = 37105.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9174.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.97; perplexity/K = 68.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.17; perplexity/K = 69.84%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  431438.1 e-6; = (1/var)*||X-X_r||^2 =  396649.7 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  34788.4 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  477963.1 e-6; = (1/var)*||X-X_r||^2 =  433029.2 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  44933.9 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  429171.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  471372.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46525.0 e-6; = (1/var)*||X-X_r||^2 val-train = 36379.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10145.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.13; perplexity/K = 69.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.49; perplexity/K = 71.81%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  426663.0 e-6; = (1/var)*||X-X_r||^2 =  393942.8 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  32720.3 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  477581.4 e-6; = (1/var)*||X-X_r||^2 =  433862.4 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  43719.0 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  426195.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  469963.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50918.4 e-6; = (1/var)*||X-X_r||^2 val-train = 39919.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10998.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.23; perplexity/K = 63.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.10; perplexity/K = 69.35%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  427471.2 e-6; = (1/var)*||X-X_r||^2 =  394477.3 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  32993.9 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  472730.0 e-6; = (1/var)*||X-X_r||^2 =  429943.4 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  42786.6 e-6 = 9.1 %)
Min.  Avg. Train Loss across Mini-Batch =  422679.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  468969.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   45258.8 e-6; = (1/var)*||X-X_r||^2 val-train = 35466.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9792.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.75; perplexity/K = 67.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.99; perplexity/K = 68.68%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  427030.7 e-6; = (1/var)*||X-X_r||^2 =  393728.4 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  33302.3 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  479178.9 e-6; = (1/var)*||X-X_r||^2 =  434518.8 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  44660.1 e-6 = 9.3 %)
Min.  Avg. Train Loss across Mini-Batch =  418920.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  468238.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52148.1 e-6; = (1/var)*||X-X_r||^2 val-train = 40790.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11357.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.30; perplexity/K = 70.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.60; perplexity/K = 66.27%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  420125.6 e-6; = (1/var)*||X-X_r||^2 =  390065.4 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  30060.1 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  467169.3 e-6; = (1/var)*||X-X_r||^2 =  426518.2 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  40651.0 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  417829.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465598.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47043.7 e-6; = (1/var)*||X-X_r||^2 val-train = 36452.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10590.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.75; perplexity/K = 67.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.03; perplexity/K = 68.94%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  424110.4 e-6; = (1/var)*||X-X_r||^2 =  391718.9 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  32391.4 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  475906.5 e-6; = (1/var)*||X-X_r||^2 =  432358.4 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  43548.2 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  415642.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465598.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51796.2 e-6; = (1/var)*||X-X_r||^2 val-train = 40639.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11156.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.46; perplexity/K = 65.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.31; perplexity/K = 64.46%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  413888.3 e-6; = (1/var)*||X-X_r||^2 =  386671.1 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  27217.2 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  471221.3 e-6; = (1/var)*||X-X_r||^2 =  433070.2 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  38151.1 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  413537.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465598.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57333.0 e-6; = (1/var)*||X-X_r||^2 val-train = 46399.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10933.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.49; perplexity/K = 71.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.03; perplexity/K = 68.95%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  411760.8 e-6; = (1/var)*||X-X_r||^2 =  385536.0 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  26224.8 e-6 = 6.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  467799.0 e-6; = (1/var)*||X-X_r||^2 =  430449.7 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  37349.3 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  411167.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  464982.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56038.2 e-6; = (1/var)*||X-X_r||^2 val-train = 44913.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11124.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.78; perplexity/K = 67.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.91; perplexity/K = 68.16%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410947.7 e-6; = (1/var)*||X-X_r||^2 =  384324.8 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  26622.9 e-6 = 6.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  469228.9 e-6; = (1/var)*||X-X_r||^2 =  430926.7 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  38302.3 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  409802.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  464584.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   58281.3 e-6; = (1/var)*||X-X_r||^2 val-train = 46601.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11679.4 e-6 

----------------------------------------------------------------------------------

Finished [16:28:36 09.01.2023] 417) Finished running for K = 16 & D = 16 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:1:29 h/m/s. 

--------------------------------------------------- 

Started [16:28:36 09.01.2023] 418) Finished running for K = 16 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(16, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 663 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.30
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.21
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.83
4                           encoder.sequential_convs.conv2d_5.weight                       131            19.76
5                                  encoder.pre_residual_stack.weight                       147            22.17
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
10                             encoder.channel_adjusting_conv.weight                         1             0.15
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                         9             1.36
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.43
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.43
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.76
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.83
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.30
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.13; perplexity/K = 75.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.72; perplexity/K = 73.23%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  746460.7 e-6; = (1/var)*||X-X_r||^2 =  604974.7 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  141486.0 e-6 = 19.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  783873.0 e-6; = (1/var)*||X-X_r||^2 =  586617.1 e-6 = 74.8 %; (1+beta)*||Z_e-Z_q||^2 =  197255.8 e-6 = 25.2 %)
Min.  Avg. Train Loss across Mini-Batch =  729874.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676183.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37412.3 e-6; = (1/var)*||X-X_r||^2 val-train = -18357.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 55769.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.01; perplexity/K = 75.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.51; perplexity/K = 71.96%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  554324.0 e-6; = (1/var)*||X-X_r||^2 =  516744.8 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  37579.2 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  543210.3 e-6; = (1/var)*||X-X_r||^2 =  506886.6 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  36323.7 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  552707.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  537973.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11113.7 e-6; = (1/var)*||X-X_r||^2 val-train = -9858.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1255.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 75.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.98; perplexity/K = 74.86%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  515070.2 e-6; = (1/var)*||X-X_r||^2 =  470248.4 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  44821.8 e-6 = 8.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  520610.6 e-6; = (1/var)*||X-X_r||^2 =  468528.2 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  52082.4 e-6 = 10.0 %)
Min.  Avg. Train Loss across Mini-Batch =  515070.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  508890.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5540.4 e-6; = (1/var)*||X-X_r||^2 val-train = -1720.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7260.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.71; perplexity/K = 79.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 76.63%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  500649.0 e-6; = (1/var)*||X-X_r||^2 =  450636.9 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  50012.0 e-6 = 10.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  501630.2 e-6; = (1/var)*||X-X_r||^2 =  448846.6 e-6 = 89.5 %; (1+beta)*||Z_e-Z_q||^2 =  52783.6 e-6 = 10.5 %)
Min.  Avg. Train Loss across Mini-Batch =  495350.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  499977.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   981.3 e-6; = (1/var)*||X-X_r||^2 val-train = -1790.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2771.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.40; perplexity/K = 77.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.88; perplexity/K = 74.28%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  475071.9 e-6; = (1/var)*||X-X_r||^2 =  427858.7 e-6 = 90.1 %; (1+beta)*||Z_e-Z_q||^2 =  47213.1 e-6 = 9.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  492608.0 e-6; = (1/var)*||X-X_r||^2 =  434081.5 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  58526.5 e-6 = 11.9 %)
Min.  Avg. Train Loss across Mini-Batch =  475071.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  488641.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17536.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6222.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11313.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.71; perplexity/K = 79.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.15; perplexity/K = 75.91%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  463090.2 e-6; = (1/var)*||X-X_r||^2 =  420603.9 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  42486.3 e-6 = 9.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  483597.1 e-6; = (1/var)*||X-X_r||^2 =  432305.6 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  51291.5 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  460391.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  478837.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20506.9 e-6; = (1/var)*||X-X_r||^2 val-train = 11701.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8805.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.33; perplexity/K = 77.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.83; perplexity/K = 80.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  461357.3 e-6; = (1/var)*||X-X_r||^2 =  419707.2 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  41650.1 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  487520.9 e-6; = (1/var)*||X-X_r||^2 =  433805.8 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  53715.1 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  446698.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  471133.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26163.6 e-6; = (1/var)*||X-X_r||^2 val-train = 14098.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12064.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.97; perplexity/K = 74.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.17; perplexity/K = 76.07%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  439541.9 e-6; = (1/var)*||X-X_r||^2 =  407560.0 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  31981.9 e-6 = 7.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  469089.3 e-6; = (1/var)*||X-X_r||^2 =  424871.0 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  44218.3 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  439066.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  466148.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29547.4 e-6; = (1/var)*||X-X_r||^2 val-train = 17311.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12236.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.73; perplexity/K = 79.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 76.89%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  433058.2 e-6; = (1/var)*||X-X_r||^2 =  403473.7 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  29584.4 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  467934.1 e-6; = (1/var)*||X-X_r||^2 =  424739.9 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  43194.3 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  431858.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  463455.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34875.9 e-6; = (1/var)*||X-X_r||^2 val-train = 21266.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13609.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.98; perplexity/K = 81.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.68; perplexity/K = 73.01%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  430743.8 e-6; = (1/var)*||X-X_r||^2 =  401456.9 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  29287.0 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  467452.4 e-6; = (1/var)*||X-X_r||^2 =  424522.1 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  42930.3 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  426778.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  461445.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36708.6 e-6; = (1/var)*||X-X_r||^2 val-train = 23065.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13643.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.13; perplexity/K = 75.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.22; perplexity/K = 76.38%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  426415.4 e-6; = (1/var)*||X-X_r||^2 =  398142.4 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  28273.0 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  463842.8 e-6; = (1/var)*||X-X_r||^2 =  424136.3 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  39706.4 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  423444.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  459741.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37427.3 e-6; = (1/var)*||X-X_r||^2 val-train = 25993.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11433.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.75; perplexity/K = 73.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.13; perplexity/K = 75.84%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  422190.8 e-6; = (1/var)*||X-X_r||^2 =  394581.5 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  27609.2 e-6 = 6.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  464937.3 e-6; = (1/var)*||X-X_r||^2 =  425097.9 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  39839.4 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  420153.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458203.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   42746.6 e-6; = (1/var)*||X-X_r||^2 val-train = 30516.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12230.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.39; perplexity/K = 77.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.50; perplexity/K = 78.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  417733.2 e-6; = (1/var)*||X-X_r||^2 =  391881.7 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  25851.4 e-6 = 6.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  460776.1 e-6; = (1/var)*||X-X_r||^2 =  424498.8 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  36277.3 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  417076.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458203.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43042.9 e-6; = (1/var)*||X-X_r||^2 val-train = 32617.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10425.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 75.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.94; perplexity/K = 74.62%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  417566.4 e-6; = (1/var)*||X-X_r||^2 =  391682.1 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  25884.3 e-6 = 6.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  466038.8 e-6; = (1/var)*||X-X_r||^2 =  428468.7 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  37570.1 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  414466.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458203.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48472.4 e-6; = (1/var)*||X-X_r||^2 val-train = 36786.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11685.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 76.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.41; perplexity/K = 77.54%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  423971.1 e-6; = (1/var)*||X-X_r||^2 =  394482.2 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  29488.9 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  466830.7 e-6; = (1/var)*||X-X_r||^2 =  427987.2 e-6 = 91.7 %; (1+beta)*||Z_e-Z_q||^2 =  38843.4 e-6 = 8.3 %)
Min.  Avg. Train Loss across Mini-Batch =  411704.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458203.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   42859.6 e-6; = (1/var)*||X-X_r||^2 val-train = 33505.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9354.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.85; perplexity/K = 74.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.04; perplexity/K = 75.27%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410613.7 e-6; = (1/var)*||X-X_r||^2 =  387573.5 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  23040.2 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  461175.0 e-6; = (1/var)*||X-X_r||^2 =  425996.7 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  35178.3 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  409091.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458203.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50561.3 e-6; = (1/var)*||X-X_r||^2 val-train = 38423.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12138.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 75.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.04; perplexity/K = 75.28%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  413588.2 e-6; = (1/var)*||X-X_r||^2 =  388883.8 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  24704.4 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  469836.2 e-6; = (1/var)*||X-X_r||^2 =  432815.4 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  37020.8 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  408309.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  457635.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   56248.0 e-6; = (1/var)*||X-X_r||^2 val-train = 43931.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12316.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 76.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.10; perplexity/K = 75.61%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  411120.0 e-6; = (1/var)*||X-X_r||^2 =  387460.0 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  23660.0 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  466777.7 e-6; = (1/var)*||X-X_r||^2 =  430294.1 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  36483.6 e-6 = 7.8 %)
Min.  Avg. Train Loss across Mini-Batch =  406570.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  456474.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   55657.7 e-6; = (1/var)*||X-X_r||^2 val-train = 42834.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12823.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.64; perplexity/K = 72.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.99; perplexity/K = 74.95%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  407252.9 e-6; = (1/var)*||X-X_r||^2 =  385054.7 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  22198.2 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  457760.7 e-6; = (1/var)*||X-X_r||^2 =  424874.6 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  32886.1 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  404788.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  454970.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50507.8 e-6; = (1/var)*||X-X_r||^2 val-train = 39819.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10687.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.09; perplexity/K = 75.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 77.01%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410568.2 e-6; = (1/var)*||X-X_r||^2 =  387062.2 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  23506.0 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  471687.2 e-6; = (1/var)*||X-X_r||^2 =  432332.3 e-6 = 91.7 %; (1+beta)*||Z_e-Z_q||^2 =  39354.9 e-6 = 8.3 %)
Min.  Avg. Train Loss across Mini-Batch =  402895.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  454591.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   61118.9 e-6; = (1/var)*||X-X_r||^2 val-train = 45270.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15848.9 e-6 

----------------------------------------------------------------------------------

Finished [17:19:18 09.01.2023] 418) Finished running for K = 16 & D = 8 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 16) change_channel_size_across_layers = True:
Total training time is = 0:1:41 h/m/s. 

--------------------------------------------------- 

Started [17:19:18 09.01.2023] 419) Finished running for K = 8 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 689 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.16
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.64
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.01
4                                  encoder.pre_residual_stack.weight                       147            21.34
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
9                              encoder.channel_adjusting_conv.weight                         4             0.58
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        36             5.22
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.22
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.22
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.01
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.64
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.16
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.44; perplexity/K = 55.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.39; perplexity/K = 54.83%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  629933.8 e-6; = (1/var)*||X-X_r||^2 =  526492.2 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  103441.6 e-6 = 16.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  632239.8 e-6; = (1/var)*||X-X_r||^2 =  514639.3 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  117600.5 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  629933.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  600853.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2305.9 e-6; = (1/var)*||X-X_r||^2 val-train = -11853.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14158.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.18; perplexity/K = 64.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.44; perplexity/K = 67.98%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  249538.0 e-6; = (1/var)*||X-X_r||^2 =  173834.1 e-6 = 69.7 %; (1+beta)*||Z_e-Z_q||^2 =  75703.9 e-6 = 30.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  247221.7 e-6; = (1/var)*||X-X_r||^2 =  177260.1 e-6 = 71.7 %; (1+beta)*||Z_e-Z_q||^2 =  69961.6 e-6 = 28.3 %)
Min.  Avg. Train Loss across Mini-Batch =  249538.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  247221.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2316.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3426.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5742.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.80; perplexity/K = 84.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.55; perplexity/K = 81.91%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  159955.6 e-6; = (1/var)*||X-X_r||^2 =  113178.2 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  46777.5 e-6 = 29.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  172068.7 e-6; = (1/var)*||X-X_r||^2 =  121269.6 e-6 = 70.5 %; (1+beta)*||Z_e-Z_q||^2 =  50799.0 e-6 = 29.5 %)
Min.  Avg. Train Loss across Mini-Batch =  159955.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  168936.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12113.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8091.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4021.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.58; perplexity/K = 82.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.71; perplexity/K = 83.89%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  128288.2 e-6; = (1/var)*||X-X_r||^2 =  88305.6 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  39982.6 e-6 = 31.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  139716.3 e-6; = (1/var)*||X-X_r||^2 =  97648.4 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  42067.9 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  125551.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  138169.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11428.1 e-6; = (1/var)*||X-X_r||^2 val-train = 9342.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2085.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.66; perplexity/K = 83.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.52; perplexity/K = 81.51%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  105636.0 e-6; = (1/var)*||X-X_r||^2 =  71396.8 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  34239.2 e-6 = 32.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  121561.5 e-6; = (1/var)*||X-X_r||^2 =  82795.9 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  38765.5 e-6 = 31.9 %)
Min.  Avg. Train Loss across Mini-Batch =  105563.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  121065.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15925.4 e-6; = (1/var)*||X-X_r||^2 val-train = 11399.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4526.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.65; perplexity/K = 83.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.60; perplexity/K = 82.52%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93819.0 e-6; = (1/var)*||X-X_r||^2 =  63254.6 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  30564.5 e-6 = 32.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  111007.4 e-6; = (1/var)*||X-X_r||^2 =  74795.5 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  36211.9 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  93259.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  110545.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17188.4 e-6; = (1/var)*||X-X_r||^2 val-train = 11540.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5647.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.72; perplexity/K = 83.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.64; perplexity/K = 83.03%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  98202.9 e-6; = (1/var)*||X-X_r||^2 =  62452.0 e-6 = 63.6 %; (1+beta)*||Z_e-Z_q||^2 =  35750.8 e-6 = 36.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  112709.1 e-6; = (1/var)*||X-X_r||^2 =  73081.6 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  39627.5 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  88988.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  105838.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14506.2 e-6; = (1/var)*||X-X_r||^2 val-train = 10629.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3876.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.65; perplexity/K = 83.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  77585.9 e-6; = (1/var)*||X-X_r||^2 =  52856.8 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  24729.1 e-6 = 31.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  96206.2 e-6; = (1/var)*||X-X_r||^2 =  65347.1 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  30859.2 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  77585.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  95975.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18620.3 e-6; = (1/var)*||X-X_r||^2 val-train = 12490.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6130.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.83; perplexity/K = 85.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.47; perplexity/K = 80.81%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  73853.7 e-6; = (1/var)*||X-X_r||^2 =  49216.6 e-6 = 66.6 %; (1+beta)*||Z_e-Z_q||^2 =  24637.1 e-6 = 33.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  90409.1 e-6; = (1/var)*||X-X_r||^2 =  60939.3 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  29469.7 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  70921.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  90274.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16555.4 e-6; = (1/var)*||X-X_r||^2 val-train = 11722.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4832.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.56; perplexity/K = 82.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.58; perplexity/K = 82.24%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  65593.5 e-6; = (1/var)*||X-X_r||^2 =  44240.8 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  21352.7 e-6 = 32.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  84703.7 e-6; = (1/var)*||X-X_r||^2 =  57423.0 e-6 = 67.8 %; (1+beta)*||Z_e-Z_q||^2 =  27280.7 e-6 = 32.2 %)
Min.  Avg. Train Loss across Mini-Batch =  63429.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  82909.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19110.2 e-6; = (1/var)*||X-X_r||^2 val-train = 13182.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5928.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.82; perplexity/K = 85.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.58; perplexity/K = 82.27%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59861.1 e-6; = (1/var)*||X-X_r||^2 =  40454.3 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  19406.8 e-6 = 32.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  80438.8 e-6; = (1/var)*||X-X_r||^2 =  55151.8 e-6 = 68.6 %; (1+beta)*||Z_e-Z_q||^2 =  25287.0 e-6 = 31.4 %)
Min.  Avg. Train Loss across Mini-Batch =  58942.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  77660.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20577.7 e-6; = (1/var)*||X-X_r||^2 val-train = 14697.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5880.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.82; perplexity/K = 85.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.51; perplexity/K = 81.35%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  56729.2 e-6; = (1/var)*||X-X_r||^2 =  38736.9 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  17992.3 e-6 = 31.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  81217.5 e-6; = (1/var)*||X-X_r||^2 =  55023.9 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  26193.6 e-6 = 32.3 %)
Min.  Avg. Train Loss across Mini-Batch =  55008.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74570.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   24488.3 e-6; = (1/var)*||X-X_r||^2 val-train = 16287.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8201.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.47; perplexity/K = 80.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.47; perplexity/K = 80.88%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68534.1 e-6; = (1/var)*||X-X_r||^2 =  42461.2 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  26072.9 e-6 = 38.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  86474.4 e-6; = (1/var)*||X-X_r||^2 =  55804.9 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  30669.5 e-6 = 35.5 %)
Min.  Avg. Train Loss across Mini-Batch =  53936.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72790.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17940.2 e-6; = (1/var)*||X-X_r||^2 val-train = 13343.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4596.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.66; perplexity/K = 83.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.59; perplexity/K = 82.36%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  58155.9 e-6; = (1/var)*||X-X_r||^2 =  38272.6 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  19883.3 e-6 = 34.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  82019.4 e-6; = (1/var)*||X-X_r||^2 =  56143.7 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  25875.6 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  53936.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72790.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23863.5 e-6; = (1/var)*||X-X_r||^2 val-train = 17871.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5992.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.58; perplexity/K = 82.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.52; perplexity/K = 81.45%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68111.9 e-6; = (1/var)*||X-X_r||^2 =  41404.2 e-6 = 60.8 %; (1+beta)*||Z_e-Z_q||^2 =  26707.8 e-6 = 39.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  82571.5 e-6; = (1/var)*||X-X_r||^2 =  53819.9 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  28751.5 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  52621.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72648.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14459.6 e-6; = (1/var)*||X-X_r||^2 val-train = 12415.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2043.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.60; perplexity/K = 82.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.50; perplexity/K = 81.31%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  58484.1 e-6; = (1/var)*||X-X_r||^2 =  36827.2 e-6 = 63.0 %; (1+beta)*||Z_e-Z_q||^2 =  21656.9 e-6 = 37.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  72873.5 e-6; = (1/var)*||X-X_r||^2 =  48070.9 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  24802.6 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  50215.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  69133.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14389.3 e-6; = (1/var)*||X-X_r||^2 val-train = 11243.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3145.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.64; perplexity/K = 82.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.62; perplexity/K = 82.73%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48813.2 e-6; = (1/var)*||X-X_r||^2 =  33912.5 e-6 = 69.5 %; (1+beta)*||Z_e-Z_q||^2 =  14900.6 e-6 = 30.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  69743.4 e-6; = (1/var)*||X-X_r||^2 =  49353.9 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  20389.4 e-6 = 29.2 %)
Min.  Avg. Train Loss across Mini-Batch =  48587.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67683.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20930.2 e-6; = (1/var)*||X-X_r||^2 val-train = 15441.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5488.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.62; perplexity/K = 82.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.73; perplexity/K = 84.15%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  58815.2 e-6; = (1/var)*||X-X_r||^2 =  35969.7 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  22845.5 e-6 = 38.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  72287.0 e-6; = (1/var)*||X-X_r||^2 =  47331.6 e-6 = 65.5 %; (1+beta)*||Z_e-Z_q||^2 =  24955.4 e-6 = 34.5 %)
Min.  Avg. Train Loss across Mini-Batch =  46639.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65142.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13471.7 e-6; = (1/var)*||X-X_r||^2 val-train = 11361.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2109.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.53; perplexity/K = 81.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.66; perplexity/K = 83.23%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45384.8 e-6; = (1/var)*||X-X_r||^2 =  32598.2 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  12786.5 e-6 = 28.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  64741.9 e-6; = (1/var)*||X-X_r||^2 =  47174.5 e-6 = 72.9 %; (1+beta)*||Z_e-Z_q||^2 =  17567.4 e-6 = 27.1 %)
Min.  Avg. Train Loss across Mini-Batch =  45376.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63852.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19357.2 e-6; = (1/var)*||X-X_r||^2 val-train = 14576.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4780.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.54; perplexity/K = 81.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.63; perplexity/K = 82.82%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60987.4 e-6; = (1/var)*||X-X_r||^2 =  37519.6 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  23467.8 e-6 = 38.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  73423.2 e-6; = (1/var)*||X-X_r||^2 =  48189.9 e-6 = 65.6 %; (1+beta)*||Z_e-Z_q||^2 =  25233.3 e-6 = 34.4 %)
Min.  Avg. Train Loss across Mini-Batch =  44682.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63691.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12435.9 e-6; = (1/var)*||X-X_r||^2 val-train = 10670.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1765.6 e-6 

----------------------------------------------------------------------------------

Finished [18:07:36 09.01.2023] 419) Finished running for K = 8 & D = 32 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:
Total training time is = 0:3:17 h/m/s. 

--------------------------------------------------- 

Started [18:07:36 09.01.2023] 420) Finished running for K = 8 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 16, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 16)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(16, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 669 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.20
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.78
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.58
4                                  encoder.pre_residual_stack.weight                       147            21.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
9                              encoder.channel_adjusting_conv.weight                         2             0.30
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                        18             2.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.38
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.60
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.38
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.60
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.58
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.78
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.20
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.52; perplexity/K = 31.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.65; perplexity/K = 33.11%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  707753.7 e-6; = (1/var)*||X-X_r||^2 =  436439.8 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  271313.9 e-6 = 38.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  659094.2 e-6; = (1/var)*||X-X_r||^2 =  422116.6 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  236977.6 e-6 = 36.0 %)
Min.  Avg. Train Loss across Mini-Batch =  707753.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  659094.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -48659.5 e-6; = (1/var)*||X-X_r||^2 val-train = -14323.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -34336.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.12; perplexity/K = 51.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.50; perplexity/K = 56.26%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  291180.6 e-6; = (1/var)*||X-X_r||^2 =  171801.8 e-6 = 59.0 %; (1+beta)*||Z_e-Z_q||^2 =  119378.7 e-6 = 41.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  298337.2 e-6; = (1/var)*||X-X_r||^2 =  177918.9 e-6 = 59.6 %; (1+beta)*||Z_e-Z_q||^2 =  120418.3 e-6 = 40.4 %)
Min.  Avg. Train Loss across Mini-Batch =  290791.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  298337.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7156.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6117.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1039.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.25; perplexity/K = 65.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.57; perplexity/K = 69.62%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  177126.4 e-6; = (1/var)*||X-X_r||^2 =  106437.6 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  70688.8 e-6 = 39.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  195108.1 e-6; = (1/var)*||X-X_r||^2 =  117852.8 e-6 = 60.4 %; (1+beta)*||Z_e-Z_q||^2 =  77255.2 e-6 = 39.6 %)
Min.  Avg. Train Loss across Mini-Batch =  177126.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  192941.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17981.6 e-6; = (1/var)*||X-X_r||^2 val-train = 11415.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6566.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.61; perplexity/K = 70.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.23; perplexity/K = 65.36%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  138822.5 e-6; = (1/var)*||X-X_r||^2 =  82545.6 e-6 = 59.5 %; (1+beta)*||Z_e-Z_q||^2 =  56277.0 e-6 = 40.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  157787.3 e-6; = (1/var)*||X-X_r||^2 =  96287.9 e-6 = 61.0 %; (1+beta)*||Z_e-Z_q||^2 =  61499.4 e-6 = 39.0 %)
Min.  Avg. Train Loss across Mini-Batch =  133665.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154766.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18964.8 e-6; = (1/var)*||X-X_r||^2 val-train = 13742.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5222.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.25; perplexity/K = 65.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.52; perplexity/K = 69.05%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  109596.8 e-6; = (1/var)*||X-X_r||^2 =  66445.8 e-6 = 60.6 %; (1+beta)*||Z_e-Z_q||^2 =  43151.0 e-6 = 39.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  135123.0 e-6; = (1/var)*||X-X_r||^2 =  83104.8 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  52018.3 e-6 = 38.5 %)
Min.  Avg. Train Loss across Mini-Batch =  109596.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  133056.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   25526.3 e-6; = (1/var)*||X-X_r||^2 val-train = 16659.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8867.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.53; perplexity/K = 69.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.42; perplexity/K = 67.72%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  111424.8 e-6; = (1/var)*||X-X_r||^2 =  62849.7 e-6 = 56.4 %; (1+beta)*||Z_e-Z_q||^2 =  48575.1 e-6 = 43.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  123151.5 e-6; = (1/var)*||X-X_r||^2 =  74442.9 e-6 = 60.4 %; (1+beta)*||Z_e-Z_q||^2 =  48708.6 e-6 = 39.6 %)
Min.  Avg. Train Loss across Mini-Batch =  92768.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  117322.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   11726.7 e-6; = (1/var)*||X-X_r||^2 val-train = 11593.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 133.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.59; perplexity/K = 69.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.64; perplexity/K = 70.48%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  86915.4 e-6; = (1/var)*||X-X_r||^2 =  52441.5 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  34473.9 e-6 = 39.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  125251.0 e-6; = (1/var)*||X-X_r||^2 =  76336.8 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  48914.2 e-6 = 39.1 %)
Min.  Avg. Train Loss across Mini-Batch =  79628.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  106371.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38335.6 e-6; = (1/var)*||X-X_r||^2 val-train = 23895.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14440.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.47; perplexity/K = 68.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.48; perplexity/K = 68.50%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72515.0 e-6; = (1/var)*||X-X_r||^2 =  46703.6 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  25811.4 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  100891.6 e-6; = (1/var)*||X-X_r||^2 =  66700.6 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  34190.9 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  71791.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  99457.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28376.6 e-6; = (1/var)*||X-X_r||^2 val-train = 19997.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8379.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.57; perplexity/K = 69.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.55; perplexity/K = 69.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  89560.3 e-6; = (1/var)*||X-X_r||^2 =  50337.0 e-6 = 56.2 %; (1+beta)*||Z_e-Z_q||^2 =  39223.3 e-6 = 43.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  105235.6 e-6; = (1/var)*||X-X_r||^2 =  64701.7 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  40533.8 e-6 = 38.5 %)
Min.  Avg. Train Loss across Mini-Batch =  66348.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  95322.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15675.3 e-6; = (1/var)*||X-X_r||^2 val-train = 14364.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1310.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.42; perplexity/K = 67.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.39; perplexity/K = 67.32%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61973.2 e-6; = (1/var)*||X-X_r||^2 =  41891.5 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  20081.7 e-6 = 32.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  90547.9 e-6; = (1/var)*||X-X_r||^2 =  62673.4 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  27874.5 e-6 = 30.8 %)
Min.  Avg. Train Loss across Mini-Batch =  61973.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  90547.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28574.7 e-6; = (1/var)*||X-X_r||^2 val-train = 20781.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7792.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.43; perplexity/K = 67.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.25; perplexity/K = 65.64%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59644.0 e-6; = (1/var)*||X-X_r||^2 =  40307.1 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  19336.9 e-6 = 32.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  87837.6 e-6; = (1/var)*||X-X_r||^2 =  60884.8 e-6 = 69.3 %; (1+beta)*||Z_e-Z_q||^2 =  26952.9 e-6 = 30.7 %)
Min.  Avg. Train Loss across Mini-Batch =  59160.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  87096.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28193.6 e-6; = (1/var)*||X-X_r||^2 val-train = 20577.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7615.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.49; perplexity/K = 68.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.27; perplexity/K = 65.82%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57483.1 e-6; = (1/var)*||X-X_r||^2 =  39906.0 e-6 = 69.4 %; (1+beta)*||Z_e-Z_q||^2 =  17577.1 e-6 = 30.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  86545.4 e-6; = (1/var)*||X-X_r||^2 =  61342.6 e-6 = 70.9 %; (1+beta)*||Z_e-Z_q||^2 =  25202.8 e-6 = 29.1 %)
Min.  Avg. Train Loss across Mini-Batch =  55674.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  85045.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29062.3 e-6; = (1/var)*||X-X_r||^2 val-train = 21436.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7625.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.57; perplexity/K = 69.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.46; perplexity/K = 68.29%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  54644.7 e-6; = (1/var)*||X-X_r||^2 =  38675.8 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  15968.9 e-6 = 29.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  85866.2 e-6; = (1/var)*||X-X_r||^2 =  61919.2 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  23947.0 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  53569.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  83574.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31221.5 e-6; = (1/var)*||X-X_r||^2 val-train = 23243.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7978.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.46; perplexity/K = 68.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.49; perplexity/K = 68.66%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52253.5 e-6; = (1/var)*||X-X_r||^2 =  37215.1 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  15038.4 e-6 = 28.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  82125.7 e-6; = (1/var)*||X-X_r||^2 =  58925.4 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  23200.3 e-6 = 28.2 %)
Min.  Avg. Train Loss across Mini-Batch =  52253.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  80946.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29872.2 e-6; = (1/var)*||X-X_r||^2 val-train = 21710.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8161.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.55; perplexity/K = 69.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.23; perplexity/K = 65.37%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52230.9 e-6; = (1/var)*||X-X_r||^2 =  37231.2 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  14999.7 e-6 = 28.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  81869.5 e-6; = (1/var)*||X-X_r||^2 =  57582.9 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  24286.6 e-6 = 29.7 %)
Min.  Avg. Train Loss across Mini-Batch =  49910.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  76449.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29638.7 e-6; = (1/var)*||X-X_r||^2 val-train = 20351.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9286.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.60; perplexity/K = 69.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.44; perplexity/K = 68.05%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48995.3 e-6; = (1/var)*||X-X_r||^2 =  35691.1 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  13304.2 e-6 = 27.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  78736.8 e-6; = (1/var)*||X-X_r||^2 =  58425.1 e-6 = 74.2 %; (1+beta)*||Z_e-Z_q||^2 =  20311.7 e-6 = 25.8 %)
Min.  Avg. Train Loss across Mini-Batch =  48995.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  75782.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29741.5 e-6; = (1/var)*||X-X_r||^2 val-train = 22734.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7007.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.40; perplexity/K = 67.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.43; perplexity/K = 67.91%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  62864.4 e-6; = (1/var)*||X-X_r||^2 =  39819.0 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  23045.4 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  109280.8 e-6; = (1/var)*||X-X_r||^2 =  66973.5 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  42307.3 e-6 = 38.7 %)
Min.  Avg. Train Loss across Mini-Batch =  47487.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  75006.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46416.4 e-6; = (1/var)*||X-X_r||^2 val-train = 27154.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19261.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.45; perplexity/K = 68.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.56; perplexity/K = 69.54%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  62632.2 e-6; = (1/var)*||X-X_r||^2 =  37167.1 e-6 = 59.3 %; (1+beta)*||Z_e-Z_q||^2 =  25465.1 e-6 = 40.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  83585.7 e-6; = (1/var)*||X-X_r||^2 =  53593.6 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  29992.1 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  46252.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74364.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20953.5 e-6; = (1/var)*||X-X_r||^2 val-train = 16426.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4527.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.64; perplexity/K = 70.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.34; perplexity/K = 66.70%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47351.6 e-6; = (1/var)*||X-X_r||^2 =  33815.8 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  13535.8 e-6 = 28.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  74446.1 e-6; = (1/var)*||X-X_r||^2 =  54018.4 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  20427.8 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  45466.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72556.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27094.5 e-6; = (1/var)*||X-X_r||^2 val-train = 20202.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6892.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.31; perplexity/K = 66.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.51; perplexity/K = 68.89%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43883.7 e-6; = (1/var)*||X-X_r||^2 =  33262.1 e-6 = 75.8 %; (1+beta)*||Z_e-Z_q||^2 =  10621.5 e-6 = 24.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  76387.5 e-6; = (1/var)*||X-X_r||^2 =  58134.4 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  18253.1 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  43883.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72556.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32503.8 e-6; = (1/var)*||X-X_r||^2 val-train = 24872.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7631.6 e-6 

----------------------------------------------------------------------------------

Finished [18:55:46 09.01.2023] 420) Finished running for K = 8 & D = 16 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:
Total training time is = 0:3:9 h/m/s. 

--------------------------------------------------- 

Started [18:55:46 09.01.2023] 421) Finished running for K = 8 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 8, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 8)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(8, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 659 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.21
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.86
3                           encoder.sequential_convs.conv2d_4.weight                       131            19.88
4                                  encoder.pre_residual_stack.weight                       147            22.31
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
9                              encoder.channel_adjusting_conv.weight                         1             0.15
10                                                       VQ.E.weight                         0             0.00
11                             decoder.channel_adjusting_conv.weight                         9             1.37
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.46
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.61
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.46
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.61
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            19.88
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.86
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.21
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.52; perplexity/K = 56.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.00; perplexity/K = 62.51%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  442426.8 e-6; = (1/var)*||X-X_r||^2 =  269323.7 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  173103.1 e-6 = 39.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  445428.4 e-6; = (1/var)*||X-X_r||^2 =  270424.6 e-6 = 60.7 %; (1+beta)*||Z_e-Z_q||^2 =  175003.8 e-6 = 39.3 %)
Min.  Avg. Train Loss across Mini-Batch =  442426.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431577.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3001.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1100.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1900.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.29; perplexity/K = 66.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.28; perplexity/K = 65.95%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  254922.4 e-6; = (1/var)*||X-X_r||^2 =  156129.1 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  98793.3 e-6 = 38.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  265002.6 e-6; = (1/var)*||X-X_r||^2 =  163720.1 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  101282.5 e-6 = 38.2 %)
Min.  Avg. Train Loss across Mini-Batch =  254922.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  259538.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10080.2 e-6; = (1/var)*||X-X_r||^2 val-train = 7591.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2489.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.62; perplexity/K = 70.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.59; perplexity/K = 69.83%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  186802.1 e-6; = (1/var)*||X-X_r||^2 =  113183.5 e-6 = 60.6 %; (1+beta)*||Z_e-Z_q||^2 =  73618.6 e-6 = 39.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  201599.3 e-6; = (1/var)*||X-X_r||^2 =  122921.8 e-6 = 61.0 %; (1+beta)*||Z_e-Z_q||^2 =  78677.5 e-6 = 39.0 %)
Min.  Avg. Train Loss across Mini-Batch =  186802.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  201462.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14797.2 e-6; = (1/var)*||X-X_r||^2 val-train = 9738.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5058.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.14; perplexity/K = 76.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.81; perplexity/K = 72.67%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  132032.9 e-6; = (1/var)*||X-X_r||^2 =  84914.6 e-6 = 64.3 %; (1+beta)*||Z_e-Z_q||^2 =  47118.4 e-6 = 35.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  147446.1 e-6; = (1/var)*||X-X_r||^2 =  97087.0 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  50359.1 e-6 = 34.2 %)
Min.  Avg. Train Loss across Mini-Batch =  131416.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  145707.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15413.1 e-6; = (1/var)*||X-X_r||^2 val-train = 12172.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3240.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.32; perplexity/K = 79.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.21; perplexity/K = 77.68%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  107344.2 e-6; = (1/var)*||X-X_r||^2 =  70602.5 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  36741.7 e-6 = 34.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  128809.4 e-6; = (1/var)*||X-X_r||^2 =  85145.1 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  43664.3 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  106973.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  126660.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21465.2 e-6; = (1/var)*||X-X_r||^2 val-train = 14542.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6922.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.23; perplexity/K = 77.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.06; perplexity/K = 75.76%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93185.2 e-6; = (1/var)*||X-X_r||^2 =  61868.5 e-6 = 66.4 %; (1+beta)*||Z_e-Z_q||^2 =  31316.8 e-6 = 33.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  114723.6 e-6; = (1/var)*||X-X_r||^2 =  76911.3 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  37812.3 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  92949.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  113534.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21538.3 e-6; = (1/var)*||X-X_r||^2 val-train = 15042.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6495.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.14; perplexity/K = 76.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.08; perplexity/K = 76.01%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  81316.1 e-6; = (1/var)*||X-X_r||^2 =  54843.9 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  26472.2 e-6 = 32.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  102376.8 e-6; = (1/var)*||X-X_r||^2 =  70153.3 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  32223.4 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  81316.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  102376.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21060.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15309.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5751.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.97; perplexity/K = 74.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.22; perplexity/K = 77.72%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  74754.2 e-6; = (1/var)*||X-X_r||^2 =  50900.6 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  23853.7 e-6 = 31.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  96627.5 e-6; = (1/var)*||X-X_r||^2 =  66362.1 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  30265.4 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  74300.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  95189.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21873.2 e-6; = (1/var)*||X-X_r||^2 val-train = 15461.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6411.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.28; perplexity/K = 78.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.32; perplexity/K = 78.99%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67850.5 e-6; = (1/var)*||X-X_r||^2 =  46687.2 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  21163.3 e-6 = 31.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  89677.9 e-6; = (1/var)*||X-X_r||^2 =  61676.9 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  28001.0 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  67815.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  89111.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21827.4 e-6; = (1/var)*||X-X_r||^2 val-train = 14989.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6837.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.16; perplexity/K = 77.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.12; perplexity/K = 76.50%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67256.7 e-6; = (1/var)*||X-X_r||^2 =  43769.7 e-6 = 65.1 %; (1+beta)*||Z_e-Z_q||^2 =  23487.0 e-6 = 34.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  86574.2 e-6; = (1/var)*||X-X_r||^2 =  57731.0 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  28843.2 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  63019.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  84745.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19317.5 e-6; = (1/var)*||X-X_r||^2 val-train = 13961.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5356.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.15; perplexity/K = 76.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.07; perplexity/K = 75.92%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57460.7 e-6; = (1/var)*||X-X_r||^2 =  40017.2 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  17443.5 e-6 = 30.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  80484.3 e-6; = (1/var)*||X-X_r||^2 =  57007.6 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  23476.7 e-6 = 29.2 %)
Min.  Avg. Train Loss across Mini-Batch =  57460.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  79618.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23023.6 e-6; = (1/var)*||X-X_r||^2 val-train = 16990.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6033.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.13; perplexity/K = 76.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.91; perplexity/K = 73.87%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  54983.7 e-6; = (1/var)*||X-X_r||^2 =  38665.9 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  16317.7 e-6 = 29.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  78389.5 e-6; = (1/var)*||X-X_r||^2 =  55977.0 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  22412.5 e-6 = 28.6 %)
Min.  Avg. Train Loss across Mini-Batch =  53582.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  76286.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23405.8 e-6; = (1/var)*||X-X_r||^2 val-train = 17311.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6094.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.04; perplexity/K = 75.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.22; perplexity/K = 77.71%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59147.0 e-6; = (1/var)*||X-X_r||^2 =  37441.8 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  21705.2 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  79327.2 e-6; = (1/var)*||X-X_r||^2 =  53528.8 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  25798.4 e-6 = 32.5 %)
Min.  Avg. Train Loss across Mini-Batch =  50092.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  73708.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20180.2 e-6; = (1/var)*||X-X_r||^2 val-train = 16087.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4093.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.11; perplexity/K = 76.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.18; perplexity/K = 77.29%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48987.8 e-6; = (1/var)*||X-X_r||^2 =  34957.9 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  14029.9 e-6 = 28.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  72948.1 e-6; = (1/var)*||X-X_r||^2 =  53744.4 e-6 = 73.7 %; (1+beta)*||Z_e-Z_q||^2 =  19203.7 e-6 = 26.3 %)
Min.  Avg. Train Loss across Mini-Batch =  47277.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  69945.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23960.2 e-6; = (1/var)*||X-X_r||^2 val-train = 18786.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5173.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.03; perplexity/K = 75.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.98; perplexity/K = 74.75%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50594.5 e-6; = (1/var)*||X-X_r||^2 =  34106.8 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  16487.7 e-6 = 32.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  68759.8 e-6; = (1/var)*||X-X_r||^2 =  48353.1 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  20406.7 e-6 = 29.7 %)
Min.  Avg. Train Loss across Mini-Batch =  45608.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67677.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18165.3 e-6; = (1/var)*||X-X_r||^2 val-train = 14246.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3919.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.07; perplexity/K = 75.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.14; perplexity/K = 76.73%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43374.2 e-6; = (1/var)*||X-X_r||^2 =  32022.3 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  11351.9 e-6 = 26.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  66119.6 e-6; = (1/var)*||X-X_r||^2 =  48980.7 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  17139.0 e-6 = 25.9 %)
Min.  Avg. Train Loss across Mini-Batch =  43150.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65222.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22745.5 e-6; = (1/var)*||X-X_r||^2 val-train = 16958.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5787.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.23; perplexity/K = 77.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.14; perplexity/K = 76.69%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44859.1 e-6; = (1/var)*||X-X_r||^2 =  31484.7 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  13374.4 e-6 = 29.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  65331.7 e-6; = (1/var)*||X-X_r||^2 =  47207.8 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  18123.9 e-6 = 27.7 %)
Min.  Avg. Train Loss across Mini-Batch =  41665.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63583.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20472.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15723.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4749.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.08; perplexity/K = 75.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.18; perplexity/K = 77.28%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40360.9 e-6; = (1/var)*||X-X_r||^2 =  30534.3 e-6 = 75.7 %; (1+beta)*||Z_e-Z_q||^2 =  9826.6 e-6 = 24.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  64939.9 e-6; = (1/var)*||X-X_r||^2 =  49663.9 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  15275.9 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  40358.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62848.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   24579.0 e-6; = (1/var)*||X-X_r||^2 val-train = 19129.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5449.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.20; perplexity/K = 77.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.95; perplexity/K = 74.36%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40585.3 e-6; = (1/var)*||X-X_r||^2 =  30054.7 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  10530.6 e-6 = 25.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  62845.4 e-6; = (1/var)*||X-X_r||^2 =  47333.5 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  15512.0 e-6 = 24.7 %)
Min.  Avg. Train Loss across Mini-Batch =  39469.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61837.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22260.1 e-6; = (1/var)*||X-X_r||^2 val-train = 17278.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4981.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.28; perplexity/K = 78.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.97; perplexity/K = 74.60%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39124.9 e-6; = (1/var)*||X-X_r||^2 =  29584.2 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  9540.7 e-6 = 24.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  61315.6 e-6; = (1/var)*||X-X_r||^2 =  46569.6 e-6 = 76.0 %; (1+beta)*||Z_e-Z_q||^2 =  14746.1 e-6 = 24.0 %)
Min.  Avg. Train Loss across Mini-Batch =  38358.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60121.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22190.7 e-6; = (1/var)*||X-X_r||^2 val-train = 16985.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5205.4 e-6 

----------------------------------------------------------------------------------

Finished [19:43:33 09.01.2023] 421) Finished running for K = 8 & D = 8 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 48) change_channel_size_across_layers = True:
Total training time is = 0:3:47 h/m/s. 

--------------------------------------------------- 

Started [19:43:33 09.01.2023] 422) Finished running for K = 8 & D = 32 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 12) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 32, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(8, 32)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(32, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 693 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.29
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.15
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.62
4                           encoder.sequential_convs.conv2d_5.weight                       131            18.90
5                                  encoder.pre_residual_stack.weight                       147            21.21
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
10                             encoder.channel_adjusting_conv.weight                         4             0.58
11                                                       VQ.E.weight                         0             0.00
12                             decoder.channel_adjusting_conv.weight                        36             5.19
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.19
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.58
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.19
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.58
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.90
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.62
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.15
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.29
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.17; perplexity/K = 39.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.12; perplexity/K = 39.01%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1257773.0 e-6; = (1/var)*||X-X_r||^2 =  915897.4 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  341875.6 e-6 = 27.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1233593.7 e-6; = (1/var)*||X-X_r||^2 =  883294.7 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  350299.0 e-6 = 28.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1020103.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  984475.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24179.3 e-6; = (1/var)*||X-X_r||^2 val-train = -32602.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8423.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.89; perplexity/K = 61.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.83; perplexity/K = 60.35%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  687674.4 e-6; = (1/var)*||X-X_r||^2 =  570222.6 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  117451.8 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  657034.8 e-6; = (1/var)*||X-X_r||^2 =  565867.1 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  91167.7 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  687674.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  649911.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30639.6 e-6; = (1/var)*||X-X_r||^2 val-train = -4355.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -26284.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.91; perplexity/K = 61.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.09; perplexity/K = 63.64%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  661864.4 e-6; = (1/var)*||X-X_r||^2 =  536235.2 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  125629.2 e-6 = 19.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  646949.6 e-6; = (1/var)*||X-X_r||^2 =  531701.9 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  115247.7 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  657289.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  630962.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -14914.8 e-6; = (1/var)*||X-X_r||^2 val-train = -4533.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10381.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.36; perplexity/K = 67.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.02; perplexity/K = 62.81%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  650419.7 e-6; = (1/var)*||X-X_r||^2 =  519094.9 e-6 = 79.8 %; (1+beta)*||Z_e-Z_q||^2 =  131324.8 e-6 = 20.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  650823.6 e-6; = (1/var)*||X-X_r||^2 =  519611.0 e-6 = 79.8 %; (1+beta)*||Z_e-Z_q||^2 =  131212.6 e-6 = 20.2 %)
Min.  Avg. Train Loss across Mini-Batch =  637547.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  630962.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   403.9 e-6; = (1/var)*||X-X_r||^2 val-train = 516.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -112.2 e-6 

----------------------------------------------------------------------------------

