current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 2/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14079718.7 e-6; = (1/var)*||X-X_r||^2 =  1128024.2 e-6 = 8.0 %; (1+beta)*||Z_e-Z_q||^2 =  12951694.6 e-6 = 92.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  88162512.3 e-6; = (1/var)*||X-X_r||^2 =  988633.7 e-6 = 1.1 %; (1+beta)*||Z_e-Z_q||^2 =  87173879.7 e-6 = 98.9 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   74082793.6 e-6; = (1/var)*||X-X_r||^2 val-train = -139390.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 74222185.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.33; perplexity/K = 0.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.34; perplexity/K = 0.26%
Epoch 4/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1939582444.8 e-6; = (1/var)*||X-X_r||^2 =  959418.4 e-6 = 0.0 %; (1+beta)*||Z_e-Z_q||^2 =  1938623005.5 e-6 = 100.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  731169519.4 e-6; = (1/var)*||X-X_r||^2 =  757965.0 e-6 = 0.1 %; (1+beta)*||Z_e-Z_q||^2 =  730411552.2 e-6 = 99.9 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1208412925.4 e-6; = (1/var)*||X-X_r||^2 val-train = -201453.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1208211453.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.96; perplexity/K = 0.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.83; perplexity/K = 0.94%
Epoch 6/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  724053149.9 e-6; = (1/var)*||X-X_r||^2 =  756559.8 e-6 = 0.1 %; (1+beta)*||Z_e-Z_q||^2 =  723296596.7 e-6 = 99.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1333411930.1 e-6; = (1/var)*||X-X_r||^2 =  689261.7 e-6 = 0.1 %; (1+beta)*||Z_e-Z_q||^2 =  1332722654.3 e-6 = 99.9 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   609358780.2 e-6; = (1/var)*||X-X_r||^2 val-train = -67298.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 609426057.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.63; perplexity/K = 1.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.69; perplexity/K = 1.11%
Epoch 8/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  185455769.7 e-6; = (1/var)*||X-X_r||^2 =  660215.3 e-6 = 0.4 %; (1+beta)*||Z_e-Z_q||^2 =  184795554.7 e-6 = 99.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  41853353.7 e-6; = (1/var)*||X-X_r||^2 =  624083.3 e-6 = 1.5 %; (1+beta)*||Z_e-Z_q||^2 =  41229270.5 e-6 = 98.5 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -143602415.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36132.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -143566284.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.02; perplexity/K = 0.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.92; perplexity/K = 0.96%
Epoch 10/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  134895168.6 e-6; = (1/var)*||X-X_r||^2 =  652563.9 e-6 = 0.5 %; (1+beta)*||Z_e-Z_q||^2 =  134242604.5 e-6 = 99.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  30308118.8 e-6; = (1/var)*||X-X_r||^2 =  655456.4 e-6 = 2.2 %; (1+beta)*||Z_e-Z_q||^2 =  29652662.9 e-6 = 97.8 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -104587049.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2892.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -104589941.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.74; perplexity/K = 1.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.75; perplexity/K = 1.12%
Epoch 12/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36718674.3 e-6; = (1/var)*||X-X_r||^2 =  637450.1 e-6 = 1.7 %; (1+beta)*||Z_e-Z_q||^2 =  36081224.0 e-6 = 98.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  21432791.1 e-6; = (1/var)*||X-X_r||^2 =  619279.3 e-6 = 2.9 %; (1+beta)*||Z_e-Z_q||^2 =  20813511.7 e-6 = 97.1 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15285883.2 e-6; = (1/var)*||X-X_r||^2 val-train = -18170.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15267712.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.06; perplexity/K = 1.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 1.18%
Epoch 14/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23664161.5 e-6; = (1/var)*||X-X_r||^2 =  618118.3 e-6 = 2.6 %; (1+beta)*||Z_e-Z_q||^2 =  23046043.2 e-6 = 97.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  23279735.8 e-6; = (1/var)*||X-X_r||^2 =  596820.2 e-6 = 2.6 %; (1+beta)*||Z_e-Z_q||^2 =  22682915.9 e-6 = 97.4 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -384425.7 e-6; = (1/var)*||X-X_r||^2 val-train = -21298.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -363127.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 1.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.02; perplexity/K = 1.18%
Epoch 16/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20623000.8 e-6; = (1/var)*||X-X_r||^2 =  603317.0 e-6 = 2.9 %; (1+beta)*||Z_e-Z_q||^2 =  20019684.0 e-6 = 97.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  20302426.7 e-6; = (1/var)*||X-X_r||^2 =  586736.8 e-6 = 2.9 %; (1+beta)*||Z_e-Z_q||^2 =  19715689.7 e-6 = 97.1 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -320574.1 e-6; = (1/var)*||X-X_r||^2 val-train = -16580.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -303994.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.22; perplexity/K = 0.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.25; perplexity/K = 0.64%
Epoch 18/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  320641646.0 e-6; = (1/var)*||X-X_r||^2 =  710042.2 e-6 = 0.2 %; (1+beta)*||Z_e-Z_q||^2 =  319931602.0 e-6 = 99.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  20016673.9 e-6; = (1/var)*||X-X_r||^2 =  753951.5 e-6 = 3.8 %; (1+beta)*||Z_e-Z_q||^2 =  19262722.6 e-6 = 96.2 %)
Min.  Avg. Train Loss across Mini-Batch =  14079718.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -300624972.1 e-6; = (1/var)*||X-X_r||^2 val-train = 43909.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -300668879.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.47; perplexity/K = 0.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.56; perplexity/K = 0.30%
Epoch 20/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8502715.8 e-6; = (1/var)*||X-X_r||^2 =  709065.1 e-6 = 8.3 %; (1+beta)*||Z_e-Z_q||^2 =  7793650.7 e-6 = 91.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2750813.5 e-6; = (1/var)*||X-X_r||^2 =  663746.9 e-6 = 24.1 %; (1+beta)*||Z_e-Z_q||^2 =  2087066.6 e-6 = 75.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8502715.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5751902.3 e-6; = (1/var)*||X-X_r||^2 val-train = -45318.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5706584.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.35; perplexity/K = 0.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.46; perplexity/K = 0.28%
Epoch 22/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6180616.0 e-6; = (1/var)*||X-X_r||^2 =  661741.4 e-6 = 10.7 %; (1+beta)*||Z_e-Z_q||^2 =  5518874.6 e-6 = 89.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5795366.1 e-6; = (1/var)*||X-X_r||^2 =  644099.6 e-6 = 11.1 %; (1+beta)*||Z_e-Z_q||^2 =  5151266.6 e-6 = 88.9 %)
Min.  Avg. Train Loss across Mini-Batch =  5792710.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -385249.8 e-6; = (1/var)*||X-X_r||^2 val-train = -17641.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -367608.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.01; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.02; perplexity/K = 0.40%
Epoch 24/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10898887.8 e-6; = (1/var)*||X-X_r||^2 =  635641.0 e-6 = 5.8 %; (1+beta)*||Z_e-Z_q||^2 =  10263246.7 e-6 = 94.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  11391422.6 e-6; = (1/var)*||X-X_r||^2 =  606977.0 e-6 = 5.3 %; (1+beta)*||Z_e-Z_q||^2 =  10784445.7 e-6 = 94.7 %)
Min.  Avg. Train Loss across Mini-Batch =  5792710.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   492534.8 e-6; = (1/var)*||X-X_r||^2 val-train = -28664.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 521199.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.06; perplexity/K = 0.40%
Epoch 26/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7719383.3 e-6; = (1/var)*||X-X_r||^2 =  602887.6 e-6 = 7.8 %; (1+beta)*||Z_e-Z_q||^2 =  7116495.6 e-6 = 92.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  6095600.2 e-6; = (1/var)*||X-X_r||^2 =  583646.9 e-6 = 9.6 %; (1+beta)*||Z_e-Z_q||^2 =  5511953.3 e-6 = 90.4 %)
Min.  Avg. Train Loss across Mini-Batch =  5792710.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1623783.1 e-6; = (1/var)*||X-X_r||^2 val-train = -19240.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1604542.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.66; perplexity/K = 0.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.60; perplexity/K = 0.51%
Epoch 28/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6108970.6 e-6; = (1/var)*||X-X_r||^2 =  586346.8 e-6 = 9.6 %; (1+beta)*||Z_e-Z_q||^2 =  5522623.9 e-6 = 90.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3556658.7 e-6; = (1/var)*||X-X_r||^2 =  578768.2 e-6 = 16.3 %; (1+beta)*||Z_e-Z_q||^2 =  2977890.4 e-6 = 83.7 %)
Min.  Avg. Train Loss across Mini-Batch =  5792710.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2552312.0 e-6; = (1/var)*||X-X_r||^2 val-train = -7578.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2544733.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.81; perplexity/K = 0.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.66; perplexity/K = 0.71%
Epoch 30/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:0:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4682227.1 e-6; = (1/var)*||X-X_r||^2 =  588416.8 e-6 = 12.6 %; (1+beta)*||Z_e-Z_q||^2 =  4093810.3 e-6 = 87.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3253193.8 e-6; = (1/var)*||X-X_r||^2 =  565016.0 e-6 = 17.4 %; (1+beta)*||Z_e-Z_q||^2 =  2688177.7 e-6 = 82.6 %)
Min.  Avg. Train Loss across Mini-Batch =  4682227.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1429033.3 e-6; = (1/var)*||X-X_r||^2 val-train = -23400.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1405632.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.65; perplexity/K = 0.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.58; perplexity/K = 0.89%
Epoch 32/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:1:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5982453.4 e-6; = (1/var)*||X-X_r||^2 =  567430.8 e-6 = 9.5 %; (1+beta)*||Z_e-Z_q||^2 =  5415022.5 e-6 = 90.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  18355558.6 e-6; = (1/var)*||X-X_r||^2 =  564166.0 e-6 = 3.1 %; (1+beta)*||Z_e-Z_q||^2 =  17791392.7 e-6 = 96.9 %)
Min.  Avg. Train Loss across Mini-Batch =  4309963.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12373105.3 e-6; = (1/var)*||X-X_r||^2 val-train = -3264.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12376370.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.56; perplexity/K = 0.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.64; perplexity/K = 0.91%
Epoch 34/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:1:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5388752.7 e-6; = (1/var)*||X-X_r||^2 =  553352.5 e-6 = 10.3 %; (1+beta)*||Z_e-Z_q||^2 =  4835400.1 e-6 = 89.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2047242.7 e-6; = (1/var)*||X-X_r||^2 =  532624.3 e-6 = 26.0 %; (1+beta)*||Z_e-Z_q||^2 =  1514618.4 e-6 = 74.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4309963.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3341510.0 e-6; = (1/var)*||X-X_r||^2 val-train = -20728.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3320781.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.58; perplexity/K = 0.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.76; perplexity/K = 0.74%
Epoch 36/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:1:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12624280.9 e-6; = (1/var)*||X-X_r||^2 =  714365.7 e-6 = 5.7 %; (1+beta)*||Z_e-Z_q||^2 =  11909915.1 e-6 = 94.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2984814.1 e-6; = (1/var)*||X-X_r||^2 =  613099.8 e-6 = 20.5 %; (1+beta)*||Z_e-Z_q||^2 =  2371714.4 e-6 = 79.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4309963.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9639466.8 e-6; = (1/var)*||X-X_r||^2 val-train = -101265.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9538200.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.89; perplexity/K = 1.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.00; perplexity/K = 1.17%
Epoch 38/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:1:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4461752.2 e-6; = (1/var)*||X-X_r||^2 =  571764.5 e-6 = 12.8 %; (1+beta)*||Z_e-Z_q||^2 =  3889987.7 e-6 = 87.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2159316.1 e-6; = (1/var)*||X-X_r||^2 =  544940.7 e-6 = 25.2 %; (1+beta)*||Z_e-Z_q||^2 =  1614375.4 e-6 = 74.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3390755.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2302436.1 e-6; = (1/var)*||X-X_r||^2 val-train = -26823.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2275612.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.42; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.61; perplexity/K = 1.29%
Epoch 40/40;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:1:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3585477.6 e-6; = (1/var)*||X-X_r||^2 =  493100.7 e-6 = 13.8 %; (1+beta)*||Z_e-Z_q||^2 =  3092376.9 e-6 = 86.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4066107.5 e-6; = (1/var)*||X-X_r||^2 =  473024.8 e-6 = 11.6 %; (1+beta)*||Z_e-Z_q||^2 =  3593082.6 e-6 = 88.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3390755.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1777329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   480629.8 e-6; = (1/var)*||X-X_r||^2 val-train = -20075.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 500705.6 e-6 

----------------------------------------------------------------------------------

