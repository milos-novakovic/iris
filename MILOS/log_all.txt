Started [11:16:27 01.01.2023] 201) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 847 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.12
1                           encoder.sequential_convs.conv2d_2.weight                        32             3.78
2                           encoder.sequential_convs.conv2d_3.weight                       131            15.47
3                                  encoder.pre_residual_stack.weight                       147            17.36
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
8                              encoder.channel_adjusting_conv.weight                         8             0.94
9                                                        VQ.E.weight                       131            15.47
10                             decoder.channel_adjusting_conv.weight                        73             8.62
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.47
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.78
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.82; perplexity/K = 2.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.99; perplexity/K = 2.59%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  267358.0 e-6; = (1/var)*||X-X_r||^2 =  185075.8 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  82282.2 e-6 = 30.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  266668.9 e-6; = (1/var)*||X-X_r||^2 =  187945.3 e-6 = 70.5 %; (1+beta)*||Z_e-Z_q||^2 =  78723.6 e-6 = 29.5 %)
Min.  Avg. Train Loss across Mini-Batch =  267358.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  266668.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -689.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2869.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3558.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.52; perplexity/K = 1.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.33; perplexity/K = 1.77%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  111956.4 e-6; = (1/var)*||X-X_r||^2 =  87536.4 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  24419.9 e-6 = 21.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  115989.4 e-6; = (1/var)*||X-X_r||^2 =  91316.4 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  24673.0 e-6 = 21.3 %)
Min.  Avg. Train Loss across Mini-Batch =  111956.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  114946.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4033.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3780.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 253.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.44; perplexity/K = 1.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.74; perplexity/K = 1.35%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47887.5 e-6; = (1/var)*||X-X_r||^2 =  36483.7 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  11403.8 e-6 = 23.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  50572.6 e-6; = (1/var)*||X-X_r||^2 =  39266.6 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  11306.0 e-6 = 22.4 %)
Min.  Avg. Train Loss across Mini-Batch =  47887.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50572.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2685.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2783.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -97.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.95; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.67; perplexity/K = 1.55%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25120.6 e-6; = (1/var)*||X-X_r||^2 =  17059.0 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  8061.7 e-6 = 32.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  28358.7 e-6; = (1/var)*||X-X_r||^2 =  19615.5 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  8743.3 e-6 = 30.8 %)
Min.  Avg. Train Loss across Mini-Batch =  25120.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28358.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3238.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2556.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 681.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.62; perplexity/K = 1.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.98; perplexity/K = 1.41%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13543.5 e-6; = (1/var)*||X-X_r||^2 =  7520.3 e-6 = 55.5 %; (1+beta)*||Z_e-Z_q||^2 =  6023.2 e-6 = 44.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  15497.4 e-6; = (1/var)*||X-X_r||^2 =  9409.8 e-6 = 60.7 %; (1+beta)*||Z_e-Z_q||^2 =  6087.6 e-6 = 39.3 %)
Min.  Avg. Train Loss across Mini-Batch =  13543.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15497.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1953.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1889.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 64.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.25; perplexity/K = 1.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.85; perplexity/K = 1.31%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8545.4 e-6; = (1/var)*||X-X_r||^2 =  3993.2 e-6 = 46.7 %; (1+beta)*||Z_e-Z_q||^2 =  4552.2 e-6 = 53.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  10410.7 e-6; = (1/var)*||X-X_r||^2 =  5507.9 e-6 = 52.9 %; (1+beta)*||Z_e-Z_q||^2 =  4902.8 e-6 = 47.1 %)
Min.  Avg. Train Loss across Mini-Batch =  8545.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10243.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1865.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1514.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 350.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.14; perplexity/K = 1.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.03; perplexity/K = 1.76%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17561.8 e-6; = (1/var)*||X-X_r||^2 =  10594.1 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  6967.7 e-6 = 39.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  19102.2 e-6; = (1/var)*||X-X_r||^2 =  12167.5 e-6 = 63.7 %; (1+beta)*||Z_e-Z_q||^2 =  6934.7 e-6 = 36.3 %)
Min.  Avg. Train Loss across Mini-Batch =  7244.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8454.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1540.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1573.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -33.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.26; perplexity/K = 1.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.57; perplexity/K = 1.49%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5541.2 e-6; = (1/var)*||X-X_r||^2 =  3049.7 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  2491.5 e-6 = 45.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  6665.3 e-6; = (1/var)*||X-X_r||^2 =  4153.1 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  2512.3 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  5541.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6665.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1124.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1103.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.45; perplexity/K = 1.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.72; perplexity/K = 1.35%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10119.4 e-6; = (1/var)*||X-X_r||^2 =  7255.7 e-6 = 71.7 %; (1+beta)*||Z_e-Z_q||^2 =  2863.7 e-6 = 28.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  11211.9 e-6; = (1/var)*||X-X_r||^2 =  8409.1 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  2802.8 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4421.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5466.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1092.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1153.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -60.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.52; perplexity/K = 1.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.20; perplexity/K = 1.04%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5928.0 e-6; = (1/var)*||X-X_r||^2 =  4309.1 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  1618.9 e-6 = 27.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  6730.3 e-6; = (1/var)*||X-X_r||^2 =  5085.5 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  1644.8 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4421.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5466.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   802.3 e-6; = (1/var)*||X-X_r||^2 val-train = 776.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.98; perplexity/K = 1.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.93; perplexity/K = 1.02%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4521.2 e-6; = (1/var)*||X-X_r||^2 =  3215.6 e-6 = 71.1 %; (1+beta)*||Z_e-Z_q||^2 =  1305.6 e-6 = 28.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5404.5 e-6; = (1/var)*||X-X_r||^2 =  3999.8 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  1404.7 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4421.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5404.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   883.3 e-6; = (1/var)*||X-X_r||^2 val-train = 784.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 99.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.15; perplexity/K = 0.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.37; perplexity/K = 1.09%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3972.4 e-6; = (1/var)*||X-X_r||^2 =  2832.6 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  1139.7 e-6 = 28.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4632.0 e-6; = (1/var)*||X-X_r||^2 =  3452.4 e-6 = 74.5 %; (1+beta)*||Z_e-Z_q||^2 =  1179.6 e-6 = 25.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3870.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4632.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   659.7 e-6; = (1/var)*||X-X_r||^2 val-train = 619.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.57; perplexity/K = 1.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.21; perplexity/K = 1.04%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3423.5 e-6; = (1/var)*||X-X_r||^2 =  2336.7 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  1086.7 e-6 = 31.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4151.7 e-6; = (1/var)*||X-X_r||^2 =  3008.7 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  1143.0 e-6 = 27.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3320.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4065.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   728.3 e-6; = (1/var)*||X-X_r||^2 val-train = 672.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 56.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.68; perplexity/K = 1.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.82; perplexity/K = 1.07%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2949.0 e-6; = (1/var)*||X-X_r||^2 =  2038.6 e-6 = 69.1 %; (1+beta)*||Z_e-Z_q||^2 =  910.4 e-6 = 30.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3648.4 e-6; = (1/var)*||X-X_r||^2 =  2668.4 e-6 = 73.1 %; (1+beta)*||Z_e-Z_q||^2 =  980.0 e-6 = 26.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2912.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3637.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   699.4 e-6; = (1/var)*||X-X_r||^2 val-train = 629.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 69.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.89; perplexity/K = 0.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.93; perplexity/K = 0.92%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2801.3 e-6; = (1/var)*||X-X_r||^2 =  1853.3 e-6 = 66.2 %; (1+beta)*||Z_e-Z_q||^2 =  947.9 e-6 = 33.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3453.8 e-6; = (1/var)*||X-X_r||^2 =  2440.4 e-6 = 70.7 %; (1+beta)*||Z_e-Z_q||^2 =  1013.4 e-6 = 29.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2788.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3433.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   652.6 e-6; = (1/var)*||X-X_r||^2 val-train = 587.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 65.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.83; perplexity/K = 0.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.44; perplexity/K = 0.95%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2814.3 e-6; = (1/var)*||X-X_r||^2 =  1936.6 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  877.7 e-6 = 31.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3306.9 e-6; = (1/var)*||X-X_r||^2 =  2385.0 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  921.9 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2580.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3200.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   492.7 e-6; = (1/var)*||X-X_r||^2 val-train = 448.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 44.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.00; perplexity/K = 0.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.89; perplexity/K = 0.87%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2740.7 e-6; = (1/var)*||X-X_r||^2 =  1618.1 e-6 = 59.0 %; (1+beta)*||Z_e-Z_q||^2 =  1122.6 e-6 = 41.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3404.2 e-6; = (1/var)*||X-X_r||^2 =  2258.6 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  1145.6 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2496.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3131.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   663.5 e-6; = (1/var)*||X-X_r||^2 val-train = 640.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.81; perplexity/K = 1.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.43; perplexity/K = 1.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3139.3 e-6; = (1/var)*||X-X_r||^2 =  2164.7 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  974.7 e-6 = 31.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4017.8 e-6; = (1/var)*||X-X_r||^2 =  3136.1 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  881.7 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2300.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2927.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   878.5 e-6; = (1/var)*||X-X_r||^2 val-train = 971.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -93.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.53; perplexity/K = 0.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.07; perplexity/K = 0.93%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2251.3 e-6; = (1/var)*||X-X_r||^2 =  1375.6 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  875.7 e-6 = 38.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2814.7 e-6; = (1/var)*||X-X_r||^2 =  1934.4 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  880.2 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2196.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2800.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   563.4 e-6; = (1/var)*||X-X_r||^2 val-train = 558.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.74; perplexity/K = 0.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.40; perplexity/K = 0.75%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2123.4 e-6; = (1/var)*||X-X_r||^2 =  1321.3 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  802.1 e-6 = 37.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2822.7 e-6; = (1/var)*||X-X_r||^2 =  1970.8 e-6 = 69.8 %; (1+beta)*||Z_e-Z_q||^2 =  852.0 e-6 = 30.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2057.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2668.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   699.3 e-6; = (1/var)*||X-X_r||^2 val-train = 649.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 49.8 e-6 

----------------------------------------------------------------------------------

Finished [12:06:36 01.01.2023] 201) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = True:
Total training time is = 0:7:9 h/m/s. 

--------------------------------------------------- 

Started [12:06:36 01.01.2023] 202) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 704) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2523 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.19
2                           encoder.sequential_convs.conv2d_3.weight                       524            20.77
3                                  encoder.pre_residual_stack.weight                       589            23.35
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.89
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.89
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
8                              encoder.channel_adjusting_conv.weight                        16             0.63
9                                                        VQ.E.weight                       131             5.19
10                             decoder.channel_adjusting_conv.weight                       147             5.83
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.89
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.89
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.77
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.19
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.08; perplexity/K = 0.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.46; perplexity/K = 0.61%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  165993.8 e-6; = (1/var)*||X-X_r||^2 =  103981.4 e-6 = 62.6 %; (1+beta)*||Z_e-Z_q||^2 =  62012.4 e-6 = 37.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  169193.0 e-6; = (1/var)*||X-X_r||^2 =  109554.1 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  59638.9 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  165993.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  169193.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3199.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5572.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2373.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.63; perplexity/K = 0.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.05; perplexity/K = 0.64%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49620.6 e-6; = (1/var)*||X-X_r||^2 =  29871.2 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  19749.5 e-6 = 39.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  54637.3 e-6; = (1/var)*||X-X_r||^2 =  33776.0 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  20861.3 e-6 = 38.2 %)
Min.  Avg. Train Loss across Mini-Batch =  49620.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52645.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5016.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3904.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1111.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.52; perplexity/K = 0.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.87; perplexity/K = 0.63%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39119.4 e-6; = (1/var)*||X-X_r||^2 =  24113.8 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  15005.5 e-6 = 38.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  41995.3 e-6; = (1/var)*||X-X_r||^2 =  27091.6 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  14903.7 e-6 = 35.5 %)
Min.  Avg. Train Loss across Mini-Batch =  39119.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41995.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2875.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2977.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -101.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.81; perplexity/K = 0.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.35; perplexity/K = 0.65%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15179.1 e-6; = (1/var)*||X-X_r||^2 =  7444.5 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  7734.6 e-6 = 51.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  16121.3 e-6; = (1/var)*||X-X_r||^2 =  8788.3 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  7333.0 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  14808.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16121.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   942.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1343.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -401.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.63; perplexity/K = 0.76%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14816.7 e-6; = (1/var)*||X-X_r||^2 =  9161.3 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  5655.4 e-6 = 38.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  16877.6 e-6; = (1/var)*||X-X_r||^2 =  11159.2 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  5718.4 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  10616.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12349.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2060.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1997.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.41; perplexity/K = 0.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.68; perplexity/K = 0.77%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7393.6 e-6; = (1/var)*||X-X_r||^2 =  4159.7 e-6 = 56.3 %; (1+beta)*||Z_e-Z_q||^2 =  3234.0 e-6 = 43.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  9051.0 e-6; = (1/var)*||X-X_r||^2 =  5800.6 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  3250.4 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  6875.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8729.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1657.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1640.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.24; perplexity/K = 0.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.70; perplexity/K = 0.67%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4719.8 e-6; = (1/var)*||X-X_r||^2 =  2645.5 e-6 = 56.1 %; (1+beta)*||Z_e-Z_q||^2 =  2074.3 e-6 = 43.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  6463.1 e-6; = (1/var)*||X-X_r||^2 =  4253.5 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  2209.6 e-6 = 34.2 %)
Min.  Avg. Train Loss across Mini-Batch =  4593.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6258.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1743.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1608.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 135.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.69; perplexity/K = 0.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.68; perplexity/K = 0.72%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6707.0 e-6; = (1/var)*||X-X_r||^2 =  3159.7 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  3547.3 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  7590.0 e-6; = (1/var)*||X-X_r||^2 =  4188.5 e-6 = 55.2 %; (1+beta)*||Z_e-Z_q||^2 =  3401.5 e-6 = 44.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4082.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5763.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   882.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1028.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -145.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.98; perplexity/K = 0.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 0.67%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3036.1 e-6; = (1/var)*||X-X_r||^2 =  1222.2 e-6 = 40.3 %; (1+beta)*||Z_e-Z_q||^2 =  1813.9 e-6 = 59.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4052.9 e-6; = (1/var)*||X-X_r||^2 =  2263.8 e-6 = 55.9 %; (1+beta)*||Z_e-Z_q||^2 =  1789.1 e-6 = 44.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2644.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3830.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1016.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1041.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -24.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.19; perplexity/K = 0.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.34; perplexity/K = 0.70%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2288.1 e-6; = (1/var)*||X-X_r||^2 =  867.8 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  1420.3 e-6 = 62.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3284.5 e-6; = (1/var)*||X-X_r||^2 =  1816.8 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  1467.6 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1862.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2865.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   996.4 e-6; = (1/var)*||X-X_r||^2 val-train = 949.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 47.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 0.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.89; perplexity/K = 0.68%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1597.3 e-6; = (1/var)*||X-X_r||^2 =  800.8 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  796.6 e-6 = 49.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2888.3 e-6; = (1/var)*||X-X_r||^2 =  2045.2 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  843.1 e-6 = 29.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1329.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2245.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1290.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1244.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 46.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.66; perplexity/K = 0.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 0.61%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1240.5 e-6; = (1/var)*||X-X_r||^2 =  474.3 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  766.2 e-6 = 61.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2091.3 e-6; = (1/var)*||X-X_r||^2 =  1298.3 e-6 = 62.1 %; (1+beta)*||Z_e-Z_q||^2 =  793.0 e-6 = 37.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1128.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1929.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   850.9 e-6; = (1/var)*||X-X_r||^2 val-train = 824.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.65; perplexity/K = 0.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.53; perplexity/K = 0.61%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1064.5 e-6; = (1/var)*||X-X_r||^2 =  499.7 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  564.8 e-6 = 53.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1903.5 e-6; = (1/var)*||X-X_r||^2 =  1340.3 e-6 = 70.4 %; (1+beta)*||Z_e-Z_q||^2 =  563.2 e-6 = 29.6 %)
Min.  Avg. Train Loss across Mini-Batch =  928.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1758.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   839.0 e-6; = (1/var)*||X-X_r||^2 val-train = 840.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.39; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.44; perplexity/K = 0.71%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  824.6 e-6; = (1/var)*||X-X_r||^2 =  335.8 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  488.8 e-6 = 59.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1786.2 e-6; = (1/var)*||X-X_r||^2 =  1212.8 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  573.4 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  798.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1616.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   961.6 e-6; = (1/var)*||X-X_r||^2 val-train = 877.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 84.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.28; perplexity/K = 0.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.09; perplexity/K = 0.79%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5594.0 e-6; = (1/var)*||X-X_r||^2 =  3349.3 e-6 = 59.9 %; (1+beta)*||Z_e-Z_q||^2 =  2244.7 e-6 = 40.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  4960.0 e-6; = (1/var)*||X-X_r||^2 =  2940.9 e-6 = 59.3 %; (1+beta)*||Z_e-Z_q||^2 =  2019.1 e-6 = 40.7 %)
Min.  Avg. Train Loss across Mini-Batch =  686.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1459.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -634.0 e-6; = (1/var)*||X-X_r||^2 val-train = -408.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -225.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.51; perplexity/K = 0.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 0.59%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  830.0 e-6; = (1/var)*||X-X_r||^2 =  327.5 e-6 = 39.5 %; (1+beta)*||Z_e-Z_q||^2 =  502.5 e-6 = 60.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1539.5 e-6; = (1/var)*||X-X_r||^2 =  1051.1 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  488.4 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  578.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1340.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   709.5 e-6; = (1/var)*||X-X_r||^2 val-train = 723.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.73; perplexity/K = 0.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.02; perplexity/K = 0.49%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  626.3 e-6; = (1/var)*||X-X_r||^2 =  249.0 e-6 = 39.8 %; (1+beta)*||Z_e-Z_q||^2 =  377.3 e-6 = 60.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1444.8 e-6; = (1/var)*||X-X_r||^2 =  1061.3 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  383.5 e-6 = 26.5 %)
Min.  Avg. Train Loss across Mini-Batch =  553.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1340.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   818.5 e-6; = (1/var)*||X-X_r||^2 val-train = 812.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.31; perplexity/K = 0.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 0.63%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  940.6 e-6; = (1/var)*||X-X_r||^2 =  504.9 e-6 = 53.7 %; (1+beta)*||Z_e-Z_q||^2 =  435.7 e-6 = 46.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1531.8 e-6; = (1/var)*||X-X_r||^2 =  1140.1 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  391.7 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  510.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1299.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   591.2 e-6; = (1/var)*||X-X_r||^2 val-train = 635.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -44.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.81; perplexity/K = 0.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.97; perplexity/K = 0.54%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  524.2 e-6; = (1/var)*||X-X_r||^2 =  201.1 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  323.0 e-6 = 61.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1338.8 e-6; = (1/var)*||X-X_r||^2 =  1005.5 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  333.3 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  474.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1299.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   814.6 e-6; = (1/var)*||X-X_r||^2 val-train = 804.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.52; perplexity/K = 0.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.69; perplexity/K = 0.72%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2215.8 e-6; = (1/var)*||X-X_r||^2 =  1625.2 e-6 = 73.3 %; (1+beta)*||Z_e-Z_q||^2 =  590.5 e-6 = 26.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  3014.5 e-6; = (1/var)*||X-X_r||^2 =  2430.5 e-6 = 80.6 %; (1+beta)*||Z_e-Z_q||^2 =  584.1 e-6 = 19.4 %)
Min.  Avg. Train Loss across Mini-Batch =  474.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1281.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   798.8 e-6; = (1/var)*||X-X_r||^2 val-train = 805.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6.5 e-6 

----------------------------------------------------------------------------------

Finished [13:14:26 01.01.2023] 202) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 704) change_channel_size_across_layers = True:
Total training time is = 0:7:49 h/m/s. 

--------------------------------------------------- 

Started [13:14:26 01.01.2023] 203) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1579 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.38
1                           encoder.sequential_convs.conv2d_2.weight                       262            16.59
2                           encoder.sequential_convs.conv2d_3.weight                       262            16.59
3                                  encoder.pre_residual_stack.weight                       147             9.31
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.28
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.25
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.28
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.25
8                              encoder.channel_adjusting_conv.weight                         8             0.51
9                                                        VQ.E.weight                       131             8.30
10                             decoder.channel_adjusting_conv.weight                        73             4.62
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.28
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.25
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.28
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.25
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            16.59
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            16.59
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.38

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.91; perplexity/K = 0.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.74; perplexity/K = 1.01%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  298887.2 e-6; = (1/var)*||X-X_r||^2 =  152569.9 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  146317.3 e-6 = 49.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  304872.6 e-6; = (1/var)*||X-X_r||^2 =  162191.0 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  142681.6 e-6 = 46.8 %)
Min.  Avg. Train Loss across Mini-Batch =  298887.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  299505.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5985.4 e-6; = (1/var)*||X-X_r||^2 val-train = 9621.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3635.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.76; perplexity/K = 1.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.01; perplexity/K = 0.98%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  129007.0 e-6; = (1/var)*||X-X_r||^2 =  71366.4 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  57640.6 e-6 = 44.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  93343.3 e-6; = (1/var)*||X-X_r||^2 =  53349.3 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  39994.0 e-6 = 42.8 %)
Min.  Avg. Train Loss across Mini-Batch =  84706.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  89907.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35663.7 e-6; = (1/var)*||X-X_r||^2 val-train = -18017.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17646.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.39; perplexity/K = 0.65%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  145591.1 e-6; = (1/var)*||X-X_r||^2 =  57302.1 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  88289.0 e-6 = 60.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  86566.8 e-6; = (1/var)*||X-X_r||^2 =  47467.6 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  39099.2 e-6 = 45.2 %)
Min.  Avg. Train Loss across Mini-Batch =  31594.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34794.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -59024.3 e-6; = (1/var)*||X-X_r||^2 val-train = -9834.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -49189.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.07; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.25; perplexity/K = 0.70%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19454.9 e-6; = (1/var)*||X-X_r||^2 =  8610.8 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  10844.2 e-6 = 55.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  21782.2 e-6; = (1/var)*||X-X_r||^2 =  11213.2 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  10569.0 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  18340.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20839.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2327.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2602.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -275.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.61; perplexity/K = 0.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.53; perplexity/K = 0.71%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28969.6 e-6; = (1/var)*||X-X_r||^2 =  14904.5 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  14065.1 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  33159.1 e-6; = (1/var)*||X-X_r||^2 =  18056.8 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  15102.3 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  13519.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15269.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4189.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3152.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1037.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.39; perplexity/K = 0.61%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13758.5 e-6; = (1/var)*||X-X_r||^2 =  6432.7 e-6 = 46.8 %; (1+beta)*||Z_e-Z_q||^2 =  7325.9 e-6 = 53.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  16514.7 e-6; = (1/var)*||X-X_r||^2 =  8954.7 e-6 = 54.2 %; (1+beta)*||Z_e-Z_q||^2 =  7560.1 e-6 = 45.8 %)
Min.  Avg. Train Loss across Mini-Batch =  13519.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15269.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2756.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2522.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 234.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 0.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.17; perplexity/K = 0.59%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9731.9 e-6; = (1/var)*||X-X_r||^2 =  4226.8 e-6 = 43.4 %; (1+beta)*||Z_e-Z_q||^2 =  5505.1 e-6 = 56.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  10855.1 e-6; = (1/var)*||X-X_r||^2 =  5870.4 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  4984.8 e-6 = 45.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8658.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10854.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1123.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1643.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -520.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.30; perplexity/K = 0.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.48; perplexity/K = 0.80%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18945.9 e-6; = (1/var)*||X-X_r||^2 =  8687.8 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  10258.1 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  20635.3 e-6; = (1/var)*||X-X_r||^2 =  10675.6 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  9959.7 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  5638.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7606.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1689.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1987.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -298.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.01; perplexity/K = 0.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.13; perplexity/K = 0.54%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6331.4 e-6; = (1/var)*||X-X_r||^2 =  3213.0 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  3118.4 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  8443.9 e-6; = (1/var)*||X-X_r||^2 =  5333.6 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  3110.3 e-6 = 36.8 %)
Min.  Avg. Train Loss across Mini-Batch =  5638.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7606.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2112.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2120.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.60; perplexity/K = 0.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.65; perplexity/K = 0.62%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5172.6 e-6; = (1/var)*||X-X_r||^2 =  2401.4 e-6 = 46.4 %; (1+beta)*||Z_e-Z_q||^2 =  2771.2 e-6 = 53.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  6485.1 e-6; = (1/var)*||X-X_r||^2 =  3840.7 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  2644.4 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4128.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5602.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1312.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1439.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -126.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.42; perplexity/K = 1.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.08; perplexity/K = 1.76%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  105263.2 e-6; = (1/var)*||X-X_r||^2 =  85525.4 e-6 = 81.2 %; (1+beta)*||Z_e-Z_q||^2 =  19737.7 e-6 = 18.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  109855.7 e-6; = (1/var)*||X-X_r||^2 =  89469.7 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  20386.0 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4592.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3944.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 648.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.31; perplexity/K = 1.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.04; perplexity/K = 1.13%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57676.7 e-6; = (1/var)*||X-X_r||^2 =  44444.4 e-6 = 77.1 %; (1+beta)*||Z_e-Z_q||^2 =  13232.3 e-6 = 22.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  61498.4 e-6; = (1/var)*||X-X_r||^2 =  48501.7 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  12996.7 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3821.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4057.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -235.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.49; perplexity/K = 1.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.38; perplexity/K = 0.99%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43598.9 e-6; = (1/var)*||X-X_r||^2 =  32839.7 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  10759.2 e-6 = 24.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  48229.0 e-6; = (1/var)*||X-X_r||^2 =  36962.9 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  11266.2 e-6 = 23.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4630.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4123.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 506.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.09; perplexity/K = 0.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.60; perplexity/K = 0.96%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36671.5 e-6; = (1/var)*||X-X_r||^2 =  26784.3 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  9887.2 e-6 = 27.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  41562.1 e-6; = (1/var)*||X-X_r||^2 =  30777.0 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  10785.1 e-6 = 25.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4890.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3992.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 897.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.52; perplexity/K = 1.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.77; perplexity/K = 0.97%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31945.2 e-6; = (1/var)*||X-X_r||^2 =  23262.0 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  8683.2 e-6 = 27.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  36037.7 e-6; = (1/var)*||X-X_r||^2 =  26896.1 e-6 = 74.6 %; (1+beta)*||Z_e-Z_q||^2 =  9141.6 e-6 = 25.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4092.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3634.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 458.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.94; perplexity/K = 1.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.90; perplexity/K = 0.97%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28334.1 e-6; = (1/var)*||X-X_r||^2 =  20281.5 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  8052.5 e-6 = 28.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  33177.9 e-6; = (1/var)*||X-X_r||^2 =  24513.9 e-6 = 73.9 %; (1+beta)*||Z_e-Z_q||^2 =  8663.9 e-6 = 26.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4843.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4232.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 611.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.92; perplexity/K = 0.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.44; perplexity/K = 0.90%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25860.9 e-6; = (1/var)*||X-X_r||^2 =  18401.8 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  7459.1 e-6 = 28.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  30873.9 e-6; = (1/var)*||X-X_r||^2 =  22664.5 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  8209.4 e-6 = 26.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5012.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4262.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 750.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.70; perplexity/K = 1.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.32; perplexity/K = 1.04%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23998.4 e-6; = (1/var)*||X-X_r||^2 =  16994.4 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  7004.1 e-6 = 29.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  28614.6 e-6; = (1/var)*||X-X_r||^2 =  20785.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  7829.6 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4616.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3790.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 825.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.12; perplexity/K = 1.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.18; perplexity/K = 0.94%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22958.2 e-6; = (1/var)*||X-X_r||^2 =  16144.5 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  6813.7 e-6 = 29.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  28123.0 e-6; = (1/var)*||X-X_r||^2 =  20190.0 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  7933.0 e-6 = 28.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5164.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4045.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1119.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.59; perplexity/K = 1.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.45; perplexity/K = 1.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21608.9 e-6; = (1/var)*||X-X_r||^2 =  15148.9 e-6 = 70.1 %; (1+beta)*||Z_e-Z_q||^2 =  6460.0 e-6 = 29.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  26768.8 e-6; = (1/var)*||X-X_r||^2 =  19427.9 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  7340.9 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3664.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5161.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5159.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4279.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 880.9 e-6 

----------------------------------------------------------------------------------

Finished [14:22:52 01.01.2023] 203) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = False:
Total training time is = 0:7:25 h/m/s. 

--------------------------------------------------- 

Started [14:22:52 01.01.2023] 204) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 704) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5423 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.22
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.33
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.33
3                                  encoder.pre_residual_stack.weight                       589            10.86
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.35
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.35
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                       131             2.42
10                             decoder.channel_adjusting_conv.weight                       147             2.71
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.35
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.35
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.33
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.33
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.22

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.41; perplexity/K = 0.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.90; perplexity/K = 0.48%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  185323.5 e-6; = (1/var)*||X-X_r||^2 =  77005.0 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  108318.6 e-6 = 58.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  186523.8 e-6; = (1/var)*||X-X_r||^2 =  83905.6 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  102618.2 e-6 = 55.0 %)
Min.  Avg. Train Loss across Mini-Batch =  184873.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186523.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1200.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6900.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5700.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.96; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.85; perplexity/K = 0.38%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  285970.7 e-6; = (1/var)*||X-X_r||^2 =  164338.1 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  121632.6 e-6 = 42.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  256803.2 e-6; = (1/var)*||X-X_r||^2 =  155047.0 e-6 = 60.4 %; (1+beta)*||Z_e-Z_q||^2 =  101756.2 e-6 = 39.6 %)
Min.  Avg. Train Loss across Mini-Batch =  26783.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29857.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -29167.5 e-6; = (1/var)*||X-X_r||^2 val-train = -9291.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19876.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.31; perplexity/K = 0.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.88; perplexity/K = 0.48%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13541.2 e-6; = (1/var)*||X-X_r||^2 =  7076.3 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  6464.9 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  16286.0 e-6; = (1/var)*||X-X_r||^2 =  9522.2 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  6763.8 e-6 = 41.5 %)
Min.  Avg. Train Loss across Mini-Batch =  13541.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16185.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2744.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2445.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 298.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.43; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.68; perplexity/K = 0.57%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5133.0 e-6; = (1/var)*||X-X_r||^2 =  2616.8 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  2516.2 e-6 = 49.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  7091.9 e-6; = (1/var)*||X-X_r||^2 =  4283.9 e-6 = 60.4 %; (1+beta)*||Z_e-Z_q||^2 =  2807.9 e-6 = 39.6 %)
Min.  Avg. Train Loss across Mini-Batch =  5133.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6848.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1958.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1667.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 291.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 0.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.69; perplexity/K = 0.62%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3390.9 e-6; = (1/var)*||X-X_r||^2 =  1679.9 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  1711.0 e-6 = 50.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  4789.5 e-6; = (1/var)*||X-X_r||^2 =  2986.2 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  1803.4 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3390.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4712.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1398.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1306.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 92.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.16; perplexity/K = 0.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.32; perplexity/K = 0.55%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1890.4 e-6; = (1/var)*||X-X_r||^2 =  1024.4 e-6 = 54.2 %; (1+beta)*||Z_e-Z_q||^2 =  866.0 e-6 = 45.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2923.3 e-6; = (1/var)*||X-X_r||^2 =  2034.3 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  889.0 e-6 = 30.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1890.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2923.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1032.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1009.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.70; perplexity/K = 0.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.98; perplexity/K = 0.49%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1331.4 e-6; = (1/var)*||X-X_r||^2 =  771.1 e-6 = 57.9 %; (1+beta)*||Z_e-Z_q||^2 =  560.2 e-6 = 42.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2359.4 e-6; = (1/var)*||X-X_r||^2 =  1703.2 e-6 = 72.2 %; (1+beta)*||Z_e-Z_q||^2 =  656.2 e-6 = 27.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1210.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2170.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1028.0 e-6; = (1/var)*||X-X_r||^2 val-train = 932.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 95.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.56; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.28; perplexity/K = 0.60%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1547.0 e-6; = (1/var)*||X-X_r||^2 =  885.7 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  661.3 e-6 = 42.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2420.9 e-6; = (1/var)*||X-X_r||^2 =  1711.2 e-6 = 70.7 %; (1+beta)*||Z_e-Z_q||^2 =  709.7 e-6 = 29.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1092.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1981.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   873.8 e-6; = (1/var)*||X-X_r||^2 val-train = 825.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 48.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.44; perplexity/K = 0.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.28; perplexity/K = 0.45%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  894.5 e-6; = (1/var)*||X-X_r||^2 =  432.8 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  461.7 e-6 = 51.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1735.4 e-6; = (1/var)*||X-X_r||^2 =  1268.5 e-6 = 73.1 %; (1+beta)*||Z_e-Z_q||^2 =  466.9 e-6 = 26.9 %)
Min.  Avg. Train Loss across Mini-Batch =  894.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1735.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   840.9 e-6; = (1/var)*||X-X_r||^2 val-train = 835.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.51; perplexity/K = 0.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.31; perplexity/K = 0.45%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  928.7 e-6; = (1/var)*||X-X_r||^2 =  466.3 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  462.4 e-6 = 49.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1839.8 e-6; = (1/var)*||X-X_r||^2 =  1370.7 e-6 = 74.5 %; (1+beta)*||Z_e-Z_q||^2 =  469.1 e-6 = 25.5 %)
Min.  Avg. Train Loss across Mini-Batch =  836.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1664.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   911.1 e-6; = (1/var)*||X-X_r||^2 val-train = 904.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.01; perplexity/K = 0.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.66; perplexity/K = 0.57%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  856.2 e-6; = (1/var)*||X-X_r||^2 =  430.3 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  426.0 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1631.6 e-6; = (1/var)*||X-X_r||^2 =  1233.5 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  398.1 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  722.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1496.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   775.3 e-6; = (1/var)*||X-X_r||^2 val-train = 803.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -27.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.86; perplexity/K = 0.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.71; perplexity/K = 0.43%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  737.6 e-6; = (1/var)*||X-X_r||^2 =  319.0 e-6 = 43.2 %; (1+beta)*||Z_e-Z_q||^2 =  418.6 e-6 = 56.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1501.4 e-6; = (1/var)*||X-X_r||^2 =  1106.0 e-6 = 73.7 %; (1+beta)*||Z_e-Z_q||^2 =  395.3 e-6 = 26.3 %)
Min.  Avg. Train Loss across Mini-Batch =  722.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1474.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   763.7 e-6; = (1/var)*||X-X_r||^2 val-train = 787.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -23.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.36; perplexity/K = 0.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.97; perplexity/K = 0.68%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1123.3 e-6; = (1/var)*||X-X_r||^2 =  545.2 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  578.1 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1854.8 e-6; = (1/var)*||X-X_r||^2 =  1280.6 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  574.3 e-6 = 31.0 %)
Min.  Avg. Train Loss across Mini-Batch =  670.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1367.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   731.5 e-6; = (1/var)*||X-X_r||^2 val-train = 735.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 0.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.71; perplexity/K = 0.57%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:36:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1303.3 e-6; = (1/var)*||X-X_r||^2 =  837.3 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  466.0 e-6 = 35.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3028.2 e-6; = (1/var)*||X-X_r||^2 =  2510.7 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  517.5 e-6 = 17.1 %)
Min.  Avg. Train Loss across Mini-Batch =  670.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1339.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1724.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1673.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 51.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.40; perplexity/K = 0.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.74; perplexity/K = 0.43%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:43:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  662.4 e-6; = (1/var)*||X-X_r||^2 =  322.2 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  340.2 e-6 = 51.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1377.7 e-6; = (1/var)*||X-X_r||^2 =  1049.7 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  327.9 e-6 = 23.8 %)
Min.  Avg. Train Loss across Mini-Batch =  583.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1248.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   715.2 e-6; = (1/var)*||X-X_r||^2 val-train = 727.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.42; perplexity/K = 0.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.44; perplexity/K = 0.41%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  554.8 e-6; = (1/var)*||X-X_r||^2 =  211.1 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  343.7 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1238.6 e-6; = (1/var)*||X-X_r||^2 =  925.8 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  312.8 e-6 = 25.3 %)
Min.  Avg. Train Loss across Mini-Batch =  546.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1194.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   683.8 e-6; = (1/var)*||X-X_r||^2 val-train = 714.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -30.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.87; perplexity/K = 0.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.16; perplexity/K = 0.45%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  621.5 e-6; = (1/var)*||X-X_r||^2 =  251.8 e-6 = 40.5 %; (1+beta)*||Z_e-Z_q||^2 =  369.8 e-6 = 59.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1345.5 e-6; = (1/var)*||X-X_r||^2 =  1009.6 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  335.9 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  546.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1194.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   724.0 e-6; = (1/var)*||X-X_r||^2 val-train = 757.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -33.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.91; perplexity/K = 0.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.53; perplexity/K = 0.42%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  527.8 e-6; = (1/var)*||X-X_r||^2 =  237.7 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  290.2 e-6 = 55.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1158.9 e-6; = (1/var)*||X-X_r||^2 =  869.0 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  289.9 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  500.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1158.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   631.0 e-6; = (1/var)*||X-X_r||^2 val-train = 631.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.63; perplexity/K = 0.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.47; perplexity/K = 0.46%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  519.6 e-6; = (1/var)*||X-X_r||^2 =  196.6 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  323.0 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1140.5 e-6; = (1/var)*||X-X_r||^2 =  832.9 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  307.6 e-6 = 27.0 %)
Min.  Avg. Train Loss across Mini-Batch =  466.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1087.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   620.9 e-6; = (1/var)*||X-X_r||^2 val-train = 636.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.03; perplexity/K = 0.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.09; perplexity/K = 0.44%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:17:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  552.9 e-6; = (1/var)*||X-X_r||^2 =  248.6 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  304.2 e-6 = 55.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1273.9 e-6; = (1/var)*||X-X_r||^2 =  998.5 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  275.4 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  454.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1087.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   721.1 e-6; = (1/var)*||X-X_r||^2 val-train = 749.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -28.8 e-6 

----------------------------------------------------------------------------------

Finished [16:41:41 01.01.2023] 204) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 704) change_channel_size_across_layers = False:
Total training time is = 0:7:49 h/m/s. 

--------------------------------------------------- 

Started [16:41:41 01.01.2023] 205) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 176) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 861 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.93
2                           encoder.sequential_convs.conv2d_3.weight                        32             3.72
3                           encoder.sequential_convs.conv2d_4.weight                       131            15.21
4                                  encoder.pre_residual_stack.weight                       147            17.07
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.18
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.18
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
9                              encoder.channel_adjusting_conv.weight                         8             0.93
10                                                       VQ.E.weight                       131            15.21
11                             decoder.channel_adjusting_conv.weight                        73             8.48
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.18
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.18
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.21
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.72
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             0.93
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.83; perplexity/K = 0.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.96; perplexity/K = 0.88%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  386731.5 e-6; = (1/var)*||X-X_r||^2 =  280645.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  106086.5 e-6 = 27.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  391864.1 e-6; = (1/var)*||X-X_r||^2 =  283008.7 e-6 = 72.2 %; (1+beta)*||Z_e-Z_q||^2 =  108855.4 e-6 = 27.8 %)
Min.  Avg. Train Loss across Mini-Batch =  386731.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  391864.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5132.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2363.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2769.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.70; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.74; perplexity/K = 1.16%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  162531.9 e-6; = (1/var)*||X-X_r||^2 =  140728.2 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  21803.6 e-6 = 13.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  161929.8 e-6; = (1/var)*||X-X_r||^2 =  142765.8 e-6 = 88.2 %; (1+beta)*||Z_e-Z_q||^2 =  19164.0 e-6 = 11.8 %)
Min.  Avg. Train Loss across Mini-Batch =  161339.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  161929.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -602.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2037.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2639.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.07; perplexity/K = 1.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.64; perplexity/K = 1.35%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  104171.0 e-6; = (1/var)*||X-X_r||^2 =  90843.7 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  13327.3 e-6 = 12.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  109134.5 e-6; = (1/var)*||X-X_r||^2 =  95476.1 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  13658.5 e-6 = 12.5 %)
Min.  Avg. Train Loss across Mini-Batch =  104171.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  109134.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4963.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4632.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 331.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.84; perplexity/K = 1.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.02; perplexity/K = 1.42%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  74199.9 e-6; = (1/var)*||X-X_r||^2 =  62721.0 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  11478.9 e-6 = 15.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  79829.4 e-6; = (1/var)*||X-X_r||^2 =  68359.2 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  11470.2 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  74199.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  79829.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5629.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5638.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.27; perplexity/K = 1.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.86; perplexity/K = 1.60%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  54931.9 e-6; = (1/var)*||X-X_r||^2 =  46873.0 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  8058.9 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  60937.9 e-6; = (1/var)*||X-X_r||^2 =  52367.0 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  8570.9 e-6 = 14.1 %)
Min.  Avg. Train Loss across Mini-Batch =  54745.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60937.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6006.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5494.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 512.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.23; perplexity/K = 1.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.55; perplexity/K = 1.49%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45416.9 e-6; = (1/var)*||X-X_r||^2 =  39377.8 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  6039.1 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  51419.7 e-6; = (1/var)*||X-X_r||^2 =  44636.0 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  6783.6 e-6 = 13.2 %)
Min.  Avg. Train Loss across Mini-Batch =  45416.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51419.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6002.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5258.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 744.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.60; perplexity/K = 1.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.93; perplexity/K = 1.56%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40704.9 e-6; = (1/var)*||X-X_r||^2 =  35799.1 e-6 = 87.9 %; (1+beta)*||Z_e-Z_q||^2 =  4905.8 e-6 = 12.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  46715.4 e-6; = (1/var)*||X-X_r||^2 =  40803.8 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  5911.6 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  40704.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46447.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6010.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5004.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1005.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.79; perplexity/K = 1.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.72; perplexity/K = 1.65%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38794.9 e-6; = (1/var)*||X-X_r||^2 =  34183.8 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  4611.1 e-6 = 11.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  44054.2 e-6; = (1/var)*||X-X_r||^2 =  38844.6 e-6 = 88.2 %; (1+beta)*||Z_e-Z_q||^2 =  5209.6 e-6 = 11.8 %)
Min.  Avg. Train Loss across Mini-Batch =  38699.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44054.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5259.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4660.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 598.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.87; perplexity/K = 1.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.04; perplexity/K = 1.56%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37486.7 e-6; = (1/var)*||X-X_r||^2 =  33405.5 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  4081.2 e-6 = 10.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  43683.1 e-6; = (1/var)*||X-X_r||^2 =  38500.7 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  5182.4 e-6 = 11.9 %)
Min.  Avg. Train Loss across Mini-Batch =  36706.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42413.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6196.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5095.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1101.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.97; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.94; perplexity/K = 1.46%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35088.7 e-6; = (1/var)*||X-X_r||^2 =  31292.6 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  3796.1 e-6 = 10.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  40569.2 e-6; = (1/var)*||X-X_r||^2 =  35876.2 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  4693.0 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  34687.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40182.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5480.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4583.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 896.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.37; perplexity/K = 1.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.85; perplexity/K = 1.56%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33587.0 e-6; = (1/var)*||X-X_r||^2 =  30231.8 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  3355.3 e-6 = 10.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  39091.7 e-6; = (1/var)*||X-X_r||^2 =  34775.6 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  4316.1 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  33169.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38861.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5504.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4543.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 960.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.84; perplexity/K = 1.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.38; perplexity/K = 1.63%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33149.3 e-6; = (1/var)*||X-X_r||^2 =  29795.6 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  3353.7 e-6 = 10.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  39937.9 e-6; = (1/var)*||X-X_r||^2 =  35373.1 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  4564.8 e-6 = 11.4 %)
Min.  Avg. Train Loss across Mini-Batch =  32185.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37685.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6788.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5577.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1211.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.10; perplexity/K = 1.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.26; perplexity/K = 1.67%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31509.3 e-6; = (1/var)*||X-X_r||^2 =  28473.5 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  3035.8 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  37465.0 e-6; = (1/var)*||X-X_r||^2 =  33396.7 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  4068.2 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  31072.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36570.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5955.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4923.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1032.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.62; perplexity/K = 1.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.54; perplexity/K = 1.59%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30471.5 e-6; = (1/var)*||X-X_r||^2 =  27557.4 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  2914.1 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  36010.0 e-6; = (1/var)*||X-X_r||^2 =  32085.6 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  3924.4 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  30267.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35874.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5538.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4528.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1010.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.54; perplexity/K = 1.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.86; perplexity/K = 1.56%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29739.9 e-6; = (1/var)*||X-X_r||^2 =  26859.8 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  2880.1 e-6 = 9.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  35308.3 e-6; = (1/var)*||X-X_r||^2 =  31426.6 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  3881.8 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  29369.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34993.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5568.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4566.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1001.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.03; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.68; perplexity/K = 1.60%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28504.4 e-6; = (1/var)*||X-X_r||^2 =  25780.8 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  2723.6 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  34192.9 e-6; = (1/var)*||X-X_r||^2 =  30431.5 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  3761.4 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  28504.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34044.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5688.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4650.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1037.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.94; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.89; perplexity/K = 1.65%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27661.2 e-6; = (1/var)*||X-X_r||^2 =  24994.9 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  2666.3 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  33214.8 e-6; = (1/var)*||X-X_r||^2 =  29554.7 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  3660.1 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  27596.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33019.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5553.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4559.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 993.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.91; perplexity/K = 1.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.39; perplexity/K = 1.53%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27101.6 e-6; = (1/var)*||X-X_r||^2 =  24537.7 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  2563.8 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  32925.3 e-6; = (1/var)*||X-X_r||^2 =  29190.9 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  3734.4 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  27101.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32462.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5823.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4653.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1170.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.90; perplexity/K = 1.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.02; perplexity/K = 1.66%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29786.5 e-6; = (1/var)*||X-X_r||^2 =  25212.1 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  4574.4 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  33854.0 e-6; = (1/var)*||X-X_r||^2 =  28958.1 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  4895.9 e-6 = 14.5 %)
Min.  Avg. Train Loss across Mini-Batch =  26635.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31960.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4067.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3745.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 321.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.73; perplexity/K = 1.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.14; perplexity/K = 1.62%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26129.8 e-6; = (1/var)*||X-X_r||^2 =  23560.1 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  2569.7 e-6 = 9.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  31208.1 e-6; = (1/var)*||X-X_r||^2 =  27906.2 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  3301.9 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  25753.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30753.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5078.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4346.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 732.2 e-6 

----------------------------------------------------------------------------------

Finished [17:29:49 01.01.2023] 205) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 176) change_channel_size_across_layers = True:
Total training time is = 0:3:7 h/m/s. 

--------------------------------------------------- 

Started [17:29:49 01.01.2023] 206) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 176) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2583 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.24
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.07
3                           encoder.sequential_convs.conv2d_4.weight                       524            20.29
4                                  encoder.pre_residual_stack.weight                       589            22.80
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
9                              encoder.channel_adjusting_conv.weight                        16             0.62
10                                                       VQ.E.weight                       131             5.07
11                             decoder.channel_adjusting_conv.weight                       147             5.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.29
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.07
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.24
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.78; perplexity/K = 0.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.59; perplexity/K = 0.71%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  377138.8 e-6; = (1/var)*||X-X_r||^2 =  238525.0 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  138613.8 e-6 = 36.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  392068.5 e-6; = (1/var)*||X-X_r||^2 =  242122.1 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  149946.4 e-6 = 38.2 %)
Min.  Avg. Train Loss across Mini-Batch =  377138.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  377854.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14929.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3597.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11332.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.94; perplexity/K = 1.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.53; perplexity/K = 1.05%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  103149.7 e-6; = (1/var)*||X-X_r||^2 =  79906.1 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  23243.6 e-6 = 22.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  110868.6 e-6; = (1/var)*||X-X_r||^2 =  88606.3 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  22262.4 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  103149.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  110868.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7719.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8700.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -981.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.99; perplexity/K = 1.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.84; perplexity/K = 1.21%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50448.7 e-6; = (1/var)*||X-X_r||^2 =  38572.1 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  11876.6 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  57259.1 e-6; = (1/var)*||X-X_r||^2 =  45106.7 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  12152.5 e-6 = 21.2 %)
Min.  Avg. Train Loss across Mini-Batch =  50448.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  57259.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6810.4 e-6; = (1/var)*||X-X_r||^2 val-train = 6534.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 275.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.48; perplexity/K = 1.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.63; perplexity/K = 1.25%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25339.0 e-6; = (1/var)*||X-X_r||^2 =  17733.6 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  7605.5 e-6 = 30.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  31540.3 e-6; = (1/var)*||X-X_r||^2 =  23273.4 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  8266.9 e-6 = 26.2 %)
Min.  Avg. Train Loss across Mini-Batch =  25339.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31540.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6201.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5539.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 661.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.29; perplexity/K = 1.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.42; perplexity/K = 1.24%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11999.0 e-6; = (1/var)*||X-X_r||^2 =  6865.0 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  5134.0 e-6 = 42.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  16940.2 e-6; = (1/var)*||X-X_r||^2 =  11135.7 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  5804.5 e-6 = 34.3 %)
Min.  Avg. Train Loss across Mini-Batch =  11999.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16940.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4941.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4270.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 670.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.53; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.42; perplexity/K = 1.29%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7074.6 e-6; = (1/var)*||X-X_r||^2 =  3889.8 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  3184.8 e-6 = 45.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  10913.5 e-6; = (1/var)*||X-X_r||^2 =  7125.2 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  3788.3 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  6973.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10913.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3838.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3235.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 603.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.67; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.67; perplexity/K = 1.30%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5129.2 e-6; = (1/var)*||X-X_r||^2 =  2479.6 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  2649.6 e-6 = 51.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  8248.2 e-6; = (1/var)*||X-X_r||^2 =  5125.8 e-6 = 62.1 %; (1+beta)*||Z_e-Z_q||^2 =  3122.5 e-6 = 37.9 %)
Min.  Avg. Train Loss across Mini-Batch =  4971.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8248.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3119.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2646.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 472.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.67; perplexity/K = 1.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.36; perplexity/K = 1.19%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5492.0 e-6; = (1/var)*||X-X_r||^2 =  2482.6 e-6 = 45.2 %; (1+beta)*||Z_e-Z_q||^2 =  3009.4 e-6 = 54.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  8055.4 e-6; = (1/var)*||X-X_r||^2 =  4784.8 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  3270.6 e-6 = 40.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3649.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6641.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2563.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2302.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 261.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.08; perplexity/K = 1.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.15; perplexity/K = 1.28%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3161.8 e-6; = (1/var)*||X-X_r||^2 =  1590.7 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  1571.1 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  5850.3 e-6; = (1/var)*||X-X_r||^2 =  3931.1 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  1919.2 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3028.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5719.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2688.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2340.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 348.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.60; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.87; perplexity/K = 1.21%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4618.0 e-6; = (1/var)*||X-X_r||^2 =  2032.7 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  2585.3 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  6231.6 e-6; = (1/var)*||X-X_r||^2 =  3783.8 e-6 = 60.7 %; (1+beta)*||Z_e-Z_q||^2 =  2447.8 e-6 = 39.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2502.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4826.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1613.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1751.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -137.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.33; perplexity/K = 1.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.11; perplexity/K = 1.27%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2171.0 e-6; = (1/var)*||X-X_r||^2 =  944.8 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  1226.2 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  4133.8 e-6; = (1/var)*||X-X_r||^2 =  2686.8 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  1447.0 e-6 = 35.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2029.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4130.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1962.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1742.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 220.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.79; perplexity/K = 1.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.22; perplexity/K = 1.28%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2100.0 e-6; = (1/var)*||X-X_r||^2 =  866.9 e-6 = 41.3 %; (1+beta)*||Z_e-Z_q||^2 =  1233.1 e-6 = 58.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  3965.4 e-6; = (1/var)*||X-X_r||^2 =  2564.1 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  1401.3 e-6 = 35.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1866.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3810.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1865.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1697.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 168.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.69; perplexity/K = 1.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.87; perplexity/K = 1.36%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2958.6 e-6; = (1/var)*||X-X_r||^2 =  1125.6 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  1833.1 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4422.0 e-6; = (1/var)*||X-X_r||^2 =  2563.9 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  1858.1 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1559.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3259.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1463.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1438.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.13; perplexity/K = 1.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.97; perplexity/K = 1.41%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1735.5 e-6; = (1/var)*||X-X_r||^2 =  639.8 e-6 = 36.9 %; (1+beta)*||Z_e-Z_q||^2 =  1095.6 e-6 = 63.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3144.2 e-6; = (1/var)*||X-X_r||^2 =  1947.9 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  1196.3 e-6 = 38.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1393.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2871.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1408.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1308.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 100.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.67; perplexity/K = 1.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.16; perplexity/K = 1.37%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2210.0 e-6; = (1/var)*||X-X_r||^2 =  746.3 e-6 = 33.8 %; (1+beta)*||Z_e-Z_q||^2 =  1463.7 e-6 = 66.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3541.3 e-6; = (1/var)*||X-X_r||^2 =  2031.2 e-6 = 57.4 %; (1+beta)*||Z_e-Z_q||^2 =  1510.1 e-6 = 42.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1201.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2667.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1331.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1284.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 46.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.38; perplexity/K = 1.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.47; perplexity/K = 1.34%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1161.8 e-6; = (1/var)*||X-X_r||^2 =  531.9 e-6 = 45.8 %; (1+beta)*||Z_e-Z_q||^2 =  629.8 e-6 = 54.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2441.3 e-6; = (1/var)*||X-X_r||^2 =  1689.9 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  751.4 e-6 = 30.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1133.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2220.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1279.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1158.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 121.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.68; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.17; perplexity/K = 1.57%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20721.2 e-6; = (1/var)*||X-X_r||^2 =  15202.6 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  5518.5 e-6 = 26.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  16536.7 e-6; = (1/var)*||X-X_r||^2 =  12034.7 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  4502.0 e-6 = 27.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1040.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2168.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4184.4 e-6; = (1/var)*||X-X_r||^2 val-train = -3167.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1016.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.29; perplexity/K = 1.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.22; perplexity/K = 1.43%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1073.1 e-6; = (1/var)*||X-X_r||^2 =  405.8 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  667.3 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2042.3 e-6; = (1/var)*||X-X_r||^2 =  1287.2 e-6 = 63.0 %; (1+beta)*||Z_e-Z_q||^2 =  755.1 e-6 = 37.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1017.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2042.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   969.2 e-6; = (1/var)*||X-X_r||^2 val-train = 881.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 87.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.35; perplexity/K = 1.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.84; perplexity/K = 1.46%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1134.1 e-6; = (1/var)*||X-X_r||^2 =  388.7 e-6 = 34.3 %; (1+beta)*||Z_e-Z_q||^2 =  745.4 e-6 = 65.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2159.2 e-6; = (1/var)*||X-X_r||^2 =  1344.9 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  814.2 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  862.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1994.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1025.1 e-6; = (1/var)*||X-X_r||^2 val-train = 956.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 68.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.14; perplexity/K = 1.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.85; perplexity/K = 1.51%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  686.6 e-6; = (1/var)*||X-X_r||^2 =  308.6 e-6 = 44.9 %; (1+beta)*||Z_e-Z_q||^2 =  378.1 e-6 = 55.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1825.7 e-6; = (1/var)*||X-X_r||^2 =  1364.3 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  461.4 e-6 = 25.3 %)
Min.  Avg. Train Loss across Mini-Batch =  674.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1679.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1139.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1055.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 83.3 e-6 

----------------------------------------------------------------------------------

Finished [18:19:30 01.01.2023] 206) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 176) change_channel_size_across_layers = True:
Total training time is = 0:3:41 h/m/s. 

--------------------------------------------------- 

Started [18:19:30 01.01.2023] 207) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 176) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2103 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.29
1                           encoder.sequential_convs.conv2d_2.weight                       262            12.46
2                           encoder.sequential_convs.conv2d_3.weight                       262            12.46
3                           encoder.sequential_convs.conv2d_4.weight                       262            12.46
4                                  encoder.pre_residual_stack.weight                       147             6.99
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.71
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.19
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.71
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.19
9                              encoder.channel_adjusting_conv.weight                         8             0.38
10                                                       VQ.E.weight                       131             6.23
11                             decoder.channel_adjusting_conv.weight                        73             3.47
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.71
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.19
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.71
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.19
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            12.46
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            12.46
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            12.46
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.29

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.64; perplexity/K = 2.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.76; perplexity/K = 2.04%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  280689.6 e-6; = (1/var)*||X-X_r||^2 =  149094.1 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  131595.6 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  279582.9 e-6; = (1/var)*||X-X_r||^2 =  156752.1 e-6 = 56.1 %; (1+beta)*||Z_e-Z_q||^2 =  122830.8 e-6 = 43.9 %)
Min.  Avg. Train Loss across Mini-Batch =  277138.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  279582.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1106.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7658.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8764.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.75; perplexity/K = 2.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.18; perplexity/K = 3.08%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59662.8 e-6; = (1/var)*||X-X_r||^2 =  35349.7 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  24313.1 e-6 = 40.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  67031.3 e-6; = (1/var)*||X-X_r||^2 =  43686.8 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  23344.5 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  58833.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67031.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7368.5 e-6; = (1/var)*||X-X_r||^2 val-train = 8337.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -968.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 72.61; perplexity/K = 3.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 74.72; perplexity/K = 3.65%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16439.6 e-6; = (1/var)*||X-X_r||^2 =  10587.7 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  5851.9 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  19461.9 e-6; = (1/var)*||X-X_r||^2 =  13590.3 e-6 = 69.8 %; (1+beta)*||Z_e-Z_q||^2 =  5871.6 e-6 = 30.2 %)
Min.  Avg. Train Loss across Mini-Batch =  15951.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  19461.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3022.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3002.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.87; perplexity/K = 3.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 78.06; perplexity/K = 3.81%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7415.6 e-6; = (1/var)*||X-X_r||^2 =  4332.0 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  3083.6 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  10027.8 e-6; = (1/var)*||X-X_r||^2 =  6811.1 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  3216.7 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  7413.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9770.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2612.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2479.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 133.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 79.66; perplexity/K = 3.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.91; perplexity/K = 3.71%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4738.1 e-6; = (1/var)*||X-X_r||^2 =  2410.3 e-6 = 50.9 %; (1+beta)*||Z_e-Z_q||^2 =  2327.8 e-6 = 49.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  6311.6 e-6; = (1/var)*||X-X_r||^2 =  3990.9 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  2320.7 e-6 = 36.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4724.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6311.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1573.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1580.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.07; perplexity/K = 3.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 81.30; perplexity/K = 3.97%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3956.7 e-6; = (1/var)*||X-X_r||^2 =  2204.2 e-6 = 55.7 %; (1+beta)*||Z_e-Z_q||^2 =  1752.5 e-6 = 44.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4900.7 e-6; = (1/var)*||X-X_r||^2 =  3184.1 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  1716.6 e-6 = 35.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2945.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4179.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   943.9 e-6; = (1/var)*||X-X_r||^2 val-train = 979.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.20; perplexity/K = 3.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.08; perplexity/K = 3.76%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2287.4 e-6; = (1/var)*||X-X_r||^2 =  1319.8 e-6 = 57.7 %; (1+beta)*||Z_e-Z_q||^2 =  967.5 e-6 = 42.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3417.8 e-6; = (1/var)*||X-X_r||^2 =  2437.8 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  979.9 e-6 = 28.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2271.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3367.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1130.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1118.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.46; perplexity/K = 3.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.41; perplexity/K = 3.58%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1799.9 e-6; = (1/var)*||X-X_r||^2 =  986.3 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  813.6 e-6 = 45.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2611.1 e-6; = (1/var)*||X-X_r||^2 =  1767.6 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  843.5 e-6 = 32.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1799.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2593.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   811.2 e-6; = (1/var)*||X-X_r||^2 val-train = 781.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.40; perplexity/K = 3.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.91; perplexity/K = 3.61%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1540.4 e-6; = (1/var)*||X-X_r||^2 =  830.1 e-6 = 53.9 %; (1+beta)*||Z_e-Z_q||^2 =  710.3 e-6 = 46.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2431.5 e-6; = (1/var)*||X-X_r||^2 =  1707.8 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  723.7 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1512.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2262.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   891.1 e-6; = (1/var)*||X-X_r||^2 val-train = 877.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.69; perplexity/K = 3.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.39; perplexity/K = 3.58%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1288.3 e-6; = (1/var)*||X-X_r||^2 =  727.8 e-6 = 56.5 %; (1+beta)*||Z_e-Z_q||^2 =  560.5 e-6 = 43.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1907.1 e-6; = (1/var)*||X-X_r||^2 =  1341.1 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  566.0 e-6 = 29.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1192.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1871.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   618.7 e-6; = (1/var)*||X-X_r||^2 val-train = 613.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 78.17; perplexity/K = 3.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.82; perplexity/K = 3.80%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1141.7 e-6; = (1/var)*||X-X_r||^2 =  603.7 e-6 = 52.9 %; (1+beta)*||Z_e-Z_q||^2 =  538.0 e-6 = 47.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1757.3 e-6; = (1/var)*||X-X_r||^2 =  1206.5 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  550.9 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1048.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1619.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   615.6 e-6; = (1/var)*||X-X_r||^2 val-train = 602.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.00; perplexity/K = 3.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 70.85; perplexity/K = 3.46%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1145.6 e-6; = (1/var)*||X-X_r||^2 =  562.5 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  583.1 e-6 = 50.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1766.2 e-6; = (1/var)*||X-X_r||^2 =  1180.5 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  585.7 e-6 = 33.2 %)
Min.  Avg. Train Loss across Mini-Batch =  916.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1463.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   620.6 e-6; = (1/var)*||X-X_r||^2 val-train = 618.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.98; perplexity/K = 3.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.45; perplexity/K = 3.49%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  935.1 e-6; = (1/var)*||X-X_r||^2 =  587.3 e-6 = 62.8 %; (1+beta)*||Z_e-Z_q||^2 =  347.8 e-6 = 37.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1475.1 e-6; = (1/var)*||X-X_r||^2 =  1116.9 e-6 = 75.7 %; (1+beta)*||Z_e-Z_q||^2 =  358.2 e-6 = 24.3 %)
Min.  Avg. Train Loss across Mini-Batch =  765.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1295.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   539.9 e-6; = (1/var)*||X-X_r||^2 val-train = 529.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.60; perplexity/K = 3.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.08; perplexity/K = 3.37%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  837.0 e-6; = (1/var)*||X-X_r||^2 =  456.0 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  381.0 e-6 = 45.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1328.8 e-6; = (1/var)*||X-X_r||^2 =  943.7 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  385.1 e-6 = 29.0 %)
Min.  Avg. Train Loss across Mini-Batch =  721.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1227.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   491.8 e-6; = (1/var)*||X-X_r||^2 val-train = 487.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 72.95; perplexity/K = 3.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 70.92; perplexity/K = 3.46%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1360.0 e-6; = (1/var)*||X-X_r||^2 =  623.9 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  736.2 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2086.3 e-6; = (1/var)*||X-X_r||^2 =  1362.8 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  723.5 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  580.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1038.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   726.3 e-6; = (1/var)*||X-X_r||^2 val-train = 738.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 70.98; perplexity/K = 3.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.68; perplexity/K = 3.06%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  699.6 e-6; = (1/var)*||X-X_r||^2 =  350.5 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  349.2 e-6 = 49.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1317.4 e-6; = (1/var)*||X-X_r||^2 =  960.5 e-6 = 72.9 %; (1+beta)*||Z_e-Z_q||^2 =  356.9 e-6 = 27.1 %)
Min.  Avg. Train Loss across Mini-Batch =  485.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   617.8 e-6; = (1/var)*||X-X_r||^2 val-train = 610.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.50; perplexity/K = 3.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.92; perplexity/K = 3.61%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  553.4 e-6; = (1/var)*||X-X_r||^2 =  330.6 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  222.8 e-6 = 40.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  981.6 e-6; = (1/var)*||X-X_r||^2 =  752.3 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  229.3 e-6 = 23.4 %)
Min.  Avg. Train Loss across Mini-Batch =  461.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  912.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   428.2 e-6; = (1/var)*||X-X_r||^2 val-train = 421.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 86.31; perplexity/K = 4.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.48; perplexity/K = 4.03%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1904.3 e-6; = (1/var)*||X-X_r||^2 =  1200.4 e-6 = 63.0 %; (1+beta)*||Z_e-Z_q||^2 =  703.8 e-6 = 37.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2674.0 e-6; = (1/var)*||X-X_r||^2 =  1898.0 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  776.1 e-6 = 29.0 %)
Min.  Avg. Train Loss across Mini-Batch =  437.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  855.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   769.8 e-6; = (1/var)*||X-X_r||^2 val-train = 697.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 72.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.24; perplexity/K = 3.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.21; perplexity/K = 3.57%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  454.3 e-6; = (1/var)*||X-X_r||^2 =  289.5 e-6 = 63.7 %; (1+beta)*||Z_e-Z_q||^2 =  164.8 e-6 = 36.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  825.7 e-6; = (1/var)*||X-X_r||^2 =  651.8 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  173.9 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  387.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  752.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   371.5 e-6; = (1/var)*||X-X_r||^2 val-train = 362.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.38; perplexity/K = 4.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.99; perplexity/K = 4.05%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8111.9 e-6; = (1/var)*||X-X_r||^2 =  3908.0 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  4203.9 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  7971.9 e-6; = (1/var)*||X-X_r||^2 =  3853.6 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  4118.3 e-6 = 51.7 %)
Min.  Avg. Train Loss across Mini-Batch =  353.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  702.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -140.0 e-6; = (1/var)*||X-X_r||^2 val-train = -54.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -85.6 e-6 

----------------------------------------------------------------------------------

Finished [19:24:43 01.01.2023] 207) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 176) change_channel_size_across_layers = False:
Total training time is = 0:3:13 h/m/s. 

--------------------------------------------------- 

Started [19:24:43 01.01.2023] 208) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 176) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7519 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            13.94
2                           encoder.sequential_convs.conv2d_3.weight                      1048            13.94
3                           encoder.sequential_convs.conv2d_4.weight                      1048            13.94
4                                  encoder.pre_residual_stack.weight                       589             7.83
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.97
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.97
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.21
10                                                       VQ.E.weight                       131             1.74
11                             decoder.channel_adjusting_conv.weight                       147             1.96
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.97
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.97
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            13.94
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            13.94
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            13.94
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.38; perplexity/K = 1.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.73; perplexity/K = 1.50%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  108746.3 e-6; = (1/var)*||X-X_r||^2 =  56815.8 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  51930.5 e-6 = 47.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  122772.5 e-6; = (1/var)*||X-X_r||^2 =  72667.0 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  50105.5 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  108746.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  122772.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14026.2 e-6; = (1/var)*||X-X_r||^2 val-train = 15851.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1825.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.08; perplexity/K = 2.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.37; perplexity/K = 1.87%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15658.9 e-6; = (1/var)*||X-X_r||^2 =  8427.7 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  7231.3 e-6 = 46.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  20897.2 e-6; = (1/var)*||X-X_r||^2 =  13637.5 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  7259.7 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  15066.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20385.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5238.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5209.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.61; perplexity/K = 2.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.38; perplexity/K = 2.36%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4040.5 e-6; = (1/var)*||X-X_r||^2 =  2165.6 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  1874.8 e-6 = 46.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  6194.3 e-6; = (1/var)*||X-X_r||^2 =  4307.7 e-6 = 69.5 %; (1+beta)*||Z_e-Z_q||^2 =  1886.6 e-6 = 30.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4040.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6194.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2153.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2142.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.14; perplexity/K = 1.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.16; perplexity/K = 1.96%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2765.0 e-6; = (1/var)*||X-X_r||^2 =  1791.4 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  973.7 e-6 = 35.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  5521.6 e-6; = (1/var)*||X-X_r||^2 =  4421.7 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  1100.0 e-6 = 19.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2113.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3795.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2756.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2630.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 126.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.33; perplexity/K = 1.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.30; perplexity/K = 2.02%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1531.9 e-6; = (1/var)*||X-X_r||^2 =  883.6 e-6 = 57.7 %; (1+beta)*||Z_e-Z_q||^2 =  648.4 e-6 = 42.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2833.7 e-6; = (1/var)*||X-X_r||^2 =  2155.4 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  678.2 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1312.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2724.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1301.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1271.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.88; perplexity/K = 2.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.53; perplexity/K = 2.22%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2684.0 e-6; = (1/var)*||X-X_r||^2 =  1375.2 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  1308.9 e-6 = 48.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3915.8 e-6; = (1/var)*||X-X_r||^2 =  2693.2 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  1222.6 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  957.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2259.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1231.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1318.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -86.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.05; perplexity/K = 2.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.92; perplexity/K = 2.00%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1314.5 e-6; = (1/var)*||X-X_r||^2 =  606.6 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  707.9 e-6 = 53.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2330.4 e-6; = (1/var)*||X-X_r||^2 =  1645.1 e-6 = 70.6 %; (1+beta)*||Z_e-Z_q||^2 =  685.3 e-6 = 29.4 %)
Min.  Avg. Train Loss across Mini-Batch =  779.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1716.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1015.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1038.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.01; perplexity/K = 2.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.85; perplexity/K = 2.19%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3435.4 e-6; = (1/var)*||X-X_r||^2 =  2342.7 e-6 = 68.2 %; (1+beta)*||Z_e-Z_q||^2 =  1092.7 e-6 = 31.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  10300.8 e-6; = (1/var)*||X-X_r||^2 =  8057.3 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  2243.4 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  626.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1505.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6865.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5714.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1150.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.05; perplexity/K = 2.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.23; perplexity/K = 2.01%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  567.2 e-6; = (1/var)*||X-X_r||^2 =  298.3 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  268.8 e-6 = 47.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1235.0 e-6; = (1/var)*||X-X_r||^2 =  957.9 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  277.1 e-6 = 22.4 %)
Min.  Avg. Train Loss across Mini-Batch =  562.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1235.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   667.8 e-6; = (1/var)*||X-X_r||^2 val-train = 659.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.58; perplexity/K = 2.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.58; perplexity/K = 1.98%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  659.5 e-6; = (1/var)*||X-X_r||^2 =  294.3 e-6 = 44.6 %; (1+beta)*||Z_e-Z_q||^2 =  365.2 e-6 = 55.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1479.2 e-6; = (1/var)*||X-X_r||^2 =  1110.9 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  368.3 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  433.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1202.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   819.8 e-6; = (1/var)*||X-X_r||^2 val-train = 816.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.62; perplexity/K = 2.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.40; perplexity/K = 2.27%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:13:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  430.2 e-6; = (1/var)*||X-X_r||^2 =  187.0 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  243.2 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1121.3 e-6; = (1/var)*||X-X_r||^2 =  868.9 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  252.4 e-6 = 22.5 %)
Min.  Avg. Train Loss across Mini-Batch =  426.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1099.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   691.0 e-6; = (1/var)*||X-X_r||^2 val-train = 681.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.37; perplexity/K = 2.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.90; perplexity/K = 2.24%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  651.6 e-6; = (1/var)*||X-X_r||^2 =  240.4 e-6 = 36.9 %; (1+beta)*||Z_e-Z_q||^2 =  411.3 e-6 = 63.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1333.7 e-6; = (1/var)*||X-X_r||^2 =  928.0 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  405.6 e-6 = 30.4 %)
Min.  Avg. Train Loss across Mini-Batch =  377.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1004.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   682.0 e-6; = (1/var)*||X-X_r||^2 val-train = 687.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.40; perplexity/K = 2.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.92; perplexity/K = 2.34%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  619.8 e-6; = (1/var)*||X-X_r||^2 =  224.0 e-6 = 36.1 %; (1+beta)*||Z_e-Z_q||^2 =  395.8 e-6 = 63.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1153.7 e-6; = (1/var)*||X-X_r||^2 =  769.5 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  384.2 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  318.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  826.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   533.9 e-6; = (1/var)*||X-X_r||^2 val-train = 545.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.81; perplexity/K = 2.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.40; perplexity/K = 2.07%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  285.5 e-6; = (1/var)*||X-X_r||^2 =  125.5 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  160.0 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  846.5 e-6; = (1/var)*||X-X_r||^2 =  672.3 e-6 = 79.4 %; (1+beta)*||Z_e-Z_q||^2 =  174.2 e-6 = 20.6 %)
Min.  Avg. Train Loss across Mini-Batch =  285.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  815.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   561.0 e-6; = (1/var)*||X-X_r||^2 val-train = 546.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.25; perplexity/K = 2.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.83; perplexity/K = 2.19%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:40:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  707.2 e-6; = (1/var)*||X-X_r||^2 =  227.6 e-6 = 32.2 %; (1+beta)*||Z_e-Z_q||^2 =  479.6 e-6 = 67.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1468.0 e-6; = (1/var)*||X-X_r||^2 =  1002.2 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  465.8 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  219.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  678.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   760.8 e-6; = (1/var)*||X-X_r||^2 val-train = 774.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.85; perplexity/K = 2.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.43; perplexity/K = 2.61%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1299.0 e-6; = (1/var)*||X-X_r||^2 =  501.3 e-6 = 38.6 %; (1+beta)*||Z_e-Z_q||^2 =  797.7 e-6 = 61.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1877.9 e-6; = (1/var)*||X-X_r||^2 =  1153.4 e-6 = 61.4 %; (1+beta)*||Z_e-Z_q||^2 =  724.5 e-6 = 38.6 %)
Min.  Avg. Train Loss across Mini-Batch =  219.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  678.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   578.9 e-6; = (1/var)*||X-X_r||^2 val-train = 652.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -73.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.92; perplexity/K = 2.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.17; perplexity/K = 2.16%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  315.8 e-6; = (1/var)*||X-X_r||^2 =  151.8 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  164.0 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  737.4 e-6; = (1/var)*||X-X_r||^2 =  562.9 e-6 = 76.3 %; (1+beta)*||Z_e-Z_q||^2 =  174.5 e-6 = 23.7 %)
Min.  Avg. Train Loss across Mini-Batch =  219.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  675.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   421.6 e-6; = (1/var)*||X-X_r||^2 val-train = 411.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.71; perplexity/K = 2.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.76; perplexity/K = 2.67%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7709.3 e-6; = (1/var)*||X-X_r||^2 =  6132.1 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  1577.2 e-6 = 20.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  6520.3 e-6; = (1/var)*||X-X_r||^2 =  4958.0 e-6 = 76.0 %; (1+beta)*||Z_e-Z_q||^2 =  1562.3 e-6 = 24.0 %)
Min.  Avg. Train Loss across Mini-Batch =  219.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  654.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1189.0 e-6; = (1/var)*||X-X_r||^2 val-train = -1174.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.81; perplexity/K = 2.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.64; perplexity/K = 2.23%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:7:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  234.8 e-6; = (1/var)*||X-X_r||^2 =  81.3 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  153.6 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  690.3 e-6; = (1/var)*||X-X_r||^2 =  526.6 e-6 = 76.3 %; (1+beta)*||Z_e-Z_q||^2 =  163.7 e-6 = 23.7 %)
Min.  Avg. Train Loss across Mini-Batch =  202.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  654.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   455.4 e-6; = (1/var)*||X-X_r||^2 val-train = 445.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.90; perplexity/K = 2.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.85; perplexity/K = 2.19%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  987.9 e-6; = (1/var)*||X-X_r||^2 =  299.2 e-6 = 30.3 %; (1+beta)*||Z_e-Z_q||^2 =  688.6 e-6 = 69.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1564.7 e-6; = (1/var)*||X-X_r||^2 =  931.2 e-6 = 59.5 %; (1+beta)*||Z_e-Z_q||^2 =  633.5 e-6 = 40.5 %)
Min.  Avg. Train Loss across Mini-Batch =  167.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  529.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   576.8 e-6; = (1/var)*||X-X_r||^2 val-train = 631.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -55.1 e-6 

----------------------------------------------------------------------------------

Finished [21:40:39 01.01.2023] 208) Finished running for K = 2048 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 176) change_channel_size_across_layers = False:
Total training time is = 0:3:56 h/m/s. 

--------------------------------------------------- 

Started [21:40:39 01.01.2023] 209) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 44) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 865 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.23
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.92
3                           encoder.sequential_convs.conv2d_4.weight                        32             3.70
4                           encoder.sequential_convs.conv2d_5.weight                       131            15.14
5                                  encoder.pre_residual_stack.weight                       147            16.99
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.16
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.16
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
10                             encoder.channel_adjusting_conv.weight                         8             0.92
11                                                       VQ.E.weight                       131            15.14
12                             decoder.channel_adjusting_conv.weight                        73             8.44
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.16
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.16
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.14
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.70
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             0.92
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.23
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.01; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.06; perplexity/K = 0.05%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999558.1 e-6; = (1/var)*||X-X_r||^2 =  998280.4 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  1277.7 e-6 = 0.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  966544.7 e-6; = (1/var)*||X-X_r||^2 =  965202.8 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  1341.9 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  989687.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954489.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -33013.4 e-6; = (1/var)*||X-X_r||^2 val-train = -33077.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 64.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.90; perplexity/K = 1.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.28; perplexity/K = 1.38%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  738068.4 e-6; = (1/var)*||X-X_r||^2 =  578238.7 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  159829.6 e-6 = 21.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  624881.2 e-6; = (1/var)*||X-X_r||^2 =  570506.1 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  54375.1 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  677580.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  613519.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -113187.2 e-6; = (1/var)*||X-X_r||^2 val-train = -7732.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -105454.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.88; perplexity/K = 2.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.75; perplexity/K = 2.23%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  535864.1 e-6; = (1/var)*||X-X_r||^2 =  493478.0 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  42386.2 e-6 = 7.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  536600.8 e-6; = (1/var)*||X-X_r||^2 =  493102.1 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  43498.7 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  535864.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  531035.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   736.6 e-6; = (1/var)*||X-X_r||^2 val-train = -375.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1112.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.58; perplexity/K = 2.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.91; perplexity/K = 2.63%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  492842.0 e-6; = (1/var)*||X-X_r||^2 =  453526.8 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  39315.1 e-6 = 8.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  502071.1 e-6; = (1/var)*||X-X_r||^2 =  453736.4 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  48334.7 e-6 = 9.6 %)
Min.  Avg. Train Loss across Mini-Batch =  489248.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  492320.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9229.1 e-6; = (1/var)*||X-X_r||^2 val-train = 209.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9019.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.35; perplexity/K = 3.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.24; perplexity/K = 3.09%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  457456.8 e-6; = (1/var)*||X-X_r||^2 =  423705.7 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  33751.1 e-6 = 7.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  464209.0 e-6; = (1/var)*||X-X_r||^2 =  427061.3 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  37147.7 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  456785.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  462280.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6752.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3355.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3396.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.05; perplexity/K = 2.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.75; perplexity/K = 2.82%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  437637.3 e-6; = (1/var)*||X-X_r||^2 =  407211.2 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30426.1 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  450682.5 e-6; = (1/var)*||X-X_r||^2 =  416833.9 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  33848.6 e-6 = 7.5 %)
Min.  Avg. Train Loss across Mini-Batch =  437637.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  447746.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13045.1 e-6; = (1/var)*||X-X_r||^2 val-train = 9622.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3422.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.41; perplexity/K = 2.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.54; perplexity/K = 3.05%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  426453.6 e-6; = (1/var)*||X-X_r||^2 =  398681.5 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  27772.1 e-6 = 6.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  444498.5 e-6; = (1/var)*||X-X_r||^2 =  411817.7 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  32680.9 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  425899.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  440934.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18044.9 e-6; = (1/var)*||X-X_r||^2 val-train = 13136.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4908.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.62; perplexity/K = 2.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.33; perplexity/K = 2.80%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  418117.6 e-6; = (1/var)*||X-X_r||^2 =  392875.0 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  25242.6 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  439703.3 e-6; = (1/var)*||X-X_r||^2 =  408981.4 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  30721.9 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  418117.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  436715.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21585.7 e-6; = (1/var)*||X-X_r||^2 val-train = 16106.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5479.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.74; perplexity/K = 3.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.59; perplexity/K = 3.01%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  409601.6 e-6; = (1/var)*||X-X_r||^2 =  387251.1 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  22350.5 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  432087.4 e-6; = (1/var)*||X-X_r||^2 =  403681.3 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  28406.2 e-6 = 6.6 %)
Min.  Avg. Train Loss across Mini-Batch =  409601.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431990.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22485.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16430.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6055.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.71; perplexity/K = 2.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.56; perplexity/K = 2.96%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  414085.5 e-6; = (1/var)*||X-X_r||^2 =  389925.8 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  24159.8 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  433511.6 e-6; = (1/var)*||X-X_r||^2 =  405745.2 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  27766.4 e-6 = 6.4 %)
Min.  Avg. Train Loss across Mini-Batch =  404407.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  428578.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19426.1 e-6; = (1/var)*||X-X_r||^2 val-train = 15819.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3606.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.24; perplexity/K = 2.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.21; perplexity/K = 2.94%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  400770.2 e-6; = (1/var)*||X-X_r||^2 =  381410.8 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  19359.4 e-6 = 4.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  427809.4 e-6; = (1/var)*||X-X_r||^2 =  401705.5 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  26103.9 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  399033.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  425316.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27039.2 e-6; = (1/var)*||X-X_r||^2 val-train = 20294.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6744.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.34; perplexity/K = 2.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.05; perplexity/K = 3.03%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399911.9 e-6; = (1/var)*||X-X_r||^2 =  380456.5 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  19455.3 e-6 = 4.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  425991.5 e-6; = (1/var)*||X-X_r||^2 =  401948.2 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  24043.3 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  395248.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  422202.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26079.7 e-6; = (1/var)*||X-X_r||^2 val-train = 21491.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4588.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.19; perplexity/K = 2.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.93; perplexity/K = 2.93%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  394429.3 e-6; = (1/var)*||X-X_r||^2 =  377421.0 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  17008.4 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  424918.7 e-6; = (1/var)*||X-X_r||^2 =  402052.0 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22866.7 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  391159.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  421404.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30489.4 e-6; = (1/var)*||X-X_r||^2 val-train = 24631.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5858.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.27; perplexity/K = 2.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.70; perplexity/K = 2.87%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388043.0 e-6; = (1/var)*||X-X_r||^2 =  373665.6 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14377.4 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  417750.5 e-6; = (1/var)*||X-X_r||^2 =  397029.9 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  20720.7 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  388023.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  417750.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29707.5 e-6; = (1/var)*||X-X_r||^2 val-train = 23364.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6343.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.71; perplexity/K = 2.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.66; perplexity/K = 2.96%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388119.7 e-6; = (1/var)*||X-X_r||^2 =  373830.6 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14289.1 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  419672.3 e-6; = (1/var)*||X-X_r||^2 =  399406.5 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  20265.8 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  386029.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  417134.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31552.6 e-6; = (1/var)*||X-X_r||^2 val-train = 25575.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5976.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.36; perplexity/K = 3.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.25; perplexity/K = 2.84%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  385458.6 e-6; = (1/var)*||X-X_r||^2 =  372118.1 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  13340.5 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  419707.0 e-6; = (1/var)*||X-X_r||^2 =  400457.1 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  19249.9 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  383702.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416775.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34248.4 e-6; = (1/var)*||X-X_r||^2 val-train = 28339.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5909.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.80; perplexity/K = 2.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.37; perplexity/K = 2.95%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388256.7 e-6; = (1/var)*||X-X_r||^2 =  373642.1 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14614.5 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  421882.1 e-6; = (1/var)*||X-X_r||^2 =  401624.2 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  20257.8 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  382294.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416341.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33625.4 e-6; = (1/var)*||X-X_r||^2 val-train = 27982.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5643.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.13; perplexity/K = 3.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.70; perplexity/K = 2.96%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  381561.0 e-6; = (1/var)*||X-X_r||^2 =  369614.4 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  11946.6 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  416941.3 e-6; = (1/var)*||X-X_r||^2 =  399358.5 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17582.8 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  380304.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415199.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35380.3 e-6; = (1/var)*||X-X_r||^2 val-train = 29744.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5636.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.61; perplexity/K = 2.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.17; perplexity/K = 2.84%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  380245.7 e-6; = (1/var)*||X-X_r||^2 =  368866.5 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11379.2 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  418035.9 e-6; = (1/var)*||X-X_r||^2 =  400740.9 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  17295.0 e-6 = 4.1 %)
Min.  Avg. Train Loss across Mini-Batch =  379191.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415199.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37790.2 e-6; = (1/var)*||X-X_r||^2 val-train = 31874.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5915.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.64; perplexity/K = 3.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.17; perplexity/K = 2.89%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  379868.2 e-6; = (1/var)*||X-X_r||^2 =  367864.5 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  12003.7 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  416429.5 e-6; = (1/var)*||X-X_r||^2 =  398820.9 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17608.6 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  378045.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  414710.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36561.3 e-6; = (1/var)*||X-X_r||^2 val-train = 30956.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5604.9 e-6 

----------------------------------------------------------------------------------

Finished [22:31:53 01.01.2023] 209) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 44) change_channel_size_across_layers = True:
Total training time is = 0:1:13 h/m/s. 

--------------------------------------------------- 

Started [22:31:53 01.01.2023] 210) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 44) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2597 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.31
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.23
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.04
4                           encoder.sequential_convs.conv2d_5.weight                       524            20.18
5                                  encoder.pre_residual_stack.weight                       589            22.68
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.81
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.81
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
10                             encoder.channel_adjusting_conv.weight                        16             0.62
11                                                       VQ.E.weight                       131             5.04
12                             decoder.channel_adjusting_conv.weight                       147             5.66
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.81
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.81
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.18
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.04
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.23
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.31
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1001603.1 e-6; = (1/var)*||X-X_r||^2 =  1001560.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  42.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  965621.8 e-6; = (1/var)*||X-X_r||^2 =  965579.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  42.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  994539.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954474.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35981.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35981.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.97; perplexity/K = 0.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.62; perplexity/K = 0.91%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1384852.6 e-6; = (1/var)*||X-X_r||^2 =  625269.1 e-6 = 45.2 %; (1+beta)*||Z_e-Z_q||^2 =  759583.5 e-6 = 54.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  710459.7 e-6; = (1/var)*||X-X_r||^2 =  615689.2 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  94770.5 e-6 = 13.3 %)
Min.  Avg. Train Loss across Mini-Batch =  980518.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  624342.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -674392.9 e-6; = (1/var)*||X-X_r||^2 val-train = -9579.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -664813.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.03; perplexity/K = 1.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.47; perplexity/K = 1.59%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  504839.1 e-6; = (1/var)*||X-X_r||^2 =  483508.3 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  21330.8 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  501496.1 e-6; = (1/var)*||X-X_r||^2 =  478889.2 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  22606.9 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  504839.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  496018.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3343.0 e-6; = (1/var)*||X-X_r||^2 val-train = -4619.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1276.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.10; perplexity/K = 1.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.58; perplexity/K = 1.44%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  427692.8 e-6; = (1/var)*||X-X_r||^2 =  409819.1 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17873.7 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  433352.2 e-6; = (1/var)*||X-X_r||^2 =  414336.7 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  19015.4 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  426604.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432932.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5659.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4517.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1141.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.22; perplexity/K = 1.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.39; perplexity/K = 1.53%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  403810.8 e-6; = (1/var)*||X-X_r||^2 =  385923.5 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  17887.3 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  413321.8 e-6; = (1/var)*||X-X_r||^2 =  394952.5 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  18369.4 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  400091.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  410556.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9511.0 e-6; = (1/var)*||X-X_r||^2 val-train = 9029.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 482.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.06; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.88; perplexity/K = 1.61%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  389107.7 e-6; = (1/var)*||X-X_r||^2 =  373411.1 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  15696.6 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  404763.5 e-6; = (1/var)*||X-X_r||^2 =  386402.6 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  18360.9 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  384481.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  398348.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15655.8 e-6; = (1/var)*||X-X_r||^2 val-train = 12991.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2664.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.01; perplexity/K = 1.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.05; perplexity/K = 1.56%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393253.5 e-6; = (1/var)*||X-X_r||^2 =  375586.1 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  17667.4 e-6 = 4.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  402992.6 e-6; = (1/var)*||X-X_r||^2 =  384716.1 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  18276.5 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  375391.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  391915.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9739.1 e-6; = (1/var)*||X-X_r||^2 val-train = 9130.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 609.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.38; perplexity/K = 1.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.15; perplexity/K = 1.57%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  367102.7 e-6; = (1/var)*||X-X_r||^2 =  356199.6 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  10903.0 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  385736.9 e-6; = (1/var)*||X-X_r||^2 =  372023.7 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  13713.1 e-6 = 3.6 %)
Min.  Avg. Train Loss across Mini-Batch =  367102.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  385395.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18634.2 e-6; = (1/var)*||X-X_r||^2 val-train = 15824.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2810.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.63; perplexity/K = 1.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.76; perplexity/K = 1.40%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  363084.8 e-6; = (1/var)*||X-X_r||^2 =  353454.8 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  9630.0 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  383021.8 e-6; = (1/var)*||X-X_r||^2 =  370420.8 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  12601.0 e-6 = 3.3 %)
Min.  Avg. Train Loss across Mini-Batch =  363084.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383021.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19937.0 e-6; = (1/var)*||X-X_r||^2 val-train = 16966.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2971.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.66; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.21; perplexity/K = 1.48%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  359314.2 e-6; = (1/var)*||X-X_r||^2 =  351188.1 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8126.1 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  380895.6 e-6; = (1/var)*||X-X_r||^2 =  369456.4 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11439.2 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  359291.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  380216.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21581.4 e-6; = (1/var)*||X-X_r||^2 val-train = 18268.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3313.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.96; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.89; perplexity/K = 1.56%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  357237.3 e-6; = (1/var)*||X-X_r||^2 =  349919.6 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  7317.8 e-6 = 2.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  380226.2 e-6; = (1/var)*||X-X_r||^2 =  369525.9 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  10700.3 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  357237.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  378004.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22988.9 e-6; = (1/var)*||X-X_r||^2 val-train = 19606.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3382.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.02; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.26; perplexity/K = 1.58%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  359615.6 e-6; = (1/var)*||X-X_r||^2 =  351595.7 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  8019.9 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  381713.0 e-6; = (1/var)*||X-X_r||^2 =  370054.4 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  11658.7 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  355241.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  376389.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22097.4 e-6; = (1/var)*||X-X_r||^2 val-train = 18458.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3638.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.95; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.48; perplexity/K = 1.54%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  353680.5 e-6; = (1/var)*||X-X_r||^2 =  347880.5 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  5799.9 e-6 = 1.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  376107.7 e-6; = (1/var)*||X-X_r||^2 =  366986.5 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  9121.2 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  353680.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  375782.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22427.3 e-6; = (1/var)*||X-X_r||^2 val-train = 19106.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3321.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.39; perplexity/K = 1.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.72; perplexity/K = 1.60%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  352484.3 e-6; = (1/var)*||X-X_r||^2 =  347098.7 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5385.6 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  375844.1 e-6; = (1/var)*||X-X_r||^2 =  367243.9 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8600.2 e-6 = 2.3 %)
Min.  Avg. Train Loss across Mini-Batch =  352484.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  375004.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23359.8 e-6; = (1/var)*||X-X_r||^2 val-train = 20145.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3214.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.11; perplexity/K = 1.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.00; perplexity/K = 1.56%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  364837.8 e-6; = (1/var)*||X-X_r||^2 =  355093.7 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  9744.2 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  383708.4 e-6; = (1/var)*||X-X_r||^2 =  370767.8 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  12940.6 e-6 = 3.4 %)
Min.  Avg. Train Loss across Mini-Batch =  351505.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  374753.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18870.6 e-6; = (1/var)*||X-X_r||^2 val-train = 15674.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3196.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.16; perplexity/K = 1.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.48; perplexity/K = 1.63%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  356196.9 e-6; = (1/var)*||X-X_r||^2 =  348584.9 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7612.0 e-6 = 2.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  375575.8 e-6; = (1/var)*||X-X_r||^2 =  365507.7 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  10068.1 e-6 = 2.7 %)
Min.  Avg. Train Loss across Mini-Batch =  350659.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  373237.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19378.9 e-6; = (1/var)*||X-X_r||^2 val-train = 16922.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2456.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.19; perplexity/K = 1.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.20; perplexity/K = 1.62%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350271.9 e-6; = (1/var)*||X-X_r||^2 =  345674.3 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  4597.6 e-6 = 1.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  373560.6 e-6; = (1/var)*||X-X_r||^2 =  365807.2 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7753.4 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  349829.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  372217.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23288.7 e-6; = (1/var)*||X-X_r||^2 val-train = 20132.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3155.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.28; perplexity/K = 1.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.84; perplexity/K = 1.55%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  351501.0 e-6; = (1/var)*||X-X_r||^2 =  346239.7 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5261.3 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  373910.8 e-6; = (1/var)*||X-X_r||^2 =  366162.7 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7748.0 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  349027.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  372217.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22409.8 e-6; = (1/var)*||X-X_r||^2 val-train = 19923.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2486.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.05; perplexity/K = 1.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.31; perplexity/K = 1.63%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  353513.9 e-6; = (1/var)*||X-X_r||^2 =  347365.2 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6148.7 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  373118.4 e-6; = (1/var)*||X-X_r||^2 =  364797.1 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  8321.3 e-6 = 2.2 %)
Min.  Avg. Train Loss across Mini-Batch =  348414.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  371498.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19604.5 e-6; = (1/var)*||X-X_r||^2 val-train = 17431.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2172.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.52; perplexity/K = 1.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.92; perplexity/K = 1.61%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  357305.5 e-6; = (1/var)*||X-X_r||^2 =  349535.6 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  7769.9 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  373547.7 e-6; = (1/var)*||X-X_r||^2 =  364582.2 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  8965.4 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  348096.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  370883.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16242.2 e-6; = (1/var)*||X-X_r||^2 val-train = 15046.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1195.5 e-6 

----------------------------------------------------------------------------------

Finished [23:22:34 01.01.2023] 210) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 44) change_channel_size_across_layers = True:
Total training time is = 0:1:40 h/m/s. 

--------------------------------------------------- 

Started [23:22:34 01.01.2023] 211) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 44) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2627 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.23
1                           encoder.sequential_convs.conv2d_2.weight                       262             9.97
2                           encoder.sequential_convs.conv2d_3.weight                       262             9.97
3                           encoder.sequential_convs.conv2d_4.weight                       262             9.97
4                           encoder.sequential_convs.conv2d_5.weight                       262             9.97
5                                  encoder.pre_residual_stack.weight                       147             5.60
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.37
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.15
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.37
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.15
10                             encoder.channel_adjusting_conv.weight                         8             0.30
11                                                       VQ.E.weight                       131             4.99
12                             decoder.channel_adjusting_conv.weight                        73             2.78
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.37
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.15
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.37
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.15
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262             9.97
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262             9.97
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262             9.97
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262             9.97
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.69; perplexity/K = 0.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.09; perplexity/K = 0.93%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  654793.6 e-6; = (1/var)*||X-X_r||^2 =  545983.8 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  108809.8 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  627488.5 e-6; = (1/var)*||X-X_r||^2 =  532998.6 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  94490.0 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  654793.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  627488.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27305.0 e-6; = (1/var)*||X-X_r||^2 val-train = -12985.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14319.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.83; perplexity/K = 1.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.87; perplexity/K = 1.46%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  339413.2 e-6; = (1/var)*||X-X_r||^2 =  200738.2 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  138675.0 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  365788.8 e-6; = (1/var)*||X-X_r||^2 =  214656.3 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  151132.5 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  320416.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  344686.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   26375.6 e-6; = (1/var)*||X-X_r||^2 val-train = 13918.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12457.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.79; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.87; perplexity/K = 1.70%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  277922.0 e-6; = (1/var)*||X-X_r||^2 =  146244.9 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  131677.1 e-6 = 47.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  279334.1 e-6; = (1/var)*||X-X_r||^2 =  154944.2 e-6 = 55.5 %; (1+beta)*||Z_e-Z_q||^2 =  124389.9 e-6 = 44.5 %)
Min.  Avg. Train Loss across Mini-Batch =  208175.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  264396.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1412.1 e-6; = (1/var)*||X-X_r||^2 val-train = 8699.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7287.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.41; perplexity/K = 1.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.32; perplexity/K = 1.72%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  146782.4 e-6; = (1/var)*||X-X_r||^2 =  55956.0 e-6 = 38.1 %; (1+beta)*||Z_e-Z_q||^2 =  90826.4 e-6 = 61.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  224897.2 e-6; = (1/var)*||X-X_r||^2 =  119112.3 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  105784.8 e-6 = 47.0 %)
Min.  Avg. Train Loss across Mini-Batch =  140954.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  223003.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   78114.8 e-6; = (1/var)*||X-X_r||^2 val-train = 63156.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14958.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.58; perplexity/K = 1.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.74; perplexity/K = 1.75%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  120802.1 e-6; = (1/var)*||X-X_r||^2 =  43369.6 e-6 = 35.9 %; (1+beta)*||Z_e-Z_q||^2 =  77432.5 e-6 = 64.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  213893.3 e-6; = (1/var)*||X-X_r||^2 =  116655.9 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  97237.3 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  84792.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  192521.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   93091.2 e-6; = (1/var)*||X-X_r||^2 val-train = 73286.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19804.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.75; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.61; perplexity/K = 1.64%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68821.1 e-6; = (1/var)*||X-X_r||^2 =  19153.1 e-6 = 27.8 %; (1+beta)*||Z_e-Z_q||^2 =  49668.0 e-6 = 72.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  180191.7 e-6; = (1/var)*||X-X_r||^2 =  105687.2 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  74504.5 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  55885.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  174281.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   111370.6 e-6; = (1/var)*||X-X_r||^2 val-train = 86534.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24836.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.33; perplexity/K = 1.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.94; perplexity/K = 1.75%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36987.3 e-6; = (1/var)*||X-X_r||^2 =  10005.2 e-6 = 27.1 %; (1+beta)*||Z_e-Z_q||^2 =  26982.1 e-6 = 72.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  160083.3 e-6; = (1/var)*||X-X_r||^2 =  102871.5 e-6 = 64.3 %; (1+beta)*||Z_e-Z_q||^2 =  57211.9 e-6 = 35.7 %)
Min.  Avg. Train Loss across Mini-Batch =  34719.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  156911.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   123096.0 e-6; = (1/var)*||X-X_r||^2 val-train = 92866.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30229.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.21; perplexity/K = 1.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.86; perplexity/K = 1.75%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36364.3 e-6; = (1/var)*||X-X_r||^2 =  8919.3 e-6 = 24.5 %; (1+beta)*||Z_e-Z_q||^2 =  27445.1 e-6 = 75.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  159870.7 e-6; = (1/var)*||X-X_r||^2 =  103394.9 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  56475.7 e-6 = 35.3 %)
Min.  Avg. Train Loss across Mini-Batch =  23341.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  152194.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   123506.3 e-6; = (1/var)*||X-X_r||^2 val-train = 94475.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29030.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.79; perplexity/K = 1.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.53; perplexity/K = 1.73%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  268338.4 e-6; = (1/var)*||X-X_r||^2 =  153554.6 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  114783.8 e-6 = 42.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  285816.6 e-6; = (1/var)*||X-X_r||^2 =  171735.4 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  114081.2 e-6 = 39.9 %)
Min.  Avg. Train Loss across Mini-Batch =  15233.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151333.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   17478.2 e-6; = (1/var)*||X-X_r||^2 val-train = 18180.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -702.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.05; perplexity/K = 1.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.34; perplexity/K = 1.73%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15010.3 e-6; = (1/var)*||X-X_r||^2 =  4534.8 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  10475.5 e-6 = 69.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  149233.4 e-6; = (1/var)*||X-X_r||^2 =  103836.4 e-6 = 69.6 %; (1+beta)*||Z_e-Z_q||^2 =  45397.0 e-6 = 30.4 %)
Min.  Avg. Train Loss across Mini-Batch =  12640.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   134223.1 e-6; = (1/var)*||X-X_r||^2 val-train = 99301.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34921.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.89; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.78; perplexity/K = 1.70%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10241.1 e-6; = (1/var)*||X-X_r||^2 =  3874.0 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  6367.1 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  150751.5 e-6; = (1/var)*||X-X_r||^2 =  109004.7 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  41746.7 e-6 = 27.7 %)
Min.  Avg. Train Loss across Mini-Batch =  9667.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   140510.4 e-6; = (1/var)*||X-X_r||^2 val-train = 105130.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35379.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.87; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.88; perplexity/K = 1.70%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8960.9 e-6; = (1/var)*||X-X_r||^2 =  3453.2 e-6 = 38.5 %; (1+beta)*||Z_e-Z_q||^2 =  5507.7 e-6 = 61.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  154604.3 e-6; = (1/var)*||X-X_r||^2 =  113454.2 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  41150.1 e-6 = 26.6 %)
Min.  Avg. Train Loss across Mini-Batch =  8793.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145643.3 e-6; = (1/var)*||X-X_r||^2 val-train = 110001.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35642.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.68; perplexity/K = 1.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.06; perplexity/K = 1.76%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  81423.0 e-6; = (1/var)*||X-X_r||^2 =  20922.0 e-6 = 25.7 %; (1+beta)*||Z_e-Z_q||^2 =  60501.0 e-6 = 74.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  177523.4 e-6; = (1/var)*||X-X_r||^2 =  102312.5 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  75211.0 e-6 = 42.4 %)
Min.  Avg. Train Loss across Mini-Batch =  7092.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   96100.4 e-6; = (1/var)*||X-X_r||^2 val-train = 81390.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14710.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.83; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.32; perplexity/K = 1.72%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8112.2 e-6; = (1/var)*||X-X_r||^2 =  2770.2 e-6 = 34.1 %; (1+beta)*||Z_e-Z_q||^2 =  5342.0 e-6 = 65.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  154009.8 e-6; = (1/var)*||X-X_r||^2 =  109785.9 e-6 = 71.3 %; (1+beta)*||Z_e-Z_q||^2 =  44223.9 e-6 = 28.7 %)
Min.  Avg. Train Loss across Mini-Batch =  6892.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145897.6 e-6; = (1/var)*||X-X_r||^2 val-train = 107015.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38881.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.61; perplexity/K = 1.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.24; perplexity/K = 1.72%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5583.2 e-6; = (1/var)*||X-X_r||^2 =  2281.7 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  3301.4 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  155992.6 e-6; = (1/var)*||X-X_r||^2 =  112695.0 e-6 = 72.2 %; (1+beta)*||Z_e-Z_q||^2 =  43297.6 e-6 = 27.8 %)
Min.  Avg. Train Loss across Mini-Batch =  5583.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   150409.4 e-6; = (1/var)*||X-X_r||^2 val-train = 110413.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39996.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.76; perplexity/K = 1.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.14; perplexity/K = 1.72%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4815.2 e-6; = (1/var)*||X-X_r||^2 =  2032.9 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  2782.3 e-6 = 57.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  156906.1 e-6; = (1/var)*||X-X_r||^2 =  113973.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  42933.1 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4815.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   152090.8 e-6; = (1/var)*||X-X_r||^2 val-train = 111940.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40150.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.19; perplexity/K = 1.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.21; perplexity/K = 1.72%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4906.9 e-6; = (1/var)*||X-X_r||^2 =  2009.2 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  2897.7 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  152582.4 e-6; = (1/var)*||X-X_r||^2 =  110306.5 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  42275.9 e-6 = 27.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4385.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   147675.6 e-6; = (1/var)*||X-X_r||^2 val-train = 108297.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39378.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.79; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.74; perplexity/K = 1.75%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4151.2 e-6; = (1/var)*||X-X_r||^2 =  1798.2 e-6 = 43.3 %; (1+beta)*||Z_e-Z_q||^2 =  2353.0 e-6 = 56.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  161638.5 e-6; = (1/var)*||X-X_r||^2 =  118493.8 e-6 = 73.3 %; (1+beta)*||Z_e-Z_q||^2 =  43144.7 e-6 = 26.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4110.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157487.3 e-6; = (1/var)*||X-X_r||^2 val-train = 116695.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40791.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.06; perplexity/K = 1.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.99; perplexity/K = 1.76%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4086.1 e-6; = (1/var)*||X-X_r||^2 =  1710.5 e-6 = 41.9 %; (1+beta)*||Z_e-Z_q||^2 =  2375.6 e-6 = 58.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  156726.8 e-6; = (1/var)*||X-X_r||^2 =  113421.6 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  43305.2 e-6 = 27.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3702.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   152640.7 e-6; = (1/var)*||X-X_r||^2 val-train = 111711.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40929.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.46; perplexity/K = 1.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.58; perplexity/K = 1.74%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32706.2 e-6; = (1/var)*||X-X_r||^2 =  5709.6 e-6 = 17.5 %; (1+beta)*||Z_e-Z_q||^2 =  26996.7 e-6 = 82.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  162568.5 e-6; = (1/var)*||X-X_r||^2 =  107829.5 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  54739.0 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3702.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  148772.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   129862.3 e-6; = (1/var)*||X-X_r||^2 val-train = 102119.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27742.3 e-6 

----------------------------------------------------------------------------------

Finished [00:28:12 02.01.2023] 211) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 44) change_channel_size_across_layers = False:
Total training time is = 0:1:37 h/m/s. 

--------------------------------------------------- 

Started [00:28:12 02.01.2023] 212) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 44) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9615 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.12
1                           encoder.sequential_convs.conv2d_2.weight                      1048            10.90
2                           encoder.sequential_convs.conv2d_3.weight                      1048            10.90
3                           encoder.sequential_convs.conv2d_4.weight                      1048            10.90
4                           encoder.sequential_convs.conv2d_5.weight                      1048            10.90
5                                  encoder.pre_residual_stack.weight                       589             6.13
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.76
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.76
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                       131             1.36
12                             decoder.channel_adjusting_conv.weight                       147             1.53
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.76
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.76
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            10.90
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            10.90
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            10.90
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            10.90
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.00; perplexity/K = 0.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.47; perplexity/K = 0.95%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  586212.7 e-6; = (1/var)*||X-X_r||^2 =  293011.9 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  293200.8 e-6 = 50.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  600063.4 e-6; = (1/var)*||X-X_r||^2 =  310993.2 e-6 = 51.8 %; (1+beta)*||Z_e-Z_q||^2 =  289070.2 e-6 = 48.2 %)
Min.  Avg. Train Loss across Mini-Batch =  586212.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  593028.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13850.7 e-6; = (1/var)*||X-X_r||^2 val-train = 17981.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4130.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.25; perplexity/K = 1.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.23; perplexity/K = 1.57%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  297429.3 e-6; = (1/var)*||X-X_r||^2 =  126193.2 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  171236.1 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  350576.5 e-6; = (1/var)*||X-X_r||^2 =  172918.4 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  177658.1 e-6 = 50.7 %)
Min.  Avg. Train Loss across Mini-Batch =  270169.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  328106.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53147.2 e-6; = (1/var)*||X-X_r||^2 val-train = 46725.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6422.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.97; perplexity/K = 1.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.75; perplexity/K = 1.99%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  200528.0 e-6; = (1/var)*||X-X_r||^2 =  73884.4 e-6 = 36.8 %; (1+beta)*||Z_e-Z_q||^2 =  126643.6 e-6 = 63.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  289017.8 e-6; = (1/var)*||X-X_r||^2 =  146431.7 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  142586.1 e-6 = 49.3 %)
Min.  Avg. Train Loss across Mini-Batch =  113399.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  220453.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   88489.7 e-6; = (1/var)*||X-X_r||^2 val-train = 72547.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15942.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.43; perplexity/K = 1.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.37; perplexity/K = 1.97%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36909.5 e-6; = (1/var)*||X-X_r||^2 =  9190.6 e-6 = 24.9 %; (1+beta)*||Z_e-Z_q||^2 =  27718.9 e-6 = 75.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  168289.1 e-6; = (1/var)*||X-X_r||^2 =  115804.8 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  52484.3 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  36909.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  168289.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   131379.7 e-6; = (1/var)*||X-X_r||^2 val-train = 106614.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24765.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.38; perplexity/K = 1.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.10; perplexity/K = 2.01%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  99063.1 e-6; = (1/var)*||X-X_r||^2 =  45850.9 e-6 = 46.3 %; (1+beta)*||Z_e-Z_q||^2 =  53212.1 e-6 = 53.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  236778.2 e-6; = (1/var)*||X-X_r||^2 =  159143.8 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  77634.4 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  15629.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  150764.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137715.1 e-6; = (1/var)*||X-X_r||^2 val-train = 113292.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24422.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.96; perplexity/K = 2.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.42; perplexity/K = 2.02%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40403.7 e-6; = (1/var)*||X-X_r||^2 =  8247.5 e-6 = 20.4 %; (1+beta)*||Z_e-Z_q||^2 =  32156.3 e-6 = 79.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  159619.4 e-6; = (1/var)*||X-X_r||^2 =  111825.8 e-6 = 70.1 %; (1+beta)*||Z_e-Z_q||^2 =  47793.6 e-6 = 29.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8434.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143085.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   119215.7 e-6; = (1/var)*||X-X_r||^2 val-train = 103578.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15637.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.45; perplexity/K = 2.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.87; perplexity/K = 2.19%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  113851.3 e-6; = (1/var)*||X-X_r||^2 =  46492.0 e-6 = 40.8 %; (1+beta)*||Z_e-Z_q||^2 =  67359.3 e-6 = 59.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  207351.8 e-6; = (1/var)*||X-X_r||^2 =  134544.2 e-6 = 64.9 %; (1+beta)*||Z_e-Z_q||^2 =  72807.6 e-6 = 35.1 %)
Min.  Avg. Train Loss across Mini-Batch =  6541.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143085.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   93500.5 e-6; = (1/var)*||X-X_r||^2 val-train = 88052.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5448.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.91; perplexity/K = 2.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.30; perplexity/K = 2.07%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5606.2 e-6; = (1/var)*||X-X_r||^2 =  2321.1 e-6 = 41.4 %; (1+beta)*||Z_e-Z_q||^2 =  3285.1 e-6 = 58.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  151554.7 e-6; = (1/var)*||X-X_r||^2 =  123339.3 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  28215.4 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  4081.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141732.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145948.5 e-6; = (1/var)*||X-X_r||^2 val-train = 121018.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24930.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.63; perplexity/K = 2.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.77; perplexity/K = 2.14%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6484.4 e-6; = (1/var)*||X-X_r||^2 =  1803.4 e-6 = 27.8 %; (1+beta)*||Z_e-Z_q||^2 =  4681.0 e-6 = 72.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  143910.3 e-6; = (1/var)*||X-X_r||^2 =  115489.4 e-6 = 80.3 %; (1+beta)*||Z_e-Z_q||^2 =  28421.0 e-6 = 19.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3232.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141732.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137425.9 e-6; = (1/var)*||X-X_r||^2 val-train = 113685.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23740.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.49; perplexity/K = 1.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.64; perplexity/K = 2.08%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3224.6 e-6; = (1/var)*||X-X_r||^2 =  1388.1 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  1836.5 e-6 = 57.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  142128.8 e-6; = (1/var)*||X-X_r||^2 =  117796.9 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  24331.9 e-6 = 17.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3224.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  140390.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   138904.1 e-6; = (1/var)*||X-X_r||^2 val-train = 116408.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22495.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.20; perplexity/K = 2.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.22; perplexity/K = 2.11%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10253.8 e-6; = (1/var)*||X-X_r||^2 =  3553.8 e-6 = 34.7 %; (1+beta)*||Z_e-Z_q||^2 =  6700.0 e-6 = 65.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  140662.0 e-6; = (1/var)*||X-X_r||^2 =  117356.3 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  23305.7 e-6 = 16.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2918.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  136890.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   130408.2 e-6; = (1/var)*||X-X_r||^2 val-train = 113802.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16605.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.06; perplexity/K = 2.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.95; perplexity/K = 2.15%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2484.2 e-6; = (1/var)*||X-X_r||^2 =  1021.7 e-6 = 41.1 %; (1+beta)*||Z_e-Z_q||^2 =  1462.5 e-6 = 58.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  145317.4 e-6; = (1/var)*||X-X_r||^2 =  124719.6 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  20597.8 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2484.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   142833.3 e-6; = (1/var)*||X-X_r||^2 val-train = 123697.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19135.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.99; perplexity/K = 1.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.99; perplexity/K = 2.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13567.3 e-6; = (1/var)*||X-X_r||^2 =  2385.5 e-6 = 17.6 %; (1+beta)*||Z_e-Z_q||^2 =  11181.8 e-6 = 82.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  148210.6 e-6; = (1/var)*||X-X_r||^2 =  117360.2 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  30850.4 e-6 = 20.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1709.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   134643.3 e-6; = (1/var)*||X-X_r||^2 val-train = 114974.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19668.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.89; perplexity/K = 2.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.76; perplexity/K = 2.14%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2675.9 e-6; = (1/var)*||X-X_r||^2 =  897.2 e-6 = 33.5 %; (1+beta)*||Z_e-Z_q||^2 =  1778.8 e-6 = 66.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  144296.5 e-6; = (1/var)*||X-X_r||^2 =  122213.4 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  22083.2 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1709.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   141620.6 e-6; = (1/var)*||X-X_r||^2 val-train = 121316.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20304.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.88; perplexity/K = 2.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.12; perplexity/K = 2.15%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11256.8 e-6; = (1/var)*||X-X_r||^2 =  2147.9 e-6 = 19.1 %; (1+beta)*||Z_e-Z_q||^2 =  9108.9 e-6 = 80.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  152512.1 e-6; = (1/var)*||X-X_r||^2 =  126638.2 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  25873.8 e-6 = 17.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1709.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   141255.3 e-6; = (1/var)*||X-X_r||^2 val-train = 124490.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16764.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.58; perplexity/K = 2.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.84; perplexity/K = 2.04%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2939.0 e-6; = (1/var)*||X-X_r||^2 =  844.7 e-6 = 28.7 %; (1+beta)*||Z_e-Z_q||^2 =  2094.3 e-6 = 71.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  144952.0 e-6; = (1/var)*||X-X_r||^2 =  123821.6 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  21130.3 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1403.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   142013.0 e-6; = (1/var)*||X-X_r||^2 val-train = 122977.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19036.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.47; perplexity/K = 2.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.50; perplexity/K = 2.22%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4626.4 e-6; = (1/var)*||X-X_r||^2 =  969.6 e-6 = 21.0 %; (1+beta)*||Z_e-Z_q||^2 =  3656.8 e-6 = 79.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  148557.3 e-6; = (1/var)*||X-X_r||^2 =  127510.1 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  21047.3 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1403.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   143930.9 e-6; = (1/var)*||X-X_r||^2 val-train = 126540.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17390.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.41; perplexity/K = 2.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.87; perplexity/K = 2.09%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1451.4 e-6; = (1/var)*||X-X_r||^2 =  573.2 e-6 = 39.5 %; (1+beta)*||Z_e-Z_q||^2 =  878.2 e-6 = 60.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  143232.1 e-6; = (1/var)*||X-X_r||^2 =  124105.6 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  19126.5 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1169.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   141780.7 e-6; = (1/var)*||X-X_r||^2 val-train = 123532.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18248.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.84; perplexity/K = 1.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.73; perplexity/K = 2.23%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1339.4 e-6; = (1/var)*||X-X_r||^2 =  559.4 e-6 = 41.8 %; (1+beta)*||Z_e-Z_q||^2 =  780.0 e-6 = 58.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  138694.1 e-6; = (1/var)*||X-X_r||^2 =  119407.2 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  19286.9 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  951.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137354.7 e-6; = (1/var)*||X-X_r||^2 val-train = 118847.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18506.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.68; perplexity/K = 2.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.07; perplexity/K = 2.10%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1571.8 e-6; = (1/var)*||X-X_r||^2 =  535.4 e-6 = 34.1 %; (1+beta)*||Z_e-Z_q||^2 =  1036.4 e-6 = 65.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  145477.6 e-6; = (1/var)*||X-X_r||^2 =  126381.0 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  19096.6 e-6 = 13.1 %)
Min.  Avg. Train Loss across Mini-Batch =  937.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  131024.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   143905.9 e-6; = (1/var)*||X-X_r||^2 val-train = 125845.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18060.3 e-6 

----------------------------------------------------------------------------------

Finished [02:45:32 02.01.2023] 212) Finished running for K = 2048 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 44) change_channel_size_across_layers = False:
Total training time is = 0:1:20 h/m/s. 

--------------------------------------------------- 

Started [02:45:32 02.01.2023] 213) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 11) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 865 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.23
3                           encoder.sequential_convs.conv2d_4.weight                         8             0.92
4                           encoder.sequential_convs.conv2d_5.weight                        32             3.70
5                           encoder.sequential_convs.conv2d_6.weight                       131            15.14
6                                  encoder.pre_residual_stack.weight                       147            16.99
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.16
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.16
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
11                             encoder.channel_adjusting_conv.weight                         8             0.92
12                                                       VQ.E.weight                       131            15.14
13                             decoder.channel_adjusting_conv.weight                        73             8.44
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.16
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.46
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.16
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.46
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.14
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.70
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             0.92
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.23
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.53; perplexity/K = 0.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.50; perplexity/K = 0.22%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2530069.5 e-6; = (1/var)*||X-X_r||^2 =  885861.4 e-6 = 35.0 %; (1+beta)*||Z_e-Z_q||^2 =  1644208.1 e-6 = 65.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1467375.5 e-6; = (1/var)*||X-X_r||^2 =  861085.4 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  606290.1 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1010828.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  946001.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1062694.1 e-6; = (1/var)*||X-X_r||^2 val-train = -24776.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1037918.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.55; perplexity/K = 0.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.52; perplexity/K = 0.46%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1475441.9 e-6; = (1/var)*||X-X_r||^2 =  741644.8 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  733797.2 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1497706.8 e-6; = (1/var)*||X-X_r||^2 =  724619.0 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  773087.9 e-6 = 51.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1010828.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  946001.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22264.9 e-6; = (1/var)*||X-X_r||^2 val-train = -17025.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39290.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.99; perplexity/K = 0.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.33; perplexity/K = 0.41%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  919212.9 e-6; = (1/var)*||X-X_r||^2 =  662998.5 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  256214.4 e-6 = 27.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  860437.4 e-6; = (1/var)*||X-X_r||^2 =  640231.7 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  220205.7 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  918611.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  817362.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -58775.4 e-6; = (1/var)*||X-X_r||^2 val-train = -22766.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36008.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.11; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.02; perplexity/K = 0.15%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  858579.3 e-6; = (1/var)*||X-X_r||^2 =  687766.6 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  170812.7 e-6 = 19.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  796148.7 e-6; = (1/var)*||X-X_r||^2 =  677490.8 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  118657.9 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  850617.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  788399.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -62430.6 e-6; = (1/var)*||X-X_r||^2 val-train = -10275.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -52154.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.50; perplexity/K = 0.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.67; perplexity/K = 0.08%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  847573.6 e-6; = (1/var)*||X-X_r||^2 =  742500.6 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  105072.9 e-6 = 12.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  811350.4 e-6; = (1/var)*||X-X_r||^2 =  720045.8 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  91304.6 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  843973.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  788399.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36223.1 e-6; = (1/var)*||X-X_r||^2 val-train = -22454.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13768.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 0.06%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  884314.4 e-6; = (1/var)*||X-X_r||^2 =  828451.9 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  55862.6 e-6 = 6.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  860657.3 e-6; = (1/var)*||X-X_r||^2 =  816940.3 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  43717.0 e-6 = 5.1 %)
Min.  Avg. Train Loss across Mini-Batch =  843973.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  788399.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23657.1 e-6; = (1/var)*||X-X_r||^2 val-train = -11511.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12145.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.81; perplexity/K = 0.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.46; perplexity/K = 0.12%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  842860.7 e-6; = (1/var)*||X-X_r||^2 =  728809.4 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  114051.3 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  829305.3 e-6; = (1/var)*||X-X_r||^2 =  725232.3 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  104072.9 e-6 = 12.5 %)
Min.  Avg. Train Loss across Mini-Batch =  830055.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  783688.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13555.4 e-6; = (1/var)*||X-X_r||^2 val-train = -3577.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9978.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.76; perplexity/K = 0.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.14; perplexity/K = 0.54%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  818429.4 e-6; = (1/var)*||X-X_r||^2 =  658027.6 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  160401.8 e-6 = 19.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  788610.1 e-6; = (1/var)*||X-X_r||^2 =  646834.8 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  141775.3 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  797702.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  779799.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -29819.3 e-6; = (1/var)*||X-X_r||^2 val-train = -11192.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18626.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.07; perplexity/K = 0.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.34; perplexity/K = 0.31%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  772404.9 e-6; = (1/var)*||X-X_r||^2 =  642409.5 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  129995.3 e-6 = 16.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  742261.6 e-6; = (1/var)*||X-X_r||^2 =  632060.2 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  110201.4 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  764791.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  735463.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30143.3 e-6; = (1/var)*||X-X_r||^2 val-train = -10349.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19793.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.91; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.48; perplexity/K = 0.32%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  744722.4 e-6; = (1/var)*||X-X_r||^2 =  642526.8 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  102195.5 e-6 = 13.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  735939.0 e-6; = (1/var)*||X-X_r||^2 =  632039.3 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  103899.6 e-6 = 14.1 %)
Min.  Avg. Train Loss across Mini-Batch =  742721.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  721721.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8783.4 e-6; = (1/var)*||X-X_r||^2 val-train = -10487.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1704.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.77; perplexity/K = 0.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.80; perplexity/K = 0.23%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  762789.5 e-6; = (1/var)*||X-X_r||^2 =  648800.0 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  113989.4 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  740138.5 e-6; = (1/var)*||X-X_r||^2 =  638207.1 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  101931.4 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  738785.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  712558.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22651.0 e-6; = (1/var)*||X-X_r||^2 val-train = -10593.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12058.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.16; perplexity/K = 0.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.48; perplexity/K = 0.46%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  768326.7 e-6; = (1/var)*||X-X_r||^2 =  638711.1 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  129615.7 e-6 = 16.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  773877.3 e-6; = (1/var)*||X-X_r||^2 =  623571.0 e-6 = 80.6 %; (1+beta)*||Z_e-Z_q||^2 =  150306.3 e-6 = 19.4 %)
Min.  Avg. Train Loss across Mini-Batch =  738785.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  712558.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5550.6 e-6; = (1/var)*||X-X_r||^2 val-train = -15140.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20690.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.30; perplexity/K = 0.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.93; perplexity/K = 0.53%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  823172.7 e-6; = (1/var)*||X-X_r||^2 =  638887.6 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  184285.0 e-6 = 22.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  713197.0 e-6; = (1/var)*||X-X_r||^2 =  613314.1 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  99882.9 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  734056.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  702681.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -109975.7 e-6; = (1/var)*||X-X_r||^2 val-train = -25573.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -84402.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.51; perplexity/K = 0.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 0.59%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  721109.1 e-6; = (1/var)*||X-X_r||^2 =  617976.8 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  103132.3 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  696998.9 e-6; = (1/var)*||X-X_r||^2 =  607444.2 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  89554.7 e-6 = 12.8 %)
Min.  Avg. Train Loss across Mini-Batch =  704072.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  686849.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24110.2 e-6; = (1/var)*||X-X_r||^2 val-train = -10532.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13577.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.82; perplexity/K = 0.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.28; perplexity/K = 0.60%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  722223.7 e-6; = (1/var)*||X-X_r||^2 =  617999.6 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  104224.0 e-6 = 14.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  709305.5 e-6; = (1/var)*||X-X_r||^2 =  609436.2 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  99869.3 e-6 = 14.1 %)
Min.  Avg. Train Loss across Mini-Batch =  702579.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676516.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12918.2 e-6; = (1/var)*||X-X_r||^2 val-train = -8563.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4354.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.52; perplexity/K = 1.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.15; perplexity/K = 1.08%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  729789.2 e-6; = (1/var)*||X-X_r||^2 =  603017.2 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  126772.0 e-6 = 17.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  709895.6 e-6; = (1/var)*||X-X_r||^2 =  591183.2 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  118712.4 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  698128.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676516.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19893.6 e-6; = (1/var)*||X-X_r||^2 val-train = -11833.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8059.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.46; perplexity/K = 1.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.03; perplexity/K = 0.83%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  699544.8 e-6; = (1/var)*||X-X_r||^2 =  601980.8 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  97563.9 e-6 = 13.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  688654.5 e-6; = (1/var)*||X-X_r||^2 =  594768.7 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  93885.7 e-6 = 13.6 %)
Min.  Avg. Train Loss across Mini-Batch =  698128.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676516.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10890.3 e-6; = (1/var)*||X-X_r||^2 val-train = -7212.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3678.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.83; perplexity/K = 0.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.12; perplexity/K = 0.64%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  704943.0 e-6; = (1/var)*||X-X_r||^2 =  601063.2 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  103879.7 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  698722.9 e-6; = (1/var)*||X-X_r||^2 =  590958.8 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  107764.0 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  690740.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  668654.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6220.1 e-6; = (1/var)*||X-X_r||^2 val-train = -10104.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3884.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.05; perplexity/K = 0.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.19; perplexity/K = 0.35%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  702797.7 e-6; = (1/var)*||X-X_r||^2 =  612271.3 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  90526.4 e-6 = 12.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  680076.3 e-6; = (1/var)*||X-X_r||^2 =  596980.9 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  83095.4 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  681779.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  662247.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22721.4 e-6; = (1/var)*||X-X_r||^2 val-train = -15290.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7431.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.07; perplexity/K = 0.44%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  730046.6 e-6; = (1/var)*||X-X_r||^2 =  614545.5 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  115501.1 e-6 = 15.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  707953.5 e-6; = (1/var)*||X-X_r||^2 =  596950.9 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  111002.6 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  681779.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  662247.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22093.1 e-6; = (1/var)*||X-X_r||^2 val-train = -17594.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4498.5 e-6 

----------------------------------------------------------------------------------

Finished [03:37:09 02.01.2023] 213) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 11) change_channel_size_across_layers = True:
Total training time is = 0:0:36 h/m/s. 

--------------------------------------------------- 

Started [03:37:09 02.01.2023] 214) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 11) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2601 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.31
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.23
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.04
5                           encoder.sequential_convs.conv2d_6.weight                       524            20.15
6                                  encoder.pre_residual_stack.weight                       589            22.65
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.81
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.81
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
11                             encoder.channel_adjusting_conv.weight                        16             0.62
12                                                       VQ.E.weight                       131             5.04
13                             decoder.channel_adjusting_conv.weight                       147             5.65
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.81
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.31
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.81
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.31
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.15
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.04
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.23
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.31
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.59; perplexity/K = 0.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.16; perplexity/K = 0.40%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1765932.0 e-6; = (1/var)*||X-X_r||^2 =  897932.5 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  867999.4 e-6 = 49.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1337631.2 e-6; = (1/var)*||X-X_r||^2 =  862747.0 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  474884.1 e-6 = 35.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1007497.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  951720.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -428300.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35185.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -393115.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.30; perplexity/K = 0.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.27; perplexity/K = 0.16%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1404750.6 e-6; = (1/var)*||X-X_r||^2 =  795192.6 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  609558.0 e-6 = 43.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1464372.5 e-6; = (1/var)*||X-X_r||^2 =  766982.6 e-6 = 52.4 %; (1+beta)*||Z_e-Z_q||^2 =  697389.9 e-6 = 47.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1007497.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  946353.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   59622.0 e-6; = (1/var)*||X-X_r||^2 val-train = -28210.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 87831.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.26; perplexity/K = 0.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.33; perplexity/K = 0.06%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  992323.8 e-6; = (1/var)*||X-X_r||^2 =  832103.7 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  160220.0 e-6 = 16.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  900058.9 e-6; = (1/var)*||X-X_r||^2 =  827583.6 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  72475.3 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  958372.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -92264.9 e-6; = (1/var)*||X-X_r||^2 val-train = -4520.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -87744.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 0.05%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  970085.5 e-6; = (1/var)*||X-X_r||^2 =  948237.6 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  21848.0 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  938711.7 e-6; = (1/var)*||X-X_r||^2 =  924874.1 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  13837.5 e-6 = 1.5 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31373.9 e-6; = (1/var)*||X-X_r||^2 val-train = -23363.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8010.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999524.0 e-6; = (1/var)*||X-X_r||^2 =  999389.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  134.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963377.7 e-6; = (1/var)*||X-X_r||^2 =  963232.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  144.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36146.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36156.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999548.2 e-6; = (1/var)*||X-X_r||^2 =  999538.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963369.5 e-6; = (1/var)*||X-X_r||^2 =  963357.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36178.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36180.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999443.8 e-6; = (1/var)*||X-X_r||^2 =  999441.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963144.3 e-6; = (1/var)*||X-X_r||^2 =  963142.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36299.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36299.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999371.7 e-6; = (1/var)*||X-X_r||^2 =  999370.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962921.6 e-6; = (1/var)*||X-X_r||^2 =  962920.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36450.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36449.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999392.5 e-6; = (1/var)*||X-X_r||^2 =  999391.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962893.9 e-6; = (1/var)*||X-X_r||^2 =  962893.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36498.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36498.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999344.7 e-6; = (1/var)*||X-X_r||^2 =  999344.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963164.7 e-6; = (1/var)*||X-X_r||^2 =  963164.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36180.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36180.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999246.1 e-6; = (1/var)*||X-X_r||^2 =  999246.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963060.1 e-6; = (1/var)*||X-X_r||^2 =  963060.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36186.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36185.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999248.0 e-6; = (1/var)*||X-X_r||^2 =  999247.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962919.9 e-6; = (1/var)*||X-X_r||^2 =  962919.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36328.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36328.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999373.5 e-6; = (1/var)*||X-X_r||^2 =  999373.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963381.9 e-6; = (1/var)*||X-X_r||^2 =  963381.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35991.6 e-6; = (1/var)*||X-X_r||^2 val-train = -35991.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999371.6 e-6; = (1/var)*||X-X_r||^2 =  999371.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963196.3 e-6; = (1/var)*||X-X_r||^2 =  963196.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36175.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36175.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999293.0 e-6; = (1/var)*||X-X_r||^2 =  999292.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962918.0 e-6; = (1/var)*||X-X_r||^2 =  962917.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36375.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36375.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999381.8 e-6; = (1/var)*||X-X_r||^2 =  999381.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963591.3 e-6; = (1/var)*||X-X_r||^2 =  963591.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35790.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35790.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999276.2 e-6; = (1/var)*||X-X_r||^2 =  999276.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963026.0 e-6; = (1/var)*||X-X_r||^2 =  963026.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36250.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36250.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999328.8 e-6; = (1/var)*||X-X_r||^2 =  999328.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963204.6 e-6; = (1/var)*||X-X_r||^2 =  963204.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36124.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36124.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999457.5 e-6; = (1/var)*||X-X_r||^2 =  999457.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962885.1 e-6; = (1/var)*||X-X_r||^2 =  962885.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36572.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36572.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999407.0 e-6; = (1/var)*||X-X_r||^2 =  999406.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964145.1 e-6; = (1/var)*||X-X_r||^2 =  964144.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  943158.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  896236.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35261.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35262.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

Finished [04:29:25 02.01.2023] 214) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 11) change_channel_size_across_layers = True:
Total training time is = 0:0:16 h/m/s. 

--------------------------------------------------- 

Started [04:29:25 02.01.2023] 215) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 11) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 3151 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.19
1                           encoder.sequential_convs.conv2d_2.weight                       262             8.31
2                           encoder.sequential_convs.conv2d_3.weight                       262             8.31
3                           encoder.sequential_convs.conv2d_4.weight                       262             8.31
4                           encoder.sequential_convs.conv2d_5.weight                       262             8.31
5                           encoder.sequential_convs.conv2d_6.weight                       262             8.31
6                                  encoder.pre_residual_stack.weight                       147             4.67
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.14
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.14
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
11                             encoder.channel_adjusting_conv.weight                         8             0.25
12                                                       VQ.E.weight                       131             4.16
13                             decoder.channel_adjusting_conv.weight                        73             2.32
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.14
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.14
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
18                    decoder.sequential_trans_convs.conv2d_1.weight                       262             8.31
19                    decoder.sequential_trans_convs.conv2d_2.weight                       262             8.31
20                    decoder.sequential_trans_convs.conv2d_3.weight                       262             8.31
21                    decoder.sequential_trans_convs.conv2d_4.weight                       262             8.31
22                    decoder.sequential_trans_convs.conv2d_5.weight                       262             8.31
23                    decoder.sequential_trans_convs.conv2d_6.weight                         6             0.19

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.31; perplexity/K = 0.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.57; perplexity/K = 0.42%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1868813.4 e-6; = (1/var)*||X-X_r||^2 =  818789.2 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  1050024.2 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1616074.4 e-6; = (1/var)*||X-X_r||^2 =  792157.5 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  823916.9 e-6 = 51.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1030112.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  960547.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -252739.1 e-6; = (1/var)*||X-X_r||^2 val-train = -26631.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -226107.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 0.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 0.06%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1008654.3 e-6; = (1/var)*||X-X_r||^2 =  870457.4 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  138196.8 e-6 = 13.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  940837.4 e-6; = (1/var)*||X-X_r||^2 =  867591.7 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  73245.7 e-6 = 7.8 %)
Min.  Avg. Train Loss across Mini-Batch =  976725.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  922769.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -67816.9 e-6; = (1/var)*||X-X_r||^2 val-train = -2865.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -64951.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 0.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.31; perplexity/K = 0.06%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  934401.4 e-6; = (1/var)*||X-X_r||^2 =  910573.3 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  23828.1 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  903813.8 e-6; = (1/var)*||X-X_r||^2 =  889497.6 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  14316.3 e-6 = 1.6 %)
Min.  Avg. Train Loss across Mini-Batch =  933491.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30587.6 e-6; = (1/var)*||X-X_r||^2 val-train = -21075.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9511.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  971564.1 e-6; = (1/var)*||X-X_r||^2 =  970635.2 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  928.9 e-6 = 0.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  956186.2 e-6; = (1/var)*||X-X_r||^2 =  955468.4 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  717.8 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15377.8 e-6; = (1/var)*||X-X_r||^2 val-train = -15166.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -211.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999561.0 e-6; = (1/var)*||X-X_r||^2 =  999551.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963967.9 e-6; = (1/var)*||X-X_r||^2 =  963957.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  10.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35593.1 e-6; = (1/var)*||X-X_r||^2 val-train = -35593.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999995.5 e-6; = (1/var)*||X-X_r||^2 =  999991.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963963.9 e-6; = (1/var)*||X-X_r||^2 =  963962.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36031.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36028.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999601.0 e-6; = (1/var)*||X-X_r||^2 =  999600.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963331.2 e-6; = (1/var)*||X-X_r||^2 =  963330.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36269.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36269.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999666.2 e-6; = (1/var)*||X-X_r||^2 =  999665.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963020.2 e-6; = (1/var)*||X-X_r||^2 =  963020.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36646.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36645.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999532.9 e-6; = (1/var)*||X-X_r||^2 =  999532.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964463.7 e-6; = (1/var)*||X-X_r||^2 =  964463.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35069.2 e-6; = (1/var)*||X-X_r||^2 val-train = -35069.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999278.9 e-6; = (1/var)*||X-X_r||^2 =  999278.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963525.9 e-6; = (1/var)*||X-X_r||^2 =  963525.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35753.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35753.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999534.5 e-6; = (1/var)*||X-X_r||^2 =  999534.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962973.9 e-6; = (1/var)*||X-X_r||^2 =  962973.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36560.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36560.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999373.1 e-6; = (1/var)*||X-X_r||^2 =  999373.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963240.9 e-6; = (1/var)*||X-X_r||^2 =  963240.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36132.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36132.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999430.4 e-6; = (1/var)*||X-X_r||^2 =  999430.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962917.8 e-6; = (1/var)*||X-X_r||^2 =  962917.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36512.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36512.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999328.2 e-6; = (1/var)*||X-X_r||^2 =  999328.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963114.8 e-6; = (1/var)*||X-X_r||^2 =  963114.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36213.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36213.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999290.4 e-6; = (1/var)*||X-X_r||^2 =  999290.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962964.2 e-6; = (1/var)*||X-X_r||^2 =  962964.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36326.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36326.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999243.6 e-6; = (1/var)*||X-X_r||^2 =  999243.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962940.2 e-6; = (1/var)*||X-X_r||^2 =  962940.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36303.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36303.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999328.3 e-6; = (1/var)*||X-X_r||^2 =  999328.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962888.6 e-6; = (1/var)*||X-X_r||^2 =  962888.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36439.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36439.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999284.8 e-6; = (1/var)*||X-X_r||^2 =  999284.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963004.0 e-6; = (1/var)*||X-X_r||^2 =  963004.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36280.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36280.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999352.8 e-6; = (1/var)*||X-X_r||^2 =  999352.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962931.7 e-6; = (1/var)*||X-X_r||^2 =  962931.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36421.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36421.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999376.0 e-6; = (1/var)*||X-X_r||^2 =  999376.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962918.6 e-6; = (1/var)*||X-X_r||^2 =  962918.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933043.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  887719.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36457.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36457.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [05:36:02 02.01.2023] 215) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 11) change_channel_size_across_layers = False:
Total training time is = 0:0:37 h/m/s. 

--------------------------------------------------- 

Started [05:36:02 02.01.2023] 216) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 11) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 11711 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.10
1                           encoder.sequential_convs.conv2d_2.weight                      1048             8.95
2                           encoder.sequential_convs.conv2d_3.weight                      1048             8.95
3                           encoder.sequential_convs.conv2d_4.weight                      1048             8.95
4                           encoder.sequential_convs.conv2d_5.weight                      1048             8.95
5                           encoder.sequential_convs.conv2d_6.weight                      1048             8.95
6                                  encoder.pre_residual_stack.weight                       589             5.03
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.62
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.62
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
11                             encoder.channel_adjusting_conv.weight                        16             0.14
12                                                       VQ.E.weight                       131             1.12
13                             decoder.channel_adjusting_conv.weight                       147             1.26
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.62
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.62
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
18                    decoder.sequential_trans_convs.conv2d_1.weight                      1048             8.95
19                    decoder.sequential_trans_convs.conv2d_2.weight                      1048             8.95
20                    decoder.sequential_trans_convs.conv2d_3.weight                      1048             8.95
21                    decoder.sequential_trans_convs.conv2d_4.weight                      1048             8.95
22                    decoder.sequential_trans_convs.conv2d_5.weight                      1048             8.95
23                    decoder.sequential_trans_convs.conv2d_6.weight                        12             0.10

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  990835.3 e-6; = (1/var)*||X-X_r||^2 =  989998.1 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  837.1 e-6 = 0.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  963419.7 e-6; = (1/var)*||X-X_r||^2 =  962902.3 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  517.4 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  983692.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  944822.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27415.6 e-6; = (1/var)*||X-X_r||^2 val-train = -27095.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -319.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.85; perplexity/K = 0.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.71; perplexity/K = 0.08%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1102593.7 e-6; = (1/var)*||X-X_r||^2 =  844045.9 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  258547.9 e-6 = 23.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  948812.3 e-6; = (1/var)*||X-X_r||^2 =  838139.0 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  110673.3 e-6 = 11.7 %)
Min.  Avg. Train Loss across Mini-Batch =  983692.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904443.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -153781.4 e-6; = (1/var)*||X-X_r||^2 val-train = -5906.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -147874.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  962591.1 e-6; = (1/var)*||X-X_r||^2 =  961438.7 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  1152.4 e-6 = 0.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  934007.2 e-6; = (1/var)*||X-X_r||^2 =  933044.7 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  962.4 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  926756.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  878278.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -28583.9 e-6; = (1/var)*||X-X_r||^2 val-train = -28393.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -190.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.95; perplexity/K = 0.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.08; perplexity/K = 0.25%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  940575.8 e-6; = (1/var)*||X-X_r||^2 =  803759.0 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  136816.8 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  899300.8 e-6; = (1/var)*||X-X_r||^2 =  785251.4 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  114049.4 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  926756.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  878278.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -41275.1 e-6; = (1/var)*||X-X_r||^2 val-train = -18507.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22767.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.75; perplexity/K = 0.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.00; perplexity/K = 0.34%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  850748.8 e-6; = (1/var)*||X-X_r||^2 =  745635.1 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  105113.6 e-6 = 12.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  838608.5 e-6; = (1/var)*||X-X_r||^2 =  723784.0 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  114824.5 e-6 = 13.7 %)
Min.  Avg. Train Loss across Mini-Batch =  835509.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  798519.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12140.3 e-6; = (1/var)*||X-X_r||^2 val-train = -21851.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9710.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.96; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.28; perplexity/K = 0.11%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  836414.8 e-6; = (1/var)*||X-X_r||^2 =  762887.3 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  73527.6 e-6 = 8.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  793270.0 e-6; = (1/var)*||X-X_r||^2 =  728854.7 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  64415.2 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  809249.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  770524.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -43144.9 e-6; = (1/var)*||X-X_r||^2 val-train = -34032.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9112.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.43; perplexity/K = 0.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.02; perplexity/K = 0.10%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  824031.9 e-6; = (1/var)*||X-X_r||^2 =  745874.5 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  78157.3 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  789582.0 e-6; = (1/var)*||X-X_r||^2 =  733538.5 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  56043.4 e-6 = 7.1 %)
Min.  Avg. Train Loss across Mini-Batch =  784847.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  759800.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34449.9 e-6; = (1/var)*||X-X_r||^2 val-train = -12336.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22113.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.60; perplexity/K = 0.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.87; perplexity/K = 0.14%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  811972.0 e-6; = (1/var)*||X-X_r||^2 =  764323.7 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  47648.3 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  794557.7 e-6; = (1/var)*||X-X_r||^2 =  749846.9 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  44710.8 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  770685.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  752895.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17414.2 e-6; = (1/var)*||X-X_r||^2 val-train = -14476.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2937.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.90; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.79; perplexity/K = 0.28%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  789148.8 e-6; = (1/var)*||X-X_r||^2 =  718073.0 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  71075.8 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  762814.0 e-6; = (1/var)*||X-X_r||^2 =  709002.2 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  53811.8 e-6 = 7.1 %)
Min.  Avg. Train Loss across Mini-Batch =  770685.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  739125.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -26334.8 e-6; = (1/var)*||X-X_r||^2 val-train = -9070.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17264.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.61; perplexity/K = 0.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.68; perplexity/K = 0.52%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  785350.5 e-6; = (1/var)*||X-X_r||^2 =  664328.4 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  121022.0 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  779788.7 e-6; = (1/var)*||X-X_r||^2 =  671992.1 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  107796.5 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  753621.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  723113.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5561.8 e-6; = (1/var)*||X-X_r||^2 val-train = 7663.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13225.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.31; perplexity/K = 0.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.85; perplexity/K = 0.29%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  738795.4 e-6; = (1/var)*||X-X_r||^2 =  657887.7 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  80907.7 e-6 = 11.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  704810.0 e-6; = (1/var)*||X-X_r||^2 =  638011.4 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  66798.6 e-6 = 9.5 %)
Min.  Avg. Train Loss across Mini-Batch =  715827.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  685647.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -33985.4 e-6; = (1/var)*||X-X_r||^2 val-train = -19876.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14109.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.77; perplexity/K = 0.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.85; perplexity/K = 0.43%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  763381.4 e-6; = (1/var)*||X-X_r||^2 =  641805.2 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  121576.2 e-6 = 15.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  735841.1 e-6; = (1/var)*||X-X_r||^2 =  625587.7 e-6 = 85.0 %; (1+beta)*||Z_e-Z_q||^2 =  110253.4 e-6 = 15.0 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  680312.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27540.3 e-6; = (1/var)*||X-X_r||^2 val-train = -16217.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11322.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.76; perplexity/K = 0.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.42; perplexity/K = 0.51%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  782573.0 e-6; = (1/var)*||X-X_r||^2 =  648582.1 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  133990.9 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  735587.8 e-6; = (1/var)*||X-X_r||^2 =  636874.7 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  98713.1 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -46985.2 e-6; = (1/var)*||X-X_r||^2 val-train = -11707.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -35277.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.57; perplexity/K = 0.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.18; perplexity/K = 0.35%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:36:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  754284.1 e-6; = (1/var)*||X-X_r||^2 =  663894.6 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  90389.4 e-6 = 12.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  762617.4 e-6; = (1/var)*||X-X_r||^2 =  675006.8 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  87610.6 e-6 = 11.5 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8333.3 e-6; = (1/var)*||X-X_r||^2 val-train = 11112.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2778.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.23; perplexity/K = 0.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.14; perplexity/K = 0.15%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:43:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  819331.1 e-6; = (1/var)*||X-X_r||^2 =  760931.0 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  58400.1 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  819684.8 e-6; = (1/var)*||X-X_r||^2 =  759605.1 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  60079.7 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   353.7 e-6; = (1/var)*||X-X_r||^2 val-train = -1325.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1679.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.05%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:50:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  905298.4 e-6; = (1/var)*||X-X_r||^2 =  898919.4 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6379.0 e-6 = 0.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  897514.1 e-6; = (1/var)*||X-X_r||^2 =  890875.1 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6639.0 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7784.3 e-6; = (1/var)*||X-X_r||^2 val-train = -8044.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 260.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.05%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  946696.1 e-6; = (1/var)*||X-X_r||^2 =  944180.5 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2515.6 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  922462.8 e-6; = (1/var)*||X-X_r||^2 =  919706.5 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2756.3 e-6 = 0.3 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24233.3 e-6; = (1/var)*||X-X_r||^2 val-train = -24474.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 240.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.05%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  966045.6 e-6; = (1/var)*||X-X_r||^2 =  965743.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  302.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  945311.0 e-6; = (1/var)*||X-X_r||^2 =  944919.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  391.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20734.6 e-6; = (1/var)*||X-X_r||^2 val-train = -20823.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 89.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999288.6 e-6; = (1/var)*||X-X_r||^2 =  999274.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  14.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962920.9 e-6; = (1/var)*||X-X_r||^2 =  962909.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  11.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36367.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36365.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.05%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:17:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999242.2 e-6; = (1/var)*||X-X_r||^2 =  999242.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962948.2 e-6; = (1/var)*||X-X_r||^2 =  962948.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  695496.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  669226.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36293.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36293.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [07:55:00 02.01.2023] 216) Finished running for K = 2048 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 11) change_channel_size_across_layers = False:
Total training time is = 0:0:57 h/m/s. 

--------------------------------------------------- 

Started [07:55:00 02.01.2023] 217) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 640) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 781 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.13
1                           encoder.sequential_convs.conv2d_2.weight                        32             4.10
2                           encoder.sequential_convs.conv2d_3.weight                       131            16.77
3                                  encoder.pre_residual_stack.weight                       147            18.82
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.61
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.51
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.61
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.51
8                              encoder.channel_adjusting_conv.weight                         8             1.02
9                                                        VQ.E.weight                        65             8.32
10                             decoder.channel_adjusting_conv.weight                        73             9.35
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.61
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.51
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.61
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.51
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            16.77
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.10
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.45; perplexity/K = 2.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.15; perplexity/K = 2.36%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  352956.8 e-6; = (1/var)*||X-X_r||^2 =  228592.2 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  124364.5 e-6 = 35.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  343035.8 e-6; = (1/var)*||X-X_r||^2 =  229674.8 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  113361.0 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  352956.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  343035.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9921.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1082.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11003.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.46; perplexity/K = 3.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.31; perplexity/K = 3.06%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  130199.2 e-6; = (1/var)*||X-X_r||^2 =  103204.1 e-6 = 79.3 %; (1+beta)*||Z_e-Z_q||^2 =  26995.1 e-6 = 20.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  132126.5 e-6; = (1/var)*||X-X_r||^2 =  105581.6 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  26544.9 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  130199.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  132126.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1927.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2377.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -450.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.99; perplexity/K = 2.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.62; perplexity/K = 2.50%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  76371.1 e-6; = (1/var)*||X-X_r||^2 =  60858.8 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  15512.3 e-6 = 20.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  79897.5 e-6; = (1/var)*||X-X_r||^2 =  63956.0 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  15941.5 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  76371.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  79897.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3526.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3097.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 429.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.11; perplexity/K = 1.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.62; perplexity/K = 1.72%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47294.9 e-6; = (1/var)*||X-X_r||^2 =  36752.7 e-6 = 77.7 %; (1+beta)*||Z_e-Z_q||^2 =  10542.2 e-6 = 22.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  51748.2 e-6; = (1/var)*||X-X_r||^2 =  40460.9 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  11287.3 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  47231.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50478.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4453.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3708.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 745.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.67; perplexity/K = 1.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.12; perplexity/K = 1.67%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37966.6 e-6; = (1/var)*||X-X_r||^2 =  27349.1 e-6 = 72.0 %; (1+beta)*||Z_e-Z_q||^2 =  10617.5 e-6 = 28.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  42029.2 e-6; = (1/var)*||X-X_r||^2 =  30289.0 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  11740.2 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  37966.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41369.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4062.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2939.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1122.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.48; perplexity/K = 1.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.57; perplexity/K = 1.33%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23065.5 e-6; = (1/var)*||X-X_r||^2 =  14481.0 e-6 = 62.8 %; (1+beta)*||Z_e-Z_q||^2 =  8584.5 e-6 = 37.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  25719.2 e-6; = (1/var)*||X-X_r||^2 =  16760.6 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  8958.6 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  23065.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25620.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2653.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2279.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 374.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.60; perplexity/K = 1.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.99; perplexity/K = 1.27%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13292.1 e-6; = (1/var)*||X-X_r||^2 =  7040.2 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  6251.9 e-6 = 47.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  15528.7 e-6; = (1/var)*||X-X_r||^2 =  9113.5 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  6415.2 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  13292.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15360.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2236.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2073.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 163.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.04; perplexity/K = 1.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.27; perplexity/K = 1.49%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9668.3 e-6; = (1/var)*||X-X_r||^2 =  5076.9 e-6 = 52.5 %; (1+beta)*||Z_e-Z_q||^2 =  4591.4 e-6 = 47.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  11063.9 e-6; = (1/var)*||X-X_r||^2 =  6499.2 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  4564.8 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  9641.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11063.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1395.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1422.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -26.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.17; perplexity/K = 1.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.34; perplexity/K = 1.30%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7604.2 e-6; = (1/var)*||X-X_r||^2 =  3795.0 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  3809.2 e-6 = 50.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8807.4 e-6; = (1/var)*||X-X_r||^2 =  4986.8 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  3820.7 e-6 = 43.4 %)
Min.  Avg. Train Loss across Mini-Batch =  7336.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8712.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1203.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1191.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.80; perplexity/K = 1.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.71; perplexity/K = 1.14%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6279.6 e-6; = (1/var)*||X-X_r||^2 =  3051.1 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  3228.5 e-6 = 51.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  7425.5 e-6; = (1/var)*||X-X_r||^2 =  4136.2 e-6 = 55.7 %; (1+beta)*||Z_e-Z_q||^2 =  3289.2 e-6 = 44.3 %)
Min.  Avg. Train Loss across Mini-Batch =  6279.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7425.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1145.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1085.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 60.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 1.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.14; perplexity/K = 1.28%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4664.7 e-6; = (1/var)*||X-X_r||^2 =  2260.2 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  2404.4 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5428.5 e-6; = (1/var)*||X-X_r||^2 =  3147.3 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  2281.2 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4344.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5093.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   763.8 e-6; = (1/var)*||X-X_r||^2 val-train = 887.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -123.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.82; perplexity/K = 0.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.45; perplexity/K = 0.92%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4311.2 e-6; = (1/var)*||X-X_r||^2 =  2074.1 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  2237.1 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4844.5 e-6; = (1/var)*||X-X_r||^2 =  2679.0 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  2165.6 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3495.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4203.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   533.3 e-6; = (1/var)*||X-X_r||^2 val-train = 604.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -71.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.03; perplexity/K = 1.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.07; perplexity/K = 1.18%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3038.2 e-6; = (1/var)*||X-X_r||^2 =  1390.4 e-6 = 45.8 %; (1+beta)*||Z_e-Z_q||^2 =  1647.8 e-6 = 54.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3596.5 e-6; = (1/var)*||X-X_r||^2 =  1938.1 e-6 = 53.9 %; (1+beta)*||Z_e-Z_q||^2 =  1658.4 e-6 = 46.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2983.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3596.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   558.3 e-6; = (1/var)*||X-X_r||^2 val-train = 547.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.04; perplexity/K = 1.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.46; perplexity/K = 1.51%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2559.7 e-6; = (1/var)*||X-X_r||^2 =  1155.9 e-6 = 45.2 %; (1+beta)*||Z_e-Z_q||^2 =  1403.7 e-6 = 54.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3208.1 e-6; = (1/var)*||X-X_r||^2 =  1701.5 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  1506.7 e-6 = 47.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2447.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3012.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   648.4 e-6; = (1/var)*||X-X_r||^2 val-train = 545.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.33; perplexity/K = 1.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.50; perplexity/K = 1.32%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2379.4 e-6; = (1/var)*||X-X_r||^2 =  922.7 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  1456.6 e-6 = 61.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2942.3 e-6; = (1/var)*||X-X_r||^2 =  1469.3 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  1473.1 e-6 = 50.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2299.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2797.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   563.0 e-6; = (1/var)*||X-X_r||^2 val-train = 546.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.34; perplexity/K = 1.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.10; perplexity/K = 0.99%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2220.7 e-6; = (1/var)*||X-X_r||^2 =  899.7 e-6 = 40.5 %; (1+beta)*||Z_e-Z_q||^2 =  1321.0 e-6 = 59.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2666.1 e-6; = (1/var)*||X-X_r||^2 =  1325.4 e-6 = 49.7 %; (1+beta)*||Z_e-Z_q||^2 =  1340.7 e-6 = 50.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2166.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2641.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   445.4 e-6; = (1/var)*||X-X_r||^2 val-train = 425.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.63; perplexity/K = 1.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.62; perplexity/K = 1.23%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6198.5 e-6; = (1/var)*||X-X_r||^2 =  3197.8 e-6 = 51.6 %; (1+beta)*||Z_e-Z_q||^2 =  3000.7 e-6 = 48.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  5069.8 e-6; = (1/var)*||X-X_r||^2 =  2903.0 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  2166.8 e-6 = 42.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1985.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2474.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1128.7 e-6; = (1/var)*||X-X_r||^2 val-train = -294.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -833.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.59; perplexity/K = 1.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.32; perplexity/K = 1.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1923.3 e-6; = (1/var)*||X-X_r||^2 =  724.7 e-6 = 37.7 %; (1+beta)*||Z_e-Z_q||^2 =  1198.6 e-6 = 62.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2308.9 e-6; = (1/var)*||X-X_r||^2 =  1118.5 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  1190.4 e-6 = 51.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1871.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2302.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   385.6 e-6; = (1/var)*||X-X_r||^2 val-train = 393.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.44; perplexity/K = 1.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 1.26%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7431.5 e-6; = (1/var)*||X-X_r||^2 =  3843.5 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  3588.0 e-6 = 48.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  7707.1 e-6; = (1/var)*||X-X_r||^2 =  4373.2 e-6 = 56.7 %; (1+beta)*||Z_e-Z_q||^2 =  3333.9 e-6 = 43.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1749.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2208.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   275.6 e-6; = (1/var)*||X-X_r||^2 val-train = 529.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -254.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.70; perplexity/K = 1.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.89; perplexity/K = 1.26%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1918.1 e-6; = (1/var)*||X-X_r||^2 =  728.9 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  1189.2 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2365.3 e-6; = (1/var)*||X-X_r||^2 =  1113.0 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  1252.4 e-6 = 52.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1749.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2208.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   447.2 e-6; = (1/var)*||X-X_r||^2 val-train = 384.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.2 e-6 

----------------------------------------------------------------------------------

Finished [08:43:43 02.01.2023] 217) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 640) change_channel_size_across_layers = True:
Total training time is = 0:7:43 h/m/s. 

--------------------------------------------------- 

Started [08:43:43 02.01.2023] 218) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 640) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2457 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.33
2                           encoder.sequential_convs.conv2d_3.weight                       524            21.33
3                                  encoder.pre_residual_stack.weight                       589            23.97
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
8                              encoder.channel_adjusting_conv.weight                        16             0.65
9                                                        VQ.E.weight                        65             2.65
10                             decoder.channel_adjusting_conv.weight                       147             5.98
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.33
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.33
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.47; perplexity/K = 2.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.95; perplexity/K = 2.34%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  204616.5 e-6; = (1/var)*||X-X_r||^2 =  139311.5 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  65305.0 e-6 = 31.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  201927.7 e-6; = (1/var)*||X-X_r||^2 =  141760.4 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  60167.3 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  204616.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  201927.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2688.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2448.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5137.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 1.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.75; perplexity/K = 1.15%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  215995.7 e-6; = (1/var)*||X-X_r||^2 =  179775.3 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  36220.3 e-6 = 16.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  210402.4 e-6; = (1/var)*||X-X_r||^2 =  175367.1 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  35035.3 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  66236.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  70868.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5593.2 e-6; = (1/var)*||X-X_r||^2 val-train = -4408.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1185.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.93; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.36; perplexity/K = 1.40%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30124.2 e-6; = (1/var)*||X-X_r||^2 =  19075.6 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  11048.6 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  32829.6 e-6; = (1/var)*||X-X_r||^2 =  22000.0 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  10829.6 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  30124.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32829.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2705.4 e-6; = (1/var)*||X-X_r||^2 val-train = 2924.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -219.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.15; perplexity/K = 1.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.34; perplexity/K = 1.40%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14919.2 e-6; = (1/var)*||X-X_r||^2 =  7686.6 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  7232.6 e-6 = 48.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  17488.5 e-6; = (1/var)*||X-X_r||^2 =  9855.2 e-6 = 56.4 %; (1+beta)*||Z_e-Z_q||^2 =  7633.3 e-6 = 43.6 %)
Min.  Avg. Train Loss across Mini-Batch =  14261.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16190.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2569.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2168.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 400.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.35; perplexity/K = 1.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.70; perplexity/K = 1.24%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18663.6 e-6; = (1/var)*||X-X_r||^2 =  10216.9 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  8446.8 e-6 = 45.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  13278.9 e-6; = (1/var)*||X-X_r||^2 =  7887.5 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  5391.4 e-6 = 40.6 %)
Min.  Avg. Train Loss across Mini-Batch =  7332.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8874.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5384.7 e-6; = (1/var)*||X-X_r||^2 val-train = -2329.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3055.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.41; perplexity/K = 1.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.58; perplexity/K = 1.23%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4393.9 e-6; = (1/var)*||X-X_r||^2 =  1923.2 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  2470.7 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  5556.9 e-6; = (1/var)*||X-X_r||^2 =  3074.6 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  2482.3 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4393.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5556.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1162.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1151.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.84; perplexity/K = 0.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.56; perplexity/K = 0.93%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2917.8 e-6; = (1/var)*||X-X_r||^2 =  1180.6 e-6 = 40.5 %; (1+beta)*||Z_e-Z_q||^2 =  1737.2 e-6 = 59.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3866.8 e-6; = (1/var)*||X-X_r||^2 =  2108.3 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  1758.5 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2916.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3852.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   949.0 e-6; = (1/var)*||X-X_r||^2 val-train = 927.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.46; perplexity/K = 1.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.48; perplexity/K = 1.12%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2592.8 e-6; = (1/var)*||X-X_r||^2 =  1001.1 e-6 = 38.6 %; (1+beta)*||Z_e-Z_q||^2 =  1591.7 e-6 = 61.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3395.8 e-6; = (1/var)*||X-X_r||^2 =  1809.9 e-6 = 53.3 %; (1+beta)*||Z_e-Z_q||^2 =  1585.9 e-6 = 46.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2247.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3110.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   803.0 e-6; = (1/var)*||X-X_r||^2 val-train = 808.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.43; perplexity/K = 1.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.41; perplexity/K = 1.11%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2038.3 e-6; = (1/var)*||X-X_r||^2 =  783.3 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  1255.1 e-6 = 61.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2934.1 e-6; = (1/var)*||X-X_r||^2 =  1622.9 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  1311.2 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1744.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2518.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   895.8 e-6; = (1/var)*||X-X_r||^2 val-train = 839.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 56.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.63; perplexity/K = 1.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.69; perplexity/K = 1.14%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1602.0 e-6; = (1/var)*||X-X_r||^2 =  653.8 e-6 = 40.8 %; (1+beta)*||Z_e-Z_q||^2 =  948.2 e-6 = 59.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2338.4 e-6; = (1/var)*||X-X_r||^2 =  1294.7 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  1043.7 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1596.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2289.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   736.4 e-6; = (1/var)*||X-X_r||^2 val-train = 640.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 95.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.88; perplexity/K = 1.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 1.28%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1346.4 e-6; = (1/var)*||X-X_r||^2 =  547.5 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  798.9 e-6 = 59.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1892.7 e-6; = (1/var)*||X-X_r||^2 =  1078.3 e-6 = 57.0 %; (1+beta)*||Z_e-Z_q||^2 =  814.4 e-6 = 43.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1346.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1892.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   546.3 e-6; = (1/var)*||X-X_r||^2 val-train = 530.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.93; perplexity/K = 1.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.36; perplexity/K = 1.01%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3584.7 e-6; = (1/var)*||X-X_r||^2 =  1379.3 e-6 = 38.5 %; (1+beta)*||Z_e-Z_q||^2 =  2205.5 e-6 = 61.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3590.2 e-6; = (1/var)*||X-X_r||^2 =  1697.1 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  1893.2 e-6 = 52.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1017.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1486.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5.5 e-6; = (1/var)*||X-X_r||^2 val-train = 317.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -312.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.23; perplexity/K = 1.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.36; perplexity/K = 1.21%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3556.2 e-6; = (1/var)*||X-X_r||^2 =  1142.4 e-6 = 32.1 %; (1+beta)*||Z_e-Z_q||^2 =  2413.8 e-6 = 67.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4031.5 e-6; = (1/var)*||X-X_r||^2 =  1833.0 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  2198.5 e-6 = 54.5 %)
Min.  Avg. Train Loss across Mini-Batch =  887.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1386.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   475.3 e-6; = (1/var)*||X-X_r||^2 val-train = 690.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -215.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.30; perplexity/K = 1.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.37; perplexity/K = 1.60%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2748.5 e-6; = (1/var)*||X-X_r||^2 =  912.7 e-6 = 33.2 %; (1+beta)*||Z_e-Z_q||^2 =  1835.8 e-6 = 66.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3113.4 e-6; = (1/var)*||X-X_r||^2 =  1376.3 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  1737.2 e-6 = 55.8 %)
Min.  Avg. Train Loss across Mini-Batch =  887.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1386.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   364.9 e-6; = (1/var)*||X-X_r||^2 val-train = 463.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -98.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.77; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.47; perplexity/K = 1.22%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1146.7 e-6; = (1/var)*||X-X_r||^2 =  383.7 e-6 = 33.5 %; (1+beta)*||Z_e-Z_q||^2 =  762.9 e-6 = 66.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1532.9 e-6; = (1/var)*||X-X_r||^2 =  749.5 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  783.3 e-6 = 51.1 %)
Min.  Avg. Train Loss across Mini-Batch =  887.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1386.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   386.2 e-6; = (1/var)*||X-X_r||^2 val-train = 365.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.48; perplexity/K = 2.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.01; perplexity/K = 1.86%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1680.9 e-6; = (1/var)*||X-X_r||^2 =  478.4 e-6 = 28.5 %; (1+beta)*||Z_e-Z_q||^2 =  1202.5 e-6 = 71.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2103.4 e-6; = (1/var)*||X-X_r||^2 =  907.9 e-6 = 43.2 %; (1+beta)*||Z_e-Z_q||^2 =  1195.5 e-6 = 56.8 %)
Min.  Avg. Train Loss across Mini-Batch =  631.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  909.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   422.5 e-6; = (1/var)*||X-X_r||^2 val-train = 429.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.76; perplexity/K = 1.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.03; perplexity/K = 1.47%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  777.4 e-6; = (1/var)*||X-X_r||^2 =  269.7 e-6 = 34.7 %; (1+beta)*||Z_e-Z_q||^2 =  507.7 e-6 = 65.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  996.2 e-6; = (1/var)*||X-X_r||^2 =  501.5 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  494.8 e-6 = 49.7 %)
Min.  Avg. Train Loss across Mini-Batch =  553.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  760.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   218.8 e-6; = (1/var)*||X-X_r||^2 val-train = 231.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.35; perplexity/K = 1.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.95; perplexity/K = 1.07%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  503.2 e-6; = (1/var)*||X-X_r||^2 =  188.9 e-6 = 37.5 %; (1+beta)*||Z_e-Z_q||^2 =  314.2 e-6 = 62.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  693.8 e-6; = (1/var)*||X-X_r||^2 =  383.3 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  310.4 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  377.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  562.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   190.6 e-6; = (1/var)*||X-X_r||^2 val-train = 194.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 1.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.06; perplexity/K = 1.18%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  319.6 e-6; = (1/var)*||X-X_r||^2 =  135.2 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  184.4 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  473.2 e-6; = (1/var)*||X-X_r||^2 =  282.1 e-6 = 59.6 %; (1+beta)*||Z_e-Z_q||^2 =  191.1 e-6 = 40.4 %)
Min.  Avg. Train Loss across Mini-Batch =  279.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  440.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   153.5 e-6; = (1/var)*||X-X_r||^2 val-train = 146.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.40; perplexity/K = 1.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.54; perplexity/K = 1.42%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  292.5 e-6; = (1/var)*||X-X_r||^2 =  117.0 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  175.5 e-6 = 60.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  442.5 e-6; = (1/var)*||X-X_r||^2 =  267.6 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  175.0 e-6 = 39.5 %)
Min.  Avg. Train Loss across Mini-Batch =  232.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  353.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   150.0 e-6; = (1/var)*||X-X_r||^2 val-train = 150.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

Finished [09:50:05 02.01.2023] 218) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 640) change_channel_size_across_layers = True:
Total training time is = 0:7:21 h/m/s. 

--------------------------------------------------- 

Started [09:50:05 02.01.2023] 219) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 640) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1513 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.40
1                           encoder.sequential_convs.conv2d_2.weight                       262            17.32
2                           encoder.sequential_convs.conv2d_3.weight                       262            17.32
3                                  encoder.pre_residual_stack.weight                       147             9.72
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.38
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.26
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.38
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.26
8                              encoder.channel_adjusting_conv.weight                         8             0.53
9                                                        VQ.E.weight                        65             4.30
10                             decoder.channel_adjusting_conv.weight                        73             4.82
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.38
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.26
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.38
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.26
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            17.32
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            17.32
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.40

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.22; perplexity/K = 2.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.76; perplexity/K = 2.32%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  214530.6 e-6; = (1/var)*||X-X_r||^2 =  118509.8 e-6 = 55.2 %; (1+beta)*||Z_e-Z_q||^2 =  96020.8 e-6 = 44.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  216970.8 e-6; = (1/var)*||X-X_r||^2 =  124256.1 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  92714.6 e-6 = 42.7 %)
Min.  Avg. Train Loss across Mini-Batch =  214530.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  215138.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2440.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5746.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3306.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.77; perplexity/K = 1.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 1.19%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  91023.5 e-6; = (1/var)*||X-X_r||^2 =  56057.7 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  34965.9 e-6 = 38.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  91461.3 e-6; = (1/var)*||X-X_r||^2 =  58481.9 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  32979.4 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  77996.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  81334.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   437.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2424.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1986.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.78; perplexity/K = 1.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.97; perplexity/K = 1.66%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21691.7 e-6; = (1/var)*||X-X_r||^2 =  13964.7 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  7727.1 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  24415.3 e-6; = (1/var)*||X-X_r||^2 =  16438.5 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  7976.8 e-6 = 32.7 %)
Min.  Avg. Train Loss across Mini-Batch =  21691.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24415.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2723.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2473.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 249.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.23; perplexity/K = 1.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.89; perplexity/K = 1.55%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11975.8 e-6; = (1/var)*||X-X_r||^2 =  7356.8 e-6 = 61.4 %; (1+beta)*||Z_e-Z_q||^2 =  4619.0 e-6 = 38.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  14662.7 e-6; = (1/var)*||X-X_r||^2 =  9758.4 e-6 = 66.6 %; (1+beta)*||Z_e-Z_q||^2 =  4904.3 e-6 = 33.4 %)
Min.  Avg. Train Loss across Mini-Batch =  11975.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14468.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2686.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2401.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 285.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.40; perplexity/K = 1.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.15; perplexity/K = 1.38%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8410.7 e-6; = (1/var)*||X-X_r||^2 =  5017.2 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  3393.5 e-6 = 40.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  10755.8 e-6; = (1/var)*||X-X_r||^2 =  7153.1 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  3602.8 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8405.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10511.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2345.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2135.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 209.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.14; perplexity/K = 1.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.51; perplexity/K = 1.42%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6979.9 e-6; = (1/var)*||X-X_r||^2 =  4116.6 e-6 = 59.0 %; (1+beta)*||Z_e-Z_q||^2 =  2863.3 e-6 = 41.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  8943.8 e-6; = (1/var)*||X-X_r||^2 =  5910.7 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  3033.1 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  6553.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8747.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1963.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1794.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 169.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.85; perplexity/K = 1.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.18; perplexity/K = 1.77%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6136.9 e-6; = (1/var)*||X-X_r||^2 =  3195.3 e-6 = 52.1 %; (1+beta)*||Z_e-Z_q||^2 =  2941.5 e-6 = 47.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  8216.4 e-6; = (1/var)*||X-X_r||^2 =  5004.5 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  3211.9 e-6 = 39.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5845.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8069.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2079.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1809.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 270.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.77; perplexity/K = 1.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.90; perplexity/K = 1.46%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47068.9 e-6; = (1/var)*||X-X_r||^2 =  31588.7 e-6 = 67.1 %; (1+beta)*||Z_e-Z_q||^2 =  15480.2 e-6 = 32.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  23227.3 e-6; = (1/var)*||X-X_r||^2 =  13953.6 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  9273.7 e-6 = 39.9 %)
Min.  Avg. Train Loss across Mini-Batch =  4962.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6876.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23841.6 e-6; = (1/var)*||X-X_r||^2 val-train = -17635.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6206.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.30; perplexity/K = 1.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.63; perplexity/K = 1.53%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5588.7 e-6; = (1/var)*||X-X_r||^2 =  3251.8 e-6 = 58.2 %; (1+beta)*||Z_e-Z_q||^2 =  2336.9 e-6 = 41.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  6972.3 e-6; = (1/var)*||X-X_r||^2 =  4507.4 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  2464.9 e-6 = 35.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4429.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6254.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1383.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1255.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 128.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.20; perplexity/K = 1.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.02; perplexity/K = 1.56%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4165.1 e-6; = (1/var)*||X-X_r||^2 =  2210.0 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  1955.1 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5882.5 e-6; = (1/var)*||X-X_r||^2 =  3813.1 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  2069.4 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3999.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5724.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1717.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1603.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 114.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.95; perplexity/K = 1.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.11; perplexity/K = 1.38%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3993.5 e-6; = (1/var)*||X-X_r||^2 =  2052.8 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  1940.7 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5566.0 e-6; = (1/var)*||X-X_r||^2 =  3518.0 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  2048.0 e-6 = 36.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3698.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5443.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1572.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1465.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 107.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.90; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.89; perplexity/K = 1.55%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3348.1 e-6; = (1/var)*||X-X_r||^2 =  1616.6 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  1731.5 e-6 = 51.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  5059.0 e-6; = (1/var)*||X-X_r||^2 =  3100.8 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  1958.1 e-6 = 38.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3348.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4946.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1710.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1484.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 226.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.16; perplexity/K = 1.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.85; perplexity/K = 1.35%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3344.3 e-6; = (1/var)*||X-X_r||^2 =  1652.9 e-6 = 49.4 %; (1+beta)*||Z_e-Z_q||^2 =  1691.4 e-6 = 50.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5013.6 e-6; = (1/var)*||X-X_r||^2 =  3170.2 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  1843.4 e-6 = 36.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3051.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4643.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1669.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1517.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 152.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.87; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.61; perplexity/K = 1.52%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3056.5 e-6; = (1/var)*||X-X_r||^2 =  1483.6 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  1572.8 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  4695.3 e-6; = (1/var)*||X-X_r||^2 =  2860.1 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  1835.2 e-6 = 39.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2972.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4456.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1638.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1376.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 262.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.93; perplexity/K = 1.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.64; perplexity/K = 1.43%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3047.5 e-6; = (1/var)*||X-X_r||^2 =  1398.3 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  1649.2 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  5606.5 e-6; = (1/var)*||X-X_r||^2 =  3528.4 e-6 = 62.9 %; (1+beta)*||Z_e-Z_q||^2 =  2078.0 e-6 = 37.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2809.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4252.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2559.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2130.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 428.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.02; perplexity/K = 1.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.71; perplexity/K = 1.63%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2683.9 e-6; = (1/var)*||X-X_r||^2 =  1367.7 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  1316.2 e-6 = 49.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4318.0 e-6; = (1/var)*||X-X_r||^2 =  2768.0 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  1550.0 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2425.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3950.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1634.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1400.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 233.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.39; perplexity/K = 1.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.71; perplexity/K = 1.63%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2436.0 e-6; = (1/var)*||X-X_r||^2 =  1162.0 e-6 = 47.7 %; (1+beta)*||Z_e-Z_q||^2 =  1274.0 e-6 = 52.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4003.8 e-6; = (1/var)*||X-X_r||^2 =  2562.9 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  1440.9 e-6 = 36.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2398.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3906.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1567.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1400.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 166.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.37; perplexity/K = 1.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.67; perplexity/K = 1.73%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2214.1 e-6; = (1/var)*||X-X_r||^2 =  1065.1 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  1148.9 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3652.7 e-6; = (1/var)*||X-X_r||^2 =  2400.9 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  1251.8 e-6 = 34.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2214.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3612.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1438.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1335.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.16; perplexity/K = 1.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.29; perplexity/K = 1.79%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6572.0 e-6; = (1/var)*||X-X_r||^2 =  4077.6 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  2494.4 e-6 = 38.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  6527.7 e-6; = (1/var)*||X-X_r||^2 =  4378.0 e-6 = 67.1 %; (1+beta)*||Z_e-Z_q||^2 =  2149.7 e-6 = 32.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2042.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3389.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -44.3 e-6; = (1/var)*||X-X_r||^2 val-train = 300.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -344.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.70; perplexity/K = 1.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.57; perplexity/K = 1.62%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1999.2 e-6; = (1/var)*||X-X_r||^2 =  907.3 e-6 = 45.4 %; (1+beta)*||Z_e-Z_q||^2 =  1091.9 e-6 = 54.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3502.5 e-6; = (1/var)*||X-X_r||^2 =  2255.2 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  1247.4 e-6 = 35.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1936.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3360.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1503.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1347.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 155.5 e-6 

----------------------------------------------------------------------------------

Finished [10:56:55 02.01.2023] 219) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 640) change_channel_size_across_layers = False:
Total training time is = 0:7:50 h/m/s. 

--------------------------------------------------- 

Started [10:56:55 02.01.2023] 220) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 640) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5357 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.22
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.56
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.56
3                                  encoder.pre_residual_stack.weight                       589            10.99
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.36
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.36
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                        65             1.21
10                             decoder.channel_adjusting_conv.weight                       147             2.74
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.36
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.36
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.56
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.56
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.22

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.53; perplexity/K = 0.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.07; perplexity/K = 0.79%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  168699.0 e-6; = (1/var)*||X-X_r||^2 =  66824.5 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  101874.5 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  171397.9 e-6; = (1/var)*||X-X_r||^2 =  73921.3 e-6 = 43.1 %; (1+beta)*||Z_e-Z_q||^2 =  97476.6 e-6 = 56.9 %)
Min.  Avg. Train Loss across Mini-Batch =  168699.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  171397.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2698.9 e-6; = (1/var)*||X-X_r||^2 val-train = 7096.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4398.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.32; perplexity/K = 1.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.07; perplexity/K = 0.98%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29014.6 e-6; = (1/var)*||X-X_r||^2 =  11504.1 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  17510.4 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  33646.1 e-6; = (1/var)*||X-X_r||^2 =  15390.2 e-6 = 45.7 %; (1+beta)*||Z_e-Z_q||^2 =  18255.9 e-6 = 54.3 %)
Min.  Avg. Train Loss across Mini-Batch =  29014.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33646.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4631.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3886.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 745.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.99; perplexity/K = 2.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.99; perplexity/K = 2.73%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  58392.7 e-6; = (1/var)*||X-X_r||^2 =  41924.9 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  16467.8 e-6 = 28.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  62996.5 e-6; = (1/var)*||X-X_r||^2 =  47262.4 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  15734.1 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  26691.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30440.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4603.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5337.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -733.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.72; perplexity/K = 2.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.49; perplexity/K = 2.10%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28743.8 e-6; = (1/var)*||X-X_r||^2 =  20570.3 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  8173.4 e-6 = 28.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  32531.5 e-6; = (1/var)*||X-X_r||^2 =  24566.7 e-6 = 75.5 %; (1+beta)*||Z_e-Z_q||^2 =  7964.8 e-6 = 24.5 %)
Min.  Avg. Train Loss across Mini-Batch =  26691.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30440.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3787.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3996.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -208.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.34; perplexity/K = 1.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.22; perplexity/K = 1.78%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24412.2 e-6; = (1/var)*||X-X_r||^2 =  16890.2 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  7522.0 e-6 = 30.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  27667.9 e-6; = (1/var)*||X-X_r||^2 =  20296.3 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  7371.6 e-6 = 26.6 %)
Min.  Avg. Train Loss across Mini-Batch =  22622.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26727.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3255.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3406.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -150.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.21; perplexity/K = 1.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.20; perplexity/K = 1.78%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17793.0 e-6; = (1/var)*||X-X_r||^2 =  12043.2 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  5749.8 e-6 = 32.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  21331.8 e-6; = (1/var)*||X-X_r||^2 =  15159.3 e-6 = 71.1 %; (1+beta)*||Z_e-Z_q||^2 =  6172.4 e-6 = 28.9 %)
Min.  Avg. Train Loss across Mini-Batch =  17405.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  21095.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3538.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3116.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 422.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.83; perplexity/K = 1.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.18; perplexity/K = 1.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15027.7 e-6; = (1/var)*||X-X_r||^2 =  9617.0 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  5410.7 e-6 = 36.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  18575.1 e-6; = (1/var)*||X-X_r||^2 =  12779.5 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  5795.6 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  15027.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  18575.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3547.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3162.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 385.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.97; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.61; perplexity/K = 1.72%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13935.9 e-6; = (1/var)*||X-X_r||^2 =  8685.7 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  5250.2 e-6 = 37.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  17718.7 e-6; = (1/var)*||X-X_r||^2 =  11848.4 e-6 = 66.9 %; (1+beta)*||Z_e-Z_q||^2 =  5870.4 e-6 = 33.1 %)
Min.  Avg. Train Loss across Mini-Batch =  13745.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17100.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3782.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3162.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 620.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.46; perplexity/K = 1.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.62; perplexity/K = 1.62%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13053.6 e-6; = (1/var)*||X-X_r||^2 =  7878.4 e-6 = 60.4 %; (1+beta)*||Z_e-Z_q||^2 =  5175.2 e-6 = 39.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  16865.5 e-6; = (1/var)*||X-X_r||^2 =  11329.3 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  5536.2 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  12799.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16248.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3811.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3450.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 361.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.46; perplexity/K = 1.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.22; perplexity/K = 1.78%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12265.9 e-6; = (1/var)*||X-X_r||^2 =  7140.7 e-6 = 58.2 %; (1+beta)*||Z_e-Z_q||^2 =  5125.2 e-6 = 41.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  15365.1 e-6; = (1/var)*||X-X_r||^2 =  10172.9 e-6 = 66.2 %; (1+beta)*||Z_e-Z_q||^2 =  5192.2 e-6 = 33.8 %)
Min.  Avg. Train Loss across Mini-Batch =  12017.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15239.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3099.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3032.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 67.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.90; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.05; perplexity/K = 1.66%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11483.9 e-6; = (1/var)*||X-X_r||^2 =  6787.0 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  4696.9 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  14785.5 e-6; = (1/var)*||X-X_r||^2 =  9773.8 e-6 = 66.1 %; (1+beta)*||Z_e-Z_q||^2 =  5011.7 e-6 = 33.9 %)
Min.  Avg. Train Loss across Mini-Batch =  11415.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14523.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3301.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2986.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 314.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.47; perplexity/K = 1.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.27; perplexity/K = 1.49%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10752.4 e-6; = (1/var)*||X-X_r||^2 =  6337.9 e-6 = 58.9 %; (1+beta)*||Z_e-Z_q||^2 =  4414.5 e-6 = 41.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  14357.7 e-6; = (1/var)*||X-X_r||^2 =  9376.1 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  4981.5 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  10604.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13941.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3605.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3038.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 567.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.37; perplexity/K = 1.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.36; perplexity/K = 1.50%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10282.2 e-6; = (1/var)*||X-X_r||^2 =  6046.1 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  4236.1 e-6 = 41.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  13932.8 e-6; = (1/var)*||X-X_r||^2 =  9069.3 e-6 = 65.1 %; (1+beta)*||Z_e-Z_q||^2 =  4863.5 e-6 = 34.9 %)
Min.  Avg. Train Loss across Mini-Batch =  10037.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13400.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3650.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3023.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 627.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.91; perplexity/K = 1.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.21; perplexity/K = 1.68%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10421.0 e-6; = (1/var)*||X-X_r||^2 =  6161.1 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  4259.9 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  13359.9 e-6; = (1/var)*||X-X_r||^2 =  8717.9 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  4642.0 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  9761.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12899.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2938.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2556.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 382.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.06; perplexity/K = 1.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.79; perplexity/K = 1.64%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9727.0 e-6; = (1/var)*||X-X_r||^2 =  5510.5 e-6 = 56.7 %; (1+beta)*||Z_e-Z_q||^2 =  4216.5 e-6 = 43.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  12982.0 e-6; = (1/var)*||X-X_r||^2 =  8371.4 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  4610.7 e-6 = 35.5 %)
Min.  Avg. Train Loss across Mini-Batch =  9420.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12519.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3255.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2860.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 394.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.09; perplexity/K = 2.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.98; perplexity/K = 2.05%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10137.9 e-6; = (1/var)*||X-X_r||^2 =  5992.5 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  4145.4 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  13389.8 e-6; = (1/var)*||X-X_r||^2 =  8629.7 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  4760.1 e-6 = 35.6 %)
Min.  Avg. Train Loss across Mini-Batch =  9309.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12519.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3252.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2637.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 614.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.27; perplexity/K = 2.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.35; perplexity/K = 2.18%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9024.5 e-6; = (1/var)*||X-X_r||^2 =  4996.5 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  4028.0 e-6 = 44.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  12378.0 e-6; = (1/var)*||X-X_r||^2 =  7813.6 e-6 = 63.1 %; (1+beta)*||Z_e-Z_q||^2 =  4564.4 e-6 = 36.9 %)
Min.  Avg. Train Loss across Mini-Batch =  9024.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12221.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3353.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2817.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 536.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.03; perplexity/K = 1.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.48; perplexity/K = 2.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9055.9 e-6; = (1/var)*||X-X_r||^2 =  5064.3 e-6 = 55.9 %; (1+beta)*||Z_e-Z_q||^2 =  3991.6 e-6 = 44.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  12529.0 e-6; = (1/var)*||X-X_r||^2 =  8030.5 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  4498.4 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8817.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12077.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3473.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2966.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 506.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.35; perplexity/K = 2.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.08; perplexity/K = 2.06%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8936.4 e-6; = (1/var)*||X-X_r||^2 =  5143.3 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  3793.0 e-6 = 42.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  14120.7 e-6; = (1/var)*||X-X_r||^2 =  9021.7 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  5099.1 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  8536.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11951.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5184.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3878.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1306.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.04; perplexity/K = 2.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.87; perplexity/K = 2.43%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:17:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8519.7 e-6; = (1/var)*||X-X_r||^2 =  4734.2 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  3785.5 e-6 = 44.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  12335.9 e-6; = (1/var)*||X-X_r||^2 =  7685.2 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  4650.7 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  8463.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11764.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3816.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2951.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 865.2 e-6 

----------------------------------------------------------------------------------

Finished [13:14:56 02.01.2023] 220) Finished running for K = 1024 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 640) change_channel_size_across_layers = False:
Total training time is = 0:7:0 h/m/s. 

--------------------------------------------------- 

Started [13:14:56 02.01.2023] 221) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 160) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 795 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.01
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.03
3                           encoder.sequential_convs.conv2d_4.weight                       131            16.48
4                                  encoder.pre_residual_stack.weight                       147            18.49
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.53
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.53
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
9                              encoder.channel_adjusting_conv.weight                         8             1.01
10                                                       VQ.E.weight                        65             8.18
11                             decoder.channel_adjusting_conv.weight                        73             9.18
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.53
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.53
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            16.48
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.03
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.01
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.02; perplexity/K = 3.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.93; perplexity/K = 3.02%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  459279.2 e-6; = (1/var)*||X-X_r||^2 =  313872.5 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  145406.7 e-6 = 31.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  447736.2 e-6; = (1/var)*||X-X_r||^2 =  317501.7 e-6 = 70.9 %; (1+beta)*||Z_e-Z_q||^2 =  130234.5 e-6 = 29.1 %)
Min.  Avg. Train Loss across Mini-Batch =  459279.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  444256.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11543.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3629.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15172.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.28; perplexity/K = 4.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.88; perplexity/K = 4.58%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  172597.8 e-6; = (1/var)*||X-X_r||^2 =  155037.1 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  17560.7 e-6 = 10.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  177085.4 e-6; = (1/var)*||X-X_r||^2 =  160272.1 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  16813.4 e-6 = 9.5 %)
Min.  Avg. Train Loss across Mini-Batch =  172597.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  177085.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4487.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5235.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -747.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.64; perplexity/K = 4.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.95; perplexity/K = 4.68%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  103881.1 e-6; = (1/var)*||X-X_r||^2 =  93828.4 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  10052.7 e-6 = 9.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  110490.4 e-6; = (1/var)*||X-X_r||^2 =  100402.9 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  10087.5 e-6 = 9.1 %)
Min.  Avg. Train Loss across Mini-Batch =  103881.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  109821.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6609.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6574.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.38; perplexity/K = 5.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.50; perplexity/K = 5.03%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  80952.7 e-6; = (1/var)*||X-X_r||^2 =  74399.4 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  6553.3 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  87131.8 e-6; = (1/var)*||X-X_r||^2 =  80276.7 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  6855.1 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  80952.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  87131.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6179.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5877.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 301.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.56; perplexity/K = 5.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.09; perplexity/K = 5.18%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68692.1 e-6; = (1/var)*||X-X_r||^2 =  63256.9 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  5435.1 e-6 = 7.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  75396.0 e-6; = (1/var)*||X-X_r||^2 =  69811.9 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  5584.1 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  68692.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  75361.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6703.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6555.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 149.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.18; perplexity/K = 4.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.34; perplexity/K = 5.01%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60981.0 e-6; = (1/var)*||X-X_r||^2 =  56058.9 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  4922.1 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  67516.2 e-6; = (1/var)*||X-X_r||^2 =  62140.8 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  5375.4 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  60981.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  67422.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6535.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6081.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 453.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.17; perplexity/K = 5.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.24; perplexity/K = 5.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  57369.2 e-6; = (1/var)*||X-X_r||^2 =  52824.8 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  4544.4 e-6 = 7.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  127305.6 e-6; = (1/var)*||X-X_r||^2 =  118713.4 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  8592.1 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  56338.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62648.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   69936.4 e-6; = (1/var)*||X-X_r||^2 val-train = 65888.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4047.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.79; perplexity/K = 5.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.20; perplexity/K = 5.20%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53209.3 e-6; = (1/var)*||X-X_r||^2 =  49175.1 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  4034.1 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  59529.6 e-6; = (1/var)*||X-X_r||^2 =  54620.5 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  4909.1 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  52886.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  59385.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6320.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5445.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 875.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.76; perplexity/K = 5.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.47; perplexity/K = 5.42%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52605.3 e-6; = (1/var)*||X-X_r||^2 =  48719.4 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  3885.9 e-6 = 7.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  66236.5 e-6; = (1/var)*||X-X_r||^2 =  61218.6 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  5017.9 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  50794.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56970.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13631.2 e-6; = (1/var)*||X-X_r||^2 val-train = 12499.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1132.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.09; perplexity/K = 5.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.74; perplexity/K = 5.64%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49420.6 e-6; = (1/var)*||X-X_r||^2 =  45978.3 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  3442.3 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  55544.4 e-6; = (1/var)*||X-X_r||^2 =  51349.0 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  4195.4 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  49083.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55222.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6123.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5370.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 753.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.74; perplexity/K = 5.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.38; perplexity/K = 5.31%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48653.1 e-6; = (1/var)*||X-X_r||^2 =  45158.5 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  3494.6 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  52838.1 e-6; = (1/var)*||X-X_r||^2 =  48774.8 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  4063.3 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  46731.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  52838.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4185.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3616.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 568.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.89; perplexity/K = 5.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.88; perplexity/K = 5.65%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44958.9 e-6; = (1/var)*||X-X_r||^2 =  41730.0 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  3228.9 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  50924.8 e-6; = (1/var)*||X-X_r||^2 =  46952.3 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  3972.4 e-6 = 7.8 %)
Min.  Avg. Train Loss across Mini-Batch =  44931.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50827.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5965.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5222.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 743.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.79; perplexity/K = 5.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.92; perplexity/K = 5.46%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44567.1 e-6; = (1/var)*||X-X_r||^2 =  41214.5 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  3352.6 e-6 = 7.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  50301.1 e-6; = (1/var)*||X-X_r||^2 =  46194.5 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  4106.6 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  44168.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50065.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5734.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4980.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 754.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.15; perplexity/K = 5.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.39; perplexity/K = 5.70%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43235.2 e-6; = (1/var)*||X-X_r||^2 =  40189.9 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  3045.3 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  48915.7 e-6; = (1/var)*||X-X_r||^2 =  45069.7 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  3846.0 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  43110.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48718.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5680.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4879.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 800.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.87; perplexity/K = 5.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.56; perplexity/K = 5.43%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42491.0 e-6; = (1/var)*||X-X_r||^2 =  39400.4 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  3090.6 e-6 = 7.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  48292.5 e-6; = (1/var)*||X-X_r||^2 =  44551.0 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  3741.4 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  42398.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47842.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5801.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5150.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 650.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.48; perplexity/K = 5.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.10; perplexity/K = 5.67%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42045.1 e-6; = (1/var)*||X-X_r||^2 =  39137.5 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  2907.6 e-6 = 6.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  49149.8 e-6; = (1/var)*||X-X_r||^2 =  45458.3 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  3691.4 e-6 = 7.5 %)
Min.  Avg. Train Loss across Mini-Batch =  41713.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47287.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7104.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6320.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 783.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.93; perplexity/K = 5.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.10; perplexity/K = 5.87%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41434.0 e-6; = (1/var)*||X-X_r||^2 =  38492.0 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  2942.0 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  48741.2 e-6; = (1/var)*||X-X_r||^2 =  44916.1 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  3825.0 e-6 = 7.8 %)
Min.  Avg. Train Loss across Mini-Batch =  41122.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46731.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7307.2 e-6; = (1/var)*||X-X_r||^2 val-train = 6424.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 883.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.06; perplexity/K = 6.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.45; perplexity/K = 6.00%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40536.7 e-6; = (1/var)*||X-X_r||^2 =  37732.9 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  2803.8 e-6 = 6.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  46334.4 e-6; = (1/var)*||X-X_r||^2 =  42881.2 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  3453.2 e-6 = 7.5 %)
Min.  Avg. Train Loss across Mini-Batch =  40390.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45945.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5797.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5148.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 649.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.07; perplexity/K = 6.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.24; perplexity/K = 5.88%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39972.5 e-6; = (1/var)*||X-X_r||^2 =  37317.9 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  2654.6 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  47397.0 e-6; = (1/var)*||X-X_r||^2 =  43988.1 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  3408.9 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  39972.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45758.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7424.5 e-6; = (1/var)*||X-X_r||^2 val-train = 6670.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 754.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.36; perplexity/K = 6.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.81; perplexity/K = 6.52%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39825.6 e-6; = (1/var)*||X-X_r||^2 =  37110.9 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  2714.7 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  46417.5 e-6; = (1/var)*||X-X_r||^2 =  43013.4 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  3404.1 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  39352.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44700.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6591.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5902.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 689.4 e-6 

----------------------------------------------------------------------------------

Finished [14:03:07 02.01.2023] 221) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 160) change_channel_size_across_layers = True:
Total training time is = 0:3:11 h/m/s. 

--------------------------------------------------- 

Started [14:03:07 02.01.2023] 222) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 160) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2517 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.27
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.20
3                           encoder.sequential_convs.conv2d_4.weight                       524            20.82
4                                  encoder.pre_residual_stack.weight                       589            23.40
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.90
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.90
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
9                              encoder.channel_adjusting_conv.weight                        16             0.64
10                                                       VQ.E.weight                        65             2.58
11                             decoder.channel_adjusting_conv.weight                       147             5.84
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.90
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.90
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.82
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.20
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.27
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.09; perplexity/K = 1.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.63; perplexity/K = 1.72%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  323191.0 e-6; = (1/var)*||X-X_r||^2 =  209102.9 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  114088.1 e-6 = 35.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  322634.4 e-6; = (1/var)*||X-X_r||^2 =  217998.6 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  104635.8 e-6 = 32.4 %)
Min.  Avg. Train Loss across Mini-Batch =  323191.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  322024.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -556.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8895.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9452.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.55; perplexity/K = 3.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.65; perplexity/K = 3.09%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  93833.5 e-6; = (1/var)*||X-X_r||^2 =  68122.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  25711.5 e-6 = 27.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  100837.2 e-6; = (1/var)*||X-X_r||^2 =  75739.1 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  25098.1 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  93833.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  100837.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7003.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7617.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -613.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.12; perplexity/K = 4.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.65; perplexity/K = 3.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33817.7 e-6; = (1/var)*||X-X_r||^2 =  27359.0 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  6458.8 e-6 = 19.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  38375.7 e-6; = (1/var)*||X-X_r||^2 =  32015.0 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  6360.7 e-6 = 16.6 %)
Min.  Avg. Train Loss across Mini-Batch =  33817.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38375.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4557.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4656.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -98.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.46; perplexity/K = 4.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.82; perplexity/K = 4.96%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15047.1 e-6; = (1/var)*||X-X_r||^2 =  12067.9 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  2979.3 e-6 = 19.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  17980.7 e-6; = (1/var)*||X-X_r||^2 =  15054.0 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  2926.7 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  15047.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17980.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2933.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2986.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -52.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.77; perplexity/K = 4.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.44; perplexity/K = 4.73%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8045.9 e-6; = (1/var)*||X-X_r||^2 =  6516.9 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  1529.1 e-6 = 19.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  10239.7 e-6; = (1/var)*||X-X_r||^2 =  8656.0 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  1583.8 e-6 = 15.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8045.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10003.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2193.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2139.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 54.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.09; perplexity/K = 5.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.93; perplexity/K = 5.17%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19577.8 e-6; = (1/var)*||X-X_r||^2 =  17011.4 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  2566.3 e-6 = 13.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  24284.1 e-6; = (1/var)*||X-X_r||^2 =  21282.7 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  3001.4 e-6 = 12.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4151.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5638.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4706.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4271.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 435.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.55; perplexity/K = 5.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.07; perplexity/K = 5.28%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4861.9 e-6; = (1/var)*||X-X_r||^2 =  3271.5 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  1590.4 e-6 = 32.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  5942.2 e-6; = (1/var)*||X-X_r||^2 =  4382.9 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  1559.4 e-6 = 26.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2371.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3559.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1080.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1111.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -31.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.12; perplexity/K = 5.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.40; perplexity/K = 5.61%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2069.5 e-6; = (1/var)*||X-X_r||^2 =  1425.1 e-6 = 68.9 %; (1+beta)*||Z_e-Z_q||^2 =  644.4 e-6 = 31.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3150.3 e-6; = (1/var)*||X-X_r||^2 =  2487.0 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  663.2 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1783.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2838.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1080.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1062.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.78; perplexity/K = 5.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.14; perplexity/K = 5.78%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1623.3 e-6; = (1/var)*||X-X_r||^2 =  1246.6 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  376.7 e-6 = 23.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2474.4 e-6; = (1/var)*||X-X_r||^2 =  2081.6 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  392.8 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1479.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2419.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   851.0 e-6; = (1/var)*||X-X_r||^2 val-train = 835.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.85; perplexity/K = 5.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.35; perplexity/K = 5.80%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1991.1 e-6; = (1/var)*||X-X_r||^2 =  1450.6 e-6 = 72.9 %; (1+beta)*||Z_e-Z_q||^2 =  540.5 e-6 = 27.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2475.8 e-6; = (1/var)*||X-X_r||^2 =  1976.2 e-6 = 79.8 %; (1+beta)*||Z_e-Z_q||^2 =  499.7 e-6 = 20.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1188.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1928.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   484.8 e-6; = (1/var)*||X-X_r||^2 val-train = 525.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -40.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.76; perplexity/K = 5.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.37; perplexity/K = 5.50%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  958.6 e-6; = (1/var)*||X-X_r||^2 =  736.0 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  222.7 e-6 = 23.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1678.6 e-6; = (1/var)*||X-X_r||^2 =  1448.6 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  230.0 e-6 = 13.7 %)
Min.  Avg. Train Loss across Mini-Batch =  958.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1627.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   719.9 e-6; = (1/var)*||X-X_r||^2 val-train = 712.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.27; perplexity/K = 5.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.19; perplexity/K = 5.49%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  988.3 e-6; = (1/var)*||X-X_r||^2 =  715.9 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  272.4 e-6 = 27.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1728.8 e-6; = (1/var)*||X-X_r||^2 =  1454.6 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  274.2 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  908.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1597.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   740.4 e-6; = (1/var)*||X-X_r||^2 val-train = 738.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.07; perplexity/K = 6.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.03; perplexity/K = 5.76%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  880.0 e-6; = (1/var)*||X-X_r||^2 =  709.8 e-6 = 80.7 %; (1+beta)*||Z_e-Z_q||^2 =  170.2 e-6 = 19.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1545.0 e-6; = (1/var)*||X-X_r||^2 =  1369.9 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  175.1 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  780.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1363.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   665.0 e-6; = (1/var)*||X-X_r||^2 val-train = 660.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.14; perplexity/K = 5.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.01; perplexity/K = 5.67%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  796.9 e-6; = (1/var)*||X-X_r||^2 =  649.6 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  147.4 e-6 = 18.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1364.9 e-6; = (1/var)*||X-X_r||^2 =  1216.8 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  148.0 e-6 = 10.8 %)
Min.  Avg. Train Loss across Mini-Batch =  673.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1246.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   567.9 e-6; = (1/var)*||X-X_r||^2 val-train = 567.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.70; perplexity/K = 5.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.03; perplexity/K = 5.77%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  626.7 e-6; = (1/var)*||X-X_r||^2 =  477.9 e-6 = 76.3 %; (1+beta)*||Z_e-Z_q||^2 =  148.8 e-6 = 23.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1238.8 e-6; = (1/var)*||X-X_r||^2 =  1083.7 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  155.1 e-6 = 12.5 %)
Min.  Avg. Train Loss across Mini-Batch =  626.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1166.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   612.1 e-6; = (1/var)*||X-X_r||^2 val-train = 605.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.81; perplexity/K = 5.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.21; perplexity/K = 5.59%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  613.3 e-6; = (1/var)*||X-X_r||^2 =  437.0 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  176.3 e-6 = 28.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1112.0 e-6; = (1/var)*||X-X_r||^2 =  935.6 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  176.4 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  532.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  958.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   498.7 e-6; = (1/var)*||X-X_r||^2 val-train = 498.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.20; perplexity/K = 5.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.14; perplexity/K = 5.29%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  827.8 e-6; = (1/var)*||X-X_r||^2 =  572.9 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  254.9 e-6 = 30.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1364.5 e-6; = (1/var)*||X-X_r||^2 =  1112.4 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  252.2 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  518.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  958.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   536.8 e-6; = (1/var)*||X-X_r||^2 val-train = 539.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 64.03; perplexity/K = 6.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.04; perplexity/K = 5.96%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  516.1 e-6; = (1/var)*||X-X_r||^2 =  418.3 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  97.8 e-6 = 19.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  948.1 e-6; = (1/var)*||X-X_r||^2 =  848.5 e-6 = 89.5 %; (1+beta)*||Z_e-Z_q||^2 =  99.5 e-6 = 10.5 %)
Min.  Avg. Train Loss across Mini-Batch =  498.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  948.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   431.9 e-6; = (1/var)*||X-X_r||^2 val-train = 430.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.44; perplexity/K = 6.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.84; perplexity/K = 6.23%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  488.9 e-6; = (1/var)*||X-X_r||^2 =  397.9 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  91.1 e-6 = 18.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  910.7 e-6; = (1/var)*||X-X_r||^2 =  816.5 e-6 = 89.7 %; (1+beta)*||Z_e-Z_q||^2 =  94.2 e-6 = 10.3 %)
Min.  Avg. Train Loss across Mini-Batch =  451.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  829.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   421.8 e-6; = (1/var)*||X-X_r||^2 val-train = 418.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.59; perplexity/K = 5.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.80; perplexity/K = 5.74%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  548.7 e-6; = (1/var)*||X-X_r||^2 =  366.7 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  182.0 e-6 = 33.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  960.5 e-6; = (1/var)*||X-X_r||^2 =  773.6 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  186.9 e-6 = 19.5 %)
Min.  Avg. Train Loss across Mini-Batch =  401.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  773.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   411.8 e-6; = (1/var)*||X-X_r||^2 val-train = 406.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

Finished [14:52:35 02.01.2023] 222) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 160) change_channel_size_across_layers = True:
Total training time is = 0:3:28 h/m/s. 

--------------------------------------------------- 

Started [14:52:35 02.01.2023] 223) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 160) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2037 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.29
1                           encoder.sequential_convs.conv2d_2.weight                       262            12.86
2                           encoder.sequential_convs.conv2d_3.weight                       262            12.86
3                           encoder.sequential_convs.conv2d_4.weight                       262            12.86
4                                  encoder.pre_residual_stack.weight                       147             7.22
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.77
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.77
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.39
10                                                       VQ.E.weight                        65             3.19
11                             decoder.channel_adjusting_conv.weight                        73             3.58
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.77
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.77
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            12.86
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            12.86
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            12.86
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.29

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.95; perplexity/K = 2.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.39; perplexity/K = 2.58%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  245098.9 e-6; = (1/var)*||X-X_r||^2 =  140866.3 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  104232.6 e-6 = 42.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  237328.4 e-6; = (1/var)*||X-X_r||^2 =  145741.3 e-6 = 61.4 %; (1+beta)*||Z_e-Z_q||^2 =  91587.1 e-6 = 38.6 %)
Min.  Avg. Train Loss across Mini-Batch =  245098.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  237328.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7770.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4875.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12645.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.50; perplexity/K = 3.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.43; perplexity/K = 3.07%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  51358.7 e-6; = (1/var)*||X-X_r||^2 =  29549.4 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  21809.3 e-6 = 42.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  58241.9 e-6; = (1/var)*||X-X_r||^2 =  37233.9 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  21008.0 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  51358.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  58241.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6883.2 e-6; = (1/var)*||X-X_r||^2 val-train = 7684.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -801.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.89; perplexity/K = 4.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.56; perplexity/K = 4.35%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13358.3 e-6; = (1/var)*||X-X_r||^2 =  7381.5 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  5976.8 e-6 = 44.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  17070.3 e-6; = (1/var)*||X-X_r||^2 =  11144.5 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  5925.8 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  13358.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17053.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3712.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3763.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -51.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.73; perplexity/K = 5.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.33; perplexity/K = 5.50%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5935.6 e-6; = (1/var)*||X-X_r||^2 =  3071.7 e-6 = 51.8 %; (1+beta)*||Z_e-Z_q||^2 =  2863.9 e-6 = 48.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  7853.7 e-6; = (1/var)*||X-X_r||^2 =  5095.7 e-6 = 64.9 %; (1+beta)*||Z_e-Z_q||^2 =  2758.0 e-6 = 35.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5602.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7800.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1918.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2024.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -105.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.05; perplexity/K = 5.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.05; perplexity/K = 5.67%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3027.2 e-6; = (1/var)*||X-X_r||^2 =  1881.3 e-6 = 62.1 %; (1+beta)*||Z_e-Z_q||^2 =  1145.9 e-6 = 37.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4699.6 e-6; = (1/var)*||X-X_r||^2 =  3487.8 e-6 = 74.2 %; (1+beta)*||Z_e-Z_q||^2 =  1211.8 e-6 = 25.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2749.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4171.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1672.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1606.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 65.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.29; perplexity/K = 5.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.55; perplexity/K = 5.72%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2124.6 e-6; = (1/var)*||X-X_r||^2 =  1017.1 e-6 = 47.9 %; (1+beta)*||Z_e-Z_q||^2 =  1107.5 e-6 = 52.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3205.9 e-6; = (1/var)*||X-X_r||^2 =  2091.0 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  1114.9 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1951.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3205.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1081.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1073.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.48; perplexity/K = 6.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.99; perplexity/K = 5.66%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1962.9 e-6; = (1/var)*||X-X_r||^2 =  883.4 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  1079.5 e-6 = 55.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2822.9 e-6; = (1/var)*||X-X_r||^2 =  1769.2 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  1053.6 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1424.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2490.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   860.0 e-6; = (1/var)*||X-X_r||^2 val-train = 885.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.47; perplexity/K = 5.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.00; perplexity/K = 6.15%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1670.3 e-6; = (1/var)*||X-X_r||^2 =  727.2 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  943.1 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2789.1 e-6; = (1/var)*||X-X_r||^2 =  1848.7 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  940.4 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1129.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1948.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1118.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1121.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.64; perplexity/K = 7.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 67.77; perplexity/K = 6.62%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2338.7 e-6; = (1/var)*||X-X_r||^2 =  1024.4 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  1314.4 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2999.3 e-6; = (1/var)*||X-X_r||^2 =  1781.1 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  1218.2 e-6 = 40.6 %)
Min.  Avg. Train Loss across Mini-Batch =  791.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1598.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   660.6 e-6; = (1/var)*||X-X_r||^2 val-train = 756.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -96.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.88; perplexity/K = 6.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 70.98; perplexity/K = 6.93%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  629.9 e-6; = (1/var)*||X-X_r||^2 =  362.3 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  267.6 e-6 = 42.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1173.5 e-6; = (1/var)*||X-X_r||^2 =  881.2 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  292.3 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  598.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1173.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   543.6 e-6; = (1/var)*||X-X_r||^2 val-train = 518.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 67.50; perplexity/K = 6.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 65.97; perplexity/K = 6.44%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  651.5 e-6; = (1/var)*||X-X_r||^2 =  340.9 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  310.6 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1404.6 e-6; = (1/var)*||X-X_r||^2 =  1078.2 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  326.5 e-6 = 23.2 %)
Min.  Avg. Train Loss across Mini-Batch =  543.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1139.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   753.1 e-6; = (1/var)*||X-X_r||^2 val-train = 737.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.70; perplexity/K = 6.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.02; perplexity/K = 6.45%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  545.6 e-6; = (1/var)*||X-X_r||^2 =  292.3 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  253.3 e-6 = 46.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1288.9 e-6; = (1/var)*||X-X_r||^2 =  1022.9 e-6 = 79.4 %; (1+beta)*||Z_e-Z_q||^2 =  266.0 e-6 = 20.6 %)
Min.  Avg. Train Loss across Mini-Batch =  459.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1139.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   743.4 e-6; = (1/var)*||X-X_r||^2 val-train = 730.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 65.47; perplexity/K = 6.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 64.37; perplexity/K = 6.29%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  492.1 e-6; = (1/var)*||X-X_r||^2 =  255.4 e-6 = 51.9 %; (1+beta)*||Z_e-Z_q||^2 =  236.7 e-6 = 48.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  951.9 e-6; = (1/var)*||X-X_r||^2 =  697.0 e-6 = 73.2 %; (1+beta)*||Z_e-Z_q||^2 =  254.9 e-6 = 26.8 %)
Min.  Avg. Train Loss across Mini-Batch =  405.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  947.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   459.8 e-6; = (1/var)*||X-X_r||^2 val-train = 441.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 74.62; perplexity/K = 7.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.91; perplexity/K = 7.02%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  566.7 e-6; = (1/var)*||X-X_r||^2 =  273.8 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  292.9 e-6 = 51.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1144.6 e-6; = (1/var)*||X-X_r||^2 =  822.9 e-6 = 71.9 %; (1+beta)*||Z_e-Z_q||^2 =  321.6 e-6 = 28.1 %)
Min.  Avg. Train Loss across Mini-Batch =  357.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  817.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   577.9 e-6; = (1/var)*||X-X_r||^2 val-train = 549.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.99; perplexity/K = 7.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.73; perplexity/K = 7.00%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  602.5 e-6; = (1/var)*||X-X_r||^2 =  265.2 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  337.2 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  927.8 e-6; = (1/var)*||X-X_r||^2 =  577.5 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  350.3 e-6 = 37.8 %)
Min.  Avg. Train Loss across Mini-Batch =  330.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  695.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   325.3 e-6; = (1/var)*||X-X_r||^2 val-train = 312.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.05; perplexity/K = 6.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 72.75; perplexity/K = 7.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  419.2 e-6; = (1/var)*||X-X_r||^2 =  198.6 e-6 = 47.4 %; (1+beta)*||Z_e-Z_q||^2 =  220.6 e-6 = 52.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1202.5 e-6; = (1/var)*||X-X_r||^2 =  966.7 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  235.8 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  279.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  695.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   783.3 e-6; = (1/var)*||X-X_r||^2 val-train = 768.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.42; perplexity/K = 7.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.22; perplexity/K = 7.15%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  260.8 e-6; = (1/var)*||X-X_r||^2 =  136.0 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  124.8 e-6 = 47.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  850.2 e-6; = (1/var)*||X-X_r||^2 =  699.7 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  150.5 e-6 = 17.7 %)
Min.  Avg. Train Loss across Mini-Batch =  244.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  695.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   589.4 e-6; = (1/var)*||X-X_r||^2 val-train = 563.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 83.58; perplexity/K = 8.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.26; perplexity/K = 8.03%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405.7 e-6; = (1/var)*||X-X_r||^2 =  162.9 e-6 = 40.2 %; (1+beta)*||Z_e-Z_q||^2 =  242.8 e-6 = 59.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  785.3 e-6; = (1/var)*||X-X_r||^2 =  536.1 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  249.2 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  217.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  695.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   379.6 e-6; = (1/var)*||X-X_r||^2 val-train = 373.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.09; perplexity/K = 7.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.94; perplexity/K = 7.61%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  273.7 e-6; = (1/var)*||X-X_r||^2 =  136.4 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  137.3 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  647.6 e-6; = (1/var)*||X-X_r||^2 =  495.9 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  151.7 e-6 = 23.4 %)
Min.  Avg. Train Loss across Mini-Batch =  179.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  576.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   373.9 e-6; = (1/var)*||X-X_r||^2 val-train = 359.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.23; perplexity/K = 7.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.66; perplexity/K = 7.19%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  535.8 e-6; = (1/var)*||X-X_r||^2 =  180.5 e-6 = 33.7 %; (1+beta)*||Z_e-Z_q||^2 =  355.3 e-6 = 66.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1111.3 e-6; = (1/var)*||X-X_r||^2 =  748.1 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  363.3 e-6 = 32.7 %)
Min.  Avg. Train Loss across Mini-Batch =  179.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  532.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   575.5 e-6; = (1/var)*||X-X_r||^2 val-train = 567.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8.0 e-6 

----------------------------------------------------------------------------------

Finished [15:57:37 02.01.2023] 223) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 160) change_channel_size_across_layers = False:
Total training time is = 0:3:1 h/m/s. 

--------------------------------------------------- 

Started [15:57:37 02.01.2023] 224) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 160) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7453 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.06
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.06
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.06
4                                  encoder.pre_residual_stack.weight                       589             7.90
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.21
10                                                       VQ.E.weight                        65             0.87
11                             decoder.channel_adjusting_conv.weight                       147             1.97
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.06
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.06
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.06
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.47; perplexity/K = 3.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.99; perplexity/K = 2.83%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  152364.5 e-6; = (1/var)*||X-X_r||^2 =  74674.0 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  77690.5 e-6 = 51.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  172727.3 e-6; = (1/var)*||X-X_r||^2 =  91974.2 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  80753.0 e-6 = 46.8 %)
Min.  Avg. Train Loss across Mini-Batch =  152364.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  166672.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20362.7 e-6; = (1/var)*||X-X_r||^2 val-train = 17300.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3062.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.51; perplexity/K = 5.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.12; perplexity/K = 5.19%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  132686.2 e-6; = (1/var)*||X-X_r||^2 =  84034.8 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  48651.4 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  130453.9 e-6; = (1/var)*||X-X_r||^2 =  76194.6 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  54259.2 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  25692.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30517.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2232.4 e-6; = (1/var)*||X-X_r||^2 val-train = -7840.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5607.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.94; perplexity/K = 6.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.61; perplexity/K = 6.70%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102082.4 e-6; = (1/var)*||X-X_r||^2 =  71578.4 e-6 = 70.1 %; (1+beta)*||Z_e-Z_q||^2 =  30504.0 e-6 = 29.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  33731.7 e-6; = (1/var)*||X-X_r||^2 =  21791.5 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  11940.2 e-6 = 35.4 %)
Min.  Avg. Train Loss across Mini-Batch =  6782.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8950.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -68350.7 e-6; = (1/var)*||X-X_r||^2 val-train = -49786.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18563.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.96; perplexity/K = 7.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.02; perplexity/K = 7.52%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4019.0 e-6; = (1/var)*||X-X_r||^2 =  1220.8 e-6 = 30.4 %; (1+beta)*||Z_e-Z_q||^2 =  2798.3 e-6 = 69.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5530.5 e-6; = (1/var)*||X-X_r||^2 =  2767.8 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  2762.7 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3149.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1511.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1547.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -35.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.70; perplexity/K = 7.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 81.68; perplexity/K = 7.98%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2551.1 e-6; = (1/var)*||X-X_r||^2 =  694.3 e-6 = 27.2 %; (1+beta)*||Z_e-Z_q||^2 =  1856.8 e-6 = 72.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3752.1 e-6; = (1/var)*||X-X_r||^2 =  1912.8 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  1839.3 e-6 = 49.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1850.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2933.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1201.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1218.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 93.46; perplexity/K = 9.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 90.35; perplexity/K = 8.82%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5709.9 e-6; = (1/var)*||X-X_r||^2 =  2421.5 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  3288.4 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5187.9 e-6; = (1/var)*||X-X_r||^2 =  2608.5 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  2579.4 e-6 = 49.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1162.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1993.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -522.0 e-6; = (1/var)*||X-X_r||^2 val-train = 186.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -708.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.22; perplexity/K = 7.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.67; perplexity/K = 6.71%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1109.7 e-6; = (1/var)*||X-X_r||^2 =  399.5 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  710.2 e-6 = 64.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1673.1 e-6; = (1/var)*||X-X_r||^2 =  963.2 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  709.9 e-6 = 42.4 %)
Min.  Avg. Train Loss across Mini-Batch =  888.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1511.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   563.4 e-6; = (1/var)*||X-X_r||^2 val-train = 563.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 72.35; perplexity/K = 7.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.35; perplexity/K = 6.97%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  704.3 e-6; = (1/var)*||X-X_r||^2 =  185.7 e-6 = 26.4 %; (1+beta)*||Z_e-Z_q||^2 =  518.6 e-6 = 73.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1436.3 e-6; = (1/var)*||X-X_r||^2 =  900.5 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  535.9 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  684.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1325.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   732.0 e-6; = (1/var)*||X-X_r||^2 val-train = 714.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 94.82; perplexity/K = 9.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 95.62; perplexity/K = 9.34%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8401.7 e-6; = (1/var)*||X-X_r||^2 =  4205.9 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  4195.8 e-6 = 49.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  6549.4 e-6; = (1/var)*||X-X_r||^2 =  3297.6 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  3251.8 e-6 = 49.7 %)
Min.  Avg. Train Loss across Mini-Batch =  632.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1246.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1852.3 e-6; = (1/var)*||X-X_r||^2 val-train = -908.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -944.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.92; perplexity/K = 6.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 74.40; perplexity/K = 7.27%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1062.8 e-6; = (1/var)*||X-X_r||^2 =  184.5 e-6 = 17.4 %; (1+beta)*||Z_e-Z_q||^2 =  878.4 e-6 = 82.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1873.0 e-6; = (1/var)*||X-X_r||^2 =  985.2 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  887.9 e-6 = 47.4 %)
Min.  Avg. Train Loss across Mini-Batch =  538.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1165.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   810.2 e-6; = (1/var)*||X-X_r||^2 val-train = 800.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 69.03; perplexity/K = 6.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 70.74; perplexity/K = 6.91%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  580.4 e-6; = (1/var)*||X-X_r||^2 =  109.0 e-6 = 18.8 %; (1+beta)*||Z_e-Z_q||^2 =  471.4 e-6 = 81.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1164.8 e-6; = (1/var)*||X-X_r||^2 =  666.3 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  498.5 e-6 = 42.8 %)
Min.  Avg. Train Loss across Mini-Batch =  538.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1164.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   584.5 e-6; = (1/var)*||X-X_r||^2 val-train = 557.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 81.83; perplexity/K = 7.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 95.70; perplexity/K = 9.35%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  71610.9 e-6; = (1/var)*||X-X_r||^2 =  28373.0 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  43237.9 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  146694.8 e-6; = (1/var)*||X-X_r||^2 =  32040.6 e-6 = 21.8 %; (1+beta)*||Z_e-Z_q||^2 =  114654.2 e-6 = 78.2 %)
Min.  Avg. Train Loss across Mini-Batch =  442.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   75083.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3667.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 71416.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.77; perplexity/K = 7.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.99; perplexity/K = 7.23%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  664.1 e-6; = (1/var)*||X-X_r||^2 =  99.0 e-6 = 14.9 %; (1+beta)*||Z_e-Z_q||^2 =  565.1 e-6 = 85.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1164.0 e-6; = (1/var)*||X-X_r||^2 =  582.2 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  581.8 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  442.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   499.9 e-6; = (1/var)*||X-X_r||^2 val-train = 483.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.24; perplexity/K = 7.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.47; perplexity/K = 7.47%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  805.6 e-6; = (1/var)*||X-X_r||^2 =  108.1 e-6 = 13.4 %; (1+beta)*||Z_e-Z_q||^2 =  697.5 e-6 = 86.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1378.9 e-6; = (1/var)*||X-X_r||^2 =  674.6 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  704.3 e-6 = 51.1 %)
Min.  Avg. Train Loss across Mini-Batch =  442.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   573.3 e-6; = (1/var)*||X-X_r||^2 val-train = 566.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 92.45; perplexity/K = 9.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 91.85; perplexity/K = 8.97%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1542.6 e-6; = (1/var)*||X-X_r||^2 =  263.3 e-6 = 17.1 %; (1+beta)*||Z_e-Z_q||^2 =  1279.3 e-6 = 82.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2135.4 e-6; = (1/var)*||X-X_r||^2 =  855.3 e-6 = 40.1 %; (1+beta)*||Z_e-Z_q||^2 =  1280.1 e-6 = 59.9 %)
Min.  Avg. Train Loss across Mini-Batch =  442.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  929.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   592.8 e-6; = (1/var)*||X-X_r||^2 val-train = 591.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 96.49; perplexity/K = 9.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 98.64; perplexity/K = 9.63%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2847.2 e-6; = (1/var)*||X-X_r||^2 =  726.5 e-6 = 25.5 %; (1+beta)*||Z_e-Z_q||^2 =  2120.7 e-6 = 74.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3732.9 e-6; = (1/var)*||X-X_r||^2 =  1938.5 e-6 = 51.9 %; (1+beta)*||Z_e-Z_q||^2 =  1794.4 e-6 = 48.1 %)
Min.  Avg. Train Loss across Mini-Batch =  441.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  929.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   885.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1212.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -326.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 94.37; perplexity/K = 9.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.28; perplexity/K = 8.52%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1499.9 e-6; = (1/var)*||X-X_r||^2 =  220.3 e-6 = 14.7 %; (1+beta)*||Z_e-Z_q||^2 =  1279.6 e-6 = 85.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2307.2 e-6; = (1/var)*||X-X_r||^2 =  1125.9 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  1181.2 e-6 = 51.2 %)
Min.  Avg. Train Loss across Mini-Batch =  370.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  929.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   807.3 e-6; = (1/var)*||X-X_r||^2 val-train = 905.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -98.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 78.52; perplexity/K = 7.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.78; perplexity/K = 7.60%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  301.8 e-6; = (1/var)*||X-X_r||^2 =  41.3 e-6 = 13.7 %; (1+beta)*||Z_e-Z_q||^2 =  260.6 e-6 = 86.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  954.4 e-6; = (1/var)*||X-X_r||^2 =  656.2 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  298.2 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  301.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  929.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   652.5 e-6; = (1/var)*||X-X_r||^2 val-train = 614.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 37.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.68; perplexity/K = 8.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 85.66; perplexity/K = 8.37%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410.7 e-6; = (1/var)*||X-X_r||^2 =  68.1 e-6 = 16.6 %; (1+beta)*||Z_e-Z_q||^2 =  342.6 e-6 = 83.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1286.6 e-6; = (1/var)*||X-X_r||^2 =  721.1 e-6 = 56.0 %; (1+beta)*||Z_e-Z_q||^2 =  565.5 e-6 = 44.0 %)
Min.  Avg. Train Loss across Mini-Batch =  228.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  837.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   875.9 e-6; = (1/var)*||X-X_r||^2 val-train = 653.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 222.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 79.05; perplexity/K = 7.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 92.73; perplexity/K = 9.06%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:15:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12741.1 e-6; = (1/var)*||X-X_r||^2 =  9128.0 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  3613.1 e-6 = 28.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  21574.2 e-6; = (1/var)*||X-X_r||^2 =  15870.0 e-6 = 73.6 %; (1+beta)*||Z_e-Z_q||^2 =  5704.2 e-6 = 26.4 %)
Min.  Avg. Train Loss across Mini-Batch =  228.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  835.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8833.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6742.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2091.1 e-6 

----------------------------------------------------------------------------------

Finished [18:13:33 02.01.2023] 224) Finished running for K = 1024 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 160) change_channel_size_across_layers = False:
Total training time is = 0:3:56 h/m/s. 

--------------------------------------------------- 

Started [18:13:33 02.01.2023] 225) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 40) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 799 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.25
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.00
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.01
4                           encoder.sequential_convs.conv2d_5.weight                       131            16.40
5                                  encoder.pre_residual_stack.weight                       147            18.40
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.51
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.51
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
10                             encoder.channel_adjusting_conv.weight                         8             1.00
11                                                       VQ.E.weight                        65             8.14
12                             decoder.channel_adjusting_conv.weight                        73             9.14
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.51
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.51
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            16.40
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.01
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.00
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.25
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1002911.9 e-6; = (1/var)*||X-X_r||^2 =  1002763.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  148.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  967713.2 e-6; = (1/var)*||X-X_r||^2 =  967566.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  146.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  964413.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35198.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35197.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.75; perplexity/K = 0.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.75; perplexity/K = 0.17%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1001432.3 e-6; = (1/var)*||X-X_r||^2 =  1001424.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  8.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963566.6 e-6; = (1/var)*||X-X_r||^2 =  963558.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963296.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -37865.6 e-6; = (1/var)*||X-X_r||^2 val-train = -37865.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000147.4 e-6; = (1/var)*||X-X_r||^2 =  1000147.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963768.7 e-6; = (1/var)*||X-X_r||^2 =  963768.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963057.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36378.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36378.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999593.9 e-6; = (1/var)*||X-X_r||^2 =  999593.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963139.8 e-6; = (1/var)*||X-X_r||^2 =  963139.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962915.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36454.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36454.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999627.3 e-6; = (1/var)*||X-X_r||^2 =  999627.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963192.8 e-6; = (1/var)*||X-X_r||^2 =  963192.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962915.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36434.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36434.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999605.3 e-6; = (1/var)*||X-X_r||^2 =  999605.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963375.3 e-6; = (1/var)*||X-X_r||^2 =  963375.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962902.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36230.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36230.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999498.1 e-6; = (1/var)*||X-X_r||^2 =  999498.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963344.4 e-6; = (1/var)*||X-X_r||^2 =  963344.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962835.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36153.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36153.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999573.5 e-6; = (1/var)*||X-X_r||^2 =  999573.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963047.3 e-6; = (1/var)*||X-X_r||^2 =  963047.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962819.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36526.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36526.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999408.0 e-6; = (1/var)*||X-X_r||^2 =  999408.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962910.2 e-6; = (1/var)*||X-X_r||^2 =  962910.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962819.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36497.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36497.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999397.9 e-6; = (1/var)*||X-X_r||^2 =  999397.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962920.8 e-6; = (1/var)*||X-X_r||^2 =  962920.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962809.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36477.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36477.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999383.3 e-6; = (1/var)*||X-X_r||^2 =  999383.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962900.3 e-6; = (1/var)*||X-X_r||^2 =  962900.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999268.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962809.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36483.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36483.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999296.5 e-6; = (1/var)*||X-X_r||^2 =  999296.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962975.9 e-6; = (1/var)*||X-X_r||^2 =  962975.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999231.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36320.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36320.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999252.3 e-6; = (1/var)*||X-X_r||^2 =  999252.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962958.8 e-6; = (1/var)*||X-X_r||^2 =  962958.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999231.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36293.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36293.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999270.3 e-6; = (1/var)*||X-X_r||^2 =  999270.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962897.2 e-6; = (1/var)*||X-X_r||^2 =  962897.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999218.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36373.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36373.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999288.8 e-6; = (1/var)*||X-X_r||^2 =  999288.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962971.0 e-6; = (1/var)*||X-X_r||^2 =  962971.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999218.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36317.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36317.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999247.0 e-6; = (1/var)*||X-X_r||^2 =  999247.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963283.2 e-6; = (1/var)*||X-X_r||^2 =  963283.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999196.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35963.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35963.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999197.1 e-6; = (1/var)*||X-X_r||^2 =  999197.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962939.8 e-6; = (1/var)*||X-X_r||^2 =  962939.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999194.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36257.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36257.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999247.4 e-6; = (1/var)*||X-X_r||^2 =  999247.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963023.3 e-6; = (1/var)*||X-X_r||^2 =  963023.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999180.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36224.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36224.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999237.2 e-6; = (1/var)*||X-X_r||^2 =  999237.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963023.8 e-6; = (1/var)*||X-X_r||^2 =  963023.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999171.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36213.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36213.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999269.6 e-6; = (1/var)*||X-X_r||^2 =  999269.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963178.8 e-6; = (1/var)*||X-X_r||^2 =  963178.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999166.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962789.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36090.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36090.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [19:03:50 02.01.2023] 225) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 40) change_channel_size_across_layers = True:
Total training time is = 0:1:17 h/m/s. 

--------------------------------------------------- 

Started [19:03:50 02.01.2023] 226) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 40) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2531 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.32
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.26
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.18
4                           encoder.sequential_convs.conv2d_5.weight                       524            20.70
5                                  encoder.pre_residual_stack.weight                       589            23.27
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.88
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.88
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
10                             encoder.channel_adjusting_conv.weight                        16             0.63
11                                                       VQ.E.weight                        65             2.57
12                             decoder.channel_adjusting_conv.weight                       147             5.81
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.88
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.88
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.70
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.18
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.26
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.96; perplexity/K = 0.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.19; perplexity/K = 0.90%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1525044.6 e-6; = (1/var)*||X-X_r||^2 =  674156.4 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  850888.2 e-6 = 55.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1460049.7 e-6; = (1/var)*||X-X_r||^2 =  651295.5 e-6 = 44.6 %; (1+beta)*||Z_e-Z_q||^2 =  808754.2 e-6 = 55.4 %)
Min.  Avg. Train Loss across Mini-Batch =  987621.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  943128.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -64994.9 e-6; = (1/var)*||X-X_r||^2 val-train = -22860.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -42134.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.83; perplexity/K = 2.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.53; perplexity/K = 2.00%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  564127.3 e-6; = (1/var)*||X-X_r||^2 =  499237.5 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  64889.9 e-6 = 11.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  548918.3 e-6; = (1/var)*||X-X_r||^2 =  492712.9 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  56205.4 e-6 = 10.2 %)
Min.  Avg. Train Loss across Mini-Batch =  563976.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  548918.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15209.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6524.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8684.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.36; perplexity/K = 3.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.43; perplexity/K = 2.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  458356.1 e-6; = (1/var)*||X-X_r||^2 =  413344.9 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  45011.2 e-6 = 9.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  463927.3 e-6; = (1/var)*||X-X_r||^2 =  420786.2 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  43141.1 e-6 = 9.3 %)
Min.  Avg. Train Loss across Mini-Batch =  454189.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  454382.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5571.2 e-6; = (1/var)*||X-X_r||^2 val-train = 7441.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1870.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.64; perplexity/K = 3.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.94; perplexity/K = 3.70%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  415935.9 e-6; = (1/var)*||X-X_r||^2 =  383825.5 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  32110.4 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  431995.2 e-6; = (1/var)*||X-X_r||^2 =  396418.7 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  35576.5 e-6 = 8.2 %)
Min.  Avg. Train Loss across Mini-Batch =  413043.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426222.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16059.3 e-6; = (1/var)*||X-X_r||^2 val-train = 12593.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3466.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.88; perplexity/K = 3.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.91; perplexity/K = 3.51%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388980.5 e-6; = (1/var)*||X-X_r||^2 =  367403.5 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  21577.0 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  411886.5 e-6; = (1/var)*||X-X_r||^2 =  385095.4 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  26791.1 e-6 = 6.5 %)
Min.  Avg. Train Loss across Mini-Batch =  387768.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  410531.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22906.0 e-6; = (1/var)*||X-X_r||^2 val-train = 17691.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5214.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.79; perplexity/K = 3.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.10; perplexity/K = 3.53%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391695.7 e-6; = (1/var)*||X-X_r||^2 =  368625.5 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  23070.1 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  423693.1 e-6; = (1/var)*||X-X_r||^2 =  393250.5 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  30442.7 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  375572.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  401596.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31997.5 e-6; = (1/var)*||X-X_r||^2 val-train = 24625.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7372.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.40; perplexity/K = 3.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.60; perplexity/K = 3.57%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  365385.1 e-6; = (1/var)*||X-X_r||^2 =  353446.6 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  11938.5 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  393414.5 e-6; = (1/var)*||X-X_r||^2 =  374849.0 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  18565.5 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  365360.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  392654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28029.4 e-6; = (1/var)*||X-X_r||^2 val-train = 21402.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6627.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.81; perplexity/K = 3.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.71; perplexity/K = 3.68%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  360222.9 e-6; = (1/var)*||X-X_r||^2 =  350893.7 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  9329.1 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  391138.1 e-6; = (1/var)*||X-X_r||^2 =  374591.3 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  16546.8 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  360141.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  389855.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30915.2 e-6; = (1/var)*||X-X_r||^2 val-train = 23697.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7217.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.94; perplexity/K = 3.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.81; perplexity/K = 3.59%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  357885.1 e-6; = (1/var)*||X-X_r||^2 =  349472.6 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  8412.5 e-6 = 2.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  386796.4 e-6; = (1/var)*||X-X_r||^2 =  372447.6 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  14348.8 e-6 = 3.7 %)
Min.  Avg. Train Loss across Mini-Batch =  356042.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  386796.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28911.2 e-6; = (1/var)*||X-X_r||^2 val-train = 22974.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5936.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.01; perplexity/K = 3.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.54; perplexity/K = 3.57%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387416.7 e-6; = (1/var)*||X-X_r||^2 =  367851.1 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  19565.6 e-6 = 5.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  415817.6 e-6; = (1/var)*||X-X_r||^2 =  393232.2 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  22585.4 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  353655.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  385016.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28400.9 e-6; = (1/var)*||X-X_r||^2 val-train = 25381.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3019.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.33; perplexity/K = 3.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.65; perplexity/K = 3.58%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  351957.8 e-6; = (1/var)*||X-X_r||^2 =  345776.3 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  6181.5 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  384998.0 e-6; = (1/var)*||X-X_r||^2 =  373025.6 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  11972.4 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  350972.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383124.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33040.2 e-6; = (1/var)*||X-X_r||^2 val-train = 27249.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5790.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.06; perplexity/K = 3.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.42; perplexity/K = 3.65%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  352601.5 e-6; = (1/var)*||X-X_r||^2 =  346533.7 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6067.9 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  386653.1 e-6; = (1/var)*||X-X_r||^2 =  375427.0 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11226.1 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  349768.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  382527.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34051.6 e-6; = (1/var)*||X-X_r||^2 val-train = 28893.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5158.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.82; perplexity/K = 3.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.67; perplexity/K = 3.58%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350906.4 e-6; = (1/var)*||X-X_r||^2 =  345603.7 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5302.7 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  385670.0 e-6; = (1/var)*||X-X_r||^2 =  374558.2 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11111.8 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  348344.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  381366.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34763.6 e-6; = (1/var)*||X-X_r||^2 val-train = 28954.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5809.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.32; perplexity/K = 3.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.15; perplexity/K = 3.63%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347510.4 e-6; = (1/var)*||X-X_r||^2 =  343498.7 e-6 = 98.8 %; (1+beta)*||Z_e-Z_q||^2 =  4011.8 e-6 = 1.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  383248.4 e-6; = (1/var)*||X-X_r||^2 =  374123.3 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  9125.0 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  347349.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  380853.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35737.9 e-6; = (1/var)*||X-X_r||^2 val-train = 30624.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5113.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.73; perplexity/K = 3.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.35; perplexity/K = 3.55%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  379166.2 e-6; = (1/var)*||X-X_r||^2 =  363997.7 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  15168.5 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  427758.5 e-6; = (1/var)*||X-X_r||^2 =  404902.4 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  22856.0 e-6 = 5.3 %)
Min.  Avg. Train Loss across Mini-Batch =  346400.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  380320.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48592.3 e-6; = (1/var)*||X-X_r||^2 val-train = 40904.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7687.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.70; perplexity/K = 3.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.28; perplexity/K = 3.74%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  345555.1 e-6; = (1/var)*||X-X_r||^2 =  342565.1 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  2990.0 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  381743.9 e-6; = (1/var)*||X-X_r||^2 =  373947.2 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  7796.6 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  345473.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  380320.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36188.8 e-6; = (1/var)*||X-X_r||^2 val-train = 31382.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4806.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.02; perplexity/K = 3.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.48; perplexity/K = 3.56%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  355829.1 e-6; = (1/var)*||X-X_r||^2 =  346509.5 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  9319.6 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  389051.5 e-6; = (1/var)*||X-X_r||^2 =  376883.2 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  12168.3 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  344896.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  379363.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33222.4 e-6; = (1/var)*||X-X_r||^2 val-train = 30373.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2848.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.63; perplexity/K = 3.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.07; perplexity/K = 3.72%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344899.0 e-6; = (1/var)*||X-X_r||^2 =  341862.0 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3037.1 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  382045.9 e-6; = (1/var)*||X-X_r||^2 =  374429.3 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  7616.6 e-6 = 2.0 %)
Min.  Avg. Train Loss across Mini-Batch =  344295.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  378875.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37146.9 e-6; = (1/var)*||X-X_r||^2 val-train = 32567.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4579.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.77; perplexity/K = 3.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.28; perplexity/K = 3.64%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344731.5 e-6; = (1/var)*||X-X_r||^2 =  341784.8 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  2946.7 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  380562.0 e-6; = (1/var)*||X-X_r||^2 =  373399.4 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  7162.6 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  343904.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  378875.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35830.5 e-6; = (1/var)*||X-X_r||^2 val-train = 31614.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4215.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.41; perplexity/K = 3.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.49; perplexity/K = 3.56%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344685.1 e-6; = (1/var)*||X-X_r||^2 =  341496.0 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  3189.0 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  382393.1 e-6; = (1/var)*||X-X_r||^2 =  375098.8 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  7294.3 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  343472.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  378875.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37708.0 e-6; = (1/var)*||X-X_r||^2 val-train = 33602.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4105.2 e-6 

----------------------------------------------------------------------------------

Finished [19:54:27 02.01.2023] 226) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 40) change_channel_size_across_layers = True:
Total training time is = 0:1:36 h/m/s. 

--------------------------------------------------- 

Started [19:54:27 02.01.2023] 227) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 40) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2561 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.23
1                           encoder.sequential_convs.conv2d_2.weight                       262            10.23
2                           encoder.sequential_convs.conv2d_3.weight                       262            10.23
3                           encoder.sequential_convs.conv2d_4.weight                       262            10.23
4                           encoder.sequential_convs.conv2d_5.weight                       262            10.23
5                                  encoder.pre_residual_stack.weight                       147             5.74
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.41
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.41
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
10                             encoder.channel_adjusting_conv.weight                         8             0.31
11                                                       VQ.E.weight                        65             2.54
12                             decoder.channel_adjusting_conv.weight                        73             2.85
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.41
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.41
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262            10.23
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262            10.23
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262            10.23
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262            10.23
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.92; perplexity/K = 0.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.13; perplexity/K = 0.70%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  777417.0 e-6; = (1/var)*||X-X_r||^2 =  492338.9 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  285078.1 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  655173.3 e-6; = (1/var)*||X-X_r||^2 =  476679.8 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  178493.5 e-6 = 27.2 %)
Min.  Avg. Train Loss across Mini-Batch =  696978.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  646955.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -122243.7 e-6; = (1/var)*||X-X_r||^2 val-train = -15659.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -106584.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.91; perplexity/K = 1.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.25; perplexity/K = 1.49%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  418004.5 e-6; = (1/var)*||X-X_r||^2 =  245745.5 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  172259.0 e-6 = 41.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  416367.8 e-6; = (1/var)*||X-X_r||^2 =  252694.4 e-6 = 60.7 %; (1+beta)*||Z_e-Z_q||^2 =  163673.4 e-6 = 39.3 %)
Min.  Avg. Train Loss across Mini-Batch =  405058.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  409396.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1636.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6948.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8585.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.82; perplexity/K = 1.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.95; perplexity/K = 1.66%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  290334.8 e-6; = (1/var)*||X-X_r||^2 =  154893.4 e-6 = 53.3 %; (1+beta)*||Z_e-Z_q||^2 =  135441.4 e-6 = 46.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  357007.4 e-6; = (1/var)*||X-X_r||^2 =  207067.3 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  149940.1 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  290334.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  339573.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   66672.6 e-6; = (1/var)*||X-X_r||^2 val-train = 52173.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14498.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.21; perplexity/K = 2.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.97; perplexity/K = 1.95%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  218400.9 e-6; = (1/var)*||X-X_r||^2 =  109908.1 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  108492.7 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  288578.6 e-6; = (1/var)*||X-X_r||^2 =  166246.0 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  122332.5 e-6 = 42.4 %)
Min.  Avg. Train Loss across Mini-Batch =  203258.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  285264.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   70177.7 e-6; = (1/var)*||X-X_r||^2 val-train = 56337.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13839.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.65; perplexity/K = 2.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.27; perplexity/K = 2.08%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  240438.4 e-6; = (1/var)*||X-X_r||^2 =  122459.9 e-6 = 50.9 %; (1+beta)*||Z_e-Z_q||^2 =  117978.4 e-6 = 49.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  292663.3 e-6; = (1/var)*||X-X_r||^2 =  170791.3 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  121872.0 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  147251.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  253166.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   52224.9 e-6; = (1/var)*||X-X_r||^2 val-train = 48331.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3893.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.64; perplexity/K = 2.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.54; perplexity/K = 2.30%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  140911.4 e-6; = (1/var)*||X-X_r||^2 =  68928.0 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  71983.4 e-6 = 51.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  248987.2 e-6; = (1/var)*||X-X_r||^2 =  157886.6 e-6 = 63.4 %; (1+beta)*||Z_e-Z_q||^2 =  91100.6 e-6 = 36.6 %)
Min.  Avg. Train Loss across Mini-Batch =  111835.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  240147.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   108075.8 e-6; = (1/var)*||X-X_r||^2 val-train = 88958.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19117.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.20; perplexity/K = 2.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.73; perplexity/K = 2.22%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  89942.3 e-6; = (1/var)*||X-X_r||^2 =  44780.3 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  45162.0 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  240880.6 e-6; = (1/var)*||X-X_r||^2 =  163335.6 e-6 = 67.8 %; (1+beta)*||Z_e-Z_q||^2 =  77545.0 e-6 = 32.2 %)
Min.  Avg. Train Loss across Mini-Batch =  89942.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  234934.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   150938.3 e-6; = (1/var)*||X-X_r||^2 val-train = 118555.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32383.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.95; perplexity/K = 2.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.50; perplexity/K = 2.29%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  71499.2 e-6; = (1/var)*||X-X_r||^2 =  37980.4 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  33518.9 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  228781.0 e-6; = (1/var)*||X-X_r||^2 =  160571.8 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  68209.2 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  71499.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  224947.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157281.8 e-6; = (1/var)*||X-X_r||^2 val-train = 122591.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34690.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.93; perplexity/K = 2.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.10; perplexity/K = 2.45%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61281.6 e-6; = (1/var)*||X-X_r||^2 =  33132.4 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  28149.3 e-6 = 45.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  223615.6 e-6; = (1/var)*||X-X_r||^2 =  162756.6 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  60859.1 e-6 = 27.2 %)
Min.  Avg. Train Loss across Mini-Batch =  61281.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  223020.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   162334.0 e-6; = (1/var)*||X-X_r||^2 val-train = 129624.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32709.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.07; perplexity/K = 2.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.95; perplexity/K = 2.24%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  66835.8 e-6; = (1/var)*||X-X_r||^2 =  34949.2 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  31886.6 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  245692.3 e-6; = (1/var)*||X-X_r||^2 =  176461.9 e-6 = 71.8 %; (1+beta)*||Z_e-Z_q||^2 =  69230.4 e-6 = 28.2 %)
Min.  Avg. Train Loss across Mini-Batch =  55010.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  221051.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   178856.5 e-6; = (1/var)*||X-X_r||^2 val-train = 141512.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 37343.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.98; perplexity/K = 2.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.70; perplexity/K = 2.22%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  62586.7 e-6; = (1/var)*||X-X_r||^2 =  29047.3 e-6 = 46.4 %; (1+beta)*||Z_e-Z_q||^2 =  33539.4 e-6 = 53.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  224195.5 e-6; = (1/var)*||X-X_r||^2 =  164024.6 e-6 = 73.2 %; (1+beta)*||Z_e-Z_q||^2 =  60170.9 e-6 = 26.8 %)
Min.  Avg. Train Loss across Mini-Batch =  48295.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  216984.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161608.7 e-6; = (1/var)*||X-X_r||^2 val-train = 134977.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26631.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.55; perplexity/K = 2.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.65; perplexity/K = 2.21%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42000.4 e-6; = (1/var)*||X-X_r||^2 =  23364.8 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  18635.7 e-6 = 44.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  216555.9 e-6; = (1/var)*||X-X_r||^2 =  166404.0 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  50151.9 e-6 = 23.2 %)
Min.  Avg. Train Loss across Mini-Batch =  41809.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  211669.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   174555.5 e-6; = (1/var)*||X-X_r||^2 val-train = 143039.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 31516.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.67; perplexity/K = 2.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.93; perplexity/K = 2.24%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39667.7 e-6; = (1/var)*||X-X_r||^2 =  21821.4 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  17846.2 e-6 = 45.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  214605.7 e-6; = (1/var)*||X-X_r||^2 =  167069.4 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  47536.2 e-6 = 22.2 %)
Min.  Avg. Train Loss across Mini-Batch =  38173.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  211414.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   174938.0 e-6; = (1/var)*||X-X_r||^2 val-train = 145248.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29690.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.25; perplexity/K = 2.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.83; perplexity/K = 2.13%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35952.2 e-6; = (1/var)*||X-X_r||^2 =  20411.0 e-6 = 56.8 %; (1+beta)*||Z_e-Z_q||^2 =  15541.2 e-6 = 43.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  210992.9 e-6; = (1/var)*||X-X_r||^2 =  166780.8 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  44212.1 e-6 = 21.0 %)
Min.  Avg. Train Loss across Mini-Batch =  35952.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  209462.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   175040.6 e-6; = (1/var)*||X-X_r||^2 val-train = 146369.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28670.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.50; perplexity/K = 2.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.78; perplexity/K = 2.13%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  48154.8 e-6; = (1/var)*||X-X_r||^2 =  22689.5 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  25465.3 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  218590.3 e-6; = (1/var)*||X-X_r||^2 =  168496.6 e-6 = 77.1 %; (1+beta)*||Z_e-Z_q||^2 =  50093.7 e-6 = 22.9 %)
Min.  Avg. Train Loss across Mini-Batch =  34446.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  207470.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   170435.5 e-6; = (1/var)*||X-X_r||^2 val-train = 145807.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24628.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.28; perplexity/K = 2.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.14; perplexity/K = 2.36%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33862.1 e-6; = (1/var)*||X-X_r||^2 =  20000.7 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  13861.4 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  211151.5 e-6; = (1/var)*||X-X_r||^2 =  171254.7 e-6 = 81.1 %; (1+beta)*||Z_e-Z_q||^2 =  39896.8 e-6 = 18.9 %)
Min.  Avg. Train Loss across Mini-Batch =  32255.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  207405.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   177289.4 e-6; = (1/var)*||X-X_r||^2 val-train = 151254.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26035.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.52; perplexity/K = 2.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.73; perplexity/K = 2.22%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67885.3 e-6; = (1/var)*||X-X_r||^2 =  36181.3 e-6 = 53.3 %; (1+beta)*||Z_e-Z_q||^2 =  31704.0 e-6 = 46.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  229114.0 e-6; = (1/var)*||X-X_r||^2 =  178964.4 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  50149.6 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  30831.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  207223.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161228.7 e-6; = (1/var)*||X-X_r||^2 val-train = 142783.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18445.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.76; perplexity/K = 2.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.65; perplexity/K = 2.31%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30346.3 e-6; = (1/var)*||X-X_r||^2 =  18482.3 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  11864.1 e-6 = 39.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  205965.4 e-6; = (1/var)*||X-X_r||^2 =  169868.8 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  36096.5 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  29678.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  204133.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   175619.0 e-6; = (1/var)*||X-X_r||^2 val-train = 151386.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24232.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.55; perplexity/K = 2.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.71; perplexity/K = 2.22%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31615.0 e-6; = (1/var)*||X-X_r||^2 =  18448.3 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  13166.6 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  202096.2 e-6; = (1/var)*||X-X_r||^2 =  165639.3 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  36456.9 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  28413.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  201272.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   170481.2 e-6; = (1/var)*||X-X_r||^2 val-train = 147190.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23290.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.64; perplexity/K = 2.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.78; perplexity/K = 2.22%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37935.1 e-6; = (1/var)*||X-X_r||^2 =  18795.5 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  19139.6 e-6 = 50.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  206788.9 e-6; = (1/var)*||X-X_r||^2 =  166470.6 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  40318.3 e-6 = 19.5 %)
Min.  Avg. Train Loss across Mini-Batch =  27266.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  200858.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   168853.8 e-6; = (1/var)*||X-X_r||^2 val-train = 147675.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21178.7 e-6 

----------------------------------------------------------------------------------

Finished [20:59:46 02.01.2023] 227) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 40) change_channel_size_across_layers = False:
Total training time is = 0:1:18 h/m/s. 

--------------------------------------------------- 

Started [23:11:10 02.01.2023] 228) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 40) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9549 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.13
1                           encoder.sequential_convs.conv2d_2.weight                      1048            10.97
2                           encoder.sequential_convs.conv2d_3.weight                      1048            10.97
3                           encoder.sequential_convs.conv2d_4.weight                      1048            10.97
4                           encoder.sequential_convs.conv2d_5.weight                      1048            10.97
5                                  encoder.pre_residual_stack.weight                       589             6.17
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.76
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.76
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                        65             0.68
12                             decoder.channel_adjusting_conv.weight                       147             1.54
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.76
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.76
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            10.97
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            10.97
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            10.97
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            10.97
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.77; perplexity/K = 1.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.94; perplexity/K = 1.26%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  571030.4 e-6; = (1/var)*||X-X_r||^2 =  291918.6 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  279111.8 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  589520.0 e-6; = (1/var)*||X-X_r||^2 =  318184.2 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  271335.8 e-6 = 46.0 %)
Min.  Avg. Train Loss across Mini-Batch =  571030.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  505632.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18489.6 e-6; = (1/var)*||X-X_r||^2 val-train = 26265.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7776.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.32; perplexity/K = 2.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.96; perplexity/K = 2.24%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  334606.4 e-6; = (1/var)*||X-X_r||^2 =  125162.2 e-6 = 37.4 %; (1+beta)*||Z_e-Z_q||^2 =  209444.2 e-6 = 62.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  384473.7 e-6; = (1/var)*||X-X_r||^2 =  186775.0 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  197698.7 e-6 = 51.4 %)
Min.  Avg. Train Loss across Mini-Batch =  334606.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384473.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   49867.3 e-6; = (1/var)*||X-X_r||^2 val-train = 61612.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11745.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.05; perplexity/K = 2.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.06; perplexity/K = 2.55%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  146638.5 e-6; = (1/var)*||X-X_r||^2 =  44968.6 e-6 = 30.7 %; (1+beta)*||Z_e-Z_q||^2 =  101669.8 e-6 = 69.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  280139.1 e-6; = (1/var)*||X-X_r||^2 =  150249.3 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  129889.8 e-6 = 46.4 %)
Min.  Avg. Train Loss across Mini-Batch =  146638.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  280139.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   133500.7 e-6; = (1/var)*||X-X_r||^2 val-train = 105280.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28220.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.79; perplexity/K = 2.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.06; perplexity/K = 2.74%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68584.3 e-6; = (1/var)*||X-X_r||^2 =  20557.7 e-6 = 30.0 %; (1+beta)*||Z_e-Z_q||^2 =  48026.6 e-6 = 70.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  221906.6 e-6; = (1/var)*||X-X_r||^2 =  146404.1 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  75502.5 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  53237.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  213363.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   153322.3 e-6; = (1/var)*||X-X_r||^2 val-train = 125846.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27475.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.55; perplexity/K = 2.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.18; perplexity/K = 3.04%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42680.0 e-6; = (1/var)*||X-X_r||^2 =  10135.7 e-6 = 23.7 %; (1+beta)*||Z_e-Z_q||^2 =  32544.3 e-6 = 76.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  200037.8 e-6; = (1/var)*||X-X_r||^2 =  143182.3 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  56855.5 e-6 = 28.4 %)
Min.  Avg. Train Loss across Mini-Batch =  27424.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  184015.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157357.8 e-6; = (1/var)*||X-X_r||^2 val-train = 133046.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24311.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.62; perplexity/K = 2.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.57; perplexity/K = 2.99%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21786.2 e-6; = (1/var)*||X-X_r||^2 =  4160.2 e-6 = 19.1 %; (1+beta)*||Z_e-Z_q||^2 =  17626.1 e-6 = 80.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  183597.8 e-6; = (1/var)*||X-X_r||^2 =  140253.0 e-6 = 76.4 %; (1+beta)*||Z_e-Z_q||^2 =  43344.8 e-6 = 23.6 %)
Min.  Avg. Train Loss across Mini-Batch =  13563.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  175119.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161811.5 e-6; = (1/var)*||X-X_r||^2 val-train = 136092.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25718.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.23; perplexity/K = 2.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.22; perplexity/K = 2.95%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31217.8 e-6; = (1/var)*||X-X_r||^2 =  10076.1 e-6 = 32.3 %; (1+beta)*||Z_e-Z_q||^2 =  21141.7 e-6 = 67.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  190519.7 e-6; = (1/var)*||X-X_r||^2 =  150513.9 e-6 = 79.0 %; (1+beta)*||Z_e-Z_q||^2 =  40005.7 e-6 = 21.0 %)
Min.  Avg. Train Loss across Mini-Batch =  11165.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  175119.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159301.8 e-6; = (1/var)*||X-X_r||^2 val-train = 140437.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18864.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.36; perplexity/K = 3.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.86; perplexity/K = 3.01%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17515.3 e-6; = (1/var)*||X-X_r||^2 =  6196.0 e-6 = 35.4 %; (1+beta)*||Z_e-Z_q||^2 =  11319.3 e-6 = 64.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  176885.2 e-6; = (1/var)*||X-X_r||^2 =  145551.6 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  31333.7 e-6 = 17.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7344.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  164966.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159369.9 e-6; = (1/var)*||X-X_r||^2 val-train = 139355.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20014.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.21; perplexity/K = 3.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.84; perplexity/K = 3.11%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55985.6 e-6; = (1/var)*||X-X_r||^2 =  19210.2 e-6 = 34.3 %; (1+beta)*||Z_e-Z_q||^2 =  36775.5 e-6 = 65.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  201812.6 e-6; = (1/var)*||X-X_r||^2 =  156391.0 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  45421.6 e-6 = 22.5 %)
Min.  Avg. Train Loss across Mini-Batch =  6782.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  164966.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145826.9 e-6; = (1/var)*||X-X_r||^2 val-train = 137180.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8646.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.39; perplexity/K = 3.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.48; perplexity/K = 3.07%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10490.0 e-6; = (1/var)*||X-X_r||^2 =  2708.1 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  7781.9 e-6 = 74.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  166512.6 e-6; = (1/var)*||X-X_r||^2 =  138915.5 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  27597.1 e-6 = 16.6 %)
Min.  Avg. Train Loss across Mini-Batch =  4118.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  164966.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   156022.6 e-6; = (1/var)*||X-X_r||^2 val-train = 136207.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19815.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.69; perplexity/K = 3.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.82; perplexity/K = 3.21%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35634.2 e-6; = (1/var)*||X-X_r||^2 =  10085.3 e-6 = 28.3 %; (1+beta)*||Z_e-Z_q||^2 =  25548.9 e-6 = 71.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  189408.3 e-6; = (1/var)*||X-X_r||^2 =  151966.6 e-6 = 80.2 %; (1+beta)*||Z_e-Z_q||^2 =  37441.7 e-6 = 19.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4118.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   153774.2 e-6; = (1/var)*||X-X_r||^2 val-train = 141881.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11892.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.25; perplexity/K = 3.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.13; perplexity/K = 3.24%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4589.0 e-6; = (1/var)*||X-X_r||^2 =  1183.2 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  3405.8 e-6 = 74.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  168759.8 e-6; = (1/var)*||X-X_r||^2 =  147366.3 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  21393.5 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3459.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   164170.8 e-6; = (1/var)*||X-X_r||^2 val-train = 146183.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17987.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.86; perplexity/K = 3.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.71; perplexity/K = 3.29%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7747.8 e-6; = (1/var)*||X-X_r||^2 =  1406.0 e-6 = 18.1 %; (1+beta)*||Z_e-Z_q||^2 =  6341.8 e-6 = 81.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  172950.8 e-6; = (1/var)*||X-X_r||^2 =  147634.3 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  25316.6 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3056.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   165203.1 e-6; = (1/var)*||X-X_r||^2 val-train = 146228.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18974.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.33; perplexity/K = 3.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.56; perplexity/K = 3.18%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10906.9 e-6; = (1/var)*||X-X_r||^2 =  3000.9 e-6 = 27.5 %; (1+beta)*||Z_e-Z_q||^2 =  7906.1 e-6 = 72.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  173671.8 e-6; = (1/var)*||X-X_r||^2 =  146361.1 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  27310.8 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2070.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   162764.9 e-6; = (1/var)*||X-X_r||^2 val-train = 143360.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19404.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.62; perplexity/K = 3.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.89; perplexity/K = 3.31%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12005.6 e-6; = (1/var)*||X-X_r||^2 =  1960.1 e-6 = 16.3 %; (1+beta)*||Z_e-Z_q||^2 =  10045.5 e-6 = 83.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  174436.4 e-6; = (1/var)*||X-X_r||^2 =  146111.2 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  28325.2 e-6 = 16.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1600.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   162430.8 e-6; = (1/var)*||X-X_r||^2 val-train = 144151.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18279.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.15; perplexity/K = 3.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.06; perplexity/K = 3.23%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1360.7 e-6; = (1/var)*||X-X_r||^2 =  624.6 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  736.1 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  167859.7 e-6; = (1/var)*||X-X_r||^2 =  145897.7 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  21962.0 e-6 = 13.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1360.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   166499.0 e-6; = (1/var)*||X-X_r||^2 val-train = 145273.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21225.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.15; perplexity/K = 3.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.02; perplexity/K = 3.42%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4818.3 e-6; = (1/var)*||X-X_r||^2 =  901.1 e-6 = 18.7 %; (1+beta)*||Z_e-Z_q||^2 =  3917.2 e-6 = 81.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  167613.3 e-6; = (1/var)*||X-X_r||^2 =  144313.8 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  23299.5 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1360.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   162794.9 e-6; = (1/var)*||X-X_r||^2 val-train = 143412.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19382.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.25; perplexity/K = 3.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.64; perplexity/K = 3.38%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17521.6 e-6; = (1/var)*||X-X_r||^2 =  3533.8 e-6 = 20.2 %; (1+beta)*||Z_e-Z_q||^2 =  13987.7 e-6 = 79.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  177991.6 e-6; = (1/var)*||X-X_r||^2 =  149752.3 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  28239.3 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1360.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   160470.1 e-6; = (1/var)*||X-X_r||^2 val-train = 146218.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14251.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.95; perplexity/K = 3.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.10; perplexity/K = 3.33%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2609.7 e-6; = (1/var)*||X-X_r||^2 =  678.6 e-6 = 26.0 %; (1+beta)*||Z_e-Z_q||^2 =  1931.0 e-6 = 74.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  168347.1 e-6; = (1/var)*||X-X_r||^2 =  145754.8 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  22592.2 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1344.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160650.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   165737.4 e-6; = (1/var)*||X-X_r||^2 val-train = 145076.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20661.2 e-6 

----------------------------------------------------------------------------------

Finished [01:26:54 03.01.2023] 228) Finished running for K = 1024 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 40) change_channel_size_across_layers = False:
Total training time is = 0:1:43 h/m/s. 

--------------------------------------------------- 

Started [01:26:54 03.01.2023] 229) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 10) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 799 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.25
3                           encoder.sequential_convs.conv2d_4.weight                         8             1.00
4                           encoder.sequential_convs.conv2d_5.weight                        32             4.01
5                           encoder.sequential_convs.conv2d_6.weight                       131            16.40
6                                  encoder.pre_residual_stack.weight                       147            18.40
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.51
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.51
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
11                             encoder.channel_adjusting_conv.weight                         8             1.00
12                                                       VQ.E.weight                        65             8.14
13                             decoder.channel_adjusting_conv.weight                        73             9.14
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.51
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.50
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.51
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.50
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            16.40
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.01
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.00
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.25
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.91; perplexity/K = 0.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.31; perplexity/K = 0.23%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1111782.1 e-6; = (1/var)*||X-X_r||^2 =  969598.3 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  142183.8 e-6 = 12.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  941249.7 e-6; = (1/var)*||X-X_r||^2 =  936808.3 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4441.4 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1001892.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  941249.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -170532.4 e-6; = (1/var)*||X-X_r||^2 val-train = -32790.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -137742.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.33; perplexity/K = 0.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.83; perplexity/K = 0.76%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1764464.1 e-6; = (1/var)*||X-X_r||^2 =  891014.0 e-6 = 50.5 %; (1+beta)*||Z_e-Z_q||^2 =  873450.1 e-6 = 49.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1320545.5 e-6; = (1/var)*||X-X_r||^2 =  873713.0 e-6 = 66.2 %; (1+beta)*||Z_e-Z_q||^2 =  446832.6 e-6 = 33.8 %)
Min.  Avg. Train Loss across Mini-Batch =  965958.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918750.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -443918.5 e-6; = (1/var)*||X-X_r||^2 val-train = -17301.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -426617.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.51; perplexity/K = 2.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.66; perplexity/K = 2.51%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1315364.7 e-6; = (1/var)*||X-X_r||^2 =  763351.1 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  552013.6 e-6 = 42.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1070692.0 e-6; = (1/var)*||X-X_r||^2 =  721883.8 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  348808.3 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  965958.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918750.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -244672.7 e-6; = (1/var)*||X-X_r||^2 val-train = -41467.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -203205.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.78; perplexity/K = 1.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.88; perplexity/K = 1.36%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  901108.1 e-6; = (1/var)*||X-X_r||^2 =  701480.1 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  199628.0 e-6 = 22.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  814391.3 e-6; = (1/var)*||X-X_r||^2 =  689654.4 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  124736.9 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  895270.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  787647.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -86716.9 e-6; = (1/var)*||X-X_r||^2 val-train = -11825.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -74891.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.03; perplexity/K = 0.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.38; perplexity/K = 0.92%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  867418.7 e-6; = (1/var)*||X-X_r||^2 =  706864.7 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  160554.0 e-6 = 18.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  826583.4 e-6; = (1/var)*||X-X_r||^2 =  684641.4 e-6 = 82.8 %; (1+beta)*||Z_e-Z_q||^2 =  141941.9 e-6 = 17.2 %)
Min.  Avg. Train Loss across Mini-Batch =  830889.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -40835.3 e-6; = (1/var)*||X-X_r||^2 val-train = -22223.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18612.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.58; perplexity/K = 0.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.98; perplexity/K = 0.58%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  823206.8 e-6; = (1/var)*||X-X_r||^2 =  706257.0 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  116949.7 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  808441.3 e-6; = (1/var)*||X-X_r||^2 =  695926.9 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  112514.5 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  819583.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -14765.4 e-6; = (1/var)*||X-X_r||^2 val-train = -10330.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4435.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.79; perplexity/K = 0.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.94; perplexity/K = 0.48%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  825782.2 e-6; = (1/var)*||X-X_r||^2 =  721573.7 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  104208.6 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  814510.0 e-6; = (1/var)*||X-X_r||^2 =  710940.5 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  103569.5 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11272.2 e-6; = (1/var)*||X-X_r||^2 val-train = -10633.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -639.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.23; perplexity/K = 0.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.79; perplexity/K = 0.17%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  814686.9 e-6; = (1/var)*||X-X_r||^2 =  751729.4 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  62957.5 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  802425.1 e-6; = (1/var)*||X-X_r||^2 =  738153.7 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  64271.4 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12261.8 e-6; = (1/var)*||X-X_r||^2 val-train = -13575.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1313.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.56; perplexity/K = 0.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.76; perplexity/K = 0.37%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  877791.9 e-6; = (1/var)*||X-X_r||^2 =  744491.9 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  133300.0 e-6 = 15.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  842848.6 e-6; = (1/var)*||X-X_r||^2 =  739184.4 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  103664.2 e-6 = 12.3 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34943.3 e-6; = (1/var)*||X-X_r||^2 val-train = -5307.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -29635.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.29; perplexity/K = 0.22%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  860250.9 e-6; = (1/var)*||X-X_r||^2 =  788149.9 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  72101.0 e-6 = 8.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  845074.1 e-6; = (1/var)*||X-X_r||^2 =  780481.8 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  64592.3 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15176.8 e-6; = (1/var)*||X-X_r||^2 val-train = -7668.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7508.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.72; perplexity/K = 0.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.56; perplexity/K = 0.25%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  825542.1 e-6; = (1/var)*||X-X_r||^2 =  757527.3 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  68014.8 e-6 = 8.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  806869.5 e-6; = (1/var)*||X-X_r||^2 =  736185.6 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  70683.9 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -18672.6 e-6; = (1/var)*||X-X_r||^2 val-train = -21341.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2669.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.25; perplexity/K = 0.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.22; perplexity/K = 0.41%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  853868.2 e-6; = (1/var)*||X-X_r||^2 =  768507.7 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  85360.4 e-6 = 10.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  841638.6 e-6; = (1/var)*||X-X_r||^2 =  759876.1 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  81762.5 e-6 = 9.7 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12229.6 e-6; = (1/var)*||X-X_r||^2 val-train = -8631.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3597.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.79; perplexity/K = 0.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.53; perplexity/K = 0.44%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  881457.4 e-6; = (1/var)*||X-X_r||^2 =  773843.8 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  107613.6 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  865205.5 e-6; = (1/var)*||X-X_r||^2 =  764848.9 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  100356.7 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16251.9 e-6; = (1/var)*||X-X_r||^2 val-train = -8994.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7256.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.33; perplexity/K = 0.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.87; perplexity/K = 0.18%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  837930.7 e-6; = (1/var)*||X-X_r||^2 =  778371.2 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  59559.4 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  815970.7 e-6; = (1/var)*||X-X_r||^2 =  767505.1 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  48465.6 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21960.0 e-6; = (1/var)*||X-X_r||^2 val-train = -10866.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11093.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 0.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.30; perplexity/K = 0.13%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  825545.4 e-6; = (1/var)*||X-X_r||^2 =  789226.7 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  36318.7 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  803203.4 e-6; = (1/var)*||X-X_r||^2 =  772695.7 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  30507.8 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22341.9 e-6; = (1/var)*||X-X_r||^2 val-train = -16531.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5810.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.95; perplexity/K = 0.19%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  847382.1 e-6; = (1/var)*||X-X_r||^2 =  808936.8 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  38445.2 e-6 = 4.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  807283.6 e-6; = (1/var)*||X-X_r||^2 =  772398.5 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  34885.1 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -40098.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36538.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3560.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 0.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.30; perplexity/K = 0.13%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  874039.4 e-6; = (1/var)*||X-X_r||^2 =  849052.5 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  24986.9 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  855170.8 e-6; = (1/var)*||X-X_r||^2 =  830146.9 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  25023.9 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -18868.6 e-6; = (1/var)*||X-X_r||^2 val-train = -18905.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 37.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.48; perplexity/K = 0.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 0.12%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  824876.7 e-6; = (1/var)*||X-X_r||^2 =  790765.0 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  34111.7 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  800601.7 e-6; = (1/var)*||X-X_r||^2 =  771315.2 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  29286.5 e-6 = 3.7 %)
Min.  Avg. Train Loss across Mini-Batch =  801003.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24275.0 e-6; = (1/var)*||X-X_r||^2 val-train = -19449.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4825.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.14; perplexity/K = 0.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.15; perplexity/K = 0.21%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  795402.2 e-6; = (1/var)*||X-X_r||^2 =  742946.3 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  52455.8 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  773830.5 e-6; = (1/var)*||X-X_r||^2 =  730092.1 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  43738.5 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  787904.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21571.6 e-6; = (1/var)*||X-X_r||^2 val-train = -12854.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8717.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.89; perplexity/K = 0.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.87; perplexity/K = 0.67%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  823345.9 e-6; = (1/var)*||X-X_r||^2 =  696511.2 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  126834.6 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  786404.9 e-6; = (1/var)*||X-X_r||^2 =  676603.7 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  109801.2 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  783170.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  757925.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36941.0 e-6; = (1/var)*||X-X_r||^2 val-train = -19907.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17033.5 e-6 

----------------------------------------------------------------------------------

Finished [02:18:21 03.01.2023] 229) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 10) change_channel_size_across_layers = True:
Total training time is = 0:0:26 h/m/s. 

--------------------------------------------------- 

Started [02:18:21 03.01.2023] 230) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 10) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2535 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.32
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.26
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.17
5                           encoder.sequential_convs.conv2d_6.weight                       524            20.67
6                                  encoder.pre_residual_stack.weight                       589            23.23
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.88
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.88
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
11                             encoder.channel_adjusting_conv.weight                        16             0.63
12                                                       VQ.E.weight                        65             2.56
13                             decoder.channel_adjusting_conv.weight                       147             5.80
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.88
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.88
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.67
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.17
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.26
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.11; perplexity/K = 0.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.21; perplexity/K = 0.31%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1573828.8 e-6; = (1/var)*||X-X_r||^2 =  900227.5 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  673601.3 e-6 = 42.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1316114.6 e-6; = (1/var)*||X-X_r||^2 =  875963.2 e-6 = 66.6 %; (1+beta)*||Z_e-Z_q||^2 =  440151.5 e-6 = 33.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1018785.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  956955.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -257714.1 e-6; = (1/var)*||X-X_r||^2 val-train = -24264.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -233449.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.84; perplexity/K = 0.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.79; perplexity/K = 0.18%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1081436.4 e-6; = (1/var)*||X-X_r||^2 =  860118.6 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  221317.7 e-6 = 20.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1001206.6 e-6; = (1/var)*||X-X_r||^2 =  849438.0 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  151768.6 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1018785.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  956955.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -80229.7 e-6; = (1/var)*||X-X_r||^2 val-train = -10680.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -69549.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.08; perplexity/K = 0.11%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  971446.2 e-6; = (1/var)*||X-X_r||^2 =  962663.5 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  8782.7 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  943205.4 e-6; = (1/var)*||X-X_r||^2 =  938223.6 e-6 = 99.5 %; (1+beta)*||Z_e-Z_q||^2 =  4981.8 e-6 = 0.5 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -28240.8 e-6; = (1/var)*||X-X_r||^2 val-train = -24439.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3800.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999472.2 e-6; = (1/var)*||X-X_r||^2 =  999407.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  65.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963519.5 e-6; = (1/var)*||X-X_r||^2 =  963462.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  57.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35952.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35945.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999385.9 e-6; = (1/var)*||X-X_r||^2 =  999376.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  9.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963094.6 e-6; = (1/var)*||X-X_r||^2 =  963086.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  7.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36291.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36289.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999499.0 e-6; = (1/var)*||X-X_r||^2 =  999498.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963168.3 e-6; = (1/var)*||X-X_r||^2 =  963168.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36330.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36330.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999489.1 e-6; = (1/var)*||X-X_r||^2 =  999488.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962926.5 e-6; = (1/var)*||X-X_r||^2 =  962926.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36562.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36562.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999515.9 e-6; = (1/var)*||X-X_r||^2 =  999515.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962978.4 e-6; = (1/var)*||X-X_r||^2 =  962978.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36537.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36537.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999369.2 e-6; = (1/var)*||X-X_r||^2 =  999369.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962957.2 e-6; = (1/var)*||X-X_r||^2 =  962957.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36411.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36411.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999354.7 e-6; = (1/var)*||X-X_r||^2 =  999354.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963911.2 e-6; = (1/var)*||X-X_r||^2 =  963911.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35443.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35443.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999284.0 e-6; = (1/var)*||X-X_r||^2 =  999284.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963099.6 e-6; = (1/var)*||X-X_r||^2 =  963099.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36184.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36184.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999382.8 e-6; = (1/var)*||X-X_r||^2 =  999382.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963804.3 e-6; = (1/var)*||X-X_r||^2 =  963804.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35578.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35578.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999256.3 e-6; = (1/var)*||X-X_r||^2 =  999256.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962947.0 e-6; = (1/var)*||X-X_r||^2 =  962947.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36309.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36309.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999339.7 e-6; = (1/var)*||X-X_r||^2 =  999339.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963438.8 e-6; = (1/var)*||X-X_r||^2 =  963438.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35900.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35900.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999357.5 e-6; = (1/var)*||X-X_r||^2 =  999357.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963587.2 e-6; = (1/var)*||X-X_r||^2 =  963587.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35770.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35770.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999248.8 e-6; = (1/var)*||X-X_r||^2 =  999248.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962935.1 e-6; = (1/var)*||X-X_r||^2 =  962935.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36313.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36313.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999288.3 e-6; = (1/var)*||X-X_r||^2 =  999288.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963132.2 e-6; = (1/var)*||X-X_r||^2 =  963132.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36156.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36156.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999315.2 e-6; = (1/var)*||X-X_r||^2 =  999315.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962984.8 e-6; = (1/var)*||X-X_r||^2 =  962984.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36330.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36330.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999212.1 e-6; = (1/var)*||X-X_r||^2 =  999212.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963096.8 e-6; = (1/var)*||X-X_r||^2 =  963096.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36115.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36115.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999225.2 e-6; = (1/var)*||X-X_r||^2 =  999225.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962915.3 e-6; = (1/var)*||X-X_r||^2 =  962915.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  959098.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925049.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36309.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36309.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [03:10:03 03.01.2023] 230) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 10) change_channel_size_across_layers = True:
Total training time is = 0:0:41 h/m/s. 

--------------------------------------------------- 

Started [03:10:03 03.01.2023] 231) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 10) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 3085 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.19
1                           encoder.sequential_convs.conv2d_2.weight                       262             8.49
2                           encoder.sequential_convs.conv2d_3.weight                       262             8.49
3                           encoder.sequential_convs.conv2d_4.weight                       262             8.49
4                           encoder.sequential_convs.conv2d_5.weight                       262             8.49
5                           encoder.sequential_convs.conv2d_6.weight                       262             8.49
6                                  encoder.pre_residual_stack.weight                       147             4.76
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.17
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.17
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
11                             encoder.channel_adjusting_conv.weight                         8             0.26
12                                                       VQ.E.weight                        65             2.11
13                             decoder.channel_adjusting_conv.weight                        73             2.37
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.17
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.17
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
18                    decoder.sequential_trans_convs.conv2d_1.weight                       262             8.49
19                    decoder.sequential_trans_convs.conv2d_2.weight                       262             8.49
20                    decoder.sequential_trans_convs.conv2d_3.weight                       262             8.49
21                    decoder.sequential_trans_convs.conv2d_4.weight                       262             8.49
22                    decoder.sequential_trans_convs.conv2d_5.weight                       262             8.49
23                    decoder.sequential_trans_convs.conv2d_6.weight                         6             0.19

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.63; perplexity/K = 0.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.65; perplexity/K = 0.16%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1207635.4 e-6; = (1/var)*||X-X_r||^2 =  885615.2 e-6 = 73.3 %; (1+beta)*||Z_e-Z_q||^2 =  322020.3 e-6 = 26.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1123296.4 e-6; = (1/var)*||X-X_r||^2 =  855505.6 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  267790.7 e-6 = 23.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1000084.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  955923.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -84339.1 e-6; = (1/var)*||X-X_r||^2 val-train = -30109.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -54229.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  982298.1 e-6; = (1/var)*||X-X_r||^2 =  974421.7 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  7876.4 e-6 = 0.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  963226.4 e-6; = (1/var)*||X-X_r||^2 =  960813.1 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2413.2 e-6 = 0.3 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19071.8 e-6; = (1/var)*||X-X_r||^2 val-train = -13608.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5463.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999600.3 e-6; = (1/var)*||X-X_r||^2 =  999568.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  32.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962947.0 e-6; = (1/var)*||X-X_r||^2 =  962917.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  29.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36653.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36650.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999760.6 e-6; = (1/var)*||X-X_r||^2 =  999754.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963115.6 e-6; = (1/var)*||X-X_r||^2 =  963110.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36645.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36643.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999484.1 e-6; = (1/var)*||X-X_r||^2 =  999482.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963115.8 e-6; = (1/var)*||X-X_r||^2 =  963114.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36368.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36368.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999711.3 e-6; = (1/var)*||X-X_r||^2 =  999710.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962945.7 e-6; = (1/var)*||X-X_r||^2 =  962944.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36765.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36765.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999665.4 e-6; = (1/var)*||X-X_r||^2 =  999664.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962986.8 e-6; = (1/var)*||X-X_r||^2 =  962986.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36678.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36678.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999448.9 e-6; = (1/var)*||X-X_r||^2 =  999448.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962984.7 e-6; = (1/var)*||X-X_r||^2 =  962984.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36464.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36464.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999371.5 e-6; = (1/var)*||X-X_r||^2 =  999371.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962895.9 e-6; = (1/var)*||X-X_r||^2 =  962895.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36475.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36475.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999448.1 e-6; = (1/var)*||X-X_r||^2 =  999442.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963860.4 e-6; = (1/var)*||X-X_r||^2 =  963856.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35587.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35585.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999417.9 e-6; = (1/var)*||X-X_r||^2 =  999417.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963197.4 e-6; = (1/var)*||X-X_r||^2 =  963197.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36220.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36220.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999311.0 e-6; = (1/var)*||X-X_r||^2 =  999310.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962871.8 e-6; = (1/var)*||X-X_r||^2 =  962871.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36439.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36439.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999423.3 e-6; = (1/var)*||X-X_r||^2 =  999423.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962985.9 e-6; = (1/var)*||X-X_r||^2 =  962985.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36437.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36437.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999211.9 e-6; = (1/var)*||X-X_r||^2 =  999211.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963207.7 e-6; = (1/var)*||X-X_r||^2 =  963207.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36004.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36004.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999479.2 e-6; = (1/var)*||X-X_r||^2 =  999479.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963174.5 e-6; = (1/var)*||X-X_r||^2 =  963174.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36304.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36304.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999205.7 e-6; = (1/var)*||X-X_r||^2 =  999205.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963004.7 e-6; = (1/var)*||X-X_r||^2 =  963004.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36201.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36201.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999242.0 e-6; = (1/var)*||X-X_r||^2 =  999241.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963016.7 e-6; = (1/var)*||X-X_r||^2 =  963016.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36225.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36225.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999411.0 e-6; = (1/var)*||X-X_r||^2 =  999410.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962872.1 e-6; = (1/var)*||X-X_r||^2 =  962871.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36538.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36538.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999270.3 e-6; = (1/var)*||X-X_r||^2 =  999270.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962925.7 e-6; = (1/var)*||X-X_r||^2 =  962925.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36344.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36344.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999209.4 e-6; = (1/var)*||X-X_r||^2 =  999209.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962906.9 e-6; = (1/var)*||X-X_r||^2 =  962906.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  956491.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36302.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36302.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [04:16:31 03.01.2023] 231) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 10) change_channel_size_across_layers = False:
Total training time is = 0:0:28 h/m/s. 

--------------------------------------------------- 

Started [04:16:31 03.01.2023] 232) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 10) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(1024, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 11645 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.10
1                           encoder.sequential_convs.conv2d_2.weight                      1048             9.00
2                           encoder.sequential_convs.conv2d_3.weight                      1048             9.00
3                           encoder.sequential_convs.conv2d_4.weight                      1048             9.00
4                           encoder.sequential_convs.conv2d_5.weight                      1048             9.00
5                           encoder.sequential_convs.conv2d_6.weight                      1048             9.00
6                                  encoder.pre_residual_stack.weight                       589             5.06
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
11                             encoder.channel_adjusting_conv.weight                        16             0.14
12                                                       VQ.E.weight                        65             0.56
13                             decoder.channel_adjusting_conv.weight                       147             1.26
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
18                    decoder.sequential_trans_convs.conv2d_1.weight                      1048             9.00
19                    decoder.sequential_trans_convs.conv2d_2.weight                      1048             9.00
20                    decoder.sequential_trans_convs.conv2d_3.weight                      1048             9.00
21                    decoder.sequential_trans_convs.conv2d_4.weight                      1048             9.00
22                    decoder.sequential_trans_convs.conv2d_5.weight                      1048             9.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                        12             0.10

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.62; perplexity/K = 0.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.62; perplexity/K = 0.16%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1223882.1 e-6; = (1/var)*||X-X_r||^2 =  942621.2 e-6 = 77.0 %; (1+beta)*||Z_e-Z_q||^2 =  281260.9 e-6 = 23.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1102015.0 e-6; = (1/var)*||X-X_r||^2 =  911669.9 e-6 = 82.7 %; (1+beta)*||Z_e-Z_q||^2 =  190345.1 e-6 = 17.3 %)
Min.  Avg. Train Loss across Mini-Batch =  999915.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  940602.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -121867.1 e-6; = (1/var)*||X-X_r||^2 val-train = -30951.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -90915.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 0.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.87; perplexity/K = 0.18%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  972250.9 e-6; = (1/var)*||X-X_r||^2 =  904627.8 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  67623.1 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  949102.0 e-6; = (1/var)*||X-X_r||^2 =  904319.4 e-6 = 95.3 %; (1+beta)*||Z_e-Z_q||^2 =  44782.5 e-6 = 4.7 %)
Min.  Avg. Train Loss across Mini-Batch =  957282.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903143.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23149.0 e-6; = (1/var)*||X-X_r||^2 val-train = -308.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22840.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.11; perplexity/K = 0.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.12; perplexity/K = 0.50%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  920042.5 e-6; = (1/var)*||X-X_r||^2 =  836201.3 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  83841.3 e-6 = 9.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  885098.8 e-6; = (1/var)*||X-X_r||^2 =  807025.4 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  78073.4 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  920042.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  883620.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34943.8 e-6; = (1/var)*||X-X_r||^2 val-train = -29175.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5767.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.24; perplexity/K = 0.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.69; perplexity/K = 0.16%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  915990.9 e-6; = (1/var)*||X-X_r||^2 =  853890.1 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  62100.8 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  894314.2 e-6; = (1/var)*||X-X_r||^2 =  847786.7 e-6 = 94.8 %; (1+beta)*||Z_e-Z_q||^2 =  46527.4 e-6 = 5.2 %)
Min.  Avg. Train Loss across Mini-Batch =  899816.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  842062.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21676.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6103.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15573.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 0.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 0.11%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  911388.8 e-6; = (1/var)*||X-X_r||^2 =  904786.0 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6602.8 e-6 = 0.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  881842.6 e-6; = (1/var)*||X-X_r||^2 =  876008.6 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  5834.1 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -29546.2 e-6; = (1/var)*||X-X_r||^2 val-train = -28777.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -768.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  959472.8 e-6; = (1/var)*||X-X_r||^2 =  958922.8 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  550.1 e-6 = 0.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  933370.8 e-6; = (1/var)*||X-X_r||^2 =  932956.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  414.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -26102.1 e-6; = (1/var)*||X-X_r||^2 val-train = -25966.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -135.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 0.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.10%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  923210.3 e-6; = (1/var)*||X-X_r||^2 =  915159.6 e-6 = 99.1 %; (1+beta)*||Z_e-Z_q||^2 =  8050.7 e-6 = 0.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  893467.4 e-6; = (1/var)*||X-X_r||^2 =  886335.9 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  7131.4 e-6 = 0.8 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -29743.0 e-6; = (1/var)*||X-X_r||^2 val-train = -28823.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -919.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  952299.3 e-6; = (1/var)*||X-X_r||^2 =  951920.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  379.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  949364.2 e-6; = (1/var)*||X-X_r||^2 =  949199.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  164.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2935.2 e-6; = (1/var)*||X-X_r||^2 val-train = -2720.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -214.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999505.9 e-6; = (1/var)*||X-X_r||^2 =  999484.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  21.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962863.6 e-6; = (1/var)*||X-X_r||^2 =  962839.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  24.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36642.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36644.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999415.9 e-6; = (1/var)*||X-X_r||^2 =  999412.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963216.6 e-6; = (1/var)*||X-X_r||^2 =  963213.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36199.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36198.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999618.5 e-6; = (1/var)*||X-X_r||^2 =  999615.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963148.0 e-6; = (1/var)*||X-X_r||^2 =  963142.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36470.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36473.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999403.9 e-6; = (1/var)*||X-X_r||^2 =  999401.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962899.9 e-6; = (1/var)*||X-X_r||^2 =  962897.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36504.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36504.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999358.8 e-6; = (1/var)*||X-X_r||^2 =  999356.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963162.2 e-6; = (1/var)*||X-X_r||^2 =  963159.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36196.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36196.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:36:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999217.2 e-6; = (1/var)*||X-X_r||^2 =  999214.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963038.1 e-6; = (1/var)*||X-X_r||^2 =  963035.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36179.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36179.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:43:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999223.6 e-6; = (1/var)*||X-X_r||^2 =  999222.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963054.6 e-6; = (1/var)*||X-X_r||^2 =  963053.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36169.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36168.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:50:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999231.1 e-6; = (1/var)*||X-X_r||^2 =  999229.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963093.7 e-6; = (1/var)*||X-X_r||^2 =  963092.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36137.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36137.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:57:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999328.4 e-6; = (1/var)*||X-X_r||^2 =  999327.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963035.8 e-6; = (1/var)*||X-X_r||^2 =  963035.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36292.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36292.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999257.5 e-6; = (1/var)*||X-X_r||^2 =  999256.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963373.2 e-6; = (1/var)*||X-X_r||^2 =  963372.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35884.2 e-6; = (1/var)*||X-X_r||^2 val-train = -35884.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999319.3 e-6; = (1/var)*||X-X_r||^2 =  999318.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963425.4 e-6; = (1/var)*||X-X_r||^2 =  963424.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35893.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35893.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.10%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:17:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999225.2 e-6; = (1/var)*||X-X_r||^2 =  999224.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962953.0 e-6; = (1/var)*||X-X_r||^2 =  962952.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  856661.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831535.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36272.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36272.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

Finished [06:35:10 03.01.2023] 232) Finished running for K = 1024 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 10) change_channel_size_across_layers = False:
Total training time is = 0:0:38 h/m/s. 

--------------------------------------------------- 

Started [06:35:10 03.01.2023] 233) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 576) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 748 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.13
1                           encoder.sequential_convs.conv2d_2.weight                        32             4.28
2                           encoder.sequential_convs.conv2d_3.weight                       131            17.51
3                                  encoder.pre_residual_stack.weight                       147            19.65
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.81
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.81
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
8                              encoder.channel_adjusting_conv.weight                         8             1.07
9                                                        VQ.E.weight                        32             4.28
10                             decoder.channel_adjusting_conv.weight                        73             9.76
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.81
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.81
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.51
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.28
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.13; perplexity/K = 5.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.18; perplexity/K = 5.11%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  349096.3 e-6; = (1/var)*||X-X_r||^2 =  207293.4 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  141802.9 e-6 = 40.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  350143.5 e-6; = (1/var)*||X-X_r||^2 =  215520.9 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  134622.7 e-6 = 38.4 %)
Min.  Avg. Train Loss across Mini-Batch =  349096.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  339201.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1047.2 e-6; = (1/var)*||X-X_r||^2 val-train = 8227.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7180.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.26; perplexity/K = 5.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.08; perplexity/K = 5.48%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  135240.3 e-6; = (1/var)*||X-X_r||^2 =  98826.0 e-6 = 73.1 %; (1+beta)*||Z_e-Z_q||^2 =  36414.3 e-6 = 26.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  136648.6 e-6; = (1/var)*||X-X_r||^2 =  101489.0 e-6 = 74.3 %; (1+beta)*||Z_e-Z_q||^2 =  35159.6 e-6 = 25.7 %)
Min.  Avg. Train Loss across Mini-Batch =  135240.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  136648.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1408.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2663.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1254.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.82; perplexity/K = 3.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.29; perplexity/K = 3.38%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  133907.6 e-6; = (1/var)*||X-X_r||^2 =  107691.7 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  26215.9 e-6 = 19.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  130725.4 e-6; = (1/var)*||X-X_r||^2 =  104404.9 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  26320.5 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  69544.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72189.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3182.2 e-6; = (1/var)*||X-X_r||^2 val-train = -3286.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 104.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.38; perplexity/K = 4.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.24; perplexity/K = 4.93%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34808.7 e-6; = (1/var)*||X-X_r||^2 =  25884.5 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  8924.2 e-6 = 25.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  37510.7 e-6; = (1/var)*||X-X_r||^2 =  28029.3 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  9481.4 e-6 = 25.3 %)
Min.  Avg. Train Loss across Mini-Batch =  34808.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37510.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2702.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2144.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 557.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.47; perplexity/K = 4.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.36; perplexity/K = 4.37%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15404.6 e-6; = (1/var)*||X-X_r||^2 =  9490.9 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  5913.7 e-6 = 38.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  17502.1 e-6; = (1/var)*||X-X_r||^2 =  11267.9 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  6234.3 e-6 = 35.6 %)
Min.  Avg. Train Loss across Mini-Batch =  15404.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17125.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2097.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1777.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 320.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.25; perplexity/K = 4.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.89; perplexity/K = 3.89%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8631.8 e-6; = (1/var)*||X-X_r||^2 =  4513.1 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  4118.7 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  9740.5 e-6; = (1/var)*||X-X_r||^2 =  5492.9 e-6 = 56.4 %; (1+beta)*||Z_e-Z_q||^2 =  4247.6 e-6 = 43.6 %)
Min.  Avg. Train Loss across Mini-Batch =  8631.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9740.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1108.7 e-6; = (1/var)*||X-X_r||^2 val-train = 979.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 128.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.11; perplexity/K = 3.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.97; perplexity/K = 3.70%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6065.2 e-6; = (1/var)*||X-X_r||^2 =  2774.3 e-6 = 45.7 %; (1+beta)*||Z_e-Z_q||^2 =  3290.9 e-6 = 54.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  6959.1 e-6; = (1/var)*||X-X_r||^2 =  3524.7 e-6 = 50.6 %; (1+beta)*||Z_e-Z_q||^2 =  3434.4 e-6 = 49.4 %)
Min.  Avg. Train Loss across Mini-Batch =  5945.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6836.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   893.9 e-6; = (1/var)*||X-X_r||^2 val-train = 750.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 143.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.51; perplexity/K = 3.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.90; perplexity/K = 3.30%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8617.2 e-6; = (1/var)*||X-X_r||^2 =  4187.3 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  4429.9 e-6 = 51.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  8382.8 e-6; = (1/var)*||X-X_r||^2 =  4274.8 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  4108.0 e-6 = 49.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4911.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5810.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -234.4 e-6; = (1/var)*||X-X_r||^2 val-train = 87.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -321.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.91; perplexity/K = 3.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.51; perplexity/K = 3.81%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4215.5 e-6; = (1/var)*||X-X_r||^2 =  1630.0 e-6 = 38.7 %; (1+beta)*||Z_e-Z_q||^2 =  2585.5 e-6 = 61.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4905.5 e-6; = (1/var)*||X-X_r||^2 =  2258.4 e-6 = 46.0 %; (1+beta)*||Z_e-Z_q||^2 =  2647.1 e-6 = 54.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4001.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4842.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   689.9 e-6; = (1/var)*||X-X_r||^2 val-train = 628.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 61.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.48; perplexity/K = 3.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.45; perplexity/K = 3.02%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4027.9 e-6; = (1/var)*||X-X_r||^2 =  1564.6 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  2463.3 e-6 = 61.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4389.0 e-6; = (1/var)*||X-X_r||^2 =  1926.9 e-6 = 43.9 %; (1+beta)*||Z_e-Z_q||^2 =  2462.1 e-6 = 56.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3694.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4323.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   361.1 e-6; = (1/var)*||X-X_r||^2 val-train = 362.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.46; perplexity/K = 3.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.05; perplexity/K = 3.33%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3804.6 e-6; = (1/var)*||X-X_r||^2 =  1610.5 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  2194.1 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4279.6 e-6; = (1/var)*||X-X_r||^2 =  2033.9 e-6 = 47.5 %; (1+beta)*||Z_e-Z_q||^2 =  2245.7 e-6 = 52.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3388.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3940.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   475.1 e-6; = (1/var)*||X-X_r||^2 val-train = 423.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 51.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.79; perplexity/K = 3.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.48; perplexity/K = 3.61%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3294.1 e-6; = (1/var)*||X-X_r||^2 =  1266.2 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  2027.9 e-6 = 61.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3592.0 e-6; = (1/var)*||X-X_r||^2 =  1620.5 e-6 = 45.1 %; (1+beta)*||Z_e-Z_q||^2 =  1971.4 e-6 = 54.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2966.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3512.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   297.9 e-6; = (1/var)*||X-X_r||^2 val-train = 354.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -56.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.36; perplexity/K = 2.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.42; perplexity/K = 2.62%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2749.8 e-6; = (1/var)*||X-X_r||^2 =  986.5 e-6 = 35.9 %; (1+beta)*||Z_e-Z_q||^2 =  1763.3 e-6 = 64.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3427.0 e-6; = (1/var)*||X-X_r||^2 =  1598.3 e-6 = 46.6 %; (1+beta)*||Z_e-Z_q||^2 =  1828.7 e-6 = 53.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2712.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3196.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   677.2 e-6; = (1/var)*||X-X_r||^2 val-train = 611.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 65.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.55; perplexity/K = 2.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.38; perplexity/K = 2.81%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2636.7 e-6; = (1/var)*||X-X_r||^2 =  924.9 e-6 = 35.1 %; (1+beta)*||Z_e-Z_q||^2 =  1711.8 e-6 = 64.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3045.8 e-6; = (1/var)*||X-X_r||^2 =  1305.4 e-6 = 42.9 %; (1+beta)*||Z_e-Z_q||^2 =  1740.3 e-6 = 57.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2463.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3043.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   409.1 e-6; = (1/var)*||X-X_r||^2 val-train = 380.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.05; perplexity/K = 3.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.44; perplexity/K = 3.01%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2476.8 e-6; = (1/var)*||X-X_r||^2 =  759.9 e-6 = 30.7 %; (1+beta)*||Z_e-Z_q||^2 =  1716.8 e-6 = 69.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3595.0 e-6; = (1/var)*||X-X_r||^2 =  1671.9 e-6 = 46.5 %; (1+beta)*||Z_e-Z_q||^2 =  1923.1 e-6 = 53.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2358.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2837.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1118.2 e-6; = (1/var)*||X-X_r||^2 val-train = 912.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 206.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.87; perplexity/K = 2.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.36; perplexity/K = 2.61%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2377.1 e-6; = (1/var)*||X-X_r||^2 =  745.8 e-6 = 31.4 %; (1+beta)*||Z_e-Z_q||^2 =  1631.3 e-6 = 68.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2815.4 e-6; = (1/var)*||X-X_r||^2 =  1224.1 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  1591.3 e-6 = 56.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2175.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2680.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   438.3 e-6; = (1/var)*||X-X_r||^2 val-train = 478.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -40.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.84; perplexity/K = 2.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.70; perplexity/K = 2.87%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2277.6 e-6; = (1/var)*||X-X_r||^2 =  740.4 e-6 = 32.5 %; (1+beta)*||Z_e-Z_q||^2 =  1537.2 e-6 = 67.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2688.8 e-6; = (1/var)*||X-X_r||^2 =  1041.8 e-6 = 38.7 %; (1+beta)*||Z_e-Z_q||^2 =  1647.0 e-6 = 61.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2016.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2516.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   411.2 e-6; = (1/var)*||X-X_r||^2 val-train = 301.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 109.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.64; perplexity/K = 2.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.53; perplexity/K = 2.64%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2105.5 e-6; = (1/var)*||X-X_r||^2 =  706.8 e-6 = 33.6 %; (1+beta)*||Z_e-Z_q||^2 =  1398.7 e-6 = 66.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2569.8 e-6; = (1/var)*||X-X_r||^2 =  1077.7 e-6 = 41.9 %; (1+beta)*||Z_e-Z_q||^2 =  1492.1 e-6 = 58.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1949.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2326.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   464.2 e-6; = (1/var)*||X-X_r||^2 val-train = 370.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 93.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.91; perplexity/K = 2.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.97; perplexity/K = 2.73%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2118.9 e-6; = (1/var)*||X-X_r||^2 =  692.9 e-6 = 32.7 %; (1+beta)*||Z_e-Z_q||^2 =  1426.0 e-6 = 67.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2559.5 e-6; = (1/var)*||X-X_r||^2 =  1031.7 e-6 = 40.3 %; (1+beta)*||Z_e-Z_q||^2 =  1527.8 e-6 = 59.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1886.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2287.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   440.6 e-6; = (1/var)*||X-X_r||^2 val-train = 338.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 101.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.35; perplexity/K = 2.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.55; perplexity/K = 2.45%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1851.2 e-6; = (1/var)*||X-X_r||^2 =  453.8 e-6 = 24.5 %; (1+beta)*||Z_e-Z_q||^2 =  1397.4 e-6 = 75.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2308.1 e-6; = (1/var)*||X-X_r||^2 =  893.3 e-6 = 38.7 %; (1+beta)*||Z_e-Z_q||^2 =  1414.8 e-6 = 61.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1777.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2198.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   456.9 e-6; = (1/var)*||X-X_r||^2 val-train = 439.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.4 e-6 

----------------------------------------------------------------------------------

Finished [07:23:49 03.01.2023] 233) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 576) change_channel_size_across_layers = True:
Total training time is = 0:7:38 h/m/s. 

--------------------------------------------------- 

Started [07:23:49 03.01.2023] 234) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 576) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2424 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.40
2                           encoder.sequential_convs.conv2d_3.weight                       524            21.62
3                                  encoder.pre_residual_stack.weight                       589            24.30
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.01
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.01
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
8                              encoder.channel_adjusting_conv.weight                        16             0.66
9                                                        VQ.E.weight                        32             1.32
10                             decoder.channel_adjusting_conv.weight                       147             6.06
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.01
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.01
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.62
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.40
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.67; perplexity/K = 3.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.09; perplexity/K = 3.14%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  216052.2 e-6; = (1/var)*||X-X_r||^2 =  114558.0 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  101494.2 e-6 = 47.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  217869.5 e-6; = (1/var)*||X-X_r||^2 =  120663.0 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  97206.5 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  216052.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  217869.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1817.3 e-6; = (1/var)*||X-X_r||^2 val-train = 6105.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4287.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.49; perplexity/K = 3.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.61; perplexity/K = 2.85%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  76007.5 e-6; = (1/var)*||X-X_r||^2 =  38558.4 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  37449.1 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  79906.8 e-6; = (1/var)*||X-X_r||^2 =  42133.7 e-6 = 52.7 %; (1+beta)*||Z_e-Z_q||^2 =  37773.2 e-6 = 47.3 %)
Min.  Avg. Train Loss across Mini-Batch =  76007.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  79658.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3899.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3575.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 324.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.73; perplexity/K = 2.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.39; perplexity/K = 2.03%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24430.9 e-6; = (1/var)*||X-X_r||^2 =  9528.6 e-6 = 39.0 %; (1+beta)*||Z_e-Z_q||^2 =  14902.3 e-6 = 61.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  27458.7 e-6; = (1/var)*||X-X_r||^2 =  11928.2 e-6 = 43.4 %; (1+beta)*||Z_e-Z_q||^2 =  15530.5 e-6 = 56.6 %)
Min.  Avg. Train Loss across Mini-Batch =  24430.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  27030.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3027.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2399.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 628.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.56; perplexity/K = 2.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.29; perplexity/K = 2.21%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17396.0 e-6; = (1/var)*||X-X_r||^2 =  6646.8 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  10749.2 e-6 = 61.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  17128.2 e-6; = (1/var)*||X-X_r||^2 =  7128.9 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  9999.3 e-6 = 58.4 %)
Min.  Avg. Train Loss across Mini-Batch =  12182.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14123.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -267.8 e-6; = (1/var)*||X-X_r||^2 val-train = 482.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -749.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.82; perplexity/K = 2.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.04; perplexity/K = 2.35%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5988.5 e-6; = (1/var)*||X-X_r||^2 =  2059.6 e-6 = 34.4 %; (1+beta)*||Z_e-Z_q||^2 =  3928.9 e-6 = 65.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  7491.6 e-6; = (1/var)*||X-X_r||^2 =  3165.1 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  4326.4 e-6 = 57.8 %)
Min.  Avg. Train Loss across Mini-Batch =  5988.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7415.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1503.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1105.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 397.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.32; perplexity/K = 2.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.67; perplexity/K = 2.86%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7208.0 e-6; = (1/var)*||X-X_r||^2 =  2410.3 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  4797.7 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  8107.9 e-6; = (1/var)*||X-X_r||^2 =  3346.5 e-6 = 41.3 %; (1+beta)*||Z_e-Z_q||^2 =  4761.4 e-6 = 58.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3628.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4768.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   899.9 e-6; = (1/var)*||X-X_r||^2 val-train = 936.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.62; perplexity/K = 3.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.93; perplexity/K = 3.31%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4567.1 e-6; = (1/var)*||X-X_r||^2 =  1343.1 e-6 = 29.4 %; (1+beta)*||Z_e-Z_q||^2 =  3223.9 e-6 = 70.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  17149.3 e-6; = (1/var)*||X-X_r||^2 =  10858.2 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  6291.0 e-6 = 36.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3372.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4422.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12582.2 e-6; = (1/var)*||X-X_r||^2 val-train = 9515.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3067.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.50; perplexity/K = 2.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.52; perplexity/K = 2.84%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2661.1 e-6; = (1/var)*||X-X_r||^2 =  806.2 e-6 = 30.3 %; (1+beta)*||Z_e-Z_q||^2 =  1854.9 e-6 = 69.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  3236.3 e-6; = (1/var)*||X-X_r||^2 =  1335.3 e-6 = 41.3 %; (1+beta)*||Z_e-Z_q||^2 =  1901.0 e-6 = 58.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2661.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3236.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   575.2 e-6; = (1/var)*||X-X_r||^2 val-train = 529.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 46.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.13; perplexity/K = 2.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.19; perplexity/K = 2.97%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2872.3 e-6; = (1/var)*||X-X_r||^2 =  674.0 e-6 = 23.5 %; (1+beta)*||Z_e-Z_q||^2 =  2198.4 e-6 = 76.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3531.2 e-6; = (1/var)*||X-X_r||^2 =  1197.7 e-6 = 33.9 %; (1+beta)*||Z_e-Z_q||^2 =  2333.5 e-6 = 66.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2000.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2577.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   658.8 e-6; = (1/var)*||X-X_r||^2 val-train = 523.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 135.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.41; perplexity/K = 3.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.11; perplexity/K = 2.95%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4897.6 e-6; = (1/var)*||X-X_r||^2 =  1452.5 e-6 = 29.7 %; (1+beta)*||Z_e-Z_q||^2 =  3445.0 e-6 = 70.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5870.6 e-6; = (1/var)*||X-X_r||^2 =  2313.4 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  3557.2 e-6 = 60.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1778.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2234.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   973.0 e-6; = (1/var)*||X-X_r||^2 val-train = 860.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 112.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.21; perplexity/K = 2.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.86; perplexity/K = 2.51%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2479.2 e-6; = (1/var)*||X-X_r||^2 =  1009.5 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  1469.7 e-6 = 59.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2406.3 e-6; = (1/var)*||X-X_r||^2 =  1004.9 e-6 = 41.8 %; (1+beta)*||Z_e-Z_q||^2 =  1401.4 e-6 = 58.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1778.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2234.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -73.0 e-6; = (1/var)*||X-X_r||^2 val-train = -4.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -68.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.89; perplexity/K = 2.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.98; perplexity/K = 2.15%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1410.6 e-6; = (1/var)*||X-X_r||^2 =  344.8 e-6 = 24.4 %; (1+beta)*||Z_e-Z_q||^2 =  1065.8 e-6 = 75.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1785.4 e-6; = (1/var)*||X-X_r||^2 =  697.1 e-6 = 39.0 %; (1+beta)*||Z_e-Z_q||^2 =  1088.2 e-6 = 61.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1181.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1595.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   374.8 e-6; = (1/var)*||X-X_r||^2 val-train = 352.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.12; perplexity/K = 2.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.32; perplexity/K = 2.99%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1698.1 e-6; = (1/var)*||X-X_r||^2 =  363.3 e-6 = 21.4 %; (1+beta)*||Z_e-Z_q||^2 =  1334.8 e-6 = 78.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1970.2 e-6; = (1/var)*||X-X_r||^2 =  714.8 e-6 = 36.3 %; (1+beta)*||Z_e-Z_q||^2 =  1255.4 e-6 = 63.7 %)
Min.  Avg. Train Loss across Mini-Batch =  979.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1293.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   272.1 e-6; = (1/var)*||X-X_r||^2 val-train = 351.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -79.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.22; perplexity/K = 2.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 2.37%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  854.3 e-6; = (1/var)*||X-X_r||^2 =  191.9 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  662.5 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1289.3 e-6; = (1/var)*||X-X_r||^2 =  553.3 e-6 = 42.9 %; (1+beta)*||Z_e-Z_q||^2 =  736.1 e-6 = 57.1 %)
Min.  Avg. Train Loss across Mini-Batch =  854.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1172.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   435.0 e-6; = (1/var)*||X-X_r||^2 val-train = 361.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 73.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.14; perplexity/K = 4.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.08; perplexity/K = 4.90%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4484.0 e-6; = (1/var)*||X-X_r||^2 =  2173.6 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  2310.4 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5265.3 e-6; = (1/var)*||X-X_r||^2 =  2904.0 e-6 = 55.2 %; (1+beta)*||Z_e-Z_q||^2 =  2361.3 e-6 = 44.8 %)
Min.  Avg. Train Loss across Mini-Batch =  774.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1045.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   781.3 e-6; = (1/var)*||X-X_r||^2 val-train = 730.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 50.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.61; perplexity/K = 3.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.47; perplexity/K = 4.00%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1864.9 e-6; = (1/var)*||X-X_r||^2 =  733.1 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  1131.8 e-6 = 60.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2261.5 e-6; = (1/var)*||X-X_r||^2 =  1158.7 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  1102.7 e-6 = 48.8 %)
Min.  Avg. Train Loss across Mini-Batch =  774.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1045.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   396.5 e-6; = (1/var)*||X-X_r||^2 val-train = 425.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -29.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.44; perplexity/K = 2.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.14; perplexity/K = 2.57%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1020.5 e-6; = (1/var)*||X-X_r||^2 =  408.5 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  612.0 e-6 = 60.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1439.6 e-6; = (1/var)*||X-X_r||^2 =  822.8 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  616.8 e-6 = 42.8 %)
Min.  Avg. Train Loss across Mini-Batch =  774.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1045.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   419.1 e-6; = (1/var)*||X-X_r||^2 val-train = 414.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.73; perplexity/K = 2.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.55; perplexity/K = 3.04%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2703.1 e-6; = (1/var)*||X-X_r||^2 =  897.7 e-6 = 33.2 %; (1+beta)*||Z_e-Z_q||^2 =  1805.4 e-6 = 66.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3098.1 e-6; = (1/var)*||X-X_r||^2 =  1309.0 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  1789.1 e-6 = 57.7 %)
Min.  Avg. Train Loss across Mini-Batch =  565.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  900.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   395.0 e-6; = (1/var)*||X-X_r||^2 val-train = 411.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -16.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.18; perplexity/K = 2.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.71; perplexity/K = 2.68%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  635.6 e-6; = (1/var)*||X-X_r||^2 =  205.1 e-6 = 32.3 %; (1+beta)*||Z_e-Z_q||^2 =  430.6 e-6 = 67.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  916.6 e-6; = (1/var)*||X-X_r||^2 =  480.0 e-6 = 52.4 %; (1+beta)*||Z_e-Z_q||^2 =  436.6 e-6 = 47.6 %)
Min.  Avg. Train Loss across Mini-Batch =  565.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  900.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   281.0 e-6; = (1/var)*||X-X_r||^2 val-train = 274.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.05; perplexity/K = 2.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.56; perplexity/K = 2.26%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  740.0 e-6; = (1/var)*||X-X_r||^2 =  196.6 e-6 = 26.6 %; (1+beta)*||Z_e-Z_q||^2 =  543.4 e-6 = 73.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1080.7 e-6; = (1/var)*||X-X_r||^2 =  529.6 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  551.1 e-6 = 51.0 %)
Min.  Avg. Train Loss across Mini-Batch =  370.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  675.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   340.7 e-6; = (1/var)*||X-X_r||^2 val-train = 332.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.7 e-6 

----------------------------------------------------------------------------------

Finished [08:29:46 03.01.2023] 234) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 576) change_channel_size_across_layers = True:
Total training time is = 0:7:57 h/m/s. 

--------------------------------------------------- 

Started [08:29:46 03.01.2023] 235) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 576) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1480 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.41
1                           encoder.sequential_convs.conv2d_2.weight                       262            17.70
2                           encoder.sequential_convs.conv2d_3.weight                       262            17.70
3                                  encoder.pre_residual_stack.weight                       147             9.93
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.43
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.43
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
8                              encoder.channel_adjusting_conv.weight                         8             0.54
9                                                        VQ.E.weight                        32             2.16
10                             decoder.channel_adjusting_conv.weight                        73             4.93
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.43
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.43
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            17.70
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            17.70
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.41

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.25; perplexity/K = 4.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.51; perplexity/K = 4.01%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  171820.1 e-6; = (1/var)*||X-X_r||^2 =  107120.9 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  64699.2 e-6 = 37.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  173396.3 e-6; = (1/var)*||X-X_r||^2 =  111092.2 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  62304.0 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  171820.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  173396.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1576.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3971.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2395.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.32; perplexity/K = 3.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.34; perplexity/K = 3.19%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40225.0 e-6; = (1/var)*||X-X_r||^2 =  25670.8 e-6 = 63.8 %; (1+beta)*||Z_e-Z_q||^2 =  14554.2 e-6 = 36.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  43200.6 e-6; = (1/var)*||X-X_r||^2 =  29093.6 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  14106.9 e-6 = 32.7 %)
Min.  Avg. Train Loss across Mini-Batch =  40183.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43200.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2975.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3422.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -447.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.41; perplexity/K = 2.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.61; perplexity/K = 2.66%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22365.0 e-6; = (1/var)*||X-X_r||^2 =  11875.6 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  10489.4 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  24733.7 e-6; = (1/var)*||X-X_r||^2 =  14372.3 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  10361.4 e-6 = 41.9 %)
Min.  Avg. Train Loss across Mini-Batch =  22365.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24733.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2368.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2496.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -128.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.68; perplexity/K = 2.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.00; perplexity/K = 1.76%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15569.3 e-6; = (1/var)*||X-X_r||^2 =  9125.5 e-6 = 58.6 %; (1+beta)*||Z_e-Z_q||^2 =  6443.8 e-6 = 41.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  14611.8 e-6; = (1/var)*||X-X_r||^2 =  9017.2 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  5594.6 e-6 = 38.3 %)
Min.  Avg. Train Loss across Mini-Batch =  7481.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8869.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -957.5 e-6; = (1/var)*||X-X_r||^2 val-train = -108.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -849.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.66; perplexity/K = 2.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.78; perplexity/K = 2.30%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  64005.4 e-6; = (1/var)*||X-X_r||^2 =  28386.5 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  35618.9 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  23497.0 e-6; = (1/var)*||X-X_r||^2 =  10648.6 e-6 = 45.3 %; (1+beta)*||Z_e-Z_q||^2 =  12848.4 e-6 = 54.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4365.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5251.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -40508.4 e-6; = (1/var)*||X-X_r||^2 val-train = -17737.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22770.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.96; perplexity/K = 2.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 2.37%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2309.0 e-6; = (1/var)*||X-X_r||^2 =  1143.1 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  1165.9 e-6 = 50.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3526.6 e-6; = (1/var)*||X-X_r||^2 =  2150.9 e-6 = 61.0 %; (1+beta)*||Z_e-Z_q||^2 =  1375.6 e-6 = 39.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2277.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2928.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1217.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1007.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 209.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.52; perplexity/K = 2.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.07; perplexity/K = 2.36%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4406.7 e-6; = (1/var)*||X-X_r||^2 =  1453.9 e-6 = 33.0 %; (1+beta)*||Z_e-Z_q||^2 =  2952.8 e-6 = 67.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4910.9 e-6; = (1/var)*||X-X_r||^2 =  2094.0 e-6 = 42.6 %; (1+beta)*||Z_e-Z_q||^2 =  2816.9 e-6 = 57.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2206.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2831.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   504.2 e-6; = (1/var)*||X-X_r||^2 val-train = 640.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -135.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.05; perplexity/K = 1.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.57; perplexity/K = 2.06%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1458.4 e-6; = (1/var)*||X-X_r||^2 =  751.4 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  707.0 e-6 = 48.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1773.4 e-6; = (1/var)*||X-X_r||^2 =  1040.8 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  732.6 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1342.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1729.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   314.9 e-6; = (1/var)*||X-X_r||^2 val-train = 289.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.41; perplexity/K = 1.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.88; perplexity/K = 1.93%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1421.0 e-6; = (1/var)*||X-X_r||^2 =  771.8 e-6 = 54.3 %; (1+beta)*||Z_e-Z_q||^2 =  649.2 e-6 = 45.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2515.7 e-6; = (1/var)*||X-X_r||^2 =  1820.3 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  695.4 e-6 = 27.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1096.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1470.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1094.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1048.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 46.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.48; perplexity/K = 3.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.23; perplexity/K = 2.97%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21366.5 e-6; = (1/var)*||X-X_r||^2 =  14040.5 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  7326.0 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  24042.5 e-6; = (1/var)*||X-X_r||^2 =  16433.7 e-6 = 68.4 %; (1+beta)*||Z_e-Z_q||^2 =  7608.8 e-6 = 31.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2676.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2393.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 282.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.25; perplexity/K = 3.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.40; perplexity/K = 3.40%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9771.7 e-6; = (1/var)*||X-X_r||^2 =  5959.5 e-6 = 61.0 %; (1+beta)*||Z_e-Z_q||^2 =  3812.3 e-6 = 39.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  11144.6 e-6; = (1/var)*||X-X_r||^2 =  7351.6 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  3793.0 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1372.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1392.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.14; perplexity/K = 2.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.36; perplexity/K = 2.61%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5928.1 e-6; = (1/var)*||X-X_r||^2 =  4218.4 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  1709.7 e-6 = 28.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  35737.1 e-6; = (1/var)*||X-X_r||^2 =  32781.0 e-6 = 91.7 %; (1+beta)*||Z_e-Z_q||^2 =  2956.1 e-6 = 8.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29808.9 e-6; = (1/var)*||X-X_r||^2 val-train = 28562.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1246.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 2.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.35; perplexity/K = 2.22%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3279.8 e-6; = (1/var)*||X-X_r||^2 =  2008.3 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  1271.5 e-6 = 38.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4050.4 e-6; = (1/var)*||X-X_r||^2 =  2743.4 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  1307.0 e-6 = 32.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   770.6 e-6; = (1/var)*||X-X_r||^2 val-train = 735.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.60; perplexity/K = 1.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.15; perplexity/K = 1.79%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2503.5 e-6; = (1/var)*||X-X_r||^2 =  1543.6 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  959.9 e-6 = 38.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3326.3 e-6; = (1/var)*||X-X_r||^2 =  2305.0 e-6 = 69.3 %; (1+beta)*||Z_e-Z_q||^2 =  1021.3 e-6 = 30.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   822.8 e-6; = (1/var)*||X-X_r||^2 val-train = 761.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 61.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.98; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.07; perplexity/K = 1.97%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2230.7 e-6; = (1/var)*||X-X_r||^2 =  1304.9 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  925.8 e-6 = 41.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3168.7 e-6; = (1/var)*||X-X_r||^2 =  2140.7 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  1028.0 e-6 = 32.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   938.0 e-6; = (1/var)*||X-X_r||^2 val-train = 835.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.14; perplexity/K = 2.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.88; perplexity/K = 2.71%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2040.7 e-6; = (1/var)*||X-X_r||^2 =  1122.2 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  918.5 e-6 = 45.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2692.6 e-6; = (1/var)*||X-X_r||^2 =  1739.2 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  953.4 e-6 = 35.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   652.0 e-6; = (1/var)*||X-X_r||^2 val-train = 617.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.21; perplexity/K = 2.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.23; perplexity/K = 2.58%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1933.7 e-6; = (1/var)*||X-X_r||^2 =  1053.0 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  880.7 e-6 = 45.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2557.6 e-6; = (1/var)*||X-X_r||^2 =  1607.8 e-6 = 62.9 %; (1+beta)*||Z_e-Z_q||^2 =  949.8 e-6 = 37.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   623.9 e-6; = (1/var)*||X-X_r||^2 val-train = 554.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 69.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.25; perplexity/K = 2.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 2.15%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1689.3 e-6; = (1/var)*||X-X_r||^2 =  768.3 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  921.0 e-6 = 54.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2380.0 e-6; = (1/var)*||X-X_r||^2 =  1382.8 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  997.2 e-6 = 41.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   690.7 e-6; = (1/var)*||X-X_r||^2 val-train = 614.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 76.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 2.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 2.39%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1397.7 e-6; = (1/var)*||X-X_r||^2 =  672.6 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  725.0 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2106.1 e-6; = (1/var)*||X-X_r||^2 =  1369.7 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  736.4 e-6 = 35.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   708.4 e-6; = (1/var)*||X-X_r||^2 val-train = 697.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.22; perplexity/K = 2.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.84; perplexity/K = 2.31%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1195.5 e-6; = (1/var)*||X-X_r||^2 =  631.9 e-6 = 52.9 %; (1+beta)*||Z_e-Z_q||^2 =  563.5 e-6 = 47.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1979.3 e-6; = (1/var)*||X-X_r||^2 =  1383.3 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  596.0 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1399.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   783.8 e-6; = (1/var)*||X-X_r||^2 val-train = 751.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32.5 e-6 

----------------------------------------------------------------------------------

Finished [09:35:55 03.01.2023] 235) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 576) change_channel_size_across_layers = False:
Total training time is = 0:7:8 h/m/s. 

--------------------------------------------------- 

Started [09:35:55 03.01.2023] 236) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 576) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5324 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.23
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.68
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.68
3                                  encoder.pre_residual_stack.weight                       589            11.06
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.37
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.37
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                        32             0.60
10                             decoder.channel_adjusting_conv.weight                       147             2.76
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.37
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.37
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.68
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.68
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.83; perplexity/K = 3.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.67; perplexity/K = 3.26%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  240857.8 e-6; = (1/var)*||X-X_r||^2 =  113540.3 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  127317.6 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  240851.6 e-6; = (1/var)*||X-X_r||^2 =  119355.6 e-6 = 49.6 %; (1+beta)*||Z_e-Z_q||^2 =  121496.1 e-6 = 50.4 %)
Min.  Avg. Train Loss across Mini-Batch =  240857.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  240851.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5815.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5821.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.87; perplexity/K = 4.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.89; perplexity/K = 3.88%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  61728.6 e-6; = (1/var)*||X-X_r||^2 =  35076.2 e-6 = 56.8 %; (1+beta)*||Z_e-Z_q||^2 =  26652.4 e-6 = 43.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  65972.3 e-6; = (1/var)*||X-X_r||^2 =  39743.3 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  26229.0 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  59615.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65972.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4243.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4667.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -423.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.76; perplexity/K = 3.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.43; perplexity/K = 3.41%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12446.3 e-6; = (1/var)*||X-X_r||^2 =  7284.3 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  5162.1 e-6 = 41.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  15045.3 e-6; = (1/var)*||X-X_r||^2 =  9684.4 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  5360.9 e-6 = 35.6 %)
Min.  Avg. Train Loss across Mini-Batch =  12090.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14601.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2599.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2400.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 198.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.83; perplexity/K = 2.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.24; perplexity/K = 2.78%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8741.6 e-6; = (1/var)*||X-X_r||^2 =  4640.8 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  4100.8 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  10974.6 e-6; = (1/var)*||X-X_r||^2 =  6587.6 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  4387.1 e-6 = 40.0 %)
Min.  Avg. Train Loss across Mini-Batch =  8727.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10719.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2233.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1946.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 286.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.29; perplexity/K = 2.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.74; perplexity/K = 2.29%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3753.7 e-6; = (1/var)*||X-X_r||^2 =  1872.4 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  1881.2 e-6 = 50.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  5042.8 e-6; = (1/var)*||X-X_r||^2 =  3156.1 e-6 = 62.6 %; (1+beta)*||Z_e-Z_q||^2 =  1886.6 e-6 = 37.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3753.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5042.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1289.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1283.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.21; perplexity/K = 2.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.98; perplexity/K = 2.73%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2406.2 e-6; = (1/var)*||X-X_r||^2 =  1289.5 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  1116.7 e-6 = 46.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3581.3 e-6; = (1/var)*||X-X_r||^2 =  2435.7 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  1145.5 e-6 = 32.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2406.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3540.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1175.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1146.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.82; perplexity/K = 2.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.27; perplexity/K = 2.79%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1988.0 e-6; = (1/var)*||X-X_r||^2 =  1089.4 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  898.7 e-6 = 45.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2854.8 e-6; = (1/var)*||X-X_r||^2 =  1923.6 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  931.2 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1801.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2804.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   866.8 e-6; = (1/var)*||X-X_r||^2 val-train = 834.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.79; perplexity/K = 2.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.25; perplexity/K = 1.81%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1411.2 e-6; = (1/var)*||X-X_r||^2 =  683.1 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  728.2 e-6 = 51.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2290.0 e-6; = (1/var)*||X-X_r||^2 =  1545.4 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  744.6 e-6 = 32.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1334.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2149.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   878.8 e-6; = (1/var)*||X-X_r||^2 val-train = 862.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.10; perplexity/K = 1.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.57; perplexity/K = 2.07%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3202.9 e-6; = (1/var)*||X-X_r||^2 =  2077.3 e-6 = 64.9 %; (1+beta)*||Z_e-Z_q||^2 =  1125.7 e-6 = 35.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3632.1 e-6; = (1/var)*||X-X_r||^2 =  2746.9 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  885.2 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1115.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1926.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   429.2 e-6; = (1/var)*||X-X_r||^2 val-train = 669.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -240.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.37; perplexity/K = 2.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.63; perplexity/K = 2.27%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  939.5 e-6; = (1/var)*||X-X_r||^2 =  468.0 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  471.5 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1744.3 e-6; = (1/var)*||X-X_r||^2 =  1261.4 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  482.9 e-6 = 27.7 %)
Min.  Avg. Train Loss across Mini-Batch =  882.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1642.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   804.8 e-6; = (1/var)*||X-X_r||^2 val-train = 793.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.69; perplexity/K = 2.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.86; perplexity/K = 2.12%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  752.3 e-6; = (1/var)*||X-X_r||^2 =  445.6 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  306.7 e-6 = 40.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1457.8 e-6; = (1/var)*||X-X_r||^2 =  1146.2 e-6 = 78.6 %; (1+beta)*||Z_e-Z_q||^2 =  311.6 e-6 = 21.4 %)
Min.  Avg. Train Loss across Mini-Batch =  701.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1392.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   705.5 e-6; = (1/var)*||X-X_r||^2 val-train = 700.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.14; perplexity/K = 2.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.52; perplexity/K = 2.25%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  642.7 e-6; = (1/var)*||X-X_r||^2 =  373.7 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  269.0 e-6 = 41.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1449.2 e-6; = (1/var)*||X-X_r||^2 =  1172.3 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  276.9 e-6 = 19.1 %)
Min.  Avg. Train Loss across Mini-Batch =  597.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1244.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   806.5 e-6; = (1/var)*||X-X_r||^2 val-train = 798.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.55; perplexity/K = 1.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.90; perplexity/K = 2.13%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  611.7 e-6; = (1/var)*||X-X_r||^2 =  363.8 e-6 = 59.5 %; (1+beta)*||Z_e-Z_q||^2 =  247.9 e-6 = 40.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1238.4 e-6; = (1/var)*||X-X_r||^2 =  977.5 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  261.0 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  571.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1216.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   626.7 e-6; = (1/var)*||X-X_r||^2 val-train = 613.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.92; perplexity/K = 1.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.06; perplexity/K = 1.38%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  674.7 e-6; = (1/var)*||X-X_r||^2 =  308.0 e-6 = 45.7 %; (1+beta)*||Z_e-Z_q||^2 =  366.6 e-6 = 54.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1273.6 e-6; = (1/var)*||X-X_r||^2 =  893.9 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  379.7 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  538.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1174.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   598.9 e-6; = (1/var)*||X-X_r||^2 val-train = 585.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.46; perplexity/K = 2.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.70; perplexity/K = 2.09%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  515.7 e-6; = (1/var)*||X-X_r||^2 =  293.3 e-6 = 56.9 %; (1+beta)*||Z_e-Z_q||^2 =  222.4 e-6 = 43.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1490.7 e-6; = (1/var)*||X-X_r||^2 =  1227.4 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  263.3 e-6 = 17.7 %)
Min.  Avg. Train Loss across Mini-Batch =  461.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1036.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   975.0 e-6; = (1/var)*||X-X_r||^2 val-train = 934.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.59; perplexity/K = 1.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.05; perplexity/K = 1.96%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  609.5 e-6; = (1/var)*||X-X_r||^2 =  281.5 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  328.0 e-6 = 53.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1178.7 e-6; = (1/var)*||X-X_r||^2 =  834.0 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  344.8 e-6 = 29.2 %)
Min.  Avg. Train Loss across Mini-Batch =  453.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1036.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   569.2 e-6; = (1/var)*||X-X_r||^2 val-train = 552.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 2.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.74; perplexity/K = 2.29%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  494.5 e-6; = (1/var)*||X-X_r||^2 =  227.8 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  266.7 e-6 = 53.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1055.6 e-6; = (1/var)*||X-X_r||^2 =  774.4 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  281.2 e-6 = 26.6 %)
Min.  Avg. Train Loss across Mini-Batch =  453.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  994.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   561.0 e-6; = (1/var)*||X-X_r||^2 val-train = 546.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.60; perplexity/K = 1.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.77; perplexity/K = 1.71%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  652.6 e-6; = (1/var)*||X-X_r||^2 =  346.0 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  306.6 e-6 = 47.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1110.6 e-6; = (1/var)*||X-X_r||^2 =  802.6 e-6 = 72.3 %; (1+beta)*||Z_e-Z_q||^2 =  308.1 e-6 = 27.7 %)
Min.  Avg. Train Loss across Mini-Batch =  380.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  912.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   458.0 e-6; = (1/var)*||X-X_r||^2 val-train = 456.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.29; perplexity/K = 1.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.51; perplexity/K = 1.47%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382.4 e-6; = (1/var)*||X-X_r||^2 =  189.7 e-6 = 49.6 %; (1+beta)*||Z_e-Z_q||^2 =  192.7 e-6 = 50.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  878.2 e-6; = (1/var)*||X-X_r||^2 =  685.1 e-6 = 78.0 %; (1+beta)*||Z_e-Z_q||^2 =  193.0 e-6 = 22.0 %)
Min.  Avg. Train Loss across Mini-Batch =  362.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  858.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   495.8 e-6; = (1/var)*||X-X_r||^2 val-train = 495.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.75; perplexity/K = 1.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.16; perplexity/K = 1.40%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  349.4 e-6; = (1/var)*||X-X_r||^2 =  182.6 e-6 = 52.3 %; (1+beta)*||Z_e-Z_q||^2 =  166.8 e-6 = 47.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  888.4 e-6; = (1/var)*||X-X_r||^2 =  714.5 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  173.9 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  349.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  858.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   538.9 e-6; = (1/var)*||X-X_r||^2 val-train = 531.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.1 e-6 

----------------------------------------------------------------------------------

Finished [11:52:44 03.01.2023] 236) Finished running for K = 512 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 576) change_channel_size_across_layers = False:
Total training time is = 0:7:49 h/m/s. 

--------------------------------------------------- 

Started [11:52:44 03.01.2023] 237) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 144) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 762 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.05
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.20
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.19
4                                  encoder.pre_residual_stack.weight                       147            19.29
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.72
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.72
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
9                              encoder.channel_adjusting_conv.weight                         8             1.05
10                                                       VQ.E.weight                        32             4.20
11                             decoder.channel_adjusting_conv.weight                        73             9.58
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.72
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.72
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.19
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.20
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.05
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.44; perplexity/K = 2.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.59; perplexity/K = 2.07%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  655508.5 e-6; = (1/var)*||X-X_r||^2 =  472756.8 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  182751.7 e-6 = 27.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  738964.8 e-6; = (1/var)*||X-X_r||^2 =  466459.1 e-6 = 63.1 %; (1+beta)*||Z_e-Z_q||^2 =  272505.7 e-6 = 36.9 %)
Min.  Avg. Train Loss across Mini-Batch =  652922.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  587631.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   83456.3 e-6; = (1/var)*||X-X_r||^2 val-train = -6297.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 89754.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.25; perplexity/K = 2.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 2.42%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  172764.6 e-6; = (1/var)*||X-X_r||^2 =  152777.3 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  19987.3 e-6 = 11.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  174284.8 e-6; = (1/var)*||X-X_r||^2 =  155350.7 e-6 = 89.1 %; (1+beta)*||Z_e-Z_q||^2 =  18934.0 e-6 = 10.9 %)
Min.  Avg. Train Loss across Mini-Batch =  172764.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  174284.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1520.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2573.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1053.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.61; perplexity/K = 3.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.74; perplexity/K = 4.05%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  97291.8 e-6; = (1/var)*||X-X_r||^2 =  85458.1 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  11833.7 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  113023.8 e-6; = (1/var)*||X-X_r||^2 =  94095.6 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  18928.3 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  97291.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  101793.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15732.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8637.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7094.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.49; perplexity/K = 4.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.77; perplexity/K = 4.45%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  58619.2 e-6; = (1/var)*||X-X_r||^2 =  49353.1 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  9266.1 e-6 = 15.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  63102.9 e-6; = (1/var)*||X-X_r||^2 =  54117.6 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  8985.3 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  58594.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  63102.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4483.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4764.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -280.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.64; perplexity/K = 4.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.90; perplexity/K = 4.67%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42654.7 e-6; = (1/var)*||X-X_r||^2 =  34877.4 e-6 = 81.8 %; (1+beta)*||Z_e-Z_q||^2 =  7777.4 e-6 = 18.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  48712.8 e-6; = (1/var)*||X-X_r||^2 =  40531.7 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  8181.1 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  42654.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48020.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6058.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5654.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 403.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.09; perplexity/K = 5.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.69; perplexity/K = 5.02%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39146.4 e-6; = (1/var)*||X-X_r||^2 =  32151.1 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  6995.3 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  44298.4 e-6; = (1/var)*||X-X_r||^2 =  36892.5 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  7405.8 e-6 = 16.7 %)
Min.  Avg. Train Loss across Mini-Batch =  39040.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44298.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5152.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4741.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 410.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.66; perplexity/K = 5.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.38; perplexity/K = 4.96%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33397.3 e-6; = (1/var)*||X-X_r||^2 =  27402.9 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  5994.4 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  38589.2 e-6; = (1/var)*||X-X_r||^2 =  32110.1 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  6479.1 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  33193.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38498.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5191.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4707.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 484.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.64; perplexity/K = 5.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.63; perplexity/K = 5.01%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30827.7 e-6; = (1/var)*||X-X_r||^2 =  25469.8 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  5357.9 e-6 = 17.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  35517.9 e-6; = (1/var)*||X-X_r||^2 =  29698.7 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  5819.2 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  30383.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35517.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4690.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4228.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 461.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.47; perplexity/K = 5.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.12; perplexity/K = 5.10%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29238.7 e-6; = (1/var)*||X-X_r||^2 =  24289.4 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  4949.3 e-6 = 16.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  34698.5 e-6; = (1/var)*||X-X_r||^2 =  29147.3 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  5551.2 e-6 = 16.0 %)
Min.  Avg. Train Loss across Mini-Batch =  27874.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33173.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5459.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4858.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 601.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.43; perplexity/K = 5.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.64; perplexity/K = 5.01%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27549.3 e-6; = (1/var)*||X-X_r||^2 =  22874.0 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  4675.3 e-6 = 17.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  32289.2 e-6; = (1/var)*||X-X_r||^2 =  27146.2 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  5143.0 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  26944.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32028.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4739.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4272.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 467.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.10; perplexity/K = 5.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.08; perplexity/K = 5.09%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41705.8 e-6; = (1/var)*||X-X_r||^2 =  35020.7 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  6685.1 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  47035.8 e-6; = (1/var)*||X-X_r||^2 =  40188.5 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  6847.4 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  24584.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29580.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5330.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5167.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 162.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.53; perplexity/K = 4.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.64; perplexity/K = 4.62%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26225.7 e-6; = (1/var)*||X-X_r||^2 =  22090.4 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  4135.3 e-6 = 15.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  289649.7 e-6; = (1/var)*||X-X_r||^2 =  281584.8 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  8065.0 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  22908.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  27783.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   263424.1 e-6; = (1/var)*||X-X_r||^2 val-train = 259494.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3929.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.02; perplexity/K = 5.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.93; perplexity/K = 5.07%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22212.2 e-6; = (1/var)*||X-X_r||^2 =  18443.8 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  3768.4 e-6 = 17.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  27004.8 e-6; = (1/var)*||X-X_r||^2 =  22690.5 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  4314.3 e-6 = 16.0 %)
Min.  Avg. Train Loss across Mini-Batch =  21922.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26784.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4792.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4246.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 546.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.48; perplexity/K = 5.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.08; perplexity/K = 5.09%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21329.4 e-6; = (1/var)*||X-X_r||^2 =  17452.1 e-6 = 81.8 %; (1+beta)*||Z_e-Z_q||^2 =  3877.4 e-6 = 18.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  25701.9 e-6; = (1/var)*||X-X_r||^2 =  21381.8 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  4320.1 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  20900.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25668.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4372.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3929.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 442.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.30; perplexity/K = 5.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.93; perplexity/K = 5.06%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21654.9 e-6; = (1/var)*||X-X_r||^2 =  18059.6 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  3595.3 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  28045.1 e-6; = (1/var)*||X-X_r||^2 =  23317.4 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  4727.7 e-6 = 16.9 %)
Min.  Avg. Train Loss across Mini-Batch =  20242.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  25106.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6390.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5257.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1132.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.84; perplexity/K = 5.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.30; perplexity/K = 5.14%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19879.4 e-6; = (1/var)*||X-X_r||^2 =  16599.4 e-6 = 83.5 %; (1+beta)*||Z_e-Z_q||^2 =  3280.0 e-6 = 16.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  24866.9 e-6; = (1/var)*||X-X_r||^2 =  20823.5 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  4043.4 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  19695.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24468.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4987.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4224.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 763.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.83; perplexity/K = 5.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.07; perplexity/K = 5.09%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19558.4 e-6; = (1/var)*||X-X_r||^2 =  16223.2 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  3335.2 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  23985.9 e-6; = (1/var)*||X-X_r||^2 =  20032.1 e-6 = 83.5 %; (1+beta)*||Z_e-Z_q||^2 =  3953.8 e-6 = 16.5 %)
Min.  Avg. Train Loss across Mini-Batch =  19493.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  23985.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4427.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3808.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 618.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.18; perplexity/K = 5.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.03; perplexity/K = 5.08%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20225.0 e-6; = (1/var)*||X-X_r||^2 =  16498.3 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  3726.7 e-6 = 18.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  24228.1 e-6; = (1/var)*||X-X_r||^2 =  20163.9 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  4064.2 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  19060.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  23782.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4003.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3665.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 337.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.41; perplexity/K = 5.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.04; perplexity/K = 5.09%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18656.8 e-6; = (1/var)*||X-X_r||^2 =  15782.6 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  2874.2 e-6 = 15.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  23635.1 e-6; = (1/var)*||X-X_r||^2 =  19983.4 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  3651.7 e-6 = 15.5 %)
Min.  Avg. Train Loss across Mini-Batch =  18616.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  23297.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4978.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4200.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 777.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.89; perplexity/K = 5.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.40; perplexity/K = 5.16%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20863.2 e-6; = (1/var)*||X-X_r||^2 =  16711.6 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  4151.6 e-6 = 19.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  25001.3 e-6; = (1/var)*||X-X_r||^2 =  20491.7 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  4509.7 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  18210.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  22953.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4138.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3780.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 358.1 e-6 

----------------------------------------------------------------------------------

Finished [12:40:43 03.01.2023] 237) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 144) change_channel_size_across_layers = True:
Total training time is = 0:3:58 h/m/s. 

--------------------------------------------------- 

Started [12:40:43 03.01.2023] 238) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 144) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2484 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.29
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.27
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.10
4                                  encoder.pre_residual_stack.weight                       589            23.71
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
9                              encoder.channel_adjusting_conv.weight                        16             0.64
10                                                       VQ.E.weight                        32             1.29
11                             decoder.channel_adjusting_conv.weight                       147             5.92
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.10
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.27
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.81; perplexity/K = 7.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.25; perplexity/K = 7.47%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  314813.6 e-6; = (1/var)*||X-X_r||^2 =  208312.2 e-6 = 66.2 %; (1+beta)*||Z_e-Z_q||^2 =  106501.4 e-6 = 33.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  310761.4 e-6; = (1/var)*||X-X_r||^2 =  210910.0 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  99851.4 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  314813.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  310761.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4052.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2597.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6650.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.11; perplexity/K = 9.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.83; perplexity/K = 9.54%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90179.9 e-6; = (1/var)*||X-X_r||^2 =  72561.0 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  17618.9 e-6 = 19.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  96052.7 e-6; = (1/var)*||X-X_r||^2 =  78959.0 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  17093.7 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  90179.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  96052.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5872.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6398.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -525.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.95; perplexity/K = 10.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.32; perplexity/K = 10.02%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45540.3 e-6; = (1/var)*||X-X_r||^2 =  37963.0 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  7577.3 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  49929.9 e-6; = (1/var)*||X-X_r||^2 =  42246.7 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  7683.2 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  45540.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49929.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4389.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4283.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 105.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.45; perplexity/K = 9.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.04; perplexity/K = 10.16%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27283.2 e-6; = (1/var)*||X-X_r||^2 =  22924.0 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  4359.2 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  30881.9 e-6; = (1/var)*||X-X_r||^2 =  26296.8 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  4585.2 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  27283.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30881.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3598.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3372.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 226.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.40; perplexity/K = 9.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.32; perplexity/K = 9.63%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17594.8 e-6; = (1/var)*||X-X_r||^2 =  14530.8 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  3063.9 e-6 = 17.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  20206.2 e-6; = (1/var)*||X-X_r||^2 =  16923.9 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  3282.3 e-6 = 16.2 %)
Min.  Avg. Train Loss across Mini-Batch =  17594.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20206.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2611.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2393.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 218.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.39; perplexity/K = 9.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.87; perplexity/K = 10.13%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13614.5 e-6; = (1/var)*||X-X_r||^2 =  10346.3 e-6 = 76.0 %; (1+beta)*||Z_e-Z_q||^2 =  3268.2 e-6 = 24.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  15210.2 e-6; = (1/var)*||X-X_r||^2 =  11944.5 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  3265.7 e-6 = 21.5 %)
Min.  Avg. Train Loss across Mini-Batch =  11688.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13724.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1595.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1598.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.43; perplexity/K = 10.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.65; perplexity/K = 10.28%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7833.7 e-6; = (1/var)*||X-X_r||^2 =  6300.5 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  1533.3 e-6 = 19.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  9437.0 e-6; = (1/var)*||X-X_r||^2 =  7796.3 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  1640.6 e-6 = 17.4 %)
Min.  Avg. Train Loss across Mini-Batch =  7700.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9362.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1603.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1495.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 107.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.93; perplexity/K = 10.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.23; perplexity/K = 10.59%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5327.0 e-6; = (1/var)*||X-X_r||^2 =  4190.1 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  1136.9 e-6 = 21.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  6433.3 e-6; = (1/var)*||X-X_r||^2 =  5237.0 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  1196.3 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  5179.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6433.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1106.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1046.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 59.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.71; perplexity/K = 11.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.90; perplexity/K = 11.11%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3753.8 e-6; = (1/var)*||X-X_r||^2 =  2921.4 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  832.4 e-6 = 22.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4869.9 e-6; = (1/var)*||X-X_r||^2 =  3993.9 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  876.0 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3753.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4707.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1116.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1072.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 43.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.32; perplexity/K = 11.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.68; perplexity/K = 11.27%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2966.7 e-6; = (1/var)*||X-X_r||^2 =  2375.6 e-6 = 80.1 %; (1+beta)*||Z_e-Z_q||^2 =  591.1 e-6 = 19.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3683.5 e-6; = (1/var)*||X-X_r||^2 =  3058.9 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  624.6 e-6 = 17.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2821.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3683.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   716.8 e-6; = (1/var)*||X-X_r||^2 val-train = 683.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 33.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.50; perplexity/K = 12.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.06; perplexity/K = 11.92%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2817.5 e-6; = (1/var)*||X-X_r||^2 =  2156.5 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  661.1 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3834.1 e-6; = (1/var)*||X-X_r||^2 =  3129.3 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  704.8 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2438.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3318.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1016.6 e-6; = (1/var)*||X-X_r||^2 val-train = 972.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 43.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.74; perplexity/K = 11.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.57; perplexity/K = 11.05%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2125.7 e-6; = (1/var)*||X-X_r||^2 =  1665.0 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  460.8 e-6 = 21.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2909.4 e-6; = (1/var)*||X-X_r||^2 =  2401.7 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  507.8 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2125.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2909.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   783.7 e-6; = (1/var)*||X-X_r||^2 val-train = 736.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 47.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.31; perplexity/K = 11.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.64; perplexity/K = 11.26%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1875.7 e-6; = (1/var)*||X-X_r||^2 =  1377.5 e-6 = 73.4 %; (1+beta)*||Z_e-Z_q||^2 =  498.2 e-6 = 26.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2542.0 e-6; = (1/var)*||X-X_r||^2 =  2026.8 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  515.2 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1700.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2354.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   666.3 e-6; = (1/var)*||X-X_r||^2 val-train = 649.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.72; perplexity/K = 12.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.75; perplexity/K = 11.47%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3057.1 e-6; = (1/var)*||X-X_r||^2 =  2264.2 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  792.9 e-6 = 25.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4073.4 e-6; = (1/var)*||X-X_r||^2 =  3245.9 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  827.5 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1626.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2247.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1016.3 e-6; = (1/var)*||X-X_r||^2 val-train = 981.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.10; perplexity/K = 13.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 67.80; perplexity/K = 13.24%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3608.2 e-6; = (1/var)*||X-X_r||^2 =  2165.3 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  1442.9 e-6 = 40.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4450.0 e-6; = (1/var)*||X-X_r||^2 =  3081.0 e-6 = 69.2 %; (1+beta)*||Z_e-Z_q||^2 =  1369.0 e-6 = 30.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1626.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2247.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   841.8 e-6; = (1/var)*||X-X_r||^2 val-train = 915.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -73.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.00; perplexity/K = 11.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.52; perplexity/K = 11.82%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1744.4 e-6; = (1/var)*||X-X_r||^2 =  1273.0 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  471.3 e-6 = 27.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2314.9 e-6; = (1/var)*||X-X_r||^2 =  1834.5 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  480.4 e-6 = 20.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1575.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2245.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   570.6 e-6; = (1/var)*||X-X_r||^2 val-train = 561.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.74; perplexity/K = 10.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.08; perplexity/K = 10.37%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1484.0 e-6; = (1/var)*||X-X_r||^2 =  1117.1 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  367.0 e-6 = 24.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1980.3 e-6; = (1/var)*||X-X_r||^2 =  1590.4 e-6 = 80.3 %; (1+beta)*||Z_e-Z_q||^2 =  389.9 e-6 = 19.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1457.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1978.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   496.3 e-6; = (1/var)*||X-X_r||^2 val-train = 473.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.98; perplexity/K = 11.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.96; perplexity/K = 11.32%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1165.0 e-6; = (1/var)*||X-X_r||^2 =  910.2 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  254.8 e-6 = 21.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1709.4 e-6; = (1/var)*||X-X_r||^2 =  1431.4 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  278.0 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1156.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1652.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   544.4 e-6; = (1/var)*||X-X_r||^2 val-train = 521.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.68; perplexity/K = 11.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.86; perplexity/K = 10.91%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1089.0 e-6; = (1/var)*||X-X_r||^2 =  822.5 e-6 = 75.5 %; (1+beta)*||Z_e-Z_q||^2 =  266.5 e-6 = 24.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1548.2 e-6; = (1/var)*||X-X_r||^2 =  1260.1 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  288.0 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1037.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1540.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   459.2 e-6; = (1/var)*||X-X_r||^2 val-train = 437.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.32; perplexity/K = 11.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.65; perplexity/K = 11.45%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1056.0 e-6; = (1/var)*||X-X_r||^2 =  867.5 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  188.5 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1527.9 e-6; = (1/var)*||X-X_r||^2 =  1301.8 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  226.1 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  875.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1290.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   471.9 e-6; = (1/var)*||X-X_r||^2 val-train = 434.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 37.6 e-6 

----------------------------------------------------------------------------------

Finished [13:30:07 03.01.2023] 238) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 144) change_channel_size_across_layers = True:
Total training time is = 0:3:23 h/m/s. 

--------------------------------------------------- 

Started [13:30:07 03.01.2023] 239) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 144) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2004 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.07
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.07
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.07
4                                  encoder.pre_residual_stack.weight                       147             7.34
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.80
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.80
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.40
10                                                       VQ.E.weight                        32             1.60
11                             decoder.channel_adjusting_conv.weight                        73             3.64
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.80
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.80
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.07
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.07
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.07
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.98; perplexity/K = 6.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.92; perplexity/K = 5.26%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  259149.3 e-6; = (1/var)*||X-X_r||^2 =  116208.7 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  142940.6 e-6 = 55.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  268779.2 e-6; = (1/var)*||X-X_r||^2 =  129301.9 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  139477.3 e-6 = 51.9 %)
Min.  Avg. Train Loss across Mini-Batch =  259149.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  263603.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9629.9 e-6; = (1/var)*||X-X_r||^2 val-train = 13093.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3463.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.72; perplexity/K = 11.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.56; perplexity/K = 11.44%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42520.2 e-6; = (1/var)*||X-X_r||^2 =  20791.0 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  21729.2 e-6 = 51.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  44607.2 e-6; = (1/var)*||X-X_r||^2 =  24337.9 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  20269.3 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  35231.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41643.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2087.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3546.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1459.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.98; perplexity/K = 14.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 74.29; perplexity/K = 14.51%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13246.3 e-6; = (1/var)*||X-X_r||^2 =  5367.9 e-6 = 40.5 %; (1+beta)*||Z_e-Z_q||^2 =  7878.4 e-6 = 59.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  14055.6 e-6; = (1/var)*||X-X_r||^2 =  6716.7 e-6 = 47.8 %; (1+beta)*||Z_e-Z_q||^2 =  7338.9 e-6 = 52.2 %)
Min.  Avg. Train Loss across Mini-Batch =  8405.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10537.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   809.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1348.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -539.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.67; perplexity/K = 15.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.40; perplexity/K = 14.92%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5964.2 e-6; = (1/var)*||X-X_r||^2 =  2424.6 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  3539.7 e-6 = 59.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  6048.7 e-6; = (1/var)*||X-X_r||^2 =  2794.8 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  3253.9 e-6 = 53.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3481.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4732.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   84.4 e-6; = (1/var)*||X-X_r||^2 val-train = 370.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -285.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 92.32; perplexity/K = 18.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 84.36; perplexity/K = 16.48%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4604.5 e-6; = (1/var)*||X-X_r||^2 =  2770.0 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  1834.5 e-6 = 39.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  10138.3 e-6; = (1/var)*||X-X_r||^2 =  7235.7 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  2902.6 e-6 = 28.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2022.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2683.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5533.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4465.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1068.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 84.96; perplexity/K = 16.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.65; perplexity/K = 17.12%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3228.8 e-6; = (1/var)*||X-X_r||^2 =  981.6 e-6 = 30.4 %; (1+beta)*||Z_e-Z_q||^2 =  2247.2 e-6 = 69.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3768.8 e-6; = (1/var)*||X-X_r||^2 =  1623.8 e-6 = 43.1 %; (1+beta)*||Z_e-Z_q||^2 =  2145.0 e-6 = 56.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1242.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1938.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   540.1 e-6; = (1/var)*||X-X_r||^2 val-train = 642.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -102.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.08; perplexity/K = 17.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.90; perplexity/K = 17.17%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1175.8 e-6; = (1/var)*||X-X_r||^2 =  367.4 e-6 = 31.2 %; (1+beta)*||Z_e-Z_q||^2 =  808.5 e-6 = 68.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1754.3 e-6; = (1/var)*||X-X_r||^2 =  931.9 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  822.4 e-6 = 46.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1008.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1611.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   578.5 e-6; = (1/var)*||X-X_r||^2 val-train = 564.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 88.50; perplexity/K = 17.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 81.02; perplexity/K = 15.82%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1194.5 e-6; = (1/var)*||X-X_r||^2 =  352.2 e-6 = 29.5 %; (1+beta)*||Z_e-Z_q||^2 =  842.3 e-6 = 70.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1727.3 e-6; = (1/var)*||X-X_r||^2 =  873.5 e-6 = 50.6 %; (1+beta)*||Z_e-Z_q||^2 =  853.9 e-6 = 49.4 %)
Min.  Avg. Train Loss across Mini-Batch =  793.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1203.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   532.8 e-6; = (1/var)*||X-X_r||^2 val-train = 521.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 91.15; perplexity/K = 17.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 88.43; perplexity/K = 17.27%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  945.2 e-6; = (1/var)*||X-X_r||^2 =  267.2 e-6 = 28.3 %; (1+beta)*||Z_e-Z_q||^2 =  678.0 e-6 = 71.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1211.9 e-6; = (1/var)*||X-X_r||^2 =  521.8 e-6 = 43.1 %; (1+beta)*||Z_e-Z_q||^2 =  690.2 e-6 = 56.9 %)
Min.  Avg. Train Loss across Mini-Batch =  645.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1128.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   266.8 e-6; = (1/var)*||X-X_r||^2 val-train = 254.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 93.30; perplexity/K = 18.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 97.65; perplexity/K = 19.07%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  677.1 e-6; = (1/var)*||X-X_r||^2 =  194.8 e-6 = 28.8 %; (1+beta)*||Z_e-Z_q||^2 =  482.3 e-6 = 71.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  974.2 e-6; = (1/var)*||X-X_r||^2 =  469.1 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  505.2 e-6 = 51.9 %)
Min.  Avg. Train Loss across Mini-Batch =  578.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  936.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   297.1 e-6; = (1/var)*||X-X_r||^2 val-train = 274.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 95.09; perplexity/K = 18.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 90.32; perplexity/K = 17.64%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  489.2 e-6; = (1/var)*||X-X_r||^2 =  116.8 e-6 = 23.9 %; (1+beta)*||Z_e-Z_q||^2 =  372.4 e-6 = 76.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  793.2 e-6; = (1/var)*||X-X_r||^2 =  391.0 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  402.2 e-6 = 50.7 %)
Min.  Avg. Train Loss across Mini-Batch =  464.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  793.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   304.0 e-6; = (1/var)*||X-X_r||^2 val-train = 274.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 105.89; perplexity/K = 20.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 106.07; perplexity/K = 20.72%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1346.0 e-6; = (1/var)*||X-X_r||^2 =  269.3 e-6 = 20.0 %; (1+beta)*||Z_e-Z_q||^2 =  1076.6 e-6 = 80.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1700.6 e-6; = (1/var)*||X-X_r||^2 =  651.2 e-6 = 38.3 %; (1+beta)*||Z_e-Z_q||^2 =  1049.3 e-6 = 61.7 %)
Min.  Avg. Train Loss across Mini-Batch =  364.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  689.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   354.6 e-6; = (1/var)*||X-X_r||^2 val-train = 381.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -27.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 122.78; perplexity/K = 23.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 123.41; perplexity/K = 24.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2703.9 e-6; = (1/var)*||X-X_r||^2 =  550.8 e-6 = 20.4 %; (1+beta)*||Z_e-Z_q||^2 =  2153.0 e-6 = 79.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2457.2 e-6; = (1/var)*||X-X_r||^2 =  723.8 e-6 = 29.5 %; (1+beta)*||Z_e-Z_q||^2 =  1733.4 e-6 = 70.5 %)
Min.  Avg. Train Loss across Mini-Batch =  364.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  689.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -246.7 e-6; = (1/var)*||X-X_r||^2 val-train = 172.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -419.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 142.05; perplexity/K = 27.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 148.50; perplexity/K = 29.00%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16085.6 e-6; = (1/var)*||X-X_r||^2 =  8748.2 e-6 = 54.4 %; (1+beta)*||Z_e-Z_q||^2 =  7337.4 e-6 = 45.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  11873.9 e-6; = (1/var)*||X-X_r||^2 =  5084.3 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  6789.6 e-6 = 57.2 %)
Min.  Avg. Train Loss across Mini-Batch =  313.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  593.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4211.8 e-6; = (1/var)*||X-X_r||^2 val-train = -3664.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -547.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 90.13; perplexity/K = 17.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 98.86; perplexity/K = 19.31%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  515.9 e-6; = (1/var)*||X-X_r||^2 =  99.6 e-6 = 19.3 %; (1+beta)*||Z_e-Z_q||^2 =  416.3 e-6 = 80.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  814.8 e-6; = (1/var)*||X-X_r||^2 =  378.0 e-6 = 46.4 %; (1+beta)*||Z_e-Z_q||^2 =  436.8 e-6 = 53.6 %)
Min.  Avg. Train Loss across Mini-Batch =  287.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   298.9 e-6; = (1/var)*||X-X_r||^2 val-train = 278.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 84.41; perplexity/K = 16.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 83.11; perplexity/K = 16.23%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  450.0 e-6; = (1/var)*||X-X_r||^2 =  71.2 e-6 = 15.8 %; (1+beta)*||Z_e-Z_q||^2 =  378.8 e-6 = 84.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1033.6 e-6; = (1/var)*||X-X_r||^2 =  643.7 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  389.9 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  268.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   583.6 e-6; = (1/var)*||X-X_r||^2 val-train = 572.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 85.90; perplexity/K = 16.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 83.33; perplexity/K = 16.28%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  449.4 e-6; = (1/var)*||X-X_r||^2 =  95.7 e-6 = 21.3 %; (1+beta)*||Z_e-Z_q||^2 =  353.7 e-6 = 78.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  722.0 e-6; = (1/var)*||X-X_r||^2 =  343.8 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  378.2 e-6 = 52.4 %)
Min.  Avg. Train Loss across Mini-Batch =  268.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   272.5 e-6; = (1/var)*||X-X_r||^2 val-train = 248.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 125.09; perplexity/K = 24.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 123.72; perplexity/K = 24.16%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2130.3 e-6; = (1/var)*||X-X_r||^2 =  235.8 e-6 = 11.1 %; (1+beta)*||Z_e-Z_q||^2 =  1894.4 e-6 = 88.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2495.7 e-6; = (1/var)*||X-X_r||^2 =  790.2 e-6 = 31.7 %; (1+beta)*||Z_e-Z_q||^2 =  1705.5 e-6 = 68.3 %)
Min.  Avg. Train Loss across Mini-Batch =  162.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   365.4 e-6; = (1/var)*||X-X_r||^2 val-train = 554.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -189.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 132.27; perplexity/K = 25.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 129.91; perplexity/K = 25.37%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7267.6 e-6; = (1/var)*||X-X_r||^2 =  1608.2 e-6 = 22.1 %; (1+beta)*||Z_e-Z_q||^2 =  5659.4 e-6 = 77.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  7442.6 e-6; = (1/var)*||X-X_r||^2 =  3053.0 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  4389.6 e-6 = 59.0 %)
Min.  Avg. Train Loss across Mini-Batch =  162.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   175.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1444.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1269.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 105.67; perplexity/K = 20.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 103.75; perplexity/K = 20.26%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1207.6 e-6; = (1/var)*||X-X_r||^2 =  139.1 e-6 = 11.5 %; (1+beta)*||Z_e-Z_q||^2 =  1068.5 e-6 = 88.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1506.7 e-6; = (1/var)*||X-X_r||^2 =  481.2 e-6 = 31.9 %; (1+beta)*||Z_e-Z_q||^2 =  1025.5 e-6 = 68.1 %)
Min.  Avg. Train Loss across Mini-Batch =  162.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  415.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   299.1 e-6; = (1/var)*||X-X_r||^2 val-train = 342.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -43.0 e-6 

----------------------------------------------------------------------------------

Finished [14:34:55 03.01.2023] 239) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 144) change_channel_size_across_layers = False:
Total training time is = 0:3:47 h/m/s. 

--------------------------------------------------- 

Started [14:34:55 03.01.2023] 240) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 144) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7420 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.12
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.12
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.12
4                                  encoder.pre_residual_stack.weight                       589             7.94
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.98
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.98
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                        32             0.43
11                             decoder.channel_adjusting_conv.weight                       147             1.98
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.98
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.98
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.12
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.12
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.12
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.50; perplexity/K = 4.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.27; perplexity/K = 4.15%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  136267.7 e-6; = (1/var)*||X-X_r||^2 =  66976.0 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  69291.7 e-6 = 50.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  147066.9 e-6; = (1/var)*||X-X_r||^2 =  79742.0 e-6 = 54.2 %; (1+beta)*||Z_e-Z_q||^2 =  67324.9 e-6 = 45.8 %)
Min.  Avg. Train Loss across Mini-Batch =  136267.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147066.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10799.2 e-6; = (1/var)*||X-X_r||^2 val-train = 12766.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1966.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.98; perplexity/K = 8.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.69; perplexity/K = 7.75%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53267.5 e-6; = (1/var)*||X-X_r||^2 =  24611.5 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  28656.0 e-6 = 53.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  46012.2 e-6; = (1/var)*||X-X_r||^2 =  23293.9 e-6 = 50.6 %; (1+beta)*||Z_e-Z_q||^2 =  22718.3 e-6 = 49.4 %)
Min.  Avg. Train Loss across Mini-Batch =  25405.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31143.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7255.3 e-6; = (1/var)*||X-X_r||^2 val-train = -1317.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5937.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.07; perplexity/K = 9.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.97; perplexity/K = 8.98%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11866.8 e-6; = (1/var)*||X-X_r||^2 =  4092.5 e-6 = 34.5 %; (1+beta)*||Z_e-Z_q||^2 =  7774.3 e-6 = 65.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  14444.3 e-6; = (1/var)*||X-X_r||^2 =  6867.5 e-6 = 47.5 %; (1+beta)*||Z_e-Z_q||^2 =  7576.8 e-6 = 52.5 %)
Min.  Avg. Train Loss across Mini-Batch =  7953.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11331.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2577.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2775.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -197.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.77; perplexity/K = 9.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.72; perplexity/K = 9.91%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5052.6 e-6; = (1/var)*||X-X_r||^2 =  2035.0 e-6 = 40.3 %; (1+beta)*||Z_e-Z_q||^2 =  3017.6 e-6 = 59.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  8244.6 e-6; = (1/var)*||X-X_r||^2 =  4963.9 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  3280.6 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4184.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6279.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3192.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2928.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 263.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.32; perplexity/K = 9.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.55; perplexity/K = 9.87%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2198.5 e-6; = (1/var)*||X-X_r||^2 =  757.3 e-6 = 34.4 %; (1+beta)*||Z_e-Z_q||^2 =  1441.2 e-6 = 65.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3789.6 e-6; = (1/var)*||X-X_r||^2 =  2240.3 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  1549.2 e-6 = 40.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2198.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3789.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1591.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1483.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 108.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 65.25; perplexity/K = 12.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 64.20; perplexity/K = 12.54%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10066.3 e-6; = (1/var)*||X-X_r||^2 =  7046.0 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  3020.3 e-6 = 30.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  53072.6 e-6; = (1/var)*||X-X_r||^2 =  41064.1 e-6 = 77.4 %; (1+beta)*||Z_e-Z_q||^2 =  12008.5 e-6 = 22.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1672.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2989.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43006.3 e-6; = (1/var)*||X-X_r||^2 val-train = 34018.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8988.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.57; perplexity/K = 9.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.53; perplexity/K = 10.26%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2016.2 e-6; = (1/var)*||X-X_r||^2 =  508.2 e-6 = 25.2 %; (1+beta)*||Z_e-Z_q||^2 =  1508.1 e-6 = 74.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2950.2 e-6; = (1/var)*||X-X_r||^2 =  1411.4 e-6 = 47.8 %; (1+beta)*||Z_e-Z_q||^2 =  1538.8 e-6 = 52.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1370.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2334.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   934.0 e-6; = (1/var)*||X-X_r||^2 val-train = 903.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.99; perplexity/K = 11.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.85; perplexity/K = 10.71%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2330.3 e-6; = (1/var)*||X-X_r||^2 =  594.6 e-6 = 25.5 %; (1+beta)*||Z_e-Z_q||^2 =  1735.7 e-6 = 74.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3262.3 e-6; = (1/var)*||X-X_r||^2 =  1516.5 e-6 = 46.5 %; (1+beta)*||Z_e-Z_q||^2 =  1745.8 e-6 = 53.5 %)
Min.  Avg. Train Loss across Mini-Batch =  989.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1867.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   931.9 e-6; = (1/var)*||X-X_r||^2 val-train = 921.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.66; perplexity/K = 9.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.39; perplexity/K = 9.84%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  967.8 e-6; = (1/var)*||X-X_r||^2 =  267.5 e-6 = 27.6 %; (1+beta)*||Z_e-Z_q||^2 =  700.2 e-6 = 72.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1686.3 e-6; = (1/var)*||X-X_r||^2 =  927.8 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  758.5 e-6 = 45.0 %)
Min.  Avg. Train Loss across Mini-Batch =  953.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1686.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   718.5 e-6; = (1/var)*||X-X_r||^2 val-train = 660.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 58.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.14; perplexity/K = 9.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.16; perplexity/K = 9.21%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  811.0 e-6; = (1/var)*||X-X_r||^2 =  212.3 e-6 = 26.2 %; (1+beta)*||Z_e-Z_q||^2 =  598.7 e-6 = 73.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1507.9 e-6; = (1/var)*||X-X_r||^2 =  849.7 e-6 = 56.3 %; (1+beta)*||Z_e-Z_q||^2 =  658.2 e-6 = 43.7 %)
Min.  Avg. Train Loss across Mini-Batch =  800.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1489.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   696.9 e-6; = (1/var)*||X-X_r||^2 val-train = 637.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 59.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.85; perplexity/K = 9.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.20; perplexity/K = 10.00%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1810.5 e-6; = (1/var)*||X-X_r||^2 =  653.3 e-6 = 36.1 %; (1+beta)*||Z_e-Z_q||^2 =  1157.3 e-6 = 63.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1914.6 e-6; = (1/var)*||X-X_r||^2 =  917.0 e-6 = 47.9 %; (1+beta)*||Z_e-Z_q||^2 =  997.6 e-6 = 52.1 %)
Min.  Avg. Train Loss across Mini-Batch =  638.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1316.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   104.1 e-6; = (1/var)*||X-X_r||^2 val-train = 263.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -159.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.70; perplexity/K = 9.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.74; perplexity/K = 9.52%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  896.3 e-6; = (1/var)*||X-X_r||^2 =  184.4 e-6 = 20.6 %; (1+beta)*||Z_e-Z_q||^2 =  712.0 e-6 = 79.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1721.4 e-6; = (1/var)*||X-X_r||^2 =  952.0 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  769.3 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  616.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1316.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   825.1 e-6; = (1/var)*||X-X_r||^2 val-train = 767.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 57.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.28; perplexity/K = 10.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.89; perplexity/K = 10.92%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1786.5 e-6; = (1/var)*||X-X_r||^2 =  367.9 e-6 = 20.6 %; (1+beta)*||Z_e-Z_q||^2 =  1418.6 e-6 = 79.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2263.7 e-6; = (1/var)*||X-X_r||^2 =  880.6 e-6 = 38.9 %; (1+beta)*||Z_e-Z_q||^2 =  1383.1 e-6 = 61.1 %)
Min.  Avg. Train Loss across Mini-Batch =  616.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1316.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   477.2 e-6; = (1/var)*||X-X_r||^2 val-train = 512.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -35.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.52; perplexity/K = 10.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.26; perplexity/K = 9.82%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  961.9 e-6; = (1/var)*||X-X_r||^2 =  184.9 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  777.0 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1510.2 e-6; = (1/var)*||X-X_r||^2 =  719.4 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  790.9 e-6 = 52.4 %)
Min.  Avg. Train Loss across Mini-Batch =  464.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1169.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   548.3 e-6; = (1/var)*||X-X_r||^2 val-train = 534.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.95; perplexity/K = 9.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.34; perplexity/K = 9.83%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  751.3 e-6; = (1/var)*||X-X_r||^2 =  127.4 e-6 = 17.0 %; (1+beta)*||Z_e-Z_q||^2 =  623.9 e-6 = 83.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1337.1 e-6; = (1/var)*||X-X_r||^2 =  670.5 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  666.6 e-6 = 49.9 %)
Min.  Avg. Train Loss across Mini-Batch =  464.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1062.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   585.8 e-6; = (1/var)*||X-X_r||^2 val-train = 543.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 42.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.88; perplexity/K = 11.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.91; perplexity/K = 11.70%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1786.6 e-6; = (1/var)*||X-X_r||^2 =  340.1 e-6 = 19.0 %; (1+beta)*||Z_e-Z_q||^2 =  1446.5 e-6 = 81.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2302.1 e-6; = (1/var)*||X-X_r||^2 =  920.6 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  1381.6 e-6 = 60.0 %)
Min.  Avg. Train Loss across Mini-Batch =  346.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   515.5 e-6; = (1/var)*||X-X_r||^2 val-train = 580.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -64.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 67.39; perplexity/K = 13.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.99; perplexity/K = 12.50%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3756.3 e-6; = (1/var)*||X-X_r||^2 =  1073.9 e-6 = 28.6 %; (1+beta)*||Z_e-Z_q||^2 =  2682.4 e-6 = 71.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3387.6 e-6; = (1/var)*||X-X_r||^2 =  1286.1 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  2101.6 e-6 = 62.0 %)
Min.  Avg. Train Loss across Mini-Batch =  312.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -368.6 e-6; = (1/var)*||X-X_r||^2 val-train = 212.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -580.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.35; perplexity/K = 10.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.92; perplexity/K = 10.34%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  549.8 e-6; = (1/var)*||X-X_r||^2 =  90.4 e-6 = 16.4 %; (1+beta)*||Z_e-Z_q||^2 =  459.4 e-6 = 83.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1887.7 e-6; = (1/var)*||X-X_r||^2 =  1366.1 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  521.6 e-6 = 27.6 %)
Min.  Avg. Train Loss across Mini-Batch =  312.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1337.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1275.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 62.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.53; perplexity/K = 10.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.37; perplexity/K = 10.81%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  872.0 e-6; = (1/var)*||X-X_r||^2 =  136.3 e-6 = 15.6 %; (1+beta)*||Z_e-Z_q||^2 =  735.7 e-6 = 84.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1651.5 e-6; = (1/var)*||X-X_r||^2 =  898.8 e-6 = 54.4 %; (1+beta)*||Z_e-Z_q||^2 =  752.7 e-6 = 45.6 %)
Min.  Avg. Train Loss across Mini-Batch =  312.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  851.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   779.5 e-6; = (1/var)*||X-X_r||^2 val-train = 762.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.88; perplexity/K = 8.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.55; perplexity/K = 8.51%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26184.4 e-6; = (1/var)*||X-X_r||^2 =  15071.6 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  11112.8 e-6 = 42.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  24263.3 e-6; = (1/var)*||X-X_r||^2 =  11240.8 e-6 = 46.3 %; (1+beta)*||Z_e-Z_q||^2 =  13022.5 e-6 = 53.7 %)
Min.  Avg. Train Loss across Mini-Batch =  306.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  819.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1921.1 e-6; = (1/var)*||X-X_r||^2 val-train = -3830.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1909.7 e-6 

----------------------------------------------------------------------------------

Finished [16:50:23 03.01.2023] 240) Finished running for K = 512 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 144) change_channel_size_across_layers = False:
Total training time is = 0:3:27 h/m/s. 

--------------------------------------------------- 

Started [16:50:23 03.01.2023] 241) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 36) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 766 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.26
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.04
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.18
4                           encoder.sequential_convs.conv2d_5.weight                       131            17.10
5                                  encoder.pre_residual_stack.weight                       147            19.19
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.70
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.70
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
10                             encoder.channel_adjusting_conv.weight                         8             1.04
11                                                       VQ.E.weight                        32             4.18
12                             decoder.channel_adjusting_conv.weight                        73             9.53
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.70
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.70
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.10
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.18
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.04
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.26
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.23; perplexity/K = 0.24%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1472139.6 e-6; = (1/var)*||X-X_r||^2 =  992327.1 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  479812.5 e-6 = 32.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  960414.3 e-6; = (1/var)*||X-X_r||^2 =  952745.2 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  7669.1 e-6 = 0.8 %)
Min.  Avg. Train Loss across Mini-Batch =  983180.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  939211.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -511725.3 e-6; = (1/var)*||X-X_r||^2 val-train = -39581.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -472143.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.25; perplexity/K = 7.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.16; perplexity/K = 7.26%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  595024.1 e-6; = (1/var)*||X-X_r||^2 =  543056.7 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  51967.4 e-6 = 8.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  577161.7 e-6; = (1/var)*||X-X_r||^2 =  534990.4 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  42171.3 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  595024.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  573303.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17862.4 e-6; = (1/var)*||X-X_r||^2 val-train = -8066.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9796.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.98; perplexity/K = 8.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.79; perplexity/K = 8.75%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  527640.1 e-6; = (1/var)*||X-X_r||^2 =  479645.2 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  47994.8 e-6 = 9.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  532371.0 e-6; = (1/var)*||X-X_r||^2 =  484893.7 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  47477.3 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  527232.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  523706.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4730.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5248.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -517.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.28; perplexity/K = 10.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.93; perplexity/K = 9.56%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  483631.8 e-6; = (1/var)*||X-X_r||^2 =  439197.8 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  44434.0 e-6 = 9.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  498233.5 e-6; = (1/var)*||X-X_r||^2 =  449008.6 e-6 = 90.1 %; (1+beta)*||Z_e-Z_q||^2 =  49224.9 e-6 = 9.9 %)
Min.  Avg. Train Loss across Mini-Batch =  483483.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  494343.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14601.7 e-6; = (1/var)*||X-X_r||^2 val-train = 9810.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4790.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.57; perplexity/K = 9.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.73; perplexity/K = 10.10%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  466550.9 e-6; = (1/var)*||X-X_r||^2 =  421881.4 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  44669.5 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  477223.9 e-6; = (1/var)*||X-X_r||^2 =  430662.5 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  46561.4 e-6 = 9.8 %)
Min.  Avg. Train Loss across Mini-Batch =  456742.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  474057.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10673.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8781.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1891.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.43; perplexity/K = 11.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.28; perplexity/K = 11.38%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  436900.3 e-6; = (1/var)*||X-X_r||^2 =  402880.5 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  34019.8 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  460767.4 e-6; = (1/var)*||X-X_r||^2 =  422201.1 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  38566.3 e-6 = 8.4 %)
Min.  Avg. Train Loss across Mini-Batch =  435802.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  455670.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23867.1 e-6; = (1/var)*||X-X_r||^2 val-train = 19320.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4546.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.13; perplexity/K = 10.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.35; perplexity/K = 11.01%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  423776.1 e-6; = (1/var)*||X-X_r||^2 =  395057.4 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  28718.8 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  452230.5 e-6; = (1/var)*||X-X_r||^2 =  419419.2 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  32811.3 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  422659.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  448337.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   28454.4 e-6; = (1/var)*||X-X_r||^2 val-train = 24361.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4092.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.85; perplexity/K = 11.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.39; perplexity/K = 10.62%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  420290.5 e-6; = (1/var)*||X-X_r||^2 =  392010.3 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  28280.2 e-6 = 6.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  441489.5 e-6; = (1/var)*||X-X_r||^2 =  408991.3 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  32498.2 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  413955.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  441489.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21199.0 e-6; = (1/var)*||X-X_r||^2 val-train = 16981.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4218.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.60; perplexity/K = 11.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.27; perplexity/K = 11.19%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405072.9 e-6; = (1/var)*||X-X_r||^2 =  383668.8 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  21404.2 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  436118.6 e-6; = (1/var)*||X-X_r||^2 =  408921.7 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  27196.8 e-6 = 6.2 %)
Min.  Avg. Train Loss across Mini-Batch =  405072.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  436118.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31045.7 e-6; = (1/var)*||X-X_r||^2 val-train = 25253.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5792.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.08; perplexity/K = 11.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.05; perplexity/K = 10.95%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  401479.1 e-6; = (1/var)*||X-X_r||^2 =  381345.4 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  20133.7 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  434141.7 e-6; = (1/var)*||X-X_r||^2 =  408784.8 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  25356.9 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  401479.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431808.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32662.6 e-6; = (1/var)*||X-X_r||^2 val-train = 27439.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5223.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.12; perplexity/K = 11.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.44; perplexity/K = 11.41%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  400878.4 e-6; = (1/var)*||X-X_r||^2 =  380944.1 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  19934.4 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  435063.6 e-6; = (1/var)*||X-X_r||^2 =  409231.4 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  25832.2 e-6 = 5.9 %)
Min.  Avg. Train Loss across Mini-Batch =  395679.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431600.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34185.2 e-6; = (1/var)*||X-X_r||^2 val-train = 28287.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5897.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.77; perplexity/K = 11.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.28; perplexity/K = 10.99%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393804.2 e-6; = (1/var)*||X-X_r||^2 =  376300.1 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  17504.1 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  428903.1 e-6; = (1/var)*||X-X_r||^2 =  405152.9 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  23750.2 e-6 = 5.5 %)
Min.  Avg. Train Loss across Mini-Batch =  393050.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426819.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35098.9 e-6; = (1/var)*||X-X_r||^2 val-train = 28852.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6246.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.48; perplexity/K = 11.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.86; perplexity/K = 11.30%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391577.6 e-6; = (1/var)*||X-X_r||^2 =  375296.6 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  16281.0 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  427640.0 e-6; = (1/var)*||X-X_r||^2 =  406593.6 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  21046.3 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  389336.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  424965.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36062.4 e-6; = (1/var)*||X-X_r||^2 val-train = 31297.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4765.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.43; perplexity/K = 11.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.81; perplexity/K = 10.70%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388253.2 e-6; = (1/var)*||X-X_r||^2 =  373198.6 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15054.6 e-6 = 3.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  423390.4 e-6; = (1/var)*||X-X_r||^2 =  403079.2 e-6 = 95.2 %; (1+beta)*||Z_e-Z_q||^2 =  20311.2 e-6 = 4.8 %)
Min.  Avg. Train Loss across Mini-Batch =  386062.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  421072.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35137.2 e-6; = (1/var)*||X-X_r||^2 val-train = 29880.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5256.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.34; perplexity/K = 11.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.84; perplexity/K = 11.10%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  383306.0 e-6; = (1/var)*||X-X_r||^2 =  370212.0 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  13094.1 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  424623.5 e-6; = (1/var)*||X-X_r||^2 =  405464.6 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  19158.9 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  383306.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  418964.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41317.5 e-6; = (1/var)*||X-X_r||^2 val-train = 35252.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6064.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.56; perplexity/K = 11.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.72; perplexity/K = 11.08%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  381733.3 e-6; = (1/var)*||X-X_r||^2 =  369057.6 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  12675.7 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  419517.6 e-6; = (1/var)*||X-X_r||^2 =  400897.8 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  18619.9 e-6 = 4.4 %)
Min.  Avg. Train Loss across Mini-Batch =  381648.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  418359.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37784.4 e-6; = (1/var)*||X-X_r||^2 val-train = 31840.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5944.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.19; perplexity/K = 11.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.57; perplexity/K = 11.05%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  383354.3 e-6; = (1/var)*||X-X_r||^2 =  369666.9 e-6 = 96.4 %; (1+beta)*||Z_e-Z_q||^2 =  13687.4 e-6 = 3.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  417584.5 e-6; = (1/var)*||X-X_r||^2 =  398770.9 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  18813.6 e-6 = 4.5 %)
Min.  Avg. Train Loss across Mini-Batch =  379821.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  417584.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34230.2 e-6; = (1/var)*||X-X_r||^2 val-train = 29104.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5126.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.03; perplexity/K = 11.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.45; perplexity/K = 11.03%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  378577.1 e-6; = (1/var)*||X-X_r||^2 =  367376.3 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11200.7 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  420317.2 e-6; = (1/var)*||X-X_r||^2 =  403677.2 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16640.0 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  378299.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  417584.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41740.2 e-6; = (1/var)*||X-X_r||^2 val-train = 36300.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5439.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.24; perplexity/K = 10.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.17; perplexity/K = 11.56%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  380366.7 e-6; = (1/var)*||X-X_r||^2 =  368030.0 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  12336.7 e-6 = 3.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  421650.3 e-6; = (1/var)*||X-X_r||^2 =  403976.7 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17673.6 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  376701.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416839.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41283.6 e-6; = (1/var)*||X-X_r||^2 val-train = 35946.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5336.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.93; perplexity/K = 11.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.83; perplexity/K = 10.90%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  378208.6 e-6; = (1/var)*||X-X_r||^2 =  367344.5 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  10864.1 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  426149.3 e-6; = (1/var)*||X-X_r||^2 =  409737.4 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  16411.9 e-6 = 3.9 %)
Min.  Avg. Train Loss across Mini-Batch =  375409.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416839.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47940.6 e-6; = (1/var)*||X-X_r||^2 val-train = 42392.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5547.8 e-6 

----------------------------------------------------------------------------------

Finished [17:41:13 03.01.2023] 241) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 36) change_channel_size_across_layers = True:
Total training time is = 0:1:50 h/m/s. 

--------------------------------------------------- 

Started [17:41:13 03.01.2023] 242) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 36) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2498 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.32
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.28
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.24
4                           encoder.sequential_convs.conv2d_5.weight                       524            20.98
5                                  encoder.pre_residual_stack.weight                       589            23.58
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.92
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.92
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
10                             encoder.channel_adjusting_conv.weight                        16             0.64
11                                                       VQ.E.weight                        32             1.28
12                             decoder.channel_adjusting_conv.weight                       147             5.88
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.92
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.92
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.98
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.24
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.28
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1002484.7 e-6; = (1/var)*||X-X_r||^2 =  1002446.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  37.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963527.1 e-6; = (1/var)*||X-X_r||^2 =  963489.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  37.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -38957.5 e-6; = (1/var)*||X-X_r||^2 val-train = -38956.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000356.2 e-6; = (1/var)*||X-X_r||^2 =  1000355.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964525.9 e-6; = (1/var)*||X-X_r||^2 =  964525.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35830.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35830.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999838.9 e-6; = (1/var)*||X-X_r||^2 =  999838.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963218.2 e-6; = (1/var)*||X-X_r||^2 =  963218.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36620.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36620.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999569.2 e-6; = (1/var)*||X-X_r||^2 =  999569.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963503.6 e-6; = (1/var)*||X-X_r||^2 =  963503.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36065.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36065.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999844.4 e-6; = (1/var)*||X-X_r||^2 =  999844.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963434.0 e-6; = (1/var)*||X-X_r||^2 =  963434.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36410.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36410.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999412.8 e-6; = (1/var)*||X-X_r||^2 =  999412.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963177.8 e-6; = (1/var)*||X-X_r||^2 =  963177.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36234.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36234.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999393.4 e-6; = (1/var)*||X-X_r||^2 =  999393.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962906.3 e-6; = (1/var)*||X-X_r||^2 =  962906.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36487.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36487.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999330.0 e-6; = (1/var)*||X-X_r||^2 =  999330.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962901.0 e-6; = (1/var)*||X-X_r||^2 =  962901.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36429.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36428.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999346.2 e-6; = (1/var)*||X-X_r||^2 =  999346.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963012.0 e-6; = (1/var)*||X-X_r||^2 =  963012.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36334.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36334.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999304.3 e-6; = (1/var)*||X-X_r||^2 =  999304.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962894.6 e-6; = (1/var)*||X-X_r||^2 =  962894.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36409.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36409.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999208.4 e-6; = (1/var)*||X-X_r||^2 =  999208.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962941.3 e-6; = (1/var)*||X-X_r||^2 =  962941.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36267.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36267.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999440.5 e-6; = (1/var)*||X-X_r||^2 =  999440.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963238.6 e-6; = (1/var)*||X-X_r||^2 =  963238.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36201.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36201.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999274.3 e-6; = (1/var)*||X-X_r||^2 =  999274.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963910.0 e-6; = (1/var)*||X-X_r||^2 =  963910.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35364.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35364.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999478.6 e-6; = (1/var)*||X-X_r||^2 =  999478.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963482.8 e-6; = (1/var)*||X-X_r||^2 =  963482.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35995.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35995.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999197.8 e-6; = (1/var)*||X-X_r||^2 =  999197.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963248.8 e-6; = (1/var)*||X-X_r||^2 =  963248.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35948.9 e-6; = (1/var)*||X-X_r||^2 val-train = -35948.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999252.0 e-6; = (1/var)*||X-X_r||^2 =  999252.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962865.3 e-6; = (1/var)*||X-X_r||^2 =  962865.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36386.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36386.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999231.0 e-6; = (1/var)*||X-X_r||^2 =  999231.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963051.4 e-6; = (1/var)*||X-X_r||^2 =  963051.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36179.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36179.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999202.2 e-6; = (1/var)*||X-X_r||^2 =  999202.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962978.3 e-6; = (1/var)*||X-X_r||^2 =  962978.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36223.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36223.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999380.6 e-6; = (1/var)*||X-X_r||^2 =  999380.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963056.7 e-6; = (1/var)*||X-X_r||^2 =  963056.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36323.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36323.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999224.1 e-6; = (1/var)*||X-X_r||^2 =  999224.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962971.9 e-6; = (1/var)*||X-X_r||^2 =  962971.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  976299.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938217.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36252.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36252.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [18:31:22 03.01.2023] 242) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 36) change_channel_size_across_layers = True:
Total training time is = 0:1:8 h/m/s. 

--------------------------------------------------- 

Started [18:31:22 03.01.2023] 243) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 36) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2528 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.24
1                           encoder.sequential_convs.conv2d_2.weight                       262            10.36
2                           encoder.sequential_convs.conv2d_3.weight                       262            10.36
3                           encoder.sequential_convs.conv2d_4.weight                       262            10.36
4                           encoder.sequential_convs.conv2d_5.weight                       262            10.36
5                                  encoder.pre_residual_stack.weight                       147             5.81
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.42
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.42
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
10                             encoder.channel_adjusting_conv.weight                         8             0.32
11                                                       VQ.E.weight                        32             1.27
12                             decoder.channel_adjusting_conv.weight                        73             2.89
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.42
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.42
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262            10.36
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262            10.36
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262            10.36
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262            10.36
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.24

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 2.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 2.39%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  785609.7 e-6; = (1/var)*||X-X_r||^2 =  468612.1 e-6 = 59.6 %; (1+beta)*||Z_e-Z_q||^2 =  316997.6 e-6 = 40.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  698719.6 e-6; = (1/var)*||X-X_r||^2 =  458067.5 e-6 = 65.6 %; (1+beta)*||Z_e-Z_q||^2 =  240652.1 e-6 = 34.4 %)
Min.  Avg. Train Loss across Mini-Batch =  680095.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  610551.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -86890.1 e-6; = (1/var)*||X-X_r||^2 val-train = -10544.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -76345.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.07; perplexity/K = 3.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.61; perplexity/K = 3.05%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  381127.9 e-6; = (1/var)*||X-X_r||^2 =  220027.0 e-6 = 57.7 %; (1+beta)*||Z_e-Z_q||^2 =  161100.9 e-6 = 42.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  431410.5 e-6; = (1/var)*||X-X_r||^2 =  275572.0 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  155838.4 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  373669.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383111.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50282.6 e-6; = (1/var)*||X-X_r||^2 val-train = 55545.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5262.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.82; perplexity/K = 4.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.30; perplexity/K = 4.55%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  267637.3 e-6; = (1/var)*||X-X_r||^2 =  139746.9 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  127890.3 e-6 = 47.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  288252.0 e-6; = (1/var)*||X-X_r||^2 =  164407.9 e-6 = 57.0 %; (1+beta)*||Z_e-Z_q||^2 =  123844.1 e-6 = 43.0 %)
Min.  Avg. Train Loss across Mini-Batch =  236815.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  283291.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20614.7 e-6; = (1/var)*||X-X_r||^2 val-train = 24660.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4046.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.04; perplexity/K = 5.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.55; perplexity/K = 4.79%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  174132.4 e-6; = (1/var)*||X-X_r||^2 =  77981.2 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  96151.1 e-6 = 55.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  246035.8 e-6; = (1/var)*||X-X_r||^2 =  138246.5 e-6 = 56.2 %; (1+beta)*||Z_e-Z_q||^2 =  107789.3 e-6 = 43.8 %)
Min.  Avg. Train Loss across Mini-Batch =  163130.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  243163.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   71903.4 e-6; = (1/var)*||X-X_r||^2 val-train = 60265.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11638.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.12; perplexity/K = 5.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.35; perplexity/K = 5.34%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  104018.2 e-6; = (1/var)*||X-X_r||^2 =  44131.3 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  59886.8 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  209640.1 e-6; = (1/var)*||X-X_r||^2 =  126183.8 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  83456.3 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  104018.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  207353.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   105621.9 e-6; = (1/var)*||X-X_r||^2 val-train = 82052.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23569.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.86; perplexity/K = 5.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.49; perplexity/K = 5.37%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  68879.6 e-6; = (1/var)*||X-X_r||^2 =  29488.0 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  39391.6 e-6 = 57.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  193528.8 e-6; = (1/var)*||X-X_r||^2 =  127310.7 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  66218.1 e-6 = 34.2 %)
Min.  Avg. Train Loss across Mini-Batch =  67679.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  187378.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   124649.1 e-6; = (1/var)*||X-X_r||^2 val-train = 97822.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26826.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.10; perplexity/K = 5.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.77; perplexity/K = 5.42%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  155453.3 e-6; = (1/var)*||X-X_r||^2 =  79177.7 e-6 = 50.9 %; (1+beta)*||Z_e-Z_q||^2 =  76275.6 e-6 = 49.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  259594.4 e-6; = (1/var)*||X-X_r||^2 =  161400.2 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  98194.2 e-6 = 37.8 %)
Min.  Avg. Train Loss across Mini-Batch =  50139.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  177540.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   104141.1 e-6; = (1/var)*||X-X_r||^2 val-train = 82222.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21918.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.75; perplexity/K = 5.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.07; perplexity/K = 5.29%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37910.4 e-6; = (1/var)*||X-X_r||^2 =  14935.9 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  22974.5 e-6 = 60.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  172320.1 e-6; = (1/var)*||X-X_r||^2 =  120184.9 e-6 = 69.7 %; (1+beta)*||Z_e-Z_q||^2 =  52135.2 e-6 = 30.3 %)
Min.  Avg. Train Loss across Mini-Batch =  36720.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  170241.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   134409.7 e-6; = (1/var)*||X-X_r||^2 val-train = 105249.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29160.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.06; perplexity/K = 5.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.52; perplexity/K = 5.38%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27188.9 e-6; = (1/var)*||X-X_r||^2 =  11175.0 e-6 = 41.1 %; (1+beta)*||Z_e-Z_q||^2 =  16013.9 e-6 = 58.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  168271.5 e-6; = (1/var)*||X-X_r||^2 =  122232.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  46039.4 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  27188.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  165654.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   141082.6 e-6; = (1/var)*||X-X_r||^2 val-train = 111057.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30025.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.58; perplexity/K = 5.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.92; perplexity/K = 5.45%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39585.2 e-6; = (1/var)*||X-X_r||^2 =  13406.9 e-6 = 33.9 %; (1+beta)*||Z_e-Z_q||^2 =  26178.3 e-6 = 66.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  166774.9 e-6; = (1/var)*||X-X_r||^2 =  117126.3 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  49648.6 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  20408.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  162060.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   127189.6 e-6; = (1/var)*||X-X_r||^2 val-train = 103719.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23470.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.39; perplexity/K = 5.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.16; perplexity/K = 5.69%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38759.2 e-6; = (1/var)*||X-X_r||^2 =  13175.7 e-6 = 34.0 %; (1+beta)*||Z_e-Z_q||^2 =  25583.5 e-6 = 66.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  167908.0 e-6; = (1/var)*||X-X_r||^2 =  119609.4 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  48298.6 e-6 = 28.8 %)
Min.  Avg. Train Loss across Mini-Batch =  16535.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160830.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   129148.8 e-6; = (1/var)*||X-X_r||^2 val-train = 106433.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22715.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.57; perplexity/K = 6.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.95; perplexity/K = 5.66%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  160123.8 e-6; = (1/var)*||X-X_r||^2 =  81756.6 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  78367.2 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  226087.9 e-6; = (1/var)*||X-X_r||^2 =  143556.2 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  82531.6 e-6 = 36.5 %)
Min.  Avg. Train Loss across Mini-Batch =  13711.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160830.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   65964.1 e-6; = (1/var)*||X-X_r||^2 val-train = 61799.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4164.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.44; perplexity/K = 5.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.77; perplexity/K = 5.42%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12645.1 e-6; = (1/var)*||X-X_r||^2 =  5608.6 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  7036.5 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  158813.6 e-6; = (1/var)*||X-X_r||^2 =  120298.4 e-6 = 75.7 %; (1+beta)*||Z_e-Z_q||^2 =  38515.2 e-6 = 24.3 %)
Min.  Avg. Train Loss across Mini-Batch =  12432.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  158161.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   146168.6 e-6; = (1/var)*||X-X_r||^2 val-train = 114689.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 31478.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.90; perplexity/K = 5.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.15; perplexity/K = 5.50%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21587.7 e-6; = (1/var)*||X-X_r||^2 =  6936.6 e-6 = 32.1 %; (1+beta)*||Z_e-Z_q||^2 =  14651.1 e-6 = 67.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  165857.5 e-6; = (1/var)*||X-X_r||^2 =  123813.1 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  42044.4 e-6 = 25.3 %)
Min.  Avg. Train Loss across Mini-Batch =  9724.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157223.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   144269.8 e-6; = (1/var)*||X-X_r||^2 val-train = 116876.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27393.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.13; perplexity/K = 5.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.89; perplexity/K = 5.45%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47447.0 e-6; = (1/var)*||X-X_r||^2 =  14538.1 e-6 = 30.6 %; (1+beta)*||Z_e-Z_q||^2 =  32908.9 e-6 = 69.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  183210.6 e-6; = (1/var)*||X-X_r||^2 =  125578.9 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  57631.7 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8793.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157223.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   135763.6 e-6; = (1/var)*||X-X_r||^2 val-train = 111040.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24722.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.09; perplexity/K = 5.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.82; perplexity/K = 5.82%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  126424.6 e-6; = (1/var)*||X-X_r||^2 =  73256.0 e-6 = 57.9 %; (1+beta)*||Z_e-Z_q||^2 =  53168.6 e-6 = 42.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  294074.8 e-6; = (1/var)*||X-X_r||^2 =  207431.1 e-6 = 70.5 %; (1+beta)*||Z_e-Z_q||^2 =  86643.6 e-6 = 29.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8139.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154624.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   167650.2 e-6; = (1/var)*||X-X_r||^2 val-train = 134175.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 33475.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.50; perplexity/K = 5.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.58; perplexity/K = 5.58%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13357.4 e-6; = (1/var)*||X-X_r||^2 =  4542.7 e-6 = 34.0 %; (1+beta)*||Z_e-Z_q||^2 =  8814.7 e-6 = 66.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  164094.6 e-6; = (1/var)*||X-X_r||^2 =  126156.6 e-6 = 76.9 %; (1+beta)*||Z_e-Z_q||^2 =  37938.0 e-6 = 23.1 %)
Min.  Avg. Train Loss across Mini-Batch =  7472.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154624.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   150737.2 e-6; = (1/var)*||X-X_r||^2 val-train = 121613.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29123.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.91; perplexity/K = 5.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.44; perplexity/K = 5.36%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7069.2 e-6; = (1/var)*||X-X_r||^2 =  3526.9 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  3542.3 e-6 = 50.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  162310.1 e-6; = (1/var)*||X-X_r||^2 =  128368.2 e-6 = 79.1 %; (1+beta)*||Z_e-Z_q||^2 =  33941.9 e-6 = 20.9 %)
Min.  Avg. Train Loss across Mini-Batch =  7069.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154624.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   155240.9 e-6; = (1/var)*||X-X_r||^2 val-train = 124841.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30399.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.64; perplexity/K = 5.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.59; perplexity/K = 6.37%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29039.3 e-6; = (1/var)*||X-X_r||^2 =  11033.9 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  18005.4 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  178811.4 e-6; = (1/var)*||X-X_r||^2 =  132335.2 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  46476.2 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  6343.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154624.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   149772.1 e-6; = (1/var)*||X-X_r||^2 val-train = 121301.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28470.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.31; perplexity/K = 5.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.92; perplexity/K = 5.45%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72496.3 e-6; = (1/var)*||X-X_r||^2 =  27342.5 e-6 = 37.7 %; (1+beta)*||Z_e-Z_q||^2 =  45153.8 e-6 = 62.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  184302.9 e-6; = (1/var)*||X-X_r||^2 =  129550.9 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  54752.0 e-6 = 29.7 %)
Min.  Avg. Train Loss across Mini-Batch =  5935.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  153904.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   111806.5 e-6; = (1/var)*||X-X_r||^2 val-train = 102208.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9598.2 e-6 

----------------------------------------------------------------------------------

Finished [19:36:46 03.01.2023] 243) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 36) change_channel_size_across_layers = False:
Total training time is = 0:1:23 h/m/s. 

--------------------------------------------------- 

Started [19:36:46 03.01.2023] 244) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 36) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9516 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.13
1                           encoder.sequential_convs.conv2d_2.weight                      1048            11.01
2                           encoder.sequential_convs.conv2d_3.weight                      1048            11.01
3                           encoder.sequential_convs.conv2d_4.weight                      1048            11.01
4                           encoder.sequential_convs.conv2d_5.weight                      1048            11.01
5                                  encoder.pre_residual_stack.weight                       589             6.19
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                        32             0.34
12                             decoder.channel_adjusting_conv.weight                       147             1.54
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            11.01
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            11.01
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            11.01
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            11.01
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.81; perplexity/K = 3.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.27; perplexity/K = 2.79%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  674093.2 e-6; = (1/var)*||X-X_r||^2 =  328254.2 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  345839.0 e-6 = 51.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  717273.5 e-6; = (1/var)*||X-X_r||^2 =  341642.2 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  375631.3 e-6 = 52.4 %)
Min.  Avg. Train Loss across Mini-Batch =  646370.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  633744.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43180.3 e-6; = (1/var)*||X-X_r||^2 val-train = 13388.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29792.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.87; perplexity/K = 5.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.15; perplexity/K = 4.91%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  337476.3 e-6; = (1/var)*||X-X_r||^2 =  131144.6 e-6 = 38.9 %; (1+beta)*||Z_e-Z_q||^2 =  206331.7 e-6 = 61.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  405037.3 e-6; = (1/var)*||X-X_r||^2 =  182892.2 e-6 = 45.2 %; (1+beta)*||Z_e-Z_q||^2 =  222145.1 e-6 = 54.8 %)
Min.  Avg. Train Loss across Mini-Batch =  337476.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  404689.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   67561.0 e-6; = (1/var)*||X-X_r||^2 val-train = 51747.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15813.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.40; perplexity/K = 6.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.47; perplexity/K = 6.73%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  197583.3 e-6; = (1/var)*||X-X_r||^2 =  68456.1 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  129127.2 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  308907.0 e-6; = (1/var)*||X-X_r||^2 =  156919.2 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  151987.7 e-6 = 49.2 %)
Min.  Avg. Train Loss across Mini-Batch =  154033.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  264042.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   111323.6 e-6; = (1/var)*||X-X_r||^2 val-train = 88463.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22860.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.20; perplexity/K = 6.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.20; perplexity/K = 7.07%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50269.3 e-6; = (1/var)*||X-X_r||^2 =  13178.1 e-6 = 26.2 %; (1+beta)*||Z_e-Z_q||^2 =  37091.2 e-6 = 73.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  199802.4 e-6; = (1/var)*||X-X_r||^2 =  132370.1 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  67432.3 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  50269.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  197553.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   149533.1 e-6; = (1/var)*||X-X_r||^2 val-train = 119192.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30341.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.47; perplexity/K = 7.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.17; perplexity/K = 7.65%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  182948.5 e-6; = (1/var)*||X-X_r||^2 =  77635.1 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  105313.4 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  298120.8 e-6; = (1/var)*||X-X_r||^2 =  182052.6 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  116068.2 e-6 = 38.9 %)
Min.  Avg. Train Loss across Mini-Batch =  20133.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  176281.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   115172.3 e-6; = (1/var)*||X-X_r||^2 val-train = 104417.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10754.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.75; perplexity/K = 6.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.51; perplexity/K = 6.94%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21211.0 e-6; = (1/var)*||X-X_r||^2 =  5545.2 e-6 = 26.1 %; (1+beta)*||Z_e-Z_q||^2 =  15665.8 e-6 = 73.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  173905.6 e-6; = (1/var)*||X-X_r||^2 =  130507.7 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  43398.0 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  11732.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  169411.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   152694.7 e-6; = (1/var)*||X-X_r||^2 val-train = 124962.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27732.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.89; perplexity/K = 7.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.07; perplexity/K = 7.44%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24689.1 e-6; = (1/var)*||X-X_r||^2 =  5271.6 e-6 = 21.4 %; (1+beta)*||Z_e-Z_q||^2 =  19417.6 e-6 = 78.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  182856.4 e-6; = (1/var)*||X-X_r||^2 =  136799.6 e-6 = 74.8 %; (1+beta)*||Z_e-Z_q||^2 =  46056.8 e-6 = 25.2 %)
Min.  Avg. Train Loss across Mini-Batch =  9031.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  167731.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   158167.3 e-6; = (1/var)*||X-X_r||^2 val-train = 131528.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26639.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.62; perplexity/K = 6.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.90; perplexity/K = 7.21%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8220.9 e-6; = (1/var)*||X-X_r||^2 =  2213.1 e-6 = 26.9 %; (1+beta)*||Z_e-Z_q||^2 =  6007.7 e-6 = 73.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  174145.7 e-6; = (1/var)*||X-X_r||^2 =  139208.9 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  34936.8 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5510.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  167731.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   165924.8 e-6; = (1/var)*||X-X_r||^2 val-train = 136995.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28929.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.62; perplexity/K = 6.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.86; perplexity/K = 6.81%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6239.6 e-6; = (1/var)*||X-X_r||^2 =  1756.1 e-6 = 28.1 %; (1+beta)*||Z_e-Z_q||^2 =  4483.5 e-6 = 71.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  173454.3 e-6; = (1/var)*||X-X_r||^2 =  139827.0 e-6 = 80.6 %; (1+beta)*||Z_e-Z_q||^2 =  33627.2 e-6 = 19.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4573.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  167731.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   167214.7 e-6; = (1/var)*||X-X_r||^2 val-train = 138071.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29143.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.80; perplexity/K = 7.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.64; perplexity/K = 7.35%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60311.8 e-6; = (1/var)*||X-X_r||^2 =  14645.2 e-6 = 24.3 %; (1+beta)*||Z_e-Z_q||^2 =  45666.6 e-6 = 75.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  192161.8 e-6; = (1/var)*||X-X_r||^2 =  133091.0 e-6 = 69.3 %; (1+beta)*||Z_e-Z_q||^2 =  59070.8 e-6 = 30.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2953.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  166791.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   131850.0 e-6; = (1/var)*||X-X_r||^2 val-train = 118445.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13404.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.72; perplexity/K = 7.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.47; perplexity/K = 7.12%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7015.4 e-6; = (1/var)*||X-X_r||^2 =  2318.6 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  4696.8 e-6 = 66.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  172934.1 e-6; = (1/var)*||X-X_r||^2 =  138313.0 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  34621.1 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2667.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  166791.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   165918.7 e-6; = (1/var)*||X-X_r||^2 val-train = 135994.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29924.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.57; perplexity/K = 7.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.95; perplexity/K = 6.83%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5465.9 e-6; = (1/var)*||X-X_r||^2 =  1430.4 e-6 = 26.2 %; (1+beta)*||Z_e-Z_q||^2 =  4035.5 e-6 = 73.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  171266.0 e-6; = (1/var)*||X-X_r||^2 =  138945.1 e-6 = 81.1 %; (1+beta)*||Z_e-Z_q||^2 =  32320.9 e-6 = 18.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2667.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  162297.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   165800.1 e-6; = (1/var)*||X-X_r||^2 val-train = 137514.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28285.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.83; perplexity/K = 7.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.04; perplexity/K = 7.04%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  111638.8 e-6; = (1/var)*||X-X_r||^2 =  43537.2 e-6 = 39.0 %; (1+beta)*||Z_e-Z_q||^2 =  68101.6 e-6 = 61.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  235904.3 e-6; = (1/var)*||X-X_r||^2 =  151417.3 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  84487.0 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2667.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  162297.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   124265.5 e-6; = (1/var)*||X-X_r||^2 val-train = 107880.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16385.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.96; perplexity/K = 6.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.05; perplexity/K = 6.85%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2825.1 e-6; = (1/var)*||X-X_r||^2 =  925.1 e-6 = 32.7 %; (1+beta)*||Z_e-Z_q||^2 =  1900.0 e-6 = 67.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  164003.8 e-6; = (1/var)*||X-X_r||^2 =  138069.9 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  25933.9 e-6 = 15.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2667.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160634.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161178.7 e-6; = (1/var)*||X-X_r||^2 val-train = 137144.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24033.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.74; perplexity/K = 6.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.93; perplexity/K = 6.82%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3629.5 e-6; = (1/var)*||X-X_r||^2 =  1488.1 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  2141.4 e-6 = 59.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  164845.5 e-6; = (1/var)*||X-X_r||^2 =  139103.7 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  25741.8 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1719.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160634.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161216.0 e-6; = (1/var)*||X-X_r||^2 val-train = 137615.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23600.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.61; perplexity/K = 6.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.26; perplexity/K = 7.47%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2303.6 e-6; = (1/var)*||X-X_r||^2 =  812.3 e-6 = 35.3 %; (1+beta)*||Z_e-Z_q||^2 =  1491.3 e-6 = 64.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  169475.9 e-6; = (1/var)*||X-X_r||^2 =  142978.3 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  26497.6 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1719.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  160634.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   167172.3 e-6; = (1/var)*||X-X_r||^2 val-train = 142166.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25006.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.69; perplexity/K = 6.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.98; perplexity/K = 7.03%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2318.4 e-6; = (1/var)*||X-X_r||^2 =  715.8 e-6 = 30.9 %; (1+beta)*||Z_e-Z_q||^2 =  1602.6 e-6 = 69.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  165667.2 e-6; = (1/var)*||X-X_r||^2 =  142708.5 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  22958.6 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1719.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  159062.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   163348.8 e-6; = (1/var)*||X-X_r||^2 val-train = 141992.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21356.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.01; perplexity/K = 7.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.61; perplexity/K = 6.76%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1616.6 e-6; = (1/var)*||X-X_r||^2 =  688.5 e-6 = 42.6 %; (1+beta)*||Z_e-Z_q||^2 =  928.1 e-6 = 57.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  160722.6 e-6; = (1/var)*||X-X_r||^2 =  141737.5 e-6 = 88.2 %; (1+beta)*||Z_e-Z_q||^2 =  18985.1 e-6 = 11.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1616.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  158230.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159106.0 e-6; = (1/var)*||X-X_r||^2 val-train = 141049.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18057.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.02; perplexity/K = 7.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.15; perplexity/K = 6.87%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6531.5 e-6; = (1/var)*||X-X_r||^2 =  1146.8 e-6 = 17.6 %; (1+beta)*||Z_e-Z_q||^2 =  5384.7 e-6 = 82.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  164522.2 e-6; = (1/var)*||X-X_r||^2 =  138858.2 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  25663.9 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1461.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  158230.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157990.7 e-6; = (1/var)*||X-X_r||^2 val-train = 137711.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20279.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.91; perplexity/K = 7.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.77; perplexity/K = 6.79%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1920.2 e-6; = (1/var)*||X-X_r||^2 =  642.2 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  1278.0 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  164526.8 e-6; = (1/var)*||X-X_r||^2 =  142139.4 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  22387.4 e-6 = 13.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1195.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  158230.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   162606.6 e-6; = (1/var)*||X-X_r||^2 val-train = 141497.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21109.4 e-6 

----------------------------------------------------------------------------------

Finished [21:53:43 03.01.2023] 244) Finished running for K = 512 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 36) change_channel_size_across_layers = False:
Total training time is = 0:1:57 h/m/s. 

--------------------------------------------------- 

Started [21:53:43 03.01.2023] 245) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 9) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 766 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.26
3                           encoder.sequential_convs.conv2d_4.weight                         8             1.04
4                           encoder.sequential_convs.conv2d_5.weight                        32             4.18
5                           encoder.sequential_convs.conv2d_6.weight                       131            17.10
6                                  encoder.pre_residual_stack.weight                       147            19.19
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.70
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.70
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
11                             encoder.channel_adjusting_conv.weight                         8             1.04
12                                                       VQ.E.weight                        32             4.18
13                             decoder.channel_adjusting_conv.weight                        73             9.53
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.70
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.52
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.70
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.52
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.10
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.18
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.04
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.26
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.01; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.25; perplexity/K = 0.44%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1045853.2 e-6; = (1/var)*||X-X_r||^2 =  967466.7 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  78386.5 e-6 = 7.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  968595.2 e-6; = (1/var)*||X-X_r||^2 =  934889.1 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  33706.1 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  990171.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  940653.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -77258.0 e-6; = (1/var)*||X-X_r||^2 val-train = -32577.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -44680.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.36; perplexity/K = 1.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.18; perplexity/K = 1.79%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1771929.8 e-6; = (1/var)*||X-X_r||^2 =  787717.2 e-6 = 44.5 %; (1+beta)*||Z_e-Z_q||^2 =  984212.6 e-6 = 55.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1390724.2 e-6; = (1/var)*||X-X_r||^2 =  758080.0 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  632644.2 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  990171.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  940653.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -381205.6 e-6; = (1/var)*||X-X_r||^2 val-train = -29637.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -351568.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.98; perplexity/K = 1.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.14; perplexity/K = 1.20%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  979148.4 e-6; = (1/var)*||X-X_r||^2 =  709751.4 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  269397.0 e-6 = 27.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  872063.8 e-6; = (1/var)*||X-X_r||^2 =  693829.6 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  178234.3 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  973137.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  872063.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -107084.6 e-6; = (1/var)*||X-X_r||^2 val-train = -15921.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -91162.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.90; perplexity/K = 0.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.38; perplexity/K = 0.66%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  890646.3 e-6; = (1/var)*||X-X_r||^2 =  735195.1 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  155451.2 e-6 = 17.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  843615.7 e-6; = (1/var)*||X-X_r||^2 =  710810.2 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  132805.5 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  890646.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  839188.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -47030.6 e-6; = (1/var)*||X-X_r||^2 val-train = -24384.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22645.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.42; perplexity/K = 0.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.57; perplexity/K = 0.31%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  888092.9 e-6; = (1/var)*||X-X_r||^2 =  815805.0 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  72287.9 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  872626.0 e-6; = (1/var)*||X-X_r||^2 =  814060.6 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  58565.4 e-6 = 6.7 %)
Min.  Avg. Train Loss across Mini-Batch =  866664.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  827346.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15466.9 e-6; = (1/var)*||X-X_r||^2 val-train = -1744.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13722.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.47; perplexity/K = 0.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.65; perplexity/K = 0.32%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  922569.0 e-6; = (1/var)*||X-X_r||^2 =  829161.1 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  93407.9 e-6 = 10.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  873791.5 e-6; = (1/var)*||X-X_r||^2 =  802794.1 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  70997.4 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  866664.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  827346.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -48777.5 e-6; = (1/var)*||X-X_r||^2 val-train = -26367.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22410.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.26; perplexity/K = 0.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.47; perplexity/K = 0.29%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  887121.7 e-6; = (1/var)*||X-X_r||^2 =  803038.7 e-6 = 90.5 %; (1+beta)*||Z_e-Z_q||^2 =  84082.9 e-6 = 9.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  894876.8 e-6; = (1/var)*||X-X_r||^2 =  794193.8 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  100683.0 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  843131.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  825274.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7755.2 e-6; = (1/var)*||X-X_r||^2 val-train = -8844.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16600.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.98; perplexity/K = 0.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.73; perplexity/K = 0.53%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  845534.7 e-6; = (1/var)*||X-X_r||^2 =  744955.3 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  100579.4 e-6 = 11.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  830348.2 e-6; = (1/var)*||X-X_r||^2 =  736804.8 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  93543.4 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  837240.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  799029.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -15186.5 e-6; = (1/var)*||X-X_r||^2 val-train = -8150.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7036.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.24; perplexity/K = 0.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.68; perplexity/K = 0.52%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  825327.0 e-6; = (1/var)*||X-X_r||^2 =  682039.8 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  143287.2 e-6 = 17.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  799860.7 e-6; = (1/var)*||X-X_r||^2 =  668593.7 e-6 = 83.6 %; (1+beta)*||Z_e-Z_q||^2 =  131267.0 e-6 = 16.4 %)
Min.  Avg. Train Loss across Mini-Batch =  816945.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  787874.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25466.2 e-6; = (1/var)*||X-X_r||^2 val-train = -13446.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12020.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.32; perplexity/K = 1.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.62; perplexity/K = 0.90%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  809406.6 e-6; = (1/var)*||X-X_r||^2 =  677472.6 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  131934.0 e-6 = 16.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  776226.0 e-6; = (1/var)*||X-X_r||^2 =  645131.7 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  131094.2 e-6 = 16.9 %)
Min.  Avg. Train Loss across Mini-Batch =  789462.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  750714.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -33180.6 e-6; = (1/var)*||X-X_r||^2 val-train = -32340.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -839.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.48; perplexity/K = 0.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.82; perplexity/K = 0.94%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  786106.4 e-6; = (1/var)*||X-X_r||^2 =  674207.6 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  111898.8 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  763977.4 e-6; = (1/var)*||X-X_r||^2 =  661343.4 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  102634.0 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  783232.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  740340.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22129.0 e-6; = (1/var)*||X-X_r||^2 val-train = -12864.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9264.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.81; perplexity/K = 0.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.83; perplexity/K = 0.94%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  803100.1 e-6; = (1/var)*||X-X_r||^2 =  688441.5 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  114658.6 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  786547.8 e-6; = (1/var)*||X-X_r||^2 =  669887.6 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  116660.2 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  783232.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  740340.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16552.3 e-6; = (1/var)*||X-X_r||^2 val-train = -18553.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2001.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.69; perplexity/K = 1.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.64; perplexity/K = 1.69%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  770696.7 e-6; = (1/var)*||X-X_r||^2 =  659937.0 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  110759.7 e-6 = 14.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  743625.9 e-6; = (1/var)*||X-X_r||^2 =  640857.4 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  102768.6 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  766349.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  729623.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27070.8 e-6; = (1/var)*||X-X_r||^2 val-train = -19079.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7991.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.95; perplexity/K = 0.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.52; perplexity/K = 1.08%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  742459.8 e-6; = (1/var)*||X-X_r||^2 =  656166.3 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  86293.4 e-6 = 11.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  720714.7 e-6; = (1/var)*||X-X_r||^2 =  650072.6 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  70642.1 e-6 = 9.8 %)
Min.  Avg. Train Loss across Mini-Batch =  735059.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  713874.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21745.1 e-6; = (1/var)*||X-X_r||^2 val-train = -6093.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15651.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.87; perplexity/K = 1.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.38; perplexity/K = 1.44%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  766817.7 e-6; = (1/var)*||X-X_r||^2 =  671745.9 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  95071.8 e-6 = 12.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  763683.0 e-6; = (1/var)*||X-X_r||^2 =  667141.2 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  96541.8 e-6 = 12.6 %)
Min.  Avg. Train Loss across Mini-Batch =  733868.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  713874.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3134.7 e-6; = (1/var)*||X-X_r||^2 val-train = -4604.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1470.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.40; perplexity/K = 1.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.41; perplexity/K = 1.25%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  792397.3 e-6; = (1/var)*||X-X_r||^2 =  663989.9 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  128407.4 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  780499.7 e-6; = (1/var)*||X-X_r||^2 =  658022.1 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  122477.6 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  733868.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  713874.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11897.7 e-6; = (1/var)*||X-X_r||^2 val-train = -5967.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5929.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.46; perplexity/K = 1.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.19; perplexity/K = 1.79%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  781976.2 e-6; = (1/var)*||X-X_r||^2 =  652899.0 e-6 = 83.5 %; (1+beta)*||Z_e-Z_q||^2 =  129077.2 e-6 = 16.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  787850.4 e-6; = (1/var)*||X-X_r||^2 =  655351.2 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  132499.2 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  733868.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  713874.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5874.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2452.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3422.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.63; perplexity/K = 2.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.53; perplexity/K = 3.42%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  745991.0 e-6; = (1/var)*||X-X_r||^2 =  635676.2 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  110314.7 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  761832.5 e-6; = (1/var)*||X-X_r||^2 =  633778.7 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  128053.8 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  722326.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  707345.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15841.6 e-6; = (1/var)*||X-X_r||^2 val-train = -1897.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17739.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.17; perplexity/K = 1.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.94; perplexity/K = 2.14%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  757149.5 e-6; = (1/var)*||X-X_r||^2 =  635410.9 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  121738.6 e-6 = 16.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  737765.9 e-6; = (1/var)*||X-X_r||^2 =  622867.2 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  114898.8 e-6 = 15.6 %)
Min.  Avg. Train Loss across Mini-Batch =  722326.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  707345.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19383.6 e-6; = (1/var)*||X-X_r||^2 val-train = -12543.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6839.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.15; perplexity/K = 2.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.77; perplexity/K = 2.69%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  790063.2 e-6; = (1/var)*||X-X_r||^2 =  641630.2 e-6 = 81.2 %; (1+beta)*||Z_e-Z_q||^2 =  148433.0 e-6 = 18.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  776664.7 e-6; = (1/var)*||X-X_r||^2 =  638586.9 e-6 = 82.2 %; (1+beta)*||Z_e-Z_q||^2 =  138077.8 e-6 = 17.8 %)
Min.  Avg. Train Loss across Mini-Batch =  722326.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  707345.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13398.5 e-6; = (1/var)*||X-X_r||^2 val-train = -3043.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10355.2 e-6 

----------------------------------------------------------------------------------

Finished [22:41:51 03.01.2023] 245) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 9) change_channel_size_across_layers = True:
Total training time is = 0:0:7 h/m/s. 

--------------------------------------------------- 

Started [22:41:51 03.01.2023] 246) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 9) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2502 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.32
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.28
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.24
5                           encoder.sequential_convs.conv2d_6.weight                       524            20.94
6                                  encoder.pre_residual_stack.weight                       589            23.54
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.92
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.92
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
11                             encoder.channel_adjusting_conv.weight                        16             0.64
12                                                       VQ.E.weight                        32             1.28
13                             decoder.channel_adjusting_conv.weight                       147             5.88
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.92
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.92
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.94
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.24
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.28
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.72; perplexity/K = 0.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.61; perplexity/K = 0.70%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2304566.5 e-6; = (1/var)*||X-X_r||^2 =  839728.0 e-6 = 36.4 %; (1+beta)*||Z_e-Z_q||^2 =  1464838.4 e-6 = 63.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1782416.9 e-6; = (1/var)*||X-X_r||^2 =  811101.1 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  971315.8 e-6 = 54.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1195718.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  943105.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -522149.6 e-6; = (1/var)*||X-X_r||^2 val-train = -28626.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -493522.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.41; perplexity/K = 0.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.28; perplexity/K = 1.03%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1279629.5 e-6; = (1/var)*||X-X_r||^2 =  700307.2 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  579322.3 e-6 = 45.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1217335.1 e-6; = (1/var)*||X-X_r||^2 =  683940.9 e-6 = 56.2 %; (1+beta)*||Z_e-Z_q||^2 =  533394.1 e-6 = 43.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1195718.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  943105.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -62294.4 e-6; = (1/var)*||X-X_r||^2 val-train = -16366.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -45928.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.48; perplexity/K = 0.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.81; perplexity/K = 0.35%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1006358.2 e-6; = (1/var)*||X-X_r||^2 =  788695.2 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  217662.9 e-6 = 21.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  953770.4 e-6; = (1/var)*||X-X_r||^2 =  778565.1 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  175205.4 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  964984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  911841.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -52587.7 e-6; = (1/var)*||X-X_r||^2 val-train = -10130.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -42457.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 0.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 0.23%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  939012.6 e-6; = (1/var)*||X-X_r||^2 =  907050.9 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  31961.6 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  914164.4 e-6; = (1/var)*||X-X_r||^2 =  892627.6 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  21536.8 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  932224.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  910925.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24848.2 e-6; = (1/var)*||X-X_r||^2 val-train = -14423.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10424.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999675.7 e-6; = (1/var)*||X-X_r||^2 =  999387.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  288.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963704.6 e-6; = (1/var)*||X-X_r||^2 =  963391.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  313.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35971.1 e-6; = (1/var)*||X-X_r||^2 val-train = -35996.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999435.4 e-6; = (1/var)*||X-X_r||^2 =  999429.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962947.7 e-6; = (1/var)*||X-X_r||^2 =  962944.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36487.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36485.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999394.1 e-6; = (1/var)*||X-X_r||^2 =  999393.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963022.9 e-6; = (1/var)*||X-X_r||^2 =  963022.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36371.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36371.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999325.2 e-6; = (1/var)*||X-X_r||^2 =  999324.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962979.7 e-6; = (1/var)*||X-X_r||^2 =  962976.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36345.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36347.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999341.8 e-6; = (1/var)*||X-X_r||^2 =  999341.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962933.5 e-6; = (1/var)*||X-X_r||^2 =  962933.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36408.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36408.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999350.9 e-6; = (1/var)*||X-X_r||^2 =  999350.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963010.4 e-6; = (1/var)*||X-X_r||^2 =  963010.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36340.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36340.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999641.7 e-6; = (1/var)*||X-X_r||^2 =  999641.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963405.4 e-6; = (1/var)*||X-X_r||^2 =  963405.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36236.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36235.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999338.1 e-6; = (1/var)*||X-X_r||^2 =  999337.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963082.1 e-6; = (1/var)*||X-X_r||^2 =  963082.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36256.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36255.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999396.4 e-6; = (1/var)*||X-X_r||^2 =  999396.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963355.3 e-6; = (1/var)*||X-X_r||^2 =  963354.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36041.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36041.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999206.6 e-6; = (1/var)*||X-X_r||^2 =  999206.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963280.9 e-6; = (1/var)*||X-X_r||^2 =  963280.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35925.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35925.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999355.5 e-6; = (1/var)*||X-X_r||^2 =  999355.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963455.9 e-6; = (1/var)*||X-X_r||^2 =  963455.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35899.6 e-6; = (1/var)*||X-X_r||^2 val-train = -35899.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999253.1 e-6; = (1/var)*||X-X_r||^2 =  999252.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962922.6 e-6; = (1/var)*||X-X_r||^2 =  962922.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36330.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36330.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999330.4 e-6; = (1/var)*||X-X_r||^2 =  999328.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963205.2 e-6; = (1/var)*||X-X_r||^2 =  963205.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36125.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36123.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999308.1 e-6; = (1/var)*||X-X_r||^2 =  999307.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964154.2 e-6; = (1/var)*||X-X_r||^2 =  964154.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35153.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35153.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999573.9 e-6; = (1/var)*||X-X_r||^2 =  999573.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963937.6 e-6; = (1/var)*||X-X_r||^2 =  963937.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35636.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35636.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999175.4 e-6; = (1/var)*||X-X_r||^2 =  999175.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963082.3 e-6; = (1/var)*||X-X_r||^2 =  963081.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922079.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  897963.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36093.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36093.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.5 e-6 

----------------------------------------------------------------------------------

Finished [23:29:57 03.01.2023] 246) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 9) change_channel_size_across_layers = True:
Total training time is = 0:0:6 h/m/s. 

--------------------------------------------------- 

Started [23:29:57 03.01.2023] 247) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 9) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 3052 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.20
1                           encoder.sequential_convs.conv2d_2.weight                       262             8.58
2                           encoder.sequential_convs.conv2d_3.weight                       262             8.58
3                           encoder.sequential_convs.conv2d_4.weight                       262             8.58
4                           encoder.sequential_convs.conv2d_5.weight                       262             8.58
5                           encoder.sequential_convs.conv2d_6.weight                       262             8.58
6                                  encoder.pre_residual_stack.weight                       147             4.82
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.18
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.18
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
11                             encoder.channel_adjusting_conv.weight                         8             0.26
12                                                       VQ.E.weight                        32             1.05
13                             decoder.channel_adjusting_conv.weight                        73             2.39
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.18
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.18
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
18                    decoder.sequential_trans_convs.conv2d_1.weight                       262             8.58
19                    decoder.sequential_trans_convs.conv2d_2.weight                       262             8.58
20                    decoder.sequential_trans_convs.conv2d_3.weight                       262             8.58
21                    decoder.sequential_trans_convs.conv2d_4.weight                       262             8.58
22                    decoder.sequential_trans_convs.conv2d_5.weight                       262             8.58
23                    decoder.sequential_trans_convs.conv2d_6.weight                         6             0.20

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 0.21%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1029312.6 e-6; = (1/var)*||X-X_r||^2 =  972054.4 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  57258.2 e-6 = 5.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  957884.1 e-6; = (1/var)*||X-X_r||^2 =  934727.1 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  23157.0 e-6 = 2.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1002575.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954974.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -71428.5 e-6; = (1/var)*||X-X_r||^2 val-train = -37327.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -34101.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.20%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  945008.1 e-6; = (1/var)*||X-X_r||^2 =  927099.0 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  17909.1 e-6 = 1.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  924448.1 e-6; = (1/var)*||X-X_r||^2 =  917691.3 e-6 = 99.3 %; (1+beta)*||Z_e-Z_q||^2 =  6756.7 e-6 = 0.7 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20560.1 e-6; = (1/var)*||X-X_r||^2 val-train = -9407.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11152.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  996513.2 e-6; = (1/var)*||X-X_r||^2 =  996336.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  176.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964048.6 e-6; = (1/var)*||X-X_r||^2 =  963576.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  472.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -32464.6 e-6; = (1/var)*||X-X_r||^2 val-train = -32760.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 296.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999606.8 e-6; = (1/var)*||X-X_r||^2 =  999603.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964524.0 e-6; = (1/var)*||X-X_r||^2 =  964519.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35082.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35083.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999610.6 e-6; = (1/var)*||X-X_r||^2 =  999609.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962931.8 e-6; = (1/var)*||X-X_r||^2 =  962930.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36678.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36678.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999375.5 e-6; = (1/var)*||X-X_r||^2 =  999374.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962915.8 e-6; = (1/var)*||X-X_r||^2 =  962913.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.9 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36459.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36461.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999458.6 e-6; = (1/var)*||X-X_r||^2 =  999458.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962955.3 e-6; = (1/var)*||X-X_r||^2 =  962954.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36503.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36503.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999489.9 e-6; = (1/var)*||X-X_r||^2 =  999489.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963126.0 e-6; = (1/var)*||X-X_r||^2 =  963125.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36363.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36364.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999612.0 e-6; = (1/var)*||X-X_r||^2 =  999611.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963180.2 e-6; = (1/var)*||X-X_r||^2 =  963179.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36431.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36431.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999402.3 e-6; = (1/var)*||X-X_r||^2 =  999402.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962886.2 e-6; = (1/var)*||X-X_r||^2 =  962886.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36516.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36516.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;  
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999482.7 e-6; = (1/var)*||X-X_r||^2 =  999482.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962933.4 e-6; = (1/var)*||X-X_r||^2 =  962933.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36549.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36549.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999370.2 e-6; = (1/var)*||X-X_r||^2 =  999370.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963076.6 e-6; = (1/var)*||X-X_r||^2 =  963076.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36293.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36293.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999281.8 e-6; = (1/var)*||X-X_r||^2 =  999281.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962903.6 e-6; = (1/var)*||X-X_r||^2 =  962903.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36378.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36378.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999270.7 e-6; = (1/var)*||X-X_r||^2 =  999270.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963191.9 e-6; = (1/var)*||X-X_r||^2 =  963191.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36078.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36078.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999289.3 e-6; = (1/var)*||X-X_r||^2 =  999289.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963095.5 e-6; = (1/var)*||X-X_r||^2 =  963095.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36193.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36193.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999484.9 e-6; = (1/var)*||X-X_r||^2 =  999484.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962905.2 e-6; = (1/var)*||X-X_r||^2 =  962905.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36579.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36579.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999227.5 e-6; = (1/var)*||X-X_r||^2 =  999227.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963176.3 e-6; = (1/var)*||X-X_r||^2 =  963176.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  933825.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885904.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36051.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36051.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

Finished [00:34:09 04.01.2023] 247) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 9) change_channel_size_across_layers = False:
Total training time is = 0:0:12 h/m/s. 

--------------------------------------------------- 

Started [00:34:09 04.01.2023] 248) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 9) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(512, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 11612 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.10
1                           encoder.sequential_convs.conv2d_2.weight                      1048             9.03
2                           encoder.sequential_convs.conv2d_3.weight                      1048             9.03
3                           encoder.sequential_convs.conv2d_4.weight                      1048             9.03
4                           encoder.sequential_convs.conv2d_5.weight                      1048             9.03
5                           encoder.sequential_convs.conv2d_6.weight                      1048             9.03
6                                  encoder.pre_residual_stack.weight                       589             5.07
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
11                             encoder.channel_adjusting_conv.weight                        16             0.14
12                                                       VQ.E.weight                        32             0.28
13                             decoder.channel_adjusting_conv.weight                       147             1.27
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
18                    decoder.sequential_trans_convs.conv2d_1.weight                      1048             9.03
19                    decoder.sequential_trans_convs.conv2d_2.weight                      1048             9.03
20                    decoder.sequential_trans_convs.conv2d_3.weight                      1048             9.03
21                    decoder.sequential_trans_convs.conv2d_4.weight                      1048             9.03
22                    decoder.sequential_trans_convs.conv2d_5.weight                      1048             9.03
23                    decoder.sequential_trans_convs.conv2d_6.weight                        12             0.10

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.65; perplexity/K = 0.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.26; perplexity/K = 0.44%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1084788.4 e-6; = (1/var)*||X-X_r||^2 =  937296.4 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  147492.0 e-6 = 13.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  932661.9 e-6; = (1/var)*||X-X_r||^2 =  912956.2 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  19705.7 e-6 = 2.1 %)
Min.  Avg. Train Loss across Mini-Batch =  999455.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925213.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -152126.5 e-6; = (1/var)*||X-X_r||^2 val-train = -24340.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -127786.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  979420.7 e-6; = (1/var)*||X-X_r||^2 =  977934.2 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1486.5 e-6 = 0.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  954936.8 e-6; = (1/var)*||X-X_r||^2 =  953794.4 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  1142.4 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24483.9 e-6; = (1/var)*||X-X_r||^2 val-train = -24139.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -344.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999679.0 e-6; = (1/var)*||X-X_r||^2 =  999662.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  16.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962933.2 e-6; = (1/var)*||X-X_r||^2 =  962920.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36745.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36741.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999466.7 e-6; = (1/var)*||X-X_r||^2 =  999465.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963020.0 e-6; = (1/var)*||X-X_r||^2 =  963018.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36446.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36447.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999347.0 e-6; = (1/var)*||X-X_r||^2 =  999345.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964276.2 e-6; = (1/var)*||X-X_r||^2 =  964275.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35070.8 e-6; = (1/var)*||X-X_r||^2 val-train = -35070.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999351.7 e-6; = (1/var)*||X-X_r||^2 =  999351.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962869.1 e-6; = (1/var)*||X-X_r||^2 =  962868.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36482.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36482.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999661.1 e-6; = (1/var)*||X-X_r||^2 =  999660.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962971.1 e-6; = (1/var)*||X-X_r||^2 =  962971.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36690.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36689.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999562.3 e-6; = (1/var)*||X-X_r||^2 =  999562.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962965.3 e-6; = (1/var)*||X-X_r||^2 =  962965.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36597.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36597.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999252.9 e-6; = (1/var)*||X-X_r||^2 =  999252.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962909.5 e-6; = (1/var)*||X-X_r||^2 =  962909.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36343.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36343.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999397.7 e-6; = (1/var)*||X-X_r||^2 =  999397.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962982.2 e-6; = (1/var)*||X-X_r||^2 =  962982.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36415.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36415.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999209.7 e-6; = (1/var)*||X-X_r||^2 =  999209.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963002.0 e-6; = (1/var)*||X-X_r||^2 =  963001.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36207.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36207.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999219.2 e-6; = (1/var)*||X-X_r||^2 =  999219.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963084.5 e-6; = (1/var)*||X-X_r||^2 =  963084.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36134.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36134.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999257.3 e-6; = (1/var)*||X-X_r||^2 =  999257.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962998.7 e-6; = (1/var)*||X-X_r||^2 =  962998.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36258.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36258.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999277.2 e-6; = (1/var)*||X-X_r||^2 =  999277.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962904.3 e-6; = (1/var)*||X-X_r||^2 =  962904.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36372.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36373.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:40:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999361.2 e-6; = (1/var)*||X-X_r||^2 =  999361.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962984.6 e-6; = (1/var)*||X-X_r||^2 =  962984.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36376.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36376.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999176.9 e-6; = (1/var)*||X-X_r||^2 =  999176.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962953.6 e-6; = (1/var)*||X-X_r||^2 =  962953.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36223.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36223.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999322.7 e-6; = (1/var)*||X-X_r||^2 =  999322.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963056.6 e-6; = (1/var)*||X-X_r||^2 =  963056.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36266.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36266.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:0:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999280.5 e-6; = (1/var)*||X-X_r||^2 =  999280.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963161.5 e-6; = (1/var)*||X-X_r||^2 =  963161.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36119.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36119.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:7:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999241.3 e-6; = (1/var)*||X-X_r||^2 =  999241.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962957.2 e-6; = (1/var)*||X-X_r||^2 =  962957.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36284.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36284.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.20%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:13:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999261.9 e-6; = (1/var)*||X-X_r||^2 =  999261.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963255.7 e-6; = (1/var)*||X-X_r||^2 =  963255.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  967625.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  918350.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36006.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36006.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [02:48:52 04.01.2023] 248) Finished running for K = 512 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 9) change_channel_size_across_layers = False:
Total training time is = 0:0:43 h/m/s. 

--------------------------------------------------- 

Started [02:48:52 04.01.2023] 249) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 512) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 732 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.14
1                           encoder.sequential_convs.conv2d_2.weight                        32             4.37
2                           encoder.sequential_convs.conv2d_3.weight                       131            17.90
3                                  encoder.pre_residual_stack.weight                       147            20.08
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
8                              encoder.channel_adjusting_conv.weight                         8             1.09
9                                                        VQ.E.weight                        16             2.19
10                             decoder.channel_adjusting_conv.weight                        73             9.97
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.92
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.92
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.90
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.37
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.14

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.07; perplexity/K = 9.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.35; perplexity/K = 8.73%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  462332.3 e-6; = (1/var)*||X-X_r||^2 =  270049.2 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  192283.1 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  465731.8 e-6; = (1/var)*||X-X_r||^2 =  272405.4 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  193326.4 e-6 = 41.5 %)
Min.  Avg. Train Loss across Mini-Batch =  462332.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  465731.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3399.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2356.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1043.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.05; perplexity/K = 12.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.12; perplexity/K = 12.94%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  134936.3 e-6; = (1/var)*||X-X_r||^2 =  103335.9 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  31600.4 e-6 = 23.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  138943.3 e-6; = (1/var)*||X-X_r||^2 =  107769.4 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  31173.9 e-6 = 22.4 %)
Min.  Avg. Train Loss across Mini-Batch =  134936.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  138068.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4007.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4433.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -426.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.64; perplexity/K = 9.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.90; perplexity/K = 9.73%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67625.4 e-6; = (1/var)*||X-X_r||^2 =  50988.1 e-6 = 75.4 %; (1+beta)*||Z_e-Z_q||^2 =  16637.3 e-6 = 24.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  72152.7 e-6; = (1/var)*||X-X_r||^2 =  55131.7 e-6 = 76.4 %; (1+beta)*||Z_e-Z_q||^2 =  17021.0 e-6 = 23.6 %)
Min.  Avg. Train Loss across Mini-Batch =  67625.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  72152.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4527.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4143.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 383.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.49; perplexity/K = 8.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.81; perplexity/K = 8.52%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:8:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43884.9 e-6; = (1/var)*||X-X_r||^2 =  30183.5 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  13701.4 e-6 = 31.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  48276.0 e-6; = (1/var)*||X-X_r||^2 =  34553.5 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  13722.5 e-6 = 28.4 %)
Min.  Avg. Train Loss across Mini-Batch =  43884.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48276.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4391.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4370.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.15; perplexity/K = 9.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.15; perplexity/K = 8.26%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31419.0 e-6; = (1/var)*||X-X_r||^2 =  19132.8 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  12286.2 e-6 = 39.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  37386.1 e-6; = (1/var)*||X-X_r||^2 =  23432.0 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  13954.1 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  31419.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35820.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5967.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4299.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1667.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.45; perplexity/K = 7.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.31; perplexity/K = 7.93%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24216.5 e-6; = (1/var)*||X-X_r||^2 =  13604.7 e-6 = 56.2 %; (1+beta)*||Z_e-Z_q||^2 =  10611.8 e-6 = 43.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  28126.0 e-6; = (1/var)*||X-X_r||^2 =  16857.9 e-6 = 59.9 %; (1+beta)*||Z_e-Z_q||^2 =  11268.1 e-6 = 40.1 %)
Min.  Avg. Train Loss across Mini-Batch =  24216.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28126.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3909.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3253.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 656.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.99; perplexity/K = 7.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.08; perplexity/K = 7.06%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18698.7 e-6; = (1/var)*||X-X_r||^2 =  9552.5 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  9146.2 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  22656.6 e-6; = (1/var)*||X-X_r||^2 =  12461.0 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  10195.6 e-6 = 45.0 %)
Min.  Avg. Train Loss across Mini-Batch =  18505.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  21777.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3957.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2908.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1049.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.39; perplexity/K = 6.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.85; perplexity/K = 6.97%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13295.1 e-6; = (1/var)*||X-X_r||^2 =  5991.4 e-6 = 45.1 %; (1+beta)*||Z_e-Z_q||^2 =  7303.6 e-6 = 54.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  17260.5 e-6; = (1/var)*||X-X_r||^2 =  8553.0 e-6 = 49.6 %; (1+beta)*||Z_e-Z_q||^2 =  8707.6 e-6 = 50.4 %)
Min.  Avg. Train Loss across Mini-Batch =  13269.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16457.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3965.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2561.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1404.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.76; perplexity/K = 7.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.18; perplexity/K = 6.71%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10329.1 e-6; = (1/var)*||X-X_r||^2 =  4234.7 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  6094.4 e-6 = 59.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  13391.9 e-6; = (1/var)*||X-X_r||^2 =  6589.3 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  6802.6 e-6 = 50.8 %)
Min.  Avg. Train Loss across Mini-Batch =  10329.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13040.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3062.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2354.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 708.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.53; perplexity/K = 7.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.75; perplexity/K = 7.71%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12788.7 e-6; = (1/var)*||X-X_r||^2 =  5061.6 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  7727.2 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  14733.9 e-6; = (1/var)*||X-X_r||^2 =  6805.9 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  7928.0 e-6 = 53.8 %)
Min.  Avg. Train Loss across Mini-Batch =  8455.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10595.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1945.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1744.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 200.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.28; perplexity/K = 6.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.73; perplexity/K = 6.14%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9702.8 e-6; = (1/var)*||X-X_r||^2 =  3519.0 e-6 = 36.3 %; (1+beta)*||Z_e-Z_q||^2 =  6183.8 e-6 = 63.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  11049.4 e-6; = (1/var)*||X-X_r||^2 =  4868.9 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  6180.5 e-6 = 55.9 %)
Min.  Avg. Train Loss across Mini-Batch =  6956.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8415.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1346.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1350.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.60; perplexity/K = 6.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.05; perplexity/K = 5.88%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6033.2 e-6; = (1/var)*||X-X_r||^2 =  1999.6 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  4033.6 e-6 = 66.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  7133.6 e-6; = (1/var)*||X-X_r||^2 =  3147.4 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  3986.1 e-6 = 55.9 %)
Min.  Avg. Train Loss across Mini-Batch =  5651.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7044.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1100.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1147.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -47.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.55; perplexity/K = 6.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.71; perplexity/K = 6.53%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16527.8 e-6; = (1/var)*||X-X_r||^2 =  6879.0 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  9648.8 e-6 = 58.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  15622.8 e-6; = (1/var)*||X-X_r||^2 =  7201.9 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  8420.9 e-6 = 53.9 %)
Min.  Avg. Train Loss across Mini-Batch =  4334.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5256.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -905.0 e-6; = (1/var)*||X-X_r||^2 val-train = 323.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1227.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.02; perplexity/K = 5.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.11; perplexity/K = 5.12%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5199.7 e-6; = (1/var)*||X-X_r||^2 =  1779.1 e-6 = 34.2 %; (1+beta)*||Z_e-Z_q||^2 =  3420.6 e-6 = 65.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  6031.0 e-6; = (1/var)*||X-X_r||^2 =  2673.8 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  3357.3 e-6 = 55.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3536.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4431.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   831.4 e-6; = (1/var)*||X-X_r||^2 val-train = 894.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -63.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.02; perplexity/K = 7.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.16; perplexity/K = 6.31%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3398.3 e-6; = (1/var)*||X-X_r||^2 =  1077.5 e-6 = 31.7 %; (1+beta)*||Z_e-Z_q||^2 =  2320.8 e-6 = 68.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4320.2 e-6; = (1/var)*||X-X_r||^2 =  1882.9 e-6 = 43.6 %; (1+beta)*||Z_e-Z_q||^2 =  2437.3 e-6 = 56.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2889.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3587.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   921.8 e-6; = (1/var)*||X-X_r||^2 val-train = 805.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 116.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.78; perplexity/K = 6.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.54; perplexity/K = 6.07%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6365.5 e-6; = (1/var)*||X-X_r||^2 =  1898.0 e-6 = 29.8 %; (1+beta)*||Z_e-Z_q||^2 =  4467.5 e-6 = 70.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  6882.3 e-6; = (1/var)*||X-X_r||^2 =  2689.7 e-6 = 39.1 %; (1+beta)*||Z_e-Z_q||^2 =  4192.6 e-6 = 60.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2412.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3046.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   516.8 e-6; = (1/var)*||X-X_r||^2 val-train = 791.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -274.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.15; perplexity/K = 6.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.86; perplexity/K = 6.98%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3170.5 e-6; = (1/var)*||X-X_r||^2 =  1018.0 e-6 = 32.1 %; (1+beta)*||Z_e-Z_q||^2 =  2152.4 e-6 = 67.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4357.6 e-6; = (1/var)*||X-X_r||^2 =  2190.1 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  2167.5 e-6 = 49.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2197.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2815.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1187.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1172.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.84; perplexity/K = 5.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.40; perplexity/K = 4.84%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2972.2 e-6; = (1/var)*||X-X_r||^2 =  933.4 e-6 = 31.4 %; (1+beta)*||Z_e-Z_q||^2 =  2038.8 e-6 = 68.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3450.1 e-6; = (1/var)*||X-X_r||^2 =  1511.2 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  1938.9 e-6 = 56.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1785.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2291.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   477.9 e-6; = (1/var)*||X-X_r||^2 val-train = 577.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -99.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.32; perplexity/K = 5.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.46; perplexity/K = 5.65%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1620.6 e-6; = (1/var)*||X-X_r||^2 =  624.0 e-6 = 38.5 %; (1+beta)*||Z_e-Z_q||^2 =  996.6 e-6 = 61.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2039.3 e-6; = (1/var)*||X-X_r||^2 =  1060.6 e-6 = 52.0 %; (1+beta)*||Z_e-Z_q||^2 =  978.7 e-6 = 48.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1529.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2023.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   418.7 e-6; = (1/var)*||X-X_r||^2 val-train = 436.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.69; perplexity/K = 8.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.27; perplexity/K = 7.14%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2187.1 e-6; = (1/var)*||X-X_r||^2 =  777.2 e-6 = 35.5 %; (1+beta)*||Z_e-Z_q||^2 =  1409.9 e-6 = 64.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2587.3 e-6; = (1/var)*||X-X_r||^2 =  1203.1 e-6 = 46.5 %; (1+beta)*||Z_e-Z_q||^2 =  1384.2 e-6 = 53.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1529.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2023.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   400.1 e-6; = (1/var)*||X-X_r||^2 val-train = 425.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.7 e-6 

----------------------------------------------------------------------------------

Finished [03:34:24 04.01.2023] 249) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 512) change_channel_size_across_layers = True:
Total training time is = 0:7:32 h/m/s. 

--------------------------------------------------- 

Started [03:34:24 04.01.2023] 250) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 512) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2408 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.44
2                           encoder.sequential_convs.conv2d_3.weight                       524            21.76
3                                  encoder.pre_residual_stack.weight                       589            24.46
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.03
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.03
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
8                              encoder.channel_adjusting_conv.weight                        16             0.66
9                                                        VQ.E.weight                        16             0.66
10                             decoder.channel_adjusting_conv.weight                       147             6.10
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.03
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.03
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.76
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.44
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.73; perplexity/K = 7.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.38; perplexity/K = 7.18%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  173190.2 e-6; = (1/var)*||X-X_r||^2 =  90961.8 e-6 = 52.5 %; (1+beta)*||Z_e-Z_q||^2 =  82228.4 e-6 = 47.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  175305.2 e-6; = (1/var)*||X-X_r||^2 =  96522.0 e-6 = 55.1 %; (1+beta)*||Z_e-Z_q||^2 =  78783.2 e-6 = 44.9 %)
Min.  Avg. Train Loss across Mini-Batch =  173190.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  175305.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2115.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5560.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3445.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.33; perplexity/K = 3.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.20; perplexity/K = 3.59%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382682.2 e-6; = (1/var)*||X-X_r||^2 =  244489.0 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  138193.2 e-6 = 36.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  362702.3 e-6; = (1/var)*||X-X_r||^2 =  241278.3 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  121423.9 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  75366.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78554.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19979.9 e-6; = (1/var)*||X-X_r||^2 val-train = -3210.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -16769.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.71; perplexity/K = 6.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.07; perplexity/K = 6.28%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  101160.7 e-6; = (1/var)*||X-X_r||^2 =  78631.4 e-6 = 77.7 %; (1+beta)*||Z_e-Z_q||^2 =  22529.3 e-6 = 22.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  104709.7 e-6; = (1/var)*||X-X_r||^2 =  83297.4 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  21412.4 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  75366.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78554.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3549.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4665.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1116.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 5.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.64; perplexity/K = 5.33%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49538.6 e-6; = (1/var)*||X-X_r||^2 =  37185.8 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  12352.8 e-6 = 24.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  54915.4 e-6; = (1/var)*||X-X_r||^2 =  42190.1 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  12725.3 e-6 = 23.2 %)
Min.  Avg. Train Loss across Mini-Batch =  49538.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  54717.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5376.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5004.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 372.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.78; perplexity/K = 4.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.31; perplexity/K = 5.20%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25751.5 e-6; = (1/var)*||X-X_r||^2 =  15766.1 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  9985.3 e-6 = 38.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  29154.6 e-6; = (1/var)*||X-X_r||^2 =  19113.1 e-6 = 65.6 %; (1+beta)*||Z_e-Z_q||^2 =  10041.5 e-6 = 34.4 %)
Min.  Avg. Train Loss across Mini-Batch =  25751.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28931.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3403.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3347.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 56.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.84; perplexity/K = 5.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.39; perplexity/K = 5.62%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18438.6 e-6; = (1/var)*||X-X_r||^2 =  9348.9 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  9089.7 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  20863.8 e-6; = (1/var)*||X-X_r||^2 =  11907.2 e-6 = 57.1 %; (1+beta)*||Z_e-Z_q||^2 =  8956.6 e-6 = 42.9 %)
Min.  Avg. Train Loss across Mini-Batch =  15263.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  18210.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2425.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2558.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -133.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.92; perplexity/K = 4.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.92; perplexity/K = 4.66%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10002.6 e-6; = (1/var)*||X-X_r||^2 =  4623.1 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  5379.4 e-6 = 53.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  12055.8 e-6; = (1/var)*||X-X_r||^2 =  6726.0 e-6 = 55.8 %; (1+beta)*||Z_e-Z_q||^2 =  5329.8 e-6 = 44.2 %)
Min.  Avg. Train Loss across Mini-Batch =  10002.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12055.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2053.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2102.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -49.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.61; perplexity/K = 4.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.70; perplexity/K = 4.96%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9529.0 e-6; = (1/var)*||X-X_r||^2 =  3995.0 e-6 = 41.9 %; (1+beta)*||Z_e-Z_q||^2 =  5534.0 e-6 = 58.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  10856.3 e-6; = (1/var)*||X-X_r||^2 =  5532.1 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  5324.1 e-6 = 49.0 %)
Min.  Avg. Train Loss across Mini-Batch =  7153.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8833.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1327.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1537.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -209.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.78; perplexity/K = 4.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.36; perplexity/K = 4.44%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6095.1 e-6; = (1/var)*||X-X_r||^2 =  2302.5 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  3792.6 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  7517.4 e-6; = (1/var)*||X-X_r||^2 =  3773.1 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  3744.3 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  5034.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6563.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1422.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1470.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -48.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.48; perplexity/K = 4.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.81; perplexity/K = 4.61%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3423.4 e-6; = (1/var)*||X-X_r||^2 =  1258.6 e-6 = 36.8 %; (1+beta)*||Z_e-Z_q||^2 =  2164.8 e-6 = 63.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  5240.7 e-6; = (1/var)*||X-X_r||^2 =  2903.5 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  2337.2 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3423.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4663.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1817.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1644.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 172.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.34; perplexity/K = 4.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.65; perplexity/K = 4.94%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3197.7 e-6; = (1/var)*||X-X_r||^2 =  1316.0 e-6 = 41.2 %; (1+beta)*||Z_e-Z_q||^2 =  1881.6 e-6 = 58.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4380.4 e-6; = (1/var)*||X-X_r||^2 =  2486.7 e-6 = 56.8 %; (1+beta)*||Z_e-Z_q||^2 =  1893.7 e-6 = 43.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3039.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4222.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1182.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1170.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.63; perplexity/K = 4.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.00; perplexity/K = 5.08%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21083.3 e-6; = (1/var)*||X-X_r||^2 =  12445.9 e-6 = 59.0 %; (1+beta)*||Z_e-Z_q||^2 =  8637.4 e-6 = 41.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  10039.3 e-6; = (1/var)*||X-X_r||^2 =  5859.8 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  4179.5 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2355.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3297.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11044.0 e-6; = (1/var)*||X-X_r||^2 val-train = -6586.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4458.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.01; perplexity/K = 6.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.04; perplexity/K = 5.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2119.8 e-6; = (1/var)*||X-X_r||^2 =  733.3 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  1386.5 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3040.9 e-6; = (1/var)*||X-X_r||^2 =  1648.5 e-6 = 54.2 %; (1+beta)*||Z_e-Z_q||^2 =  1392.4 e-6 = 45.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1752.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2700.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   921.1 e-6; = (1/var)*||X-X_r||^2 val-train = 915.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.90; perplexity/K = 6.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.07; perplexity/K = 5.89%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1820.7 e-6; = (1/var)*||X-X_r||^2 =  659.2 e-6 = 36.2 %; (1+beta)*||Z_e-Z_q||^2 =  1161.5 e-6 = 63.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2716.0 e-6; = (1/var)*||X-X_r||^2 =  1564.9 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  1151.2 e-6 = 42.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1422.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2156.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   895.4 e-6; = (1/var)*||X-X_r||^2 val-train = 905.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.45; perplexity/K = 6.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.90; perplexity/K = 5.43%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2041.9 e-6; = (1/var)*||X-X_r||^2 =  1021.8 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  1020.2 e-6 = 50.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2830.2 e-6; = (1/var)*||X-X_r||^2 =  1744.3 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  1085.9 e-6 = 38.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1304.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1962.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   788.3 e-6; = (1/var)*||X-X_r||^2 val-train = 722.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 65.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.60; perplexity/K = 5.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.16; perplexity/K = 5.14%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6747.2 e-6; = (1/var)*||X-X_r||^2 =  4652.2 e-6 = 68.9 %; (1+beta)*||Z_e-Z_q||^2 =  2095.1 e-6 = 31.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3999.1 e-6; = (1/var)*||X-X_r||^2 =  2652.4 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  1346.7 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1140.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1726.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2748.1 e-6; = (1/var)*||X-X_r||^2 val-train = -1999.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -748.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.82; perplexity/K = 4.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 4.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  946.1 e-6; = (1/var)*||X-X_r||^2 =  347.5 e-6 = 36.7 %; (1+beta)*||Z_e-Z_q||^2 =  598.7 e-6 = 63.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1519.2 e-6; = (1/var)*||X-X_r||^2 =  902.7 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  616.6 e-6 = 40.6 %)
Min.  Avg. Train Loss across Mini-Batch =  946.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1490.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   573.1 e-6; = (1/var)*||X-X_r||^2 val-train = 555.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.62; perplexity/K = 4.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.19; perplexity/K = 4.76%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2145.8 e-6; = (1/var)*||X-X_r||^2 =  962.3 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  1183.5 e-6 = 55.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2298.1 e-6; = (1/var)*||X-X_r||^2 =  1263.5 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  1034.6 e-6 = 45.0 %)
Min.  Avg. Train Loss across Mini-Batch =  750.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1218.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   152.3 e-6; = (1/var)*||X-X_r||^2 val-train = 301.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -148.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.04; perplexity/K = 5.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.57; perplexity/K = 5.30%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1299.5 e-6; = (1/var)*||X-X_r||^2 =  473.6 e-6 = 36.4 %; (1+beta)*||Z_e-Z_q||^2 =  825.9 e-6 = 63.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1785.3 e-6; = (1/var)*||X-X_r||^2 =  975.2 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  810.0 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  689.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1118.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   485.7 e-6; = (1/var)*||X-X_r||^2 val-train = 501.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.67; perplexity/K = 4.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 4.90%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1173.3 e-6; = (1/var)*||X-X_r||^2 =  707.9 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  465.4 e-6 = 39.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1752.8 e-6; = (1/var)*||X-X_r||^2 =  1304.8 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  448.1 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  645.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1072.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   579.5 e-6; = (1/var)*||X-X_r||^2 val-train = 596.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.3 e-6 

----------------------------------------------------------------------------------

Finished [04:38:05 04.01.2023] 250) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 512) change_channel_size_across_layers = True:
Total training time is = 0:7:40 h/m/s. 

--------------------------------------------------- 

Started [04:38:05 04.01.2023] 251) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 512) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1464 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.41
1                           encoder.sequential_convs.conv2d_2.weight                       262            17.90
2                           encoder.sequential_convs.conv2d_3.weight                       262            17.90
3                                  encoder.pre_residual_stack.weight                       147            10.04
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.46
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.46
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
8                              encoder.channel_adjusting_conv.weight                         8             0.55
9                                                        VQ.E.weight                        16             1.09
10                             decoder.channel_adjusting_conv.weight                        73             4.99
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.46
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.46
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            17.90
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            17.90
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.41

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.97; perplexity/K = 8.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.76; perplexity/K = 8.11%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  178683.5 e-6; = (1/var)*||X-X_r||^2 =  108079.7 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  70603.8 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  182045.1 e-6; = (1/var)*||X-X_r||^2 =  114171.5 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  67873.6 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  178683.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  182045.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3361.6 e-6; = (1/var)*||X-X_r||^2 val-train = 6091.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2730.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.64; perplexity/K = 7.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.13; perplexity/K = 7.47%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  82701.5 e-6; = (1/var)*||X-X_r||^2 =  50022.8 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  32678.7 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  85478.1 e-6; = (1/var)*||X-X_r||^2 =  54310.6 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  31167.5 e-6 = 36.5 %)
Min.  Avg. Train Loss across Mini-Batch =  57120.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  61526.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2776.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4287.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1511.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.73; perplexity/K = 5.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.90; perplexity/K = 5.43%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18051.6 e-6; = (1/var)*||X-X_r||^2 =  9583.4 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  8468.2 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  20427.8 e-6; = (1/var)*||X-X_r||^2 =  11948.3 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  8479.5 e-6 = 41.5 %)
Min.  Avg. Train Loss across Mini-Batch =  18051.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20306.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2376.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2365.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.44; perplexity/K = 3.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.55; perplexity/K = 3.73%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8955.7 e-6; = (1/var)*||X-X_r||^2 =  4204.9 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  4750.8 e-6 = 53.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  10303.8 e-6; = (1/var)*||X-X_r||^2 =  5621.4 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  4682.4 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  8851.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10303.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1348.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1416.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -68.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.37; perplexity/K = 4.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.15; perplexity/K = 4.75%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4399.3 e-6; = (1/var)*||X-X_r||^2 =  2098.7 e-6 = 47.7 %; (1+beta)*||Z_e-Z_q||^2 =  2300.6 e-6 = 52.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5317.2 e-6; = (1/var)*||X-X_r||^2 =  3121.6 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  2195.6 e-6 = 41.3 %)
Min.  Avg. Train Loss across Mini-Batch =  4357.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5317.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   917.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1022.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -105.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.44; perplexity/K = 5.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.09; perplexity/K = 5.11%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2645.3 e-6; = (1/var)*||X-X_r||^2 =  1246.0 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  1399.3 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3310.7 e-6; = (1/var)*||X-X_r||^2 =  1856.0 e-6 = 56.1 %; (1+beta)*||Z_e-Z_q||^2 =  1454.7 e-6 = 43.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2545.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3310.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   665.4 e-6; = (1/var)*||X-X_r||^2 val-train = 610.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 55.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.22; perplexity/K = 5.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.92; perplexity/K = 5.44%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2101.4 e-6; = (1/var)*||X-X_r||^2 =  957.4 e-6 = 45.6 %; (1+beta)*||Z_e-Z_q||^2 =  1144.0 e-6 = 54.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2687.9 e-6; = (1/var)*||X-X_r||^2 =  1537.0 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  1150.9 e-6 = 42.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2101.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2687.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   586.5 e-6; = (1/var)*||X-X_r||^2 val-train = 579.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.28; perplexity/K = 6.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.00; perplexity/K = 10.16%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  95714.7 e-6; = (1/var)*||X-X_r||^2 =  57358.7 e-6 = 59.9 %; (1+beta)*||Z_e-Z_q||^2 =  38356.0 e-6 = 40.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1895081.9 e-6; = (1/var)*||X-X_r||^2 =  216646.4 e-6 = 11.4 %; (1+beta)*||Z_e-Z_q||^2 =  1678435.5 e-6 = 88.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1321.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1816.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1799367.2 e-6; = (1/var)*||X-X_r||^2 val-train = 159287.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1640079.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.97; perplexity/K = 5.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.18; perplexity/K = 5.15%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4594.0 e-6; = (1/var)*||X-X_r||^2 =  2826.2 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  1767.9 e-6 = 38.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5641.7 e-6; = (1/var)*||X-X_r||^2 =  3648.1 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  1993.6 e-6 = 35.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1321.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1816.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1047.7 e-6; = (1/var)*||X-X_r||^2 val-train = 821.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 225.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.86; perplexity/K = 5.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.79; perplexity/K = 5.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1999.5 e-6; = (1/var)*||X-X_r||^2 =  1023.2 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  976.3 e-6 = 48.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2799.8 e-6; = (1/var)*||X-X_r||^2 =  1793.2 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  1006.6 e-6 = 36.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1321.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1816.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   800.3 e-6; = (1/var)*||X-X_r||^2 val-train = 770.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.46; perplexity/K = 6.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.54; perplexity/K = 6.85%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1666.2 e-6; = (1/var)*||X-X_r||^2 =  861.5 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  804.6 e-6 = 48.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2474.3 e-6; = (1/var)*||X-X_r||^2 =  1592.1 e-6 = 64.3 %; (1+beta)*||Z_e-Z_q||^2 =  882.3 e-6 = 35.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1321.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1816.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   808.2 e-6; = (1/var)*||X-X_r||^2 val-train = 730.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 77.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.24; perplexity/K = 5.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.12; perplexity/K = 5.52%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2200.6 e-6; = (1/var)*||X-X_r||^2 =  1137.2 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  1063.5 e-6 = 48.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2840.6 e-6; = (1/var)*||X-X_r||^2 =  1796.5 e-6 = 63.2 %; (1+beta)*||Z_e-Z_q||^2 =  1044.1 e-6 = 36.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1132.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1750.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   639.9 e-6; = (1/var)*||X-X_r||^2 val-train = 659.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.44; perplexity/K = 5.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.19; perplexity/K = 5.15%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  973.8 e-6; = (1/var)*||X-X_r||^2 =  522.2 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  451.6 e-6 = 46.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1518.0 e-6; = (1/var)*||X-X_r||^2 =  1010.1 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  507.9 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  937.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1493.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   544.2 e-6; = (1/var)*||X-X_r||^2 val-train = 487.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 56.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.87; perplexity/K = 5.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 4.79%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1558.4 e-6; = (1/var)*||X-X_r||^2 =  972.8 e-6 = 62.4 %; (1+beta)*||Z_e-Z_q||^2 =  585.6 e-6 = 37.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1604.9 e-6; = (1/var)*||X-X_r||^2 =  1120.5 e-6 = 69.8 %; (1+beta)*||Z_e-Z_q||^2 =  484.5 e-6 = 30.2 %)
Min.  Avg. Train Loss across Mini-Batch =  719.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1306.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46.6 e-6; = (1/var)*||X-X_r||^2 val-train = 147.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -101.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.25; perplexity/K = 5.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.53; perplexity/K = 5.29%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  652.0 e-6; = (1/var)*||X-X_r||^2 =  356.1 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  295.9 e-6 = 45.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1267.2 e-6; = (1/var)*||X-X_r||^2 =  887.2 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  379.9 e-6 = 30.0 %)
Min.  Avg. Train Loss across Mini-Batch =  600.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1127.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   615.2 e-6; = (1/var)*||X-X_r||^2 val-train = 531.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 84.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.22; perplexity/K = 7.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.52; perplexity/K = 6.85%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  641.3 e-6; = (1/var)*||X-X_r||^2 =  357.4 e-6 = 55.7 %; (1+beta)*||Z_e-Z_q||^2 =  283.8 e-6 = 44.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1174.1 e-6; = (1/var)*||X-X_r||^2 =  851.9 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  322.3 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  596.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1110.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   532.9 e-6; = (1/var)*||X-X_r||^2 val-train = 494.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.05; perplexity/K = 5.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.43; perplexity/K = 5.64%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  657.5 e-6; = (1/var)*||X-X_r||^2 =  309.4 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  348.1 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1124.7 e-6; = (1/var)*||X-X_r||^2 =  756.6 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  368.2 e-6 = 32.7 %)
Min.  Avg. Train Loss across Mini-Batch =  521.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  986.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   467.2 e-6; = (1/var)*||X-X_r||^2 val-train = 447.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.50; perplexity/K = 6.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.71; perplexity/K = 5.36%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  506.1 e-6; = (1/var)*||X-X_r||^2 =  273.7 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  232.4 e-6 = 45.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  941.8 e-6; = (1/var)*||X-X_r||^2 =  685.3 e-6 = 72.8 %; (1+beta)*||Z_e-Z_q||^2 =  256.6 e-6 = 27.2 %)
Min.  Avg. Train Loss across Mini-Batch =  460.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  882.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   435.8 e-6; = (1/var)*||X-X_r||^2 val-train = 411.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.86; perplexity/K = 5.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.16; perplexity/K = 5.53%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  475.7 e-6; = (1/var)*||X-X_r||^2 =  214.1 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  261.6 e-6 = 55.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  885.6 e-6; = (1/var)*||X-X_r||^2 =  603.7 e-6 = 68.2 %; (1+beta)*||Z_e-Z_q||^2 =  281.9 e-6 = 31.8 %)
Min.  Avg. Train Loss across Mini-Batch =  393.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  795.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   409.9 e-6; = (1/var)*||X-X_r||^2 val-train = 389.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.57; perplexity/K = 5.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.78; perplexity/K = 4.99%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  404.9 e-6; = (1/var)*||X-X_r||^2 =  222.2 e-6 = 54.9 %; (1+beta)*||Z_e-Z_q||^2 =  182.7 e-6 = 45.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  840.5 e-6; = (1/var)*||X-X_r||^2 =  622.0 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  218.5 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  378.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  794.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   435.6 e-6; = (1/var)*||X-X_r||^2 val-train = 399.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35.8 e-6 

----------------------------------------------------------------------------------

Finished [05:42:02 04.01.2023] 251) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 512) change_channel_size_across_layers = False:
Total training time is = 0:7:57 h/m/s. 

--------------------------------------------------- 

Started [05:42:02 04.01.2023] 252) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 512) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5308 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.23
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.74
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.74
3                                  encoder.pre_residual_stack.weight                       589            11.10
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                        16             0.30
10                             decoder.channel_adjusting_conv.weight                       147             2.77
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.74
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.74
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.50; perplexity/K = 3.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.54; perplexity/K = 3.34%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  428589.1 e-6; = (1/var)*||X-X_r||^2 =  155735.6 e-6 = 36.3 %; (1+beta)*||Z_e-Z_q||^2 =  272853.5 e-6 = 63.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  421553.2 e-6; = (1/var)*||X-X_r||^2 =  159779.5 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  261773.7 e-6 = 62.1 %)
Min.  Avg. Train Loss across Mini-Batch =  428589.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  421553.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7035.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4043.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11079.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.33; perplexity/K = 2.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.46; perplexity/K = 2.52%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  81364.4 e-6; = (1/var)*||X-X_r||^2 =  46645.9 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  34718.5 e-6 = 42.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  85046.8 e-6; = (1/var)*||X-X_r||^2 =  51295.2 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  33751.5 e-6 = 39.7 %)
Min.  Avg. Train Loss across Mini-Batch =  72953.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  79197.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3682.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4649.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -966.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.84; perplexity/K = 3.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.37; perplexity/K = 3.27%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22718.1 e-6; = (1/var)*||X-X_r||^2 =  11418.9 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  11299.2 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  26739.5 e-6; = (1/var)*||X-X_r||^2 =  15208.9 e-6 = 56.9 %; (1+beta)*||Z_e-Z_q||^2 =  11530.6 e-6 = 43.1 %)
Min.  Avg. Train Loss across Mini-Batch =  22718.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26739.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4021.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3790.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 231.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.64; perplexity/K = 2.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.50; perplexity/K = 2.93%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9650.4 e-6; = (1/var)*||X-X_r||^2 =  4842.9 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  4807.4 e-6 = 49.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  12964.7 e-6; = (1/var)*||X-X_r||^2 =  7697.5 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  5267.2 e-6 = 40.6 %)
Min.  Avg. Train Loss across Mini-Batch =  9650.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12596.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3314.4 e-6; = (1/var)*||X-X_r||^2 val-train = 2854.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 459.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.64; perplexity/K = 3.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.10; perplexity/K = 3.55%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35715.5 e-6; = (1/var)*||X-X_r||^2 =  23301.5 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  12414.1 e-6 = 34.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  17848.0 e-6; = (1/var)*||X-X_r||^2 =  10558.0 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  7290.0 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  6167.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8745.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17867.5 e-6; = (1/var)*||X-X_r||^2 val-train = -12743.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5124.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.72; perplexity/K = 3.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.98; perplexity/K = 3.51%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4678.0 e-6; = (1/var)*||X-X_r||^2 =  2179.0 e-6 = 46.6 %; (1+beta)*||Z_e-Z_q||^2 =  2499.0 e-6 = 53.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  7165.9 e-6; = (1/var)*||X-X_r||^2 =  4455.9 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  2710.0 e-6 = 37.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4678.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7156.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2488.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2276.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 211.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.28; perplexity/K = 3.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.88; perplexity/K = 3.08%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3231.1 e-6; = (1/var)*||X-X_r||^2 =  1590.6 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  1640.5 e-6 = 50.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  5609.6 e-6; = (1/var)*||X-X_r||^2 =  3646.5 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  1963.0 e-6 = 35.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3196.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5370.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2378.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2056.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 322.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.69; perplexity/K = 3.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.32; perplexity/K = 3.25%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2507.1 e-6; = (1/var)*||X-X_r||^2 =  1153.6 e-6 = 46.0 %; (1+beta)*||Z_e-Z_q||^2 =  1353.4 e-6 = 54.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4725.2 e-6; = (1/var)*||X-X_r||^2 =  3177.1 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  1548.1 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2507.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4468.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2218.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2023.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 194.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.09; perplexity/K = 3.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.22; perplexity/K = 3.21%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2132.1 e-6; = (1/var)*||X-X_r||^2 =  925.8 e-6 = 43.4 %; (1+beta)*||Z_e-Z_q||^2 =  1206.3 e-6 = 56.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3899.4 e-6; = (1/var)*||X-X_r||^2 =  2557.6 e-6 = 65.6 %; (1+beta)*||Z_e-Z_q||^2 =  1341.7 e-6 = 34.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2093.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3896.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1767.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1631.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 135.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.17; perplexity/K = 2.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.37; perplexity/K = 2.88%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1902.2 e-6; = (1/var)*||X-X_r||^2 =  858.9 e-6 = 45.2 %; (1+beta)*||Z_e-Z_q||^2 =  1043.3 e-6 = 54.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3589.3 e-6; = (1/var)*||X-X_r||^2 =  2403.4 e-6 = 67.0 %; (1+beta)*||Z_e-Z_q||^2 =  1185.9 e-6 = 33.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1826.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3575.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1687.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1544.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 142.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 4.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.07; perplexity/K = 4.33%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:12:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1608.4 e-6; = (1/var)*||X-X_r||^2 =  739.0 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  869.4 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3241.8 e-6; = (1/var)*||X-X_r||^2 =  2228.1 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  1013.7 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1524.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3229.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1633.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1489.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 144.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.52; perplexity/K = 3.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.96; perplexity/K = 3.89%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:19:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1476.6 e-6; = (1/var)*||X-X_r||^2 =  694.2 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  782.4 e-6 = 53.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3171.6 e-6; = (1/var)*||X-X_r||^2 =  2227.5 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  944.1 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1410.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3049.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1695.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1533.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 161.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 4.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.74; perplexity/K = 4.59%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:26:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1414.5 e-6; = (1/var)*||X-X_r||^2 =  540.2 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  874.3 e-6 = 61.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3106.1 e-6; = (1/var)*||X-X_r||^2 =  2089.3 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  1016.8 e-6 = 32.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1352.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2964.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1691.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1549.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 142.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.99; perplexity/K = 4.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.88; perplexity/K = 4.25%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:32:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1265.1 e-6; = (1/var)*||X-X_r||^2 =  481.1 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  784.0 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2805.2 e-6; = (1/var)*||X-X_r||^2 =  1866.8 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  938.4 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1219.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2725.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1540.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1385.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 154.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.26; perplexity/K = 4.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.57; perplexity/K = 3.74%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:39:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1275.4 e-6; = (1/var)*||X-X_r||^2 =  467.9 e-6 = 36.7 %; (1+beta)*||Z_e-Z_q||^2 =  807.5 e-6 = 63.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2662.0 e-6; = (1/var)*||X-X_r||^2 =  1756.1 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  905.9 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1067.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2512.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1386.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1288.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 98.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.96; perplexity/K = 5.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.47; perplexity/K = 5.26%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:45:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1245.7 e-6; = (1/var)*||X-X_r||^2 =  553.1 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  692.5 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2722.5 e-6; = (1/var)*||X-X_r||^2 =  1932.9 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  789.6 e-6 = 29.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1067.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2511.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1476.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1379.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 97.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.42; perplexity/K = 4.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.97; perplexity/K = 3.89%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:52:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1053.8 e-6; = (1/var)*||X-X_r||^2 =  389.4 e-6 = 37.0 %; (1+beta)*||Z_e-Z_q||^2 =  664.4 e-6 = 63.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2579.6 e-6; = (1/var)*||X-X_r||^2 =  1804.6 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  775.0 e-6 = 30.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2370.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1525.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1415.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 110.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.32; perplexity/K = 4.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.03; perplexity/K = 3.92%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:59:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1308.5 e-6; = (1/var)*||X-X_r||^2 =  664.0 e-6 = 50.7 %; (1+beta)*||Z_e-Z_q||^2 =  644.5 e-6 = 49.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2902.8 e-6; = (1/var)*||X-X_r||^2 =  2133.0 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  769.7 e-6 = 26.5 %)
Min.  Avg. Train Loss across Mini-Batch =  985.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2366.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1594.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1469.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 125.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.21; perplexity/K = 5.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.95; perplexity/K = 5.06%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:5:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1310.4 e-6; = (1/var)*||X-X_r||^2 =  418.3 e-6 = 31.9 %; (1+beta)*||Z_e-Z_q||^2 =  892.1 e-6 = 68.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2657.9 e-6; = (1/var)*||X-X_r||^2 =  1625.8 e-6 = 61.2 %; (1+beta)*||Z_e-Z_q||^2 =  1032.1 e-6 = 38.8 %)
Min.  Avg. Train Loss across Mini-Batch =  936.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2251.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1347.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1207.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 140.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.54; perplexity/K = 4.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.84; perplexity/K = 4.23%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:12:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  928.6 e-6; = (1/var)*||X-X_r||^2 =  311.2 e-6 = 33.5 %; (1+beta)*||Z_e-Z_q||^2 =  617.4 e-6 = 66.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2236.6 e-6; = (1/var)*||X-X_r||^2 =  1457.6 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  779.0 e-6 = 34.8 %)
Min.  Avg. Train Loss across Mini-Batch =  901.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2176.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1308.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1146.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 161.6 e-6 

----------------------------------------------------------------------------------

Finished [07:55:06 04.01.2023] 252) Finished running for K = 256 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 512) change_channel_size_across_layers = False:
Total training time is = 0:7:4 h/m/s. 

--------------------------------------------------- 

Started [07:55:06 04.01.2023] 253) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 128) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 746 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.07
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.29
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.56
4                                  encoder.pre_residual_stack.weight                       147            19.71
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.83
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.83
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
9                              encoder.channel_adjusting_conv.weight                         8             1.07
10                                                       VQ.E.weight                        16             2.14
11                             decoder.channel_adjusting_conv.weight                        73             9.79
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.83
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.83
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.56
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.29
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.07
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.79; perplexity/K = 3.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.60; perplexity/K = 3.36%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1234847.9 e-6; = (1/var)*||X-X_r||^2 =  546167.1 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  688680.9 e-6 = 55.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  833865.5 e-6; = (1/var)*||X-X_r||^2 =  510916.1 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  322949.4 e-6 = 38.7 %)
Min.  Avg. Train Loss across Mini-Batch =  988703.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  833865.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -400982.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35250.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -365731.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.27; perplexity/K = 8.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.88; perplexity/K = 8.16%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  182243.3 e-6; = (1/var)*||X-X_r||^2 =  163905.5 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  18337.8 e-6 = 10.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  188673.9 e-6; = (1/var)*||X-X_r||^2 =  168993.6 e-6 = 89.6 %; (1+beta)*||Z_e-Z_q||^2 =  19680.4 e-6 = 10.4 %)
Min.  Avg. Train Loss across Mini-Batch =  182243.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  188673.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6430.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5088.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1342.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.03; perplexity/K = 10.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.65; perplexity/K = 10.02%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102720.7 e-6; = (1/var)*||X-X_r||^2 =  94340.7 e-6 = 91.8 %; (1+beta)*||Z_e-Z_q||^2 =  8380.0 e-6 = 8.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  112106.2 e-6; = (1/var)*||X-X_r||^2 =  101344.0 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  10762.2 e-6 = 9.6 %)
Min.  Avg. Train Loss across Mini-Batch =  102352.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  108844.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9385.5 e-6; = (1/var)*||X-X_r||^2 val-train = 7003.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2382.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.09; perplexity/K = 14.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.64; perplexity/K = 14.31%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:8:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  77951.0 e-6; = (1/var)*||X-X_r||^2 =  71354.1 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  6596.9 e-6 = 8.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  83815.1 e-6; = (1/var)*||X-X_r||^2 =  77375.1 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  6439.9 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  77491.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  83785.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5864.1 e-6; = (1/var)*||X-X_r||^2 val-train = 6021.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -157.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.08; perplexity/K = 17.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.39; perplexity/K = 16.95%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  63961.7 e-6; = (1/var)*||X-X_r||^2 =  59623.1 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  4338.6 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  69830.8 e-6; = (1/var)*||X-X_r||^2 =  65327.5 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  4503.3 e-6 = 6.4 %)
Min.  Avg. Train Loss across Mini-Batch =  63961.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  69830.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5869.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5704.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 164.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.91; perplexity/K = 18.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.77; perplexity/K = 18.27%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55742.7 e-6; = (1/var)*||X-X_r||^2 =  52908.4 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  2834.3 e-6 = 5.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  60884.8 e-6; = (1/var)*||X-X_r||^2 =  58112.4 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  2772.4 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  55742.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60884.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5142.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5204.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -61.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.62; perplexity/K = 20.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.78; perplexity/K = 21.40%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50263.9 e-6; = (1/var)*||X-X_r||^2 =  48174.3 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  2089.6 e-6 = 4.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  54974.4 e-6; = (1/var)*||X-X_r||^2 =  52869.4 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  2105.0 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  50263.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  54974.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4710.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4695.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.73; perplexity/K = 20.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.46; perplexity/K = 20.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46395.0 e-6; = (1/var)*||X-X_r||^2 =  44668.8 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  1726.2 e-6 = 3.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  51437.8 e-6; = (1/var)*||X-X_r||^2 =  49509.5 e-6 = 96.3 %; (1+beta)*||Z_e-Z_q||^2 =  1928.2 e-6 = 3.7 %)
Min.  Avg. Train Loss across Mini-Batch =  46395.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51437.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5042.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4840.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 202.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.92; perplexity/K = 21.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.40; perplexity/K = 21.25%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42605.2 e-6; = (1/var)*||X-X_r||^2 =  41151.6 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  1453.7 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  47587.9 e-6; = (1/var)*||X-X_r||^2 =  46009.0 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  1578.9 e-6 = 3.3 %)
Min.  Avg. Train Loss across Mini-Batch =  42545.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47587.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4982.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4857.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 125.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.49; perplexity/K = 22.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.36; perplexity/K = 20.84%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40126.0 e-6; = (1/var)*||X-X_r||^2 =  38725.5 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  1400.6 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  44976.1 e-6; = (1/var)*||X-X_r||^2 =  43389.9 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  1586.2 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  40126.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44976.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4850.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4664.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 185.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.09; perplexity/K = 22.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.04; perplexity/K = 22.28%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38539.0 e-6; = (1/var)*||X-X_r||^2 =  37342.4 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  1196.6 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  43435.7 e-6; = (1/var)*||X-X_r||^2 =  42142.3 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  1293.4 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  38405.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43282.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4896.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4799.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 96.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.56; perplexity/K = 21.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.45; perplexity/K = 22.05%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37291.1 e-6; = (1/var)*||X-X_r||^2 =  36149.5 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  1141.6 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  42270.9 e-6; = (1/var)*||X-X_r||^2 =  41038.9 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  1232.0 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  37201.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  41888.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4979.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4889.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 90.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.84; perplexity/K = 22.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.29; perplexity/K = 22.77%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36288.2 e-6; = (1/var)*||X-X_r||^2 =  35200.6 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  1087.6 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  40776.1 e-6; = (1/var)*||X-X_r||^2 =  39602.2 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  1173.8 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  36288.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40729.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4487.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4401.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 86.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.49; perplexity/K = 23.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.47; perplexity/K = 22.45%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35568.4 e-6; = (1/var)*||X-X_r||^2 =  34520.8 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  1047.6 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  40161.1 e-6; = (1/var)*||X-X_r||^2 =  39046.8 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  1114.3 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  35477.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  40102.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4592.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4526.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 66.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.62; perplexity/K = 23.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.81; perplexity/K = 22.58%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34886.0 e-6; = (1/var)*||X-X_r||^2 =  33883.8 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  1002.2 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  39252.0 e-6; = (1/var)*||X-X_r||^2 =  38228.5 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  1023.5 e-6 = 2.6 %)
Min.  Avg. Train Loss across Mini-Batch =  34886.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  39154.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4366.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4344.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.43; perplexity/K = 22.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.20; perplexity/K = 22.74%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35194.7 e-6; = (1/var)*||X-X_r||^2 =  34157.5 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  1037.2 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  39107.5 e-6; = (1/var)*||X-X_r||^2 =  38085.5 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  1022.0 e-6 = 2.6 %)
Min.  Avg. Train Loss across Mini-Batch =  34388.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38953.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3912.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3927.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.51; perplexity/K = 23.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.96; perplexity/K = 22.64%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34213.3 e-6; = (1/var)*||X-X_r||^2 =  33300.4 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  913.0 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  38334.6 e-6; = (1/var)*||X-X_r||^2 =  37379.0 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  955.6 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  33617.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38019.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4121.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4078.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 42.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.40; perplexity/K = 22.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.37; perplexity/K = 23.19%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33146.4 e-6; = (1/var)*||X-X_r||^2 =  32280.3 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  866.1 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  37720.9 e-6; = (1/var)*||X-X_r||^2 =  36768.9 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  952.0 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  33079.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37554.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4574.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4488.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 85.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.50; perplexity/K = 22.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.98; perplexity/K = 22.65%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32972.6 e-6; = (1/var)*||X-X_r||^2 =  32114.9 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  857.7 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  37161.2 e-6; = (1/var)*||X-X_r||^2 =  36221.3 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  939.9 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  32901.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37161.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4188.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4106.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 82.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.72; perplexity/K = 21.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.97; perplexity/K = 21.86%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:0 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41352.0 e-6; = (1/var)*||X-X_r||^2 =  39645.3 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  1706.7 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  41155.5 e-6; = (1/var)*||X-X_r||^2 =  39779.3 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  1376.3 e-6 = 3.3 %)
Min.  Avg. Train Loss across Mini-Batch =  32365.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  36457.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -196.4 e-6; = (1/var)*||X-X_r||^2 val-train = 134.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -330.4 e-6 

----------------------------------------------------------------------------------

Finished [08:39:13 04.01.2023] 253) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 128) change_channel_size_across_layers = True:
Total training time is = 0:3:6 h/m/s. 

--------------------------------------------------- 

Started [08:39:13 04.01.2023] 254) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 128) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2468 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.30
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.31
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.23
4                                  encoder.pre_residual_stack.weight                       589            23.87
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.96
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.96
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                        16             0.65
11                             decoder.channel_adjusting_conv.weight                       147             5.96
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.96
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.96
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.23
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.31
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.95; perplexity/K = 10.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.66; perplexity/K = 10.80%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  262902.7 e-6; = (1/var)*||X-X_r||^2 =  178714.8 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  84187.9 e-6 = 32.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  263093.8 e-6; = (1/var)*||X-X_r||^2 =  186354.1 e-6 = 70.8 %; (1+beta)*||Z_e-Z_q||^2 =  76739.7 e-6 = 29.2 %)
Min.  Avg. Train Loss across Mini-Batch =  262902.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  263093.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   191.1 e-6; = (1/var)*||X-X_r||^2 val-train = 7639.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7448.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.59; perplexity/K = 14.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.54; perplexity/K = 14.27%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  104958.1 e-6; = (1/var)*||X-X_r||^2 =  66615.2 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  38342.9 e-6 = 36.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  114144.0 e-6; = (1/var)*||X-X_r||^2 =  75015.6 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  39128.4 e-6 = 34.3 %)
Min.  Avg. Train Loss across Mini-Batch =  104958.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  114068.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9185.9 e-6; = (1/var)*||X-X_r||^2 val-train = 8400.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 785.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.86; perplexity/K = 16.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.44; perplexity/K = 16.97%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  37638.6 e-6; = (1/var)*||X-X_r||^2 =  21913.7 e-6 = 58.2 %; (1+beta)*||Z_e-Z_q||^2 =  15724.8 e-6 = 41.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  42985.8 e-6; = (1/var)*||X-X_r||^2 =  26716.9 e-6 = 62.2 %; (1+beta)*||Z_e-Z_q||^2 =  16268.9 e-6 = 37.8 %)
Min.  Avg. Train Loss across Mini-Batch =  37638.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42985.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5347.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4803.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 544.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.23; perplexity/K = 20.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.91; perplexity/K = 19.89%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16472.6 e-6; = (1/var)*||X-X_r||^2 =  8302.2 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  8170.4 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  20436.4 e-6; = (1/var)*||X-X_r||^2 =  11826.8 e-6 = 57.9 %; (1+beta)*||Z_e-Z_q||^2 =  8609.6 e-6 = 42.1 %)
Min.  Avg. Train Loss across Mini-Batch =  16472.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  20436.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3963.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3524.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 439.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.14; perplexity/K = 20.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.10; perplexity/K = 18.79%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6778.7 e-6; = (1/var)*||X-X_r||^2 =  2682.0 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  4096.7 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  9067.5 e-6; = (1/var)*||X-X_r||^2 =  4643.5 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  4424.1 e-6 = 48.8 %)
Min.  Avg. Train Loss across Mini-Batch =  6778.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9067.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2288.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1961.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 327.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.07; perplexity/K = 19.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.03; perplexity/K = 18.76%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6835.1 e-6; = (1/var)*||X-X_r||^2 =  2109.7 e-6 = 30.9 %; (1+beta)*||Z_e-Z_q||^2 =  4725.4 e-6 = 69.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8076.9 e-6; = (1/var)*||X-X_r||^2 =  3405.5 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  4671.4 e-6 = 57.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4002.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5832.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1241.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1295.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -54.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.36; perplexity/K = 18.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.20; perplexity/K = 18.44%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3785.3 e-6; = (1/var)*||X-X_r||^2 =  1133.4 e-6 = 29.9 %; (1+beta)*||Z_e-Z_q||^2 =  2651.9 e-6 = 70.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  4720.1 e-6; = (1/var)*||X-X_r||^2 =  2068.0 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  2652.1 e-6 = 56.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3182.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4391.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   934.8 e-6; = (1/var)*||X-X_r||^2 val-train = 934.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 65.48; perplexity/K = 25.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.87; perplexity/K = 23.78%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21636.7 e-6; = (1/var)*||X-X_r||^2 =  10882.8 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  10753.9 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  22008.2 e-6; = (1/var)*||X-X_r||^2 =  11777.7 e-6 = 53.5 %; (1+beta)*||Z_e-Z_q||^2 =  10230.5 e-6 = 46.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2008.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3048.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   371.6 e-6; = (1/var)*||X-X_r||^2 val-train = 894.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -523.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.70; perplexity/K = 22.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.69; perplexity/K = 20.58%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4776.1 e-6; = (1/var)*||X-X_r||^2 =  2024.4 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  2751.7 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5831.1 e-6; = (1/var)*||X-X_r||^2 =  3059.2 e-6 = 52.5 %; (1+beta)*||Z_e-Z_q||^2 =  2771.9 e-6 = 47.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2008.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3048.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1055.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1034.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.18; perplexity/K = 22.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.31; perplexity/K = 22.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5036.7 e-6; = (1/var)*||X-X_r||^2 =  1744.3 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  3292.4 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  5484.2 e-6; = (1/var)*||X-X_r||^2 =  2402.9 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  3081.3 e-6 = 56.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2008.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3048.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   447.5 e-6; = (1/var)*||X-X_r||^2 val-train = 658.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -211.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.17; perplexity/K = 24.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.99; perplexity/K = 21.87%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4221.5 e-6; = (1/var)*||X-X_r||^2 =  1274.0 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  2947.5 e-6 = 69.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4868.3 e-6; = (1/var)*||X-X_r||^2 =  2020.1 e-6 = 41.5 %; (1+beta)*||Z_e-Z_q||^2 =  2848.2 e-6 = 58.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1875.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2689.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   646.8 e-6; = (1/var)*||X-X_r||^2 val-train = 746.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -99.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.94; perplexity/K = 24.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.57; perplexity/K = 23.27%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4942.8 e-6; = (1/var)*||X-X_r||^2 =  1566.4 e-6 = 31.7 %; (1+beta)*||Z_e-Z_q||^2 =  3376.4 e-6 = 68.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5333.1 e-6; = (1/var)*||X-X_r||^2 =  2294.9 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  3038.2 e-6 = 57.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1381.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2011.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   390.4 e-6; = (1/var)*||X-X_r||^2 val-train = 728.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -338.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.16; perplexity/K = 21.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.57; perplexity/K = 21.32%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2180.9 e-6; = (1/var)*||X-X_r||^2 =  634.3 e-6 = 29.1 %; (1+beta)*||Z_e-Z_q||^2 =  1546.6 e-6 = 70.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2648.0 e-6; = (1/var)*||X-X_r||^2 =  1129.7 e-6 = 42.7 %; (1+beta)*||Z_e-Z_q||^2 =  1518.4 e-6 = 57.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1037.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1769.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   467.1 e-6; = (1/var)*||X-X_r||^2 val-train = 495.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -28.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.26; perplexity/K = 23.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.46; perplexity/K = 24.01%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3716.7 e-6; = (1/var)*||X-X_r||^2 =  895.9 e-6 = 24.1 %; (1+beta)*||Z_e-Z_q||^2 =  2820.9 e-6 = 75.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5166.3 e-6; = (1/var)*||X-X_r||^2 =  2448.8 e-6 = 47.4 %; (1+beta)*||Z_e-Z_q||^2 =  2717.5 e-6 = 52.6 %)
Min.  Avg. Train Loss across Mini-Batch =  873.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1648.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1449.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1552.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -103.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.24; perplexity/K = 23.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.05; perplexity/K = 21.89%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1278.3 e-6; = (1/var)*||X-X_r||^2 =  323.9 e-6 = 25.3 %; (1+beta)*||Z_e-Z_q||^2 =  954.4 e-6 = 74.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1786.4 e-6; = (1/var)*||X-X_r||^2 =  797.8 e-6 = 44.7 %; (1+beta)*||Z_e-Z_q||^2 =  988.7 e-6 = 55.3 %)
Min.  Avg. Train Loss across Mini-Batch =  873.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1648.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   508.2 e-6; = (1/var)*||X-X_r||^2 val-train = 473.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.18; perplexity/K = 21.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.54; perplexity/K = 20.52%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1512.9 e-6; = (1/var)*||X-X_r||^2 =  331.9 e-6 = 21.9 %; (1+beta)*||Z_e-Z_q||^2 =  1181.0 e-6 = 78.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2186.4 e-6; = (1/var)*||X-X_r||^2 =  980.8 e-6 = 44.9 %; (1+beta)*||Z_e-Z_q||^2 =  1205.6 e-6 = 55.1 %)
Min.  Avg. Train Loss across Mini-Batch =  873.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1401.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   673.5 e-6; = (1/var)*||X-X_r||^2 val-train = 648.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.56; perplexity/K = 23.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 62.69; perplexity/K = 24.49%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3388.7 e-6; = (1/var)*||X-X_r||^2 =  1667.3 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  1721.4 e-6 = 50.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2547.8 e-6; = (1/var)*||X-X_r||^2 =  1004.9 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  1542.9 e-6 = 60.6 %)
Min.  Avg. Train Loss across Mini-Batch =  838.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1401.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -840.9 e-6; = (1/var)*||X-X_r||^2 val-train = -662.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -178.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.37; perplexity/K = 22.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.38; perplexity/K = 21.63%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  815.9 e-6; = (1/var)*||X-X_r||^2 =  205.4 e-6 = 25.2 %; (1+beta)*||Z_e-Z_q||^2 =  610.6 e-6 = 74.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1241.7 e-6; = (1/var)*||X-X_r||^2 =  591.9 e-6 = 47.7 %; (1+beta)*||Z_e-Z_q||^2 =  649.8 e-6 = 52.3 %)
Min.  Avg. Train Loss across Mini-Batch =  815.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1166.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   425.7 e-6; = (1/var)*||X-X_r||^2 val-train = 386.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 65.65; perplexity/K = 25.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.56; perplexity/K = 26.00%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5492.5 e-6; = (1/var)*||X-X_r||^2 =  2344.9 e-6 = 42.7 %; (1+beta)*||Z_e-Z_q||^2 =  3147.6 e-6 = 57.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5295.6 e-6; = (1/var)*||X-X_r||^2 =  2651.1 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  2644.5 e-6 = 49.9 %)
Min.  Avg. Train Loss across Mini-Batch =  601.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1064.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -196.9 e-6; = (1/var)*||X-X_r||^2 val-train = 306.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -503.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.78; perplexity/K = 22.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.80; perplexity/K = 23.36%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1537.6 e-6; = (1/var)*||X-X_r||^2 =  308.4 e-6 = 20.1 %; (1+beta)*||Z_e-Z_q||^2 =  1229.2 e-6 = 79.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1951.9 e-6; = (1/var)*||X-X_r||^2 =  743.7 e-6 = 38.1 %; (1+beta)*||Z_e-Z_q||^2 =  1208.2 e-6 = 61.9 %)
Min.  Avg. Train Loss across Mini-Batch =  601.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1064.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   414.3 e-6; = (1/var)*||X-X_r||^2 val-train = 435.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -21.0 e-6 

----------------------------------------------------------------------------------

Finished [09:25:20 04.01.2023] 254) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 128) change_channel_size_across_layers = True:
Total training time is = 0:3:6 h/m/s. 

--------------------------------------------------- 

Started [09:25:20 04.01.2023] 255) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 128) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1988 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.18
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.18
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.18
4                                  encoder.pre_residual_stack.weight                       147             7.39
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.81
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.81
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.40
10                                                       VQ.E.weight                        16             0.80
11                             decoder.channel_adjusting_conv.weight                        73             3.67
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.81
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.81
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.18
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.18
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.18
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.98; perplexity/K = 14.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.57; perplexity/K = 14.29%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  277872.1 e-6; = (1/var)*||X-X_r||^2 =  147572.0 e-6 = 53.1 %; (1+beta)*||Z_e-Z_q||^2 =  130300.1 e-6 = 46.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  284879.2 e-6; = (1/var)*||X-X_r||^2 =  163146.1 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  121733.1 e-6 = 42.7 %)
Min.  Avg. Train Loss across Mini-Batch =  277872.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  281921.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7007.1 e-6; = (1/var)*||X-X_r||^2 val-train = 15574.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8567.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.80; perplexity/K = 21.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.97; perplexity/K = 19.52%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  128925.9 e-6; = (1/var)*||X-X_r||^2 =  92026.6 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  36899.3 e-6 = 28.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  210038.9 e-6; = (1/var)*||X-X_r||^2 =  145034.0 e-6 = 69.1 %; (1+beta)*||Z_e-Z_q||^2 =  65004.9 e-6 = 30.9 %)
Min.  Avg. Train Loss across Mini-Batch =  49919.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56146.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   81113.0 e-6; = (1/var)*||X-X_r||^2 val-train = 53007.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28105.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.88; perplexity/K = 30.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.65; perplexity/K = 29.94%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12535.6 e-6; = (1/var)*||X-X_r||^2 =  6501.3 e-6 = 51.9 %; (1+beta)*||Z_e-Z_q||^2 =  6034.3 e-6 = 48.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  15440.1 e-6; = (1/var)*||X-X_r||^2 =  9457.8 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  5982.3 e-6 = 38.7 %)
Min.  Avg. Train Loss across Mini-Batch =  12535.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15440.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2904.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2956.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -52.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 87.54; perplexity/K = 34.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 88.10; perplexity/K = 34.42%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5369.4 e-6; = (1/var)*||X-X_r||^2 =  2558.0 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  2811.4 e-6 = 52.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  7170.1 e-6; = (1/var)*||X-X_r||^2 =  4318.2 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  2851.9 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  5369.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7102.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1800.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1760.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 83.93; perplexity/K = 32.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 83.46; perplexity/K = 32.60%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3650.1 e-6; = (1/var)*||X-X_r||^2 =  1593.6 e-6 = 43.7 %; (1+beta)*||Z_e-Z_q||^2 =  2056.4 e-6 = 56.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4711.2 e-6; = (1/var)*||X-X_r||^2 =  2660.4 e-6 = 56.5 %; (1+beta)*||Z_e-Z_q||^2 =  2050.8 e-6 = 43.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3638.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4711.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1061.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1066.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 86.41; perplexity/K = 33.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 84.22; perplexity/K = 32.90%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2150.8 e-6; = (1/var)*||X-X_r||^2 =  1029.3 e-6 = 47.9 %; (1+beta)*||Z_e-Z_q||^2 =  1121.5 e-6 = 52.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2822.8 e-6; = (1/var)*||X-X_r||^2 =  1672.5 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  1150.3 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2150.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2822.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   672.0 e-6; = (1/var)*||X-X_r||^2 val-train = 643.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 84.39; perplexity/K = 32.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.95; perplexity/K = 32.40%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1666.2 e-6; = (1/var)*||X-X_r||^2 =  803.3 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  862.9 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2273.0 e-6; = (1/var)*||X-X_r||^2 =  1374.9 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  898.1 e-6 = 39.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1662.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2273.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   606.9 e-6; = (1/var)*||X-X_r||^2 val-train = 571.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.69; perplexity/K = 32.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 82.37; perplexity/K = 32.18%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1237.8 e-6; = (1/var)*||X-X_r||^2 =  624.3 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  613.5 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1798.5 e-6; = (1/var)*||X-X_r||^2 =  1155.2 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  643.2 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1230.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1784.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   560.7 e-6; = (1/var)*||X-X_r||^2 val-train = 530.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 85.29; perplexity/K = 33.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.49; perplexity/K = 31.44%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1016.2 e-6; = (1/var)*||X-X_r||^2 =  512.7 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  503.6 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1451.0 e-6; = (1/var)*||X-X_r||^2 =  922.5 e-6 = 63.6 %; (1+beta)*||Z_e-Z_q||^2 =  528.5 e-6 = 36.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1006.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1451.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   434.8 e-6; = (1/var)*||X-X_r||^2 val-train = 409.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 85.24; perplexity/K = 33.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.31; perplexity/K = 31.37%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  896.5 e-6; = (1/var)*||X-X_r||^2 =  466.9 e-6 = 52.1 %; (1+beta)*||Z_e-Z_q||^2 =  429.6 e-6 = 47.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1443.6 e-6; = (1/var)*||X-X_r||^2 =  1006.6 e-6 = 69.7 %; (1+beta)*||Z_e-Z_q||^2 =  437.0 e-6 = 30.3 %)
Min.  Avg. Train Loss across Mini-Batch =  836.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1268.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   547.1 e-6; = (1/var)*||X-X_r||^2 val-train = 539.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 89.38; perplexity/K = 34.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 86.55; perplexity/K = 33.81%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1536.4 e-6; = (1/var)*||X-X_r||^2 =  633.2 e-6 = 41.2 %; (1+beta)*||Z_e-Z_q||^2 =  903.3 e-6 = 58.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1906.2 e-6; = (1/var)*||X-X_r||^2 =  1031.5 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  874.7 e-6 = 45.9 %)
Min.  Avg. Train Loss across Mini-Batch =  713.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1102.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   369.7 e-6; = (1/var)*||X-X_r||^2 val-train = 398.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -28.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.03; perplexity/K = 29.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 79.78; perplexity/K = 31.17%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  671.6 e-6; = (1/var)*||X-X_r||^2 =  331.6 e-6 = 49.4 %; (1+beta)*||Z_e-Z_q||^2 =  340.0 e-6 = 50.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  994.9 e-6; = (1/var)*||X-X_r||^2 =  642.8 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  352.2 e-6 = 35.4 %)
Min.  Avg. Train Loss across Mini-Batch =  628.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  976.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   323.3 e-6; = (1/var)*||X-X_r||^2 val-train = 311.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 97.92; perplexity/K = 38.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 99.77; perplexity/K = 38.97%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6195.4 e-6; = (1/var)*||X-X_r||^2 =  4071.0 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  2124.4 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3931.2 e-6; = (1/var)*||X-X_r||^2 =  2290.0 e-6 = 58.3 %; (1+beta)*||Z_e-Z_q||^2 =  1641.1 e-6 = 41.7 %)
Min.  Avg. Train Loss across Mini-Batch =  575.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  925.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2264.3 e-6; = (1/var)*||X-X_r||^2 val-train = -1780.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -483.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.61; perplexity/K = 31.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.20; perplexity/K = 31.33%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  909.1 e-6; = (1/var)*||X-X_r||^2 =  337.8 e-6 = 37.2 %; (1+beta)*||Z_e-Z_q||^2 =  571.2 e-6 = 62.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1273.4 e-6; = (1/var)*||X-X_r||^2 =  693.6 e-6 = 54.5 %; (1+beta)*||Z_e-Z_q||^2 =  579.8 e-6 = 45.5 %)
Min.  Avg. Train Loss across Mini-Batch =  519.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  792.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   364.4 e-6; = (1/var)*||X-X_r||^2 val-train = 355.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 77.43; perplexity/K = 30.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.98; perplexity/K = 28.12%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  522.4 e-6; = (1/var)*||X-X_r||^2 =  223.1 e-6 = 42.7 %; (1+beta)*||Z_e-Z_q||^2 =  299.3 e-6 = 57.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  824.7 e-6; = (1/var)*||X-X_r||^2 =  503.6 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  321.1 e-6 = 38.9 %)
Min.  Avg. Train Loss across Mini-Batch =  519.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  792.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   302.3 e-6; = (1/var)*||X-X_r||^2 val-train = 280.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 79.95; perplexity/K = 31.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 80.41; perplexity/K = 31.41%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393.7 e-6; = (1/var)*||X-X_r||^2 =  211.8 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  181.8 e-6 = 46.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  615.5 e-6; = (1/var)*||X-X_r||^2 =  417.6 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  197.8 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  382.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  603.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   221.8 e-6; = (1/var)*||X-X_r||^2 val-train = 205.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.05; perplexity/K = 29.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.77; perplexity/K = 29.60%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  784.8 e-6; = (1/var)*||X-X_r||^2 =  256.6 e-6 = 32.7 %; (1+beta)*||Z_e-Z_q||^2 =  528.2 e-6 = 67.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1051.4 e-6; = (1/var)*||X-X_r||^2 =  525.9 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  525.5 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  341.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  588.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   266.6 e-6; = (1/var)*||X-X_r||^2 val-train = 269.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 78.85; perplexity/K = 30.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.56; perplexity/K = 28.73%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  518.8 e-6; = (1/var)*||X-X_r||^2 =  199.5 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  319.4 e-6 = 61.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  771.2 e-6; = (1/var)*||X-X_r||^2 =  427.3 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  343.9 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  341.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  588.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   252.4 e-6; = (1/var)*||X-X_r||^2 val-train = 227.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 75.94; perplexity/K = 29.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.89; perplexity/K = 28.86%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  616.0 e-6; = (1/var)*||X-X_r||^2 =  228.2 e-6 = 37.0 %; (1+beta)*||Z_e-Z_q||^2 =  387.8 e-6 = 63.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  777.8 e-6; = (1/var)*||X-X_r||^2 =  390.7 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  387.1 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  341.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  588.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161.8 e-6; = (1/var)*||X-X_r||^2 val-train = 162.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 95.27; perplexity/K = 37.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 93.60; perplexity/K = 36.56%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1590.2 e-6; = (1/var)*||X-X_r||^2 =  477.5 e-6 = 30.0 %; (1+beta)*||Z_e-Z_q||^2 =  1112.8 e-6 = 70.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1825.9 e-6; = (1/var)*||X-X_r||^2 =  791.0 e-6 = 43.3 %; (1+beta)*||Z_e-Z_q||^2 =  1034.9 e-6 = 56.7 %)
Min.  Avg. Train Loss across Mini-Batch =  321.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  515.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   235.7 e-6; = (1/var)*||X-X_r||^2 val-train = 313.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -77.8 e-6 

----------------------------------------------------------------------------------

Finished [10:27:09 04.01.2023] 255) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 128) change_channel_size_across_layers = False:
Total training time is = 0:3:49 h/m/s. 

--------------------------------------------------- 

Started [10:27:09 04.01.2023] 256) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 128) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7404 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.15
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.15
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.15
4                                  encoder.pre_residual_stack.weight                       589             7.96
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                        16             0.22
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.15
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.15
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.15
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.95; perplexity/K = 10.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.03; perplexity/K = 10.17%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  174055.1 e-6; = (1/var)*||X-X_r||^2 =  86914.7 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  87140.4 e-6 = 50.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  187435.8 e-6; = (1/var)*||X-X_r||^2 =  101296.6 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  86139.2 e-6 = 46.0 %)
Min.  Avg. Train Loss across Mini-Batch =  174055.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  184388.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13380.7 e-6; = (1/var)*||X-X_r||^2 val-train = 14381.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1001.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.05; perplexity/K = 17.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.44; perplexity/K = 18.53%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22819.0 e-6; = (1/var)*||X-X_r||^2 =  8642.4 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  14176.6 e-6 = 62.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  27887.1 e-6; = (1/var)*||X-X_r||^2 =  14395.2 e-6 = 51.6 %; (1+beta)*||Z_e-Z_q||^2 =  13492.0 e-6 = 48.4 %)
Min.  Avg. Train Loss across Mini-Batch =  22819.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  27887.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5068.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5752.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -684.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.95; perplexity/K = 21.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.46; perplexity/K = 20.49%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7379.3 e-6; = (1/var)*||X-X_r||^2 =  2487.6 e-6 = 33.7 %; (1+beta)*||Z_e-Z_q||^2 =  4891.7 e-6 = 66.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  10662.0 e-6; = (1/var)*||X-X_r||^2 =  5628.9 e-6 = 52.8 %; (1+beta)*||Z_e-Z_q||^2 =  5033.0 e-6 = 47.2 %)
Min.  Avg. Train Loss across Mini-Batch =  7379.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10662.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3282.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3141.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 141.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.70; perplexity/K = 22.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.45; perplexity/K = 21.27%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7261.4 e-6; = (1/var)*||X-X_r||^2 =  2407.3 e-6 = 33.2 %; (1+beta)*||Z_e-Z_q||^2 =  4854.1 e-6 = 66.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  9095.9 e-6; = (1/var)*||X-X_r||^2 =  4400.3 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  4695.6 e-6 = 51.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3654.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5954.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1834.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1993.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -158.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.77; perplexity/K = 22.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.73; perplexity/K = 22.94%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3511.0 e-6; = (1/var)*||X-X_r||^2 =  964.2 e-6 = 27.5 %; (1+beta)*||Z_e-Z_q||^2 =  2546.8 e-6 = 72.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5345.5 e-6; = (1/var)*||X-X_r||^2 =  2799.4 e-6 = 52.4 %; (1+beta)*||Z_e-Z_q||^2 =  2546.2 e-6 = 47.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2240.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4115.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1834.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1835.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.13; perplexity/K = 22.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.98; perplexity/K = 23.04%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3196.3 e-6; = (1/var)*||X-X_r||^2 =  771.1 e-6 = 24.1 %; (1+beta)*||Z_e-Z_q||^2 =  2425.2 e-6 = 75.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4496.9 e-6; = (1/var)*||X-X_r||^2 =  2107.7 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  2389.1 e-6 = 53.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1901.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3564.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1300.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1336.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.36; perplexity/K = 22.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.00; perplexity/K = 21.88%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1912.9 e-6; = (1/var)*||X-X_r||^2 =  454.6 e-6 = 23.8 %; (1+beta)*||Z_e-Z_q||^2 =  1458.4 e-6 = 76.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3031.0 e-6; = (1/var)*||X-X_r||^2 =  1532.5 e-6 = 50.6 %; (1+beta)*||Z_e-Z_q||^2 =  1498.5 e-6 = 49.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1676.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2895.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1118.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1077.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.25; perplexity/K = 21.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.54; perplexity/K = 22.48%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2259.7 e-6; = (1/var)*||X-X_r||^2 =  465.6 e-6 = 20.6 %; (1+beta)*||Z_e-Z_q||^2 =  1794.0 e-6 = 79.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3843.4 e-6; = (1/var)*||X-X_r||^2 =  2034.6 e-6 = 52.9 %; (1+beta)*||Z_e-Z_q||^2 =  1808.8 e-6 = 47.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1100.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2285.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1583.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1569.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.99; perplexity/K = 21.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.68; perplexity/K = 20.19%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1477.9 e-6; = (1/var)*||X-X_r||^2 =  287.7 e-6 = 19.5 %; (1+beta)*||Z_e-Z_q||^2 =  1190.2 e-6 = 80.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2444.9 e-6; = (1/var)*||X-X_r||^2 =  1217.8 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  1227.1 e-6 = 50.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1060.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2000.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   967.0 e-6; = (1/var)*||X-X_r||^2 val-train = 930.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 36.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.36; perplexity/K = 21.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.13; perplexity/K = 20.75%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1251.6 e-6; = (1/var)*||X-X_r||^2 =  232.9 e-6 = 18.6 %; (1+beta)*||Z_e-Z_q||^2 =  1018.8 e-6 = 81.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2279.6 e-6; = (1/var)*||X-X_r||^2 =  1219.5 e-6 = 53.5 %; (1+beta)*||Z_e-Z_q||^2 =  1060.1 e-6 = 46.5 %)
Min.  Avg. Train Loss across Mini-Batch =  939.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1961.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1028.0 e-6; = (1/var)*||X-X_r||^2 val-train = 986.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 41.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 53.10; perplexity/K = 20.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.69; perplexity/K = 20.19%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:12:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1607.2 e-6; = (1/var)*||X-X_r||^2 =  254.2 e-6 = 15.8 %; (1+beta)*||Z_e-Z_q||^2 =  1353.0 e-6 = 84.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2467.6 e-6; = (1/var)*||X-X_r||^2 =  1105.6 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  1362.0 e-6 = 55.2 %)
Min.  Avg. Train Loss across Mini-Batch =  627.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1660.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   860.4 e-6; = (1/var)*||X-X_r||^2 val-train = 851.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.00; perplexity/K = 25.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.19; perplexity/K = 24.68%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:19:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2470.6 e-6; = (1/var)*||X-X_r||^2 =  1216.2 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  1254.4 e-6 = 50.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  15013.4 e-6; = (1/var)*||X-X_r||^2 =  9558.5 e-6 = 63.7 %; (1+beta)*||Z_e-Z_q||^2 =  5454.9 e-6 = 36.3 %)
Min.  Avg. Train Loss across Mini-Batch =  627.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1468.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12542.8 e-6; = (1/var)*||X-X_r||^2 val-train = 8342.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4200.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.54; perplexity/K = 19.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.49; perplexity/K = 18.94%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:25:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1017.9 e-6; = (1/var)*||X-X_r||^2 =  195.6 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  822.2 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1730.4 e-6; = (1/var)*||X-X_r||^2 =  866.3 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  864.1 e-6 = 49.9 %)
Min.  Avg. Train Loss across Mini-Batch =  483.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   712.5 e-6; = (1/var)*||X-X_r||^2 val-train = 670.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 41.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 76.33; perplexity/K = 29.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 74.77; perplexity/K = 29.21%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:32:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12292.1 e-6; = (1/var)*||X-X_r||^2 =  3132.1 e-6 = 25.5 %; (1+beta)*||Z_e-Z_q||^2 =  9160.1 e-6 = 74.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  10472.5 e-6; = (1/var)*||X-X_r||^2 =  4150.2 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  6322.3 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1819.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1018.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2837.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.81; perplexity/K = 20.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.55; perplexity/K = 19.75%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:38:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  844.6 e-6; = (1/var)*||X-X_r||^2 =  150.7 e-6 = 17.8 %; (1+beta)*||Z_e-Z_q||^2 =  693.8 e-6 = 82.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1963.9 e-6; = (1/var)*||X-X_r||^2 =  1240.2 e-6 = 63.1 %; (1+beta)*||Z_e-Z_q||^2 =  723.7 e-6 = 36.9 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1119.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1089.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 68.53; perplexity/K = 26.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 67.73; perplexity/K = 26.46%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:45:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2758.9 e-6; = (1/var)*||X-X_r||^2 =  530.0 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  2228.9 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3022.8 e-6; = (1/var)*||X-X_r||^2 =  1060.2 e-6 = 35.1 %; (1+beta)*||Z_e-Z_q||^2 =  1962.6 e-6 = 64.9 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   264.0 e-6; = (1/var)*||X-X_r||^2 val-train = 530.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -266.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.00; perplexity/K = 23.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.91; perplexity/K = 22.23%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:52:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1258.0 e-6; = (1/var)*||X-X_r||^2 =  154.3 e-6 = 12.3 %; (1+beta)*||Z_e-Z_q||^2 =  1103.6 e-6 = 87.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1751.3 e-6; = (1/var)*||X-X_r||^2 =  664.4 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  1086.9 e-6 = 62.1 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   493.3 e-6; = (1/var)*||X-X_r||^2 val-train = 510.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -16.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 61.68; perplexity/K = 24.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.77; perplexity/K = 23.74%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:58:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  854.8 e-6; = (1/var)*||X-X_r||^2 =  116.5 e-6 = 13.6 %; (1+beta)*||Z_e-Z_q||^2 =  738.3 e-6 = 86.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1609.1 e-6; = (1/var)*||X-X_r||^2 =  864.1 e-6 = 53.7 %; (1+beta)*||Z_e-Z_q||^2 =  744.9 e-6 = 46.3 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   754.3 e-6; = (1/var)*||X-X_r||^2 val-train = 747.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.61; perplexity/K = 21.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.95; perplexity/K = 20.69%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:5:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  697.7 e-6; = (1/var)*||X-X_r||^2 =  137.9 e-6 = 19.8 %; (1+beta)*||Z_e-Z_q||^2 =  559.8 e-6 = 80.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1387.8 e-6; = (1/var)*||X-X_r||^2 =  821.5 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  566.3 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  338.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1122.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   690.1 e-6; = (1/var)*||X-X_r||^2 val-train = 683.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 73.66; perplexity/K = 28.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 71.00; perplexity/K = 27.74%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:11:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2971.1 e-6; = (1/var)*||X-X_r||^2 =  668.8 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  2302.3 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5166.3 e-6; = (1/var)*||X-X_r||^2 =  3241.0 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  1925.3 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  205.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  997.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2195.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2572.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -377.0 e-6 

----------------------------------------------------------------------------------

Finished [12:39:45 04.01.2023] 256) Finished running for K = 256 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 128) change_channel_size_across_layers = False:
Total training time is = 0:3:35 h/m/s. 

--------------------------------------------------- 

Started [12:39:45 04.01.2023] 257) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 750 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.27
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.07
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.27
4                           encoder.sequential_convs.conv2d_5.weight                       131            17.47
5                                  encoder.pre_residual_stack.weight                       147            19.60
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.80
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.80
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
10                             encoder.channel_adjusting_conv.weight                         8             1.07
11                                                       VQ.E.weight                        16             2.13
12                             decoder.channel_adjusting_conv.weight                        73             9.73
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.80
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.80
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.47
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.27
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.07
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.78%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1002273.5 e-6; = (1/var)*||X-X_r||^2 =  1002192.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  80.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  965386.6 e-6; = (1/var)*||X-X_r||^2 =  965288.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  97.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1001569.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963985.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36886.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36904.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.78%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1001495.3 e-6; = (1/var)*||X-X_r||^2 =  1001494.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  967613.5 e-6; = (1/var)*||X-X_r||^2 =  967612.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1000339.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963150.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -33881.8 e-6; = (1/var)*||X-X_r||^2 val-train = -33881.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999521.1 e-6; = (1/var)*||X-X_r||^2 =  999521.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963227.9 e-6; = (1/var)*||X-X_r||^2 =  963227.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999463.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  963003.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36293.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36293.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999569.7 e-6; = (1/var)*||X-X_r||^2 =  999569.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964020.0 e-6; = (1/var)*||X-X_r||^2 =  964020.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999368.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962900.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35549.7 e-6; = (1/var)*||X-X_r||^2 val-train = -35549.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999524.8 e-6; = (1/var)*||X-X_r||^2 =  999524.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963126.5 e-6; = (1/var)*||X-X_r||^2 =  963126.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999368.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962865.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36398.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36398.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999692.0 e-6; = (1/var)*||X-X_r||^2 =  999692.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963100.5 e-6; = (1/var)*||X-X_r||^2 =  963100.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999343.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962842.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36591.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36591.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000198.5 e-6; = (1/var)*||X-X_r||^2 =  1000198.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  965639.1 e-6; = (1/var)*||X-X_r||^2 =  965639.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999340.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962842.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34559.4 e-6; = (1/var)*||X-X_r||^2 val-train = -34559.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999785.3 e-6; = (1/var)*||X-X_r||^2 =  999785.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962897.6 e-6; = (1/var)*||X-X_r||^2 =  962897.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999323.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962842.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36887.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36887.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999407.2 e-6; = (1/var)*||X-X_r||^2 =  999407.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962875.5 e-6; = (1/var)*||X-X_r||^2 =  962875.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999323.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962842.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36531.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36531.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999354.3 e-6; = (1/var)*||X-X_r||^2 =  999354.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963139.5 e-6; = (1/var)*||X-X_r||^2 =  963139.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999323.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36214.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36214.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999454.8 e-6; = (1/var)*||X-X_r||^2 =  999454.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962933.5 e-6; = (1/var)*||X-X_r||^2 =  962933.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999281.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36521.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36521.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999568.8 e-6; = (1/var)*||X-X_r||^2 =  999568.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963539.4 e-6; = (1/var)*||X-X_r||^2 =  963539.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999267.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36029.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36029.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999348.6 e-6; = (1/var)*||X-X_r||^2 =  999348.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962928.7 e-6; = (1/var)*||X-X_r||^2 =  962928.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999245.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36419.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36419.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999246.7 e-6; = (1/var)*||X-X_r||^2 =  999246.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962859.8 e-6; = (1/var)*||X-X_r||^2 =  962859.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999225.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36386.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36386.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999449.7 e-6; = (1/var)*||X-X_r||^2 =  999449.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963296.2 e-6; = (1/var)*||X-X_r||^2 =  963296.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999216.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36153.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36153.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999341.6 e-6; = (1/var)*||X-X_r||^2 =  999341.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963021.4 e-6; = (1/var)*||X-X_r||^2 =  963021.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999216.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36320.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36320.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999294.0 e-6; = (1/var)*||X-X_r||^2 =  999294.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963045.4 e-6; = (1/var)*||X-X_r||^2 =  963045.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999210.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36248.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36248.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999241.5 e-6; = (1/var)*||X-X_r||^2 =  999241.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962907.7 e-6; = (1/var)*||X-X_r||^2 =  962907.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999164.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36333.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36333.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999502.6 e-6; = (1/var)*||X-X_r||^2 =  999502.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963309.1 e-6; = (1/var)*||X-X_r||^2 =  963309.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999164.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36193.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36193.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999305.3 e-6; = (1/var)*||X-X_r||^2 =  999305.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962881.5 e-6; = (1/var)*||X-X_r||^2 =  962881.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999164.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962832.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36423.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36423.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [13:28:34 04.01.2023] 257) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = True:
Total training time is = 0:1:49 h/m/s. 

--------------------------------------------------- 

Started [13:28:34 04.01.2023] 258) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2482 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.32
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.29
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.28
4                           encoder.sequential_convs.conv2d_5.weight                       524            21.11
5                                  encoder.pre_residual_stack.weight                       589            23.73
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
10                             encoder.channel_adjusting_conv.weight                        16             0.64
11                                                       VQ.E.weight                        16             0.64
12                             decoder.channel_adjusting_conv.weight                       147             5.92
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.11
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.28
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.61; perplexity/K = 10.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.71; perplexity/K = 10.43%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  657330.6 e-6; = (1/var)*||X-X_r||^2 =  609150.9 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  48179.7 e-6 = 7.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  706153.8 e-6; = (1/var)*||X-X_r||^2 =  616533.7 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  89620.1 e-6 = 12.7 %)
Min.  Avg. Train Loss across Mini-Batch =  644691.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  607611.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   48823.3 e-6; = (1/var)*||X-X_r||^2 val-train = 7382.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 41440.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.88; perplexity/K = 13.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.53; perplexity/K = 13.49%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  527546.4 e-6; = (1/var)*||X-X_r||^2 =  507476.1 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  20070.3 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  511485.3 e-6; = (1/var)*||X-X_r||^2 =  498715.2 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  12770.1 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  525853.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511485.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16061.1 e-6; = (1/var)*||X-X_r||^2 val-train = -8760.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7300.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.19; perplexity/K = 16.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.91; perplexity/K = 17.15%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  465632.5 e-6; = (1/var)*||X-X_r||^2 =  451017.5 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  14615.0 e-6 = 3.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  476025.1 e-6; = (1/var)*||X-X_r||^2 =  462495.9 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  13529.2 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  459357.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  458945.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10392.6 e-6; = (1/var)*||X-X_r||^2 val-train = 11478.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1085.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.39; perplexity/K = 17.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.46; perplexity/K = 17.76%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  428916.0 e-6; = (1/var)*||X-X_r||^2 =  414436.3 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  14479.7 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  449441.5 e-6; = (1/var)*||X-X_r||^2 =  435379.0 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  14062.5 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  427487.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432106.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20525.5 e-6; = (1/var)*||X-X_r||^2 val-train = 20942.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -417.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.10; perplexity/K = 17.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.25; perplexity/K = 20.02%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  411045.6 e-6; = (1/var)*||X-X_r||^2 =  398703.6 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  12342.0 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  421868.8 e-6; = (1/var)*||X-X_r||^2 =  409149.4 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  12719.4 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  404292.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  416925.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10823.2 e-6; = (1/var)*||X-X_r||^2 val-train = 10445.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 377.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.89; perplexity/K = 17.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.34; perplexity/K = 18.10%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  280304.6 e-6; = (1/var)*||X-X_r||^2 =  263606.5 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  16698.1 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  296664.4 e-6; = (1/var)*||X-X_r||^2 =  279679.9 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  16984.5 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  277749.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  296664.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16359.8 e-6; = (1/var)*||X-X_r||^2 val-train = 16073.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 286.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.91; perplexity/K = 19.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.98; perplexity/K = 18.74%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  263824.9 e-6; = (1/var)*||X-X_r||^2 =  248386.8 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  15438.1 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  298119.9 e-6; = (1/var)*||X-X_r||^2 =  279095.0 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  19025.0 e-6 = 6.4 %)
Min.  Avg. Train Loss across Mini-Batch =  259200.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  287669.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34295.0 e-6; = (1/var)*||X-X_r||^2 val-train = 30708.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3586.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.52; perplexity/K = 18.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.49; perplexity/K = 17.77%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  250711.3 e-6; = (1/var)*||X-X_r||^2 =  235582.6 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  15128.8 e-6 = 6.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  284757.7 e-6; = (1/var)*||X-X_r||^2 =  267267.7 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  17490.0 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  249601.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  283518.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34046.4 e-6; = (1/var)*||X-X_r||^2 val-train = 31685.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2361.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.05; perplexity/K = 19.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.60; perplexity/K = 18.98%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  243087.9 e-6; = (1/var)*||X-X_r||^2 =  227760.8 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  15327.1 e-6 = 6.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  277075.2 e-6; = (1/var)*||X-X_r||^2 =  260158.8 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  16916.3 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  239555.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  275372.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33987.2 e-6; = (1/var)*||X-X_r||^2 val-train = 32398.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1589.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.37; perplexity/K = 20.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.57; perplexity/K = 18.97%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  234199.8 e-6; = (1/var)*||X-X_r||^2 =  219848.3 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  14351.4 e-6 = 6.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  272491.2 e-6; = (1/var)*||X-X_r||^2 =  255776.4 e-6 = 93.9 %; (1+beta)*||Z_e-Z_q||^2 =  16714.8 e-6 = 6.1 %)
Min.  Avg. Train Loss across Mini-Batch =  233645.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  270074.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38291.4 e-6; = (1/var)*||X-X_r||^2 val-train = 35928.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2363.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.35; perplexity/K = 19.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.84; perplexity/K = 20.64%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  229563.0 e-6; = (1/var)*||X-X_r||^2 =  216087.8 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  13475.2 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  268115.4 e-6; = (1/var)*||X-X_r||^2 =  252091.0 e-6 = 94.0 %; (1+beta)*||Z_e-Z_q||^2 =  16024.4 e-6 = 6.0 %)
Min.  Avg. Train Loss across Mini-Batch =  229458.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  265268.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38552.5 e-6; = (1/var)*||X-X_r||^2 val-train = 36003.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2549.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.78; perplexity/K = 20.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.74; perplexity/K = 20.60%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  105161.6 e-6; = (1/var)*||X-X_r||^2 =  83018.8 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  22142.8 e-6 = 21.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  157031.9 e-6; = (1/var)*||X-X_r||^2 =  133036.0 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  23996.0 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  105161.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151755.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51870.3 e-6; = (1/var)*||X-X_r||^2 val-train = 50017.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1853.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.29; perplexity/K = 20.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.42; perplexity/K = 21.26%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  106144.4 e-6; = (1/var)*||X-X_r||^2 =  82447.7 e-6 = 77.7 %; (1+beta)*||Z_e-Z_q||^2 =  23696.7 e-6 = 22.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  153123.6 e-6; = (1/var)*||X-X_r||^2 =  128425.5 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  24698.1 e-6 = 16.1 %)
Min.  Avg. Train Loss across Mini-Batch =  93192.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141519.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   46979.2 e-6; = (1/var)*||X-X_r||^2 val-train = 45977.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1001.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.12; perplexity/K = 20.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.59; perplexity/K = 20.15%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  89123.5 e-6; = (1/var)*||X-X_r||^2 =  69693.3 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  19430.2 e-6 = 21.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  143094.8 e-6; = (1/var)*||X-X_r||^2 =  121190.6 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  21904.1 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  87337.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  138231.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53971.2 e-6; = (1/var)*||X-X_r||^2 val-train = 51497.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2473.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.14; perplexity/K = 19.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.20; perplexity/K = 20.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  88244.6 e-6; = (1/var)*||X-X_r||^2 =  68130.1 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  20114.4 e-6 = 22.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  138351.4 e-6; = (1/var)*||X-X_r||^2 =  116194.4 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  22156.9 e-6 = 16.0 %)
Min.  Avg. Train Loss across Mini-Batch =  86214.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  135984.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50106.8 e-6; = (1/var)*||X-X_r||^2 val-train = 48064.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2042.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.26; perplexity/K = 19.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.70; perplexity/K = 20.19%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  83852.6 e-6; = (1/var)*||X-X_r||^2 =  65093.4 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  18759.2 e-6 = 22.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  136877.5 e-6; = (1/var)*||X-X_r||^2 =  115175.2 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  21702.3 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  82598.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  133604.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   53024.9 e-6; = (1/var)*||X-X_r||^2 val-train = 50081.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2943.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.08; perplexity/K = 19.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.12; perplexity/K = 22.31%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  125297.3 e-6; = (1/var)*||X-X_r||^2 =  93568.1 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  31729.2 e-6 = 25.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  172967.7 e-6; = (1/var)*||X-X_r||^2 =  140029.4 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  32938.3 e-6 = 19.0 %)
Min.  Avg. Train Loss across Mini-Batch =  79945.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  133604.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   47670.4 e-6; = (1/var)*||X-X_r||^2 val-train = 46461.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1209.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.21; perplexity/K = 20.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.52; perplexity/K = 19.34%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  80197.6 e-6; = (1/var)*||X-X_r||^2 =  60919.6 e-6 = 76.0 %; (1+beta)*||Z_e-Z_q||^2 =  19278.0 e-6 = 24.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  134546.3 e-6; = (1/var)*||X-X_r||^2 =  112608.5 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  21937.8 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  77222.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  132106.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54348.7 e-6; = (1/var)*||X-X_r||^2 val-train = 51688.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2659.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.65; perplexity/K = 22.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.53; perplexity/K = 21.69%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  84650.1 e-6; = (1/var)*||X-X_r||^2 =  66409.0 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  18241.1 e-6 = 21.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  136230.5 e-6; = (1/var)*||X-X_r||^2 =  116200.4 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  20030.0 e-6 = 14.7 %)
Min.  Avg. Train Loss across Mini-Batch =  75392.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  127874.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   51580.4 e-6; = (1/var)*||X-X_r||^2 val-train = 49791.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1788.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.36; perplexity/K = 21.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.84; perplexity/K = 19.86%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  125487.4 e-6; = (1/var)*||X-X_r||^2 =  97646.2 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  27841.1 e-6 = 22.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  203021.0 e-6; = (1/var)*||X-X_r||^2 =  170997.7 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  32023.3 e-6 = 15.8 %)
Min.  Avg. Train Loss across Mini-Batch =  72184.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  127874.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   77533.6 e-6; = (1/var)*||X-X_r||^2 val-train = 73351.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4182.1 e-6 

----------------------------------------------------------------------------------

Finished [14:17:29 04.01.2023] 258) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = True:
Total training time is = 0:1:54 h/m/s. 

--------------------------------------------------- 

Started [14:17:29 04.01.2023] 259) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2512 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.24
1                           encoder.sequential_convs.conv2d_2.weight                       262            10.43
2                           encoder.sequential_convs.conv2d_3.weight                       262            10.43
3                           encoder.sequential_convs.conv2d_4.weight                       262            10.43
4                           encoder.sequential_convs.conv2d_5.weight                       262            10.43
5                                  encoder.pre_residual_stack.weight                       147             5.85
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.43
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.43
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
10                             encoder.channel_adjusting_conv.weight                         8             0.32
11                                                       VQ.E.weight                        16             0.64
12                             decoder.channel_adjusting_conv.weight                        73             2.91
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.43
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.43
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262            10.43
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262            10.43
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262            10.43
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262            10.43
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.24

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.28; perplexity/K = 2.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.10; perplexity/K = 2.38%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  774770.8 e-6; = (1/var)*||X-X_r||^2 =  590294.1 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  184476.7 e-6 = 23.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  723551.1 e-6; = (1/var)*||X-X_r||^2 =  580833.4 e-6 = 80.3 %; (1+beta)*||Z_e-Z_q||^2 =  142717.7 e-6 = 19.7 %)
Min.  Avg. Train Loss across Mini-Batch =  766927.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  705975.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -51219.7 e-6; = (1/var)*||X-X_r||^2 val-train = -9460.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -41759.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.05; perplexity/K = 8.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.62; perplexity/K = 8.44%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  401803.1 e-6; = (1/var)*||X-X_r||^2 =  234672.2 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  167130.9 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  433157.1 e-6; = (1/var)*||X-X_r||^2 =  256066.7 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  177090.4 e-6 = 40.9 %)
Min.  Avg. Train Loss across Mini-Batch =  385706.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  394235.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31354.0 e-6; = (1/var)*||X-X_r||^2 val-train = 21394.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9959.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.17; perplexity/K = 12.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.52; perplexity/K = 12.31%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  245123.6 e-6; = (1/var)*||X-X_r||^2 =  124603.2 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  120520.4 e-6 = 49.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  277655.1 e-6; = (1/var)*||X-X_r||^2 =  155208.0 e-6 = 55.9 %; (1+beta)*||Z_e-Z_q||^2 =  122447.1 e-6 = 44.1 %)
Min.  Avg. Train Loss across Mini-Batch =  222154.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  264671.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32531.4 e-6; = (1/var)*||X-X_r||^2 val-train = 30604.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1926.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.89; perplexity/K = 13.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.12; perplexity/K = 13.72%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  157620.1 e-6; = (1/var)*||X-X_r||^2 =  70999.4 e-6 = 45.0 %; (1+beta)*||Z_e-Z_q||^2 =  86620.7 e-6 = 55.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  227913.1 e-6; = (1/var)*||X-X_r||^2 =  129487.7 e-6 = 56.8 %; (1+beta)*||Z_e-Z_q||^2 =  98425.4 e-6 = 43.2 %)
Min.  Avg. Train Loss across Mini-Batch =  140824.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  211663.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   70293.1 e-6; = (1/var)*||X-X_r||^2 val-train = 58488.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11804.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.09; perplexity/K = 13.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.14; perplexity/K = 13.34%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94008.8 e-6; = (1/var)*||X-X_r||^2 =  36947.2 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  57061.6 e-6 = 60.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  189825.6 e-6; = (1/var)*||X-X_r||^2 =  111838.0 e-6 = 58.9 %; (1+beta)*||Z_e-Z_q||^2 =  77987.6 e-6 = 41.1 %)
Min.  Avg. Train Loss across Mini-Batch =  90023.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186032.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   95816.8 e-6; = (1/var)*||X-X_r||^2 val-train = 74890.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20926.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.08; perplexity/K = 13.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.49; perplexity/K = 13.47%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60119.8 e-6; = (1/var)*||X-X_r||^2 =  23882.9 e-6 = 39.7 %; (1+beta)*||Z_e-Z_q||^2 =  36237.0 e-6 = 60.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  174889.4 e-6; = (1/var)*||X-X_r||^2 =  111869.7 e-6 = 64.0 %; (1+beta)*||Z_e-Z_q||^2 =  63019.7 e-6 = 36.0 %)
Min.  Avg. Train Loss across Mini-Batch =  50939.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  166834.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   114769.6 e-6; = (1/var)*||X-X_r||^2 val-train = 87986.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26782.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.51; perplexity/K = 13.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.57; perplexity/K = 13.11%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  35069.4 e-6; = (1/var)*||X-X_r||^2 =  12519.9 e-6 = 35.7 %; (1+beta)*||Z_e-Z_q||^2 =  22549.4 e-6 = 64.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  165192.0 e-6; = (1/var)*||X-X_r||^2 =  113529.2 e-6 = 68.7 %; (1+beta)*||Z_e-Z_q||^2 =  51662.9 e-6 = 31.3 %)
Min.  Avg. Train Loss across Mini-Batch =  34448.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  163187.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   130122.7 e-6; = (1/var)*||X-X_r||^2 val-train = 101009.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29113.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.60; perplexity/K = 13.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.32; perplexity/K = 13.41%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32638.1 e-6; = (1/var)*||X-X_r||^2 =  10580.5 e-6 = 32.4 %; (1+beta)*||Z_e-Z_q||^2 =  22057.5 e-6 = 67.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  157383.8 e-6; = (1/var)*||X-X_r||^2 =  107004.0 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  50379.7 e-6 = 32.0 %)
Min.  Avg. Train Loss across Mini-Batch =  22800.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157022.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   124745.7 e-6; = (1/var)*||X-X_r||^2 val-train = 96423.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28322.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.34; perplexity/K = 13.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.38; perplexity/K = 13.82%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59739.2 e-6; = (1/var)*||X-X_r||^2 =  25092.7 e-6 = 42.0 %; (1+beta)*||Z_e-Z_q||^2 =  34646.5 e-6 = 58.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  172800.3 e-6; = (1/var)*||X-X_r||^2 =  119815.9 e-6 = 69.3 %; (1+beta)*||Z_e-Z_q||^2 =  52984.4 e-6 = 30.7 %)
Min.  Avg. Train Loss across Mini-Batch =  16453.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151527.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   113061.1 e-6; = (1/var)*||X-X_r||^2 val-train = 94723.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18337.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.53; perplexity/K = 13.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.53; perplexity/K = 13.49%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12556.9 e-6; = (1/var)*||X-X_r||^2 =  5316.2 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  7240.6 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  145721.5 e-6; = (1/var)*||X-X_r||^2 =  109113.6 e-6 = 74.9 %; (1+beta)*||Z_e-Z_q||^2 =  36608.0 e-6 = 25.1 %)
Min.  Avg. Train Loss across Mini-Batch =  12220.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143647.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   133164.7 e-6; = (1/var)*||X-X_r||^2 val-train = 103797.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29367.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.28; perplexity/K = 13.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.36; perplexity/K = 13.03%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30294.6 e-6; = (1/var)*||X-X_r||^2 =  7575.5 e-6 = 25.0 %; (1+beta)*||Z_e-Z_q||^2 =  22719.1 e-6 = 75.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  152486.0 e-6; = (1/var)*||X-X_r||^2 =  107690.8 e-6 = 70.6 %; (1+beta)*||Z_e-Z_q||^2 =  44795.2 e-6 = 29.4 %)
Min.  Avg. Train Loss across Mini-Batch =  9814.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142521.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   122191.4 e-6; = (1/var)*||X-X_r||^2 val-train = 100115.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22076.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.84; perplexity/K = 13.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.75; perplexity/K = 13.58%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19848.4 e-6; = (1/var)*||X-X_r||^2 =  8150.5 e-6 = 41.1 %; (1+beta)*||Z_e-Z_q||^2 =  11697.9 e-6 = 58.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  172928.3 e-6; = (1/var)*||X-X_r||^2 =  130761.1 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  42167.2 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  8481.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  139277.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   153079.9 e-6; = (1/var)*||X-X_r||^2 val-train = 122610.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30469.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.40; perplexity/K = 13.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.50; perplexity/K = 13.48%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14019.9 e-6; = (1/var)*||X-X_r||^2 =  4238.8 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  9781.2 e-6 = 69.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  145059.8 e-6; = (1/var)*||X-X_r||^2 =  109173.5 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  35886.3 e-6 = 24.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7205.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  139277.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   131039.9 e-6; = (1/var)*||X-X_r||^2 val-train = 104934.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26105.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.29; perplexity/K = 12.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.24; perplexity/K = 12.98%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6962.8 e-6; = (1/var)*||X-X_r||^2 =  3028.3 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  3934.5 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  141894.7 e-6; = (1/var)*||X-X_r||^2 =  109515.2 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  32379.5 e-6 = 22.8 %)
Min.  Avg. Train Loss across Mini-Batch =  6808.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  139277.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   134931.9 e-6; = (1/var)*||X-X_r||^2 val-train = 106486.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28445.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.64; perplexity/K = 13.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.00; perplexity/K = 13.28%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  137954.5 e-6; = (1/var)*||X-X_r||^2 =  68588.6 e-6 = 49.7 %; (1+beta)*||Z_e-Z_q||^2 =  69365.9 e-6 = 50.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  195704.3 e-6; = (1/var)*||X-X_r||^2 =  128946.0 e-6 = 65.9 %; (1+beta)*||Z_e-Z_q||^2 =  66758.4 e-6 = 34.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5849.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  139277.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   57749.9 e-6; = (1/var)*||X-X_r||^2 val-train = 60357.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2607.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.86; perplexity/K = 13.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.45; perplexity/K = 13.46%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7182.4 e-6; = (1/var)*||X-X_r||^2 =  2539.3 e-6 = 35.4 %; (1+beta)*||Z_e-Z_q||^2 =  4643.1 e-6 = 64.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  139517.4 e-6; = (1/var)*||X-X_r||^2 =  107480.8 e-6 = 77.0 %; (1+beta)*||Z_e-Z_q||^2 =  32036.6 e-6 = 23.0 %)
Min.  Avg. Train Loss across Mini-Batch =  5398.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  137853.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   132335.0 e-6; = (1/var)*||X-X_r||^2 val-train = 104941.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27393.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.71; perplexity/K = 13.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.07; perplexity/K = 13.31%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4796.3 e-6; = (1/var)*||X-X_r||^2 =  2127.5 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  2668.8 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  141911.3 e-6; = (1/var)*||X-X_r||^2 =  111406.8 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  30504.4 e-6 = 21.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4692.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  136975.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137114.9 e-6; = (1/var)*||X-X_r||^2 val-train = 109279.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27835.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.25; perplexity/K = 13.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.44; perplexity/K = 13.84%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40296.8 e-6; = (1/var)*||X-X_r||^2 =  9945.1 e-6 = 24.7 %; (1+beta)*||Z_e-Z_q||^2 =  30351.7 e-6 = 75.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  143663.5 e-6; = (1/var)*||X-X_r||^2 =  97567.3 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  46096.2 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  4168.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  135999.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   103366.7 e-6; = (1/var)*||X-X_r||^2 val-train = 87622.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15744.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.63; perplexity/K = 13.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.68; perplexity/K = 13.16%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7026.9 e-6; = (1/var)*||X-X_r||^2 =  2247.6 e-6 = 32.0 %; (1+beta)*||Z_e-Z_q||^2 =  4779.3 e-6 = 68.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  137448.1 e-6; = (1/var)*||X-X_r||^2 =  105456.8 e-6 = 76.7 %; (1+beta)*||Z_e-Z_q||^2 =  31991.3 e-6 = 23.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3766.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  134737.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   130421.2 e-6; = (1/var)*||X-X_r||^2 val-train = 103209.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27212.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.28; perplexity/K = 13.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.50; perplexity/K = 13.09%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4217.2 e-6; = (1/var)*||X-X_r||^2 =  1803.5 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  2413.7 e-6 = 57.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  141321.5 e-6; = (1/var)*||X-X_r||^2 =  109918.6 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  31402.9 e-6 = 22.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3462.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  134737.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137104.3 e-6; = (1/var)*||X-X_r||^2 val-train = 108115.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28989.1 e-6 

----------------------------------------------------------------------------------

Finished [15:20:57 04.01.2023] 259) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 32) change_channel_size_across_layers = False:
Total training time is = 0:1:28 h/m/s. 

--------------------------------------------------- 

Started [15:20:57 04.01.2023] 260) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9500 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.13
1                           encoder.sequential_convs.conv2d_2.weight                      1048            11.03
2                           encoder.sequential_convs.conv2d_3.weight                      1048            11.03
3                           encoder.sequential_convs.conv2d_4.weight                      1048            11.03
4                           encoder.sequential_convs.conv2d_5.weight                      1048            11.03
5                                  encoder.pre_residual_stack.weight                       589             6.20
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                        16             0.17
12                             decoder.channel_adjusting_conv.weight                       147             1.55
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            11.03
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            11.03
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            11.03
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            11.03
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.16; perplexity/K = 4.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.56; perplexity/K = 4.91%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  656790.3 e-6; = (1/var)*||X-X_r||^2 =  337449.5 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  319340.8 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  577299.3 e-6; = (1/var)*||X-X_r||^2 =  334785.3 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  242514.0 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  604184.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  571249.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -79491.0 e-6; = (1/var)*||X-X_r||^2 val-train = -2664.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -76826.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.08; perplexity/K = 7.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.07; perplexity/K = 7.06%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  392454.2 e-6; = (1/var)*||X-X_r||^2 =  171360.5 e-6 = 43.7 %; (1+beta)*||Z_e-Z_q||^2 =  221093.7 e-6 = 56.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  428656.8 e-6; = (1/var)*||X-X_r||^2 =  219781.1 e-6 = 51.3 %; (1+beta)*||Z_e-Z_q||^2 =  208875.7 e-6 = 48.7 %)
Min.  Avg. Train Loss across Mini-Batch =  392454.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  413393.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36202.6 e-6; = (1/var)*||X-X_r||^2 val-train = 48420.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12218.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.98; perplexity/K = 9.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.90; perplexity/K = 9.34%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  191201.8 e-6; = (1/var)*||X-X_r||^2 =  67485.9 e-6 = 35.3 %; (1+beta)*||Z_e-Z_q||^2 =  123715.9 e-6 = 64.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  287212.3 e-6; = (1/var)*||X-X_r||^2 =  148006.8 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  139205.5 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  189157.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  287212.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   96010.5 e-6; = (1/var)*||X-X_r||^2 val-train = 80520.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15489.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.97; perplexity/K = 9.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.59; perplexity/K = 10.00%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  89869.9 e-6; = (1/var)*||X-X_r||^2 =  30044.0 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  59825.9 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  227612.6 e-6; = (1/var)*||X-X_r||^2 =  140028.8 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  87583.8 e-6 = 38.5 %)
Min.  Avg. Train Loss across Mini-Batch =  72023.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  220906.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137742.7 e-6; = (1/var)*||X-X_r||^2 val-train = 109984.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27757.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.73; perplexity/K = 10.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.85; perplexity/K = 10.10%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  65888.9 e-6; = (1/var)*||X-X_r||^2 =  18601.1 e-6 = 28.2 %; (1+beta)*||Z_e-Z_q||^2 =  47287.8 e-6 = 71.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  205870.8 e-6; = (1/var)*||X-X_r||^2 =  135814.3 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  70056.5 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  27284.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  195955.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   139981.9 e-6; = (1/var)*||X-X_r||^2 val-train = 117213.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22768.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.17; perplexity/K = 9.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.90; perplexity/K = 10.51%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13427.5 e-6; = (1/var)*||X-X_r||^2 =  4811.4 e-6 = 35.8 %; (1+beta)*||Z_e-Z_q||^2 =  8616.1 e-6 = 64.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  179615.0 e-6; = (1/var)*||X-X_r||^2 =  136852.6 e-6 = 76.2 %; (1+beta)*||Z_e-Z_q||^2 =  42762.4 e-6 = 23.8 %)
Min.  Avg. Train Loss across Mini-Batch =  13427.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  179314.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   166187.6 e-6; = (1/var)*||X-X_r||^2 val-train = 132041.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34146.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.96; perplexity/K = 10.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.07; perplexity/K = 10.18%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11823.9 e-6; = (1/var)*||X-X_r||^2 =  4514.6 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  7309.3 e-6 = 61.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  172753.1 e-6; = (1/var)*||X-X_r||^2 =  139143.0 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  33610.2 e-6 = 19.5 %)
Min.  Avg. Train Loss across Mini-Batch =  10866.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  170459.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   160929.2 e-6; = (1/var)*||X-X_r||^2 val-train = 134628.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26300.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.57; perplexity/K = 10.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.86; perplexity/K = 10.49%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24287.3 e-6; = (1/var)*||X-X_r||^2 =  7191.7 e-6 = 29.6 %; (1+beta)*||Z_e-Z_q||^2 =  17095.6 e-6 = 70.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  183365.3 e-6; = (1/var)*||X-X_r||^2 =  143132.6 e-6 = 78.1 %; (1+beta)*||Z_e-Z_q||^2 =  40232.6 e-6 = 21.9 %)
Min.  Avg. Train Loss across Mini-Batch =  5993.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  167532.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159077.9 e-6; = (1/var)*||X-X_r||^2 val-train = 135940.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23137.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.62; perplexity/K = 10.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.03; perplexity/K = 10.95%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55698.8 e-6; = (1/var)*||X-X_r||^2 =  17254.5 e-6 = 31.0 %; (1+beta)*||Z_e-Z_q||^2 =  38444.3 e-6 = 69.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  190874.1 e-6; = (1/var)*||X-X_r||^2 =  139069.5 e-6 = 72.9 %; (1+beta)*||Z_e-Z_q||^2 =  51804.6 e-6 = 27.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5993.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  167532.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   135175.3 e-6; = (1/var)*||X-X_r||^2 val-train = 121815.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13360.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.72; perplexity/K = 11.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.36; perplexity/K = 11.08%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10399.2 e-6; = (1/var)*||X-X_r||^2 =  3660.2 e-6 = 35.2 %; (1+beta)*||Z_e-Z_q||^2 =  6739.1 e-6 = 64.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  165060.0 e-6; = (1/var)*||X-X_r||^2 =  135211.6 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  29848.4 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5014.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  159434.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   154660.8 e-6; = (1/var)*||X-X_r||^2 val-train = 131551.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23109.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.68; perplexity/K = 10.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.56; perplexity/K = 11.55%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:13:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43445.1 e-6; = (1/var)*||X-X_r||^2 =  11572.5 e-6 = 26.6 %; (1+beta)*||Z_e-Z_q||^2 =  31872.6 e-6 = 73.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  180582.2 e-6; = (1/var)*||X-X_r||^2 =  135847.5 e-6 = 75.2 %; (1+beta)*||Z_e-Z_q||^2 =  44734.7 e-6 = 24.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4122.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137137.1 e-6; = (1/var)*||X-X_r||^2 val-train = 124275.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12862.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.25; perplexity/K = 11.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.12; perplexity/K = 10.60%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4329.7 e-6; = (1/var)*||X-X_r||^2 =  1374.9 e-6 = 31.8 %; (1+beta)*||Z_e-Z_q||^2 =  2954.8 e-6 = 68.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  164122.5 e-6; = (1/var)*||X-X_r||^2 =  138982.7 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  25139.8 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3373.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159792.8 e-6; = (1/var)*||X-X_r||^2 val-train = 137607.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22185.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.36; perplexity/K = 11.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.61; perplexity/K = 10.79%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29835.2 e-6; = (1/var)*||X-X_r||^2 =  6716.8 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  23118.4 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  181639.7 e-6; = (1/var)*||X-X_r||^2 =  143850.8 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  37788.9 e-6 = 20.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2866.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   151804.5 e-6; = (1/var)*||X-X_r||^2 val-train = 137134.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14670.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.53; perplexity/K = 10.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.22; perplexity/K = 10.63%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6139.3 e-6; = (1/var)*||X-X_r||^2 =  1542.3 e-6 = 25.1 %; (1+beta)*||Z_e-Z_q||^2 =  4597.0 e-6 = 74.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  163332.4 e-6; = (1/var)*||X-X_r||^2 =  139801.6 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  23530.8 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2866.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   157193.1 e-6; = (1/var)*||X-X_r||^2 val-train = 138259.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18933.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.80; perplexity/K = 10.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.58; perplexity/K = 11.56%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:40:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10147.5 e-6; = (1/var)*||X-X_r||^2 =  3722.3 e-6 = 36.7 %; (1+beta)*||Z_e-Z_q||^2 =  6425.1 e-6 = 63.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  174079.5 e-6; = (1/var)*||X-X_r||^2 =  142212.6 e-6 = 81.7 %; (1+beta)*||Z_e-Z_q||^2 =  31866.9 e-6 = 18.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2412.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   163932.1 e-6; = (1/var)*||X-X_r||^2 val-train = 138490.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25441.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.98; perplexity/K = 10.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.74; perplexity/K = 10.83%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4764.6 e-6; = (1/var)*||X-X_r||^2 =  1249.9 e-6 = 26.2 %; (1+beta)*||Z_e-Z_q||^2 =  3514.7 e-6 = 73.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  169751.9 e-6; = (1/var)*||X-X_r||^2 =  144984.8 e-6 = 85.4 %; (1+beta)*||Z_e-Z_q||^2 =  24767.0 e-6 = 14.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1971.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   164987.2 e-6; = (1/var)*||X-X_r||^2 val-train = 143734.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21252.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.22; perplexity/K = 11.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.33; perplexity/K = 11.07%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5837.4 e-6; = (1/var)*||X-X_r||^2 =  1392.3 e-6 = 23.9 %; (1+beta)*||Z_e-Z_q||^2 =  4445.1 e-6 = 76.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  164112.3 e-6; = (1/var)*||X-X_r||^2 =  141394.0 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  22718.3 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1923.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   158274.8 e-6; = (1/var)*||X-X_r||^2 val-train = 140001.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18273.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.88; perplexity/K = 11.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.34; perplexity/K = 11.46%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6713.2 e-6; = (1/var)*||X-X_r||^2 =  1429.4 e-6 = 21.3 %; (1+beta)*||Z_e-Z_q||^2 =  5283.8 e-6 = 78.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  169733.5 e-6; = (1/var)*||X-X_r||^2 =  144444.6 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  25289.0 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1505.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   163020.3 e-6; = (1/var)*||X-X_r||^2 val-train = 143015.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20005.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.72; perplexity/K = 10.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.69; perplexity/K = 10.42%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:7:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4636.4 e-6; = (1/var)*||X-X_r||^2 =  1833.3 e-6 = 39.5 %; (1+beta)*||Z_e-Z_q||^2 =  2803.1 e-6 = 60.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  161256.7 e-6; = (1/var)*||X-X_r||^2 =  139161.8 e-6 = 86.3 %; (1+beta)*||Z_e-Z_q||^2 =  22095.0 e-6 = 13.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1505.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155705.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   156620.3 e-6; = (1/var)*||X-X_r||^2 val-train = 137328.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19291.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.70; perplexity/K = 10.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.92; perplexity/K = 10.51%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1854.8 e-6; = (1/var)*||X-X_r||^2 =  689.0 e-6 = 37.1 %; (1+beta)*||Z_e-Z_q||^2 =  1165.8 e-6 = 62.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  158331.8 e-6; = (1/var)*||X-X_r||^2 =  138789.2 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  19542.6 e-6 = 12.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1505.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  155348.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   156477.0 e-6; = (1/var)*||X-X_r||^2 val-train = 138100.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18376.8 e-6 

----------------------------------------------------------------------------------

Finished [17:36:05 04.01.2023] 260) Finished running for K = 256 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 32) change_channel_size_across_layers = False:
Total training time is = 0:1:8 h/m/s. 

--------------------------------------------------- 

Started [17:36:05 04.01.2023] 261) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 8) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 750 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.27
3                           encoder.sequential_convs.conv2d_4.weight                         8             1.07
4                           encoder.sequential_convs.conv2d_5.weight                        32             4.27
5                           encoder.sequential_convs.conv2d_6.weight                       131            17.47
6                                  encoder.pre_residual_stack.weight                       147            19.60
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.80
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.80
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
11                             encoder.channel_adjusting_conv.weight                         8             1.07
12                                                       VQ.E.weight                        16             2.13
13                             decoder.channel_adjusting_conv.weight                        73             9.73
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.80
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.53
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.80
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.53
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.47
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.27
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.07
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.22; perplexity/K = 2.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.86; perplexity/K = 1.90%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1699507.8 e-6; = (1/var)*||X-X_r||^2 =  914172.8 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  785335.0 e-6 = 46.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1155930.2 e-6; = (1/var)*||X-X_r||^2 =  885236.4 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  270693.8 e-6 = 23.4 %)
Min.  Avg. Train Loss across Mini-Batch =  995960.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  950445.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -543577.6 e-6; = (1/var)*||X-X_r||^2 val-train = -28936.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -514641.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.96; perplexity/K = 4.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.45; perplexity/K = 4.47%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1311644.4 e-6; = (1/var)*||X-X_r||^2 =  709117.1 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  602527.3 e-6 = 45.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1179786.9 e-6; = (1/var)*||X-X_r||^2 =  691408.6 e-6 = 58.6 %; (1+beta)*||Z_e-Z_q||^2 =  488378.3 e-6 = 41.4 %)
Min.  Avg. Train Loss across Mini-Batch =  995960.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  950445.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -131857.5 e-6; = (1/var)*||X-X_r||^2 val-train = -17708.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -114149.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.46; perplexity/K = 4.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.38; perplexity/K = 4.44%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  891455.5 e-6; = (1/var)*||X-X_r||^2 =  656879.3 e-6 = 73.7 %; (1+beta)*||Z_e-Z_q||^2 =  234576.2 e-6 = 26.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  858686.9 e-6; = (1/var)*||X-X_r||^2 =  644030.5 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  214656.4 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  889838.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  799088.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -32768.6 e-6; = (1/var)*||X-X_r||^2 val-train = -12848.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19919.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.51; perplexity/K = 1.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.04; perplexity/K = 1.97%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  830486.1 e-6; = (1/var)*||X-X_r||^2 =  675072.1 e-6 = 81.3 %; (1+beta)*||Z_e-Z_q||^2 =  155414.0 e-6 = 18.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  821414.4 e-6; = (1/var)*||X-X_r||^2 =  660578.6 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  160835.8 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  826785.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  769788.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9071.7 e-6; = (1/var)*||X-X_r||^2 val-train = -14493.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5421.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.00; perplexity/K = 1.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.00; perplexity/K = 0.78%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  827095.5 e-6; = (1/var)*||X-X_r||^2 =  704319.5 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  122776.0 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  815471.6 e-6; = (1/var)*||X-X_r||^2 =  690592.1 e-6 = 84.7 %; (1+beta)*||Z_e-Z_q||^2 =  124879.5 e-6 = 15.3 %)
Min.  Avg. Train Loss across Mini-Batch =  815947.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  769788.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11623.8 e-6; = (1/var)*||X-X_r||^2 val-train = -13727.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2103.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.05; perplexity/K = 1.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.50; perplexity/K = 0.97%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  830269.3 e-6; = (1/var)*||X-X_r||^2 =  713839.1 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  116430.2 e-6 = 14.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  804492.2 e-6; = (1/var)*||X-X_r||^2 =  703750.0 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  100742.2 e-6 = 12.5 %)
Min.  Avg. Train Loss across Mini-Batch =  815947.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  769788.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25777.2 e-6; = (1/var)*||X-X_r||^2 val-train = -10089.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15688.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.95; perplexity/K = 1.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.99; perplexity/K = 2.34%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  796775.4 e-6; = (1/var)*||X-X_r||^2 =  681163.8 e-6 = 85.5 %; (1+beta)*||Z_e-Z_q||^2 =  115611.6 e-6 = 14.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  766761.2 e-6; = (1/var)*||X-X_r||^2 =  659727.0 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  107034.1 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  779707.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  746839.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30014.3 e-6; = (1/var)*||X-X_r||^2 val-train = -21436.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8577.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.08; perplexity/K = 1.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.86; perplexity/K = 1.51%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  807386.2 e-6; = (1/var)*||X-X_r||^2 =  691708.1 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  115678.1 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  792645.1 e-6; = (1/var)*||X-X_r||^2 =  688940.7 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  103704.4 e-6 = 13.1 %)
Min.  Avg. Train Loss across Mini-Batch =  779707.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  746839.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -14741.1 e-6; = (1/var)*||X-X_r||^2 val-train = -2767.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11973.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.13; perplexity/K = 4.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.86; perplexity/K = 3.07%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  791245.8 e-6; = (1/var)*||X-X_r||^2 =  647200.6 e-6 = 81.8 %; (1+beta)*||Z_e-Z_q||^2 =  144045.2 e-6 = 18.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  773595.5 e-6; = (1/var)*||X-X_r||^2 =  637921.2 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  135674.3 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  763366.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  741199.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17650.3 e-6; = (1/var)*||X-X_r||^2 val-train = -9279.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8370.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.82; perplexity/K = 1.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.04; perplexity/K = 1.19%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  766913.4 e-6; = (1/var)*||X-X_r||^2 =  677833.3 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  89080.1 e-6 = 11.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  747587.9 e-6; = (1/var)*||X-X_r||^2 =  660837.3 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  86750.7 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  763366.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  741199.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19325.5 e-6; = (1/var)*||X-X_r||^2 val-train = -16996.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2329.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.51; perplexity/K = 1.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.52; perplexity/K = 0.98%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  787676.6 e-6; = (1/var)*||X-X_r||^2 =  700304.0 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  87372.6 e-6 = 11.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  774203.7 e-6; = (1/var)*||X-X_r||^2 =  688964.0 e-6 = 89.0 %; (1+beta)*||Z_e-Z_q||^2 =  85239.7 e-6 = 11.0 %)
Min.  Avg. Train Loss across Mini-Batch =  763366.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  741199.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13472.9 e-6; = (1/var)*||X-X_r||^2 val-train = -11340.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2132.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.11; perplexity/K = 0.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.79; perplexity/K = 1.09%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  775489.7 e-6; = (1/var)*||X-X_r||^2 =  696332.4 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  79157.3 e-6 = 10.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  767345.4 e-6; = (1/var)*||X-X_r||^2 =  685594.1 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  81751.4 e-6 = 10.7 %)
Min.  Avg. Train Loss across Mini-Batch =  755054.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  731982.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -8144.3 e-6; = (1/var)*||X-X_r||^2 val-train = -10738.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2594.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.55; perplexity/K = 2.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.31; perplexity/K = 2.08%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  794680.0 e-6; = (1/var)*||X-X_r||^2 =  681053.1 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  113626.9 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  769077.3 e-6; = (1/var)*||X-X_r||^2 =  669883.3 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  99194.0 e-6 = 12.9 %)
Min.  Avg. Train Loss across Mini-Batch =  749453.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  729875.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25602.7 e-6; = (1/var)*||X-X_r||^2 val-train = -11169.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14432.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.43; perplexity/K = 2.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.25; perplexity/K = 1.66%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  744107.2 e-6; = (1/var)*||X-X_r||^2 =  651493.5 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  92613.7 e-6 = 12.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  733118.9 e-6; = (1/var)*||X-X_r||^2 =  633528.8 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  99590.1 e-6 = 13.6 %)
Min.  Avg. Train Loss across Mini-Batch =  739768.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  719955.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10988.3 e-6; = (1/var)*||X-X_r||^2 val-train = -17964.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6976.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.46; perplexity/K = 2.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.72; perplexity/K = 3.41%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  781919.6 e-6; = (1/var)*||X-X_r||^2 =  648427.7 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  133491.9 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  767385.7 e-6; = (1/var)*||X-X_r||^2 =  646671.8 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  120713.9 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  739768.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  719955.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -14533.9 e-6; = (1/var)*||X-X_r||^2 val-train = -1755.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12778.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.59; perplexity/K = 2.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.02; perplexity/K = 4.69%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  799977.5 e-6; = (1/var)*||X-X_r||^2 =  671507.3 e-6 = 83.9 %; (1+beta)*||Z_e-Z_q||^2 =  128470.2 e-6 = 16.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  780717.5 e-6; = (1/var)*||X-X_r||^2 =  652283.9 e-6 = 83.5 %; (1+beta)*||Z_e-Z_q||^2 =  128433.6 e-6 = 16.5 %)
Min.  Avg. Train Loss across Mini-Batch =  739768.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  719955.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -19260.0 e-6; = (1/var)*||X-X_r||^2 val-train = -19223.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.92; perplexity/K = 6.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.61; perplexity/K = 6.88%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  783054.3 e-6; = (1/var)*||X-X_r||^2 =  644837.1 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  138217.1 e-6 = 17.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  761528.3 e-6; = (1/var)*||X-X_r||^2 =  623813.6 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  137714.7 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  739768.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  719955.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21526.0 e-6; = (1/var)*||X-X_r||^2 val-train = -21023.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -502.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.36; perplexity/K = 6.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.24; perplexity/K = 5.56%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  725937.8 e-6; = (1/var)*||X-X_r||^2 =  636895.2 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  89042.6 e-6 = 12.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  703881.1 e-6; = (1/var)*||X-X_r||^2 =  616571.5 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  87309.6 e-6 = 12.4 %)
Min.  Avg. Train Loss across Mini-Batch =  701632.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  685609.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22056.8 e-6; = (1/var)*||X-X_r||^2 val-train = -20323.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1733.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.79; perplexity/K = 4.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.43; perplexity/K = 4.86%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  708868.0 e-6; = (1/var)*||X-X_r||^2 =  620410.2 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  88457.8 e-6 = 12.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  695209.5 e-6; = (1/var)*||X-X_r||^2 =  608188.0 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  87021.5 e-6 = 12.5 %)
Min.  Avg. Train Loss across Mini-Batch =  686808.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676709.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13658.5 e-6; = (1/var)*||X-X_r||^2 val-train = -12222.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1436.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.05; perplexity/K = 7.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.14; perplexity/K = 5.52%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  700787.8 e-6; = (1/var)*||X-X_r||^2 =  605401.4 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  95386.4 e-6 = 13.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  697381.8 e-6; = (1/var)*||X-X_r||^2 =  601064.0 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  96317.8 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  686808.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  676709.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3406.0 e-6; = (1/var)*||X-X_r||^2 val-train = -4337.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 931.4 e-6 

----------------------------------------------------------------------------------

Finished [18:26:00 04.01.2023] 261) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 8) change_channel_size_across_layers = True:
Total training time is = 0:0:54 h/m/s. 

--------------------------------------------------- 

Started [18:26:00 04.01.2023] 262) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 8) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2486 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.32
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.29
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.27
5                           encoder.sequential_convs.conv2d_6.weight                       524            21.08
6                                  encoder.pre_residual_stack.weight                       589            23.69
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
11                             encoder.channel_adjusting_conv.weight                        16             0.64
12                                                       VQ.E.weight                        16             0.64
13                             decoder.channel_adjusting_conv.weight                       147             5.91
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.94
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.94
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.08
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.27
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.31; perplexity/K = 1.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.58; perplexity/K = 1.79%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1990311.2 e-6; = (1/var)*||X-X_r||^2 =  912366.9 e-6 = 45.8 %; (1+beta)*||Z_e-Z_q||^2 =  1077944.3 e-6 = 54.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1607871.8 e-6; = (1/var)*||X-X_r||^2 =  877704.8 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  730167.0 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1001344.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  959364.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -382439.4 e-6; = (1/var)*||X-X_r||^2 val-train = -34662.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -347777.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.00; perplexity/K = 5.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 5.34%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1478207.2 e-6; = (1/var)*||X-X_r||^2 =  699362.7 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  778844.5 e-6 = 52.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1551105.0 e-6; = (1/var)*||X-X_r||^2 =  683357.5 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  867747.5 e-6 = 55.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1001344.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  959364.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   72897.8 e-6; = (1/var)*||X-X_r||^2 val-train = -16005.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 88903.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.29; perplexity/K = 2.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.26; perplexity/K = 1.66%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  928979.4 e-6; = (1/var)*||X-X_r||^2 =  673566.2 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  255413.3 e-6 = 27.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  848672.0 e-6; = (1/var)*||X-X_r||^2 =  668654.5 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  180017.5 e-6 = 21.2 %)
Min.  Avg. Train Loss across Mini-Batch =  928979.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  817748.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -80307.5 e-6; = (1/var)*||X-X_r||^2 val-train = -4911.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -75395.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.48; perplexity/K = 1.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.97; perplexity/K = 1.16%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  905644.9 e-6; = (1/var)*||X-X_r||^2 =  735043.4 e-6 = 81.2 %; (1+beta)*||Z_e-Z_q||^2 =  170601.5 e-6 = 18.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  902069.4 e-6; = (1/var)*||X-X_r||^2 =  739180.5 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  162888.9 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3575.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4137.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7712.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.83; perplexity/K = 0.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.59; perplexity/K = 0.62%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  911649.3 e-6; = (1/var)*||X-X_r||^2 =  813290.1 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  98359.1 e-6 = 10.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  921288.6 e-6; = (1/var)*||X-X_r||^2 =  798360.8 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  122927.8 e-6 = 13.3 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   9639.4 e-6; = (1/var)*||X-X_r||^2 val-train = -14929.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24568.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  997326.8 e-6; = (1/var)*||X-X_r||^2 =  996895.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  431.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  955676.1 e-6; = (1/var)*||X-X_r||^2 =  955165.1 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  511.0 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -41650.8 e-6; = (1/var)*||X-X_r||^2 val-train = -41730.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 79.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999385.1 e-6; = (1/var)*||X-X_r||^2 =  999349.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  35.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963118.9 e-6; = (1/var)*||X-X_r||^2 =  963078.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  40.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36266.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36270.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999306.2 e-6; = (1/var)*||X-X_r||^2 =  999302.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963167.5 e-6; = (1/var)*||X-X_r||^2 =  963164.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36138.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36138.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999461.3 e-6; = (1/var)*||X-X_r||^2 =  999456.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  5.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963258.0 e-6; = (1/var)*||X-X_r||^2 =  963257.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36203.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36199.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999417.7 e-6; = (1/var)*||X-X_r||^2 =  999416.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963506.3 e-6; = (1/var)*||X-X_r||^2 =  963505.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35911.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35911.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999315.2 e-6; = (1/var)*||X-X_r||^2 =  999314.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962942.5 e-6; = (1/var)*||X-X_r||^2 =  962942.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36372.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36372.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999405.1 e-6; = (1/var)*||X-X_r||^2 =  999404.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962962.5 e-6; = (1/var)*||X-X_r||^2 =  962961.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36442.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36442.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999279.0 e-6; = (1/var)*||X-X_r||^2 =  999278.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963279.0 e-6; = (1/var)*||X-X_r||^2 =  963278.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36000.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36000.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999264.5 e-6; = (1/var)*||X-X_r||^2 =  999264.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962923.2 e-6; = (1/var)*||X-X_r||^2 =  962923.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36341.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36341.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999325.7 e-6; = (1/var)*||X-X_r||^2 =  999325.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963122.8 e-6; = (1/var)*||X-X_r||^2 =  963122.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36202.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36202.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999257.3 e-6; = (1/var)*||X-X_r||^2 =  999257.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962923.8 e-6; = (1/var)*||X-X_r||^2 =  962923.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36333.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36333.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999186.9 e-6; = (1/var)*||X-X_r||^2 =  999186.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962987.1 e-6; = (1/var)*||X-X_r||^2 =  962987.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36199.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36199.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999315.8 e-6; = (1/var)*||X-X_r||^2 =  999315.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962899.2 e-6; = (1/var)*||X-X_r||^2 =  962899.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36416.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36416.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999181.0 e-6; = (1/var)*||X-X_r||^2 =  999180.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963086.9 e-6; = (1/var)*||X-X_r||^2 =  963086.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36094.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36093.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999193.7 e-6; = (1/var)*||X-X_r||^2 =  999193.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962976.6 e-6; = (1/var)*||X-X_r||^2 =  962976.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  848225.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  790517.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36217.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36217.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [19:16:25 04.01.2023] 262) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 8) change_channel_size_across_layers = True:
Total training time is = 0:0:24 h/m/s. 

--------------------------------------------------- 

Started [19:16:25 04.01.2023] 263) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 8) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 3036 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.20
1                           encoder.sequential_convs.conv2d_2.weight                       262             8.63
2                           encoder.sequential_convs.conv2d_3.weight                       262             8.63
3                           encoder.sequential_convs.conv2d_4.weight                       262             8.63
4                           encoder.sequential_convs.conv2d_5.weight                       262             8.63
5                           encoder.sequential_convs.conv2d_6.weight                       262             8.63
6                                  encoder.pre_residual_stack.weight                       147             4.84
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.19
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.19
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
11                             encoder.channel_adjusting_conv.weight                         8             0.26
12                                                       VQ.E.weight                        16             0.53
13                             decoder.channel_adjusting_conv.weight                        73             2.40
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.19
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.19
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
18                    decoder.sequential_trans_convs.conv2d_1.weight                       262             8.63
19                    decoder.sequential_trans_convs.conv2d_2.weight                       262             8.63
20                    decoder.sequential_trans_convs.conv2d_3.weight                       262             8.63
21                    decoder.sequential_trans_convs.conv2d_4.weight                       262             8.63
22                    decoder.sequential_trans_convs.conv2d_5.weight                       262             8.63
23                    decoder.sequential_trans_convs.conv2d_6.weight                         6             0.20

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.06; perplexity/K = 1.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.81; perplexity/K = 1.88%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1717892.4 e-6; = (1/var)*||X-X_r||^2 =  885404.9 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  832487.5 e-6 = 48.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1517978.9 e-6; = (1/var)*||X-X_r||^2 =  857193.4 e-6 = 56.5 %; (1+beta)*||Z_e-Z_q||^2 =  660785.5 e-6 = 43.5 %)
Min.  Avg. Train Loss across Mini-Batch =  991643.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  958482.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -199913.5 e-6; = (1/var)*||X-X_r||^2 val-train = -28211.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -171702.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.24; perplexity/K = 0.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.21; perplexity/K = 0.47%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  964337.5 e-6; = (1/var)*||X-X_r||^2 =  899410.7 e-6 = 93.3 %; (1+beta)*||Z_e-Z_q||^2 =  64926.8 e-6 = 6.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  909808.9 e-6; = (1/var)*||X-X_r||^2 =  885914.2 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  23894.6 e-6 = 2.6 %)
Min.  Avg. Train Loss across Mini-Batch =  964337.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  899258.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -54528.7 e-6; = (1/var)*||X-X_r||^2 val-train = -13496.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -41032.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  970983.0 e-6; = (1/var)*||X-X_r||^2 =  968820.3 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  2162.7 e-6 = 0.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  942470.2 e-6; = (1/var)*||X-X_r||^2 =  940986.2 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  1484.0 e-6 = 0.2 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -28512.8 e-6; = (1/var)*||X-X_r||^2 val-train = -27834.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -678.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999841.4 e-6; = (1/var)*||X-X_r||^2 =  999810.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  31.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  965139.4 e-6; = (1/var)*||X-X_r||^2 =  965113.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  26.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34702.0 e-6; = (1/var)*||X-X_r||^2 val-train = -34696.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999397.0 e-6; = (1/var)*||X-X_r||^2 =  999395.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962897.8 e-6; = (1/var)*||X-X_r||^2 =  962896.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.7 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36499.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36499.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999871.7 e-6; = (1/var)*||X-X_r||^2 =  999870.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964052.4 e-6; = (1/var)*||X-X_r||^2 =  964052.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35819.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35818.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999413.7 e-6; = (1/var)*||X-X_r||^2 =  999413.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963249.7 e-6; = (1/var)*||X-X_r||^2 =  963249.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36164.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36163.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999366.8 e-6; = (1/var)*||X-X_r||^2 =  999366.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963207.5 e-6; = (1/var)*||X-X_r||^2 =  963207.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36159.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36159.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999527.2 e-6; = (1/var)*||X-X_r||^2 =  999526.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963769.2 e-6; = (1/var)*||X-X_r||^2 =  963768.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35758.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35758.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999228.2 e-6; = (1/var)*||X-X_r||^2 =  999227.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963584.2 e-6; = (1/var)*||X-X_r||^2 =  963583.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35644.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35644.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999381.3 e-6; = (1/var)*||X-X_r||^2 =  999381.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963331.7 e-6; = (1/var)*||X-X_r||^2 =  963331.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36049.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36049.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999328.9 e-6; = (1/var)*||X-X_r||^2 =  999328.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963461.6 e-6; = (1/var)*||X-X_r||^2 =  963461.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35867.3 e-6; = (1/var)*||X-X_r||^2 val-train = -35867.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999258.1 e-6; = (1/var)*||X-X_r||^2 =  999257.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962877.5 e-6; = (1/var)*||X-X_r||^2 =  962877.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36380.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36379.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999371.4 e-6; = (1/var)*||X-X_r||^2 =  999371.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963010.0 e-6; = (1/var)*||X-X_r||^2 =  963009.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36361.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36361.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999375.7 e-6; = (1/var)*||X-X_r||^2 =  999375.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963503.1 e-6; = (1/var)*||X-X_r||^2 =  963503.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35872.6 e-6; = (1/var)*||X-X_r||^2 val-train = -35872.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999255.1 e-6; = (1/var)*||X-X_r||^2 =  999255.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962926.4 e-6; = (1/var)*||X-X_r||^2 =  962926.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36328.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36328.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999319.7 e-6; = (1/var)*||X-X_r||^2 =  999319.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963250.5 e-6; = (1/var)*||X-X_r||^2 =  963250.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36069.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36069.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:57:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999342.6 e-6; = (1/var)*||X-X_r||^2 =  999340.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962916.5 e-6; = (1/var)*||X-X_r||^2 =  962915.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36426.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36425.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999218.1 e-6; = (1/var)*||X-X_r||^2 =  999217.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962918.6 e-6; = (1/var)*||X-X_r||^2 =  962918.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36299.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36299.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999306.3 e-6; = (1/var)*||X-X_r||^2 =  999306.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962976.5 e-6; = (1/var)*||X-X_r||^2 =  962976.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  929984.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  874431.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36329.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36329.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [20:21:22 04.01.2023] 263) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 8) change_channel_size_across_layers = False:
Total training time is = 0:0:56 h/m/s. 

--------------------------------------------------- 

Started [20:21:22 04.01.2023] 264) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 8) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(256, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 11596 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.10
1                           encoder.sequential_convs.conv2d_2.weight                      1048             9.04
2                           encoder.sequential_convs.conv2d_3.weight                      1048             9.04
3                           encoder.sequential_convs.conv2d_4.weight                      1048             9.04
4                           encoder.sequential_convs.conv2d_5.weight                      1048             9.04
5                           encoder.sequential_convs.conv2d_6.weight                      1048             9.04
6                                  encoder.pre_residual_stack.weight                       589             5.08
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
11                             encoder.channel_adjusting_conv.weight                        16             0.14
12                                                       VQ.E.weight                        16             0.14
13                             decoder.channel_adjusting_conv.weight                       147             1.27
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
18                    decoder.sequential_trans_convs.conv2d_1.weight                      1048             9.04
19                    decoder.sequential_trans_convs.conv2d_2.weight                      1048             9.04
20                    decoder.sequential_trans_convs.conv2d_3.weight                      1048             9.04
21                    decoder.sequential_trans_convs.conv2d_4.weight                      1048             9.04
22                    decoder.sequential_trans_convs.conv2d_5.weight                      1048             9.04
23                    decoder.sequential_trans_convs.conv2d_6.weight                        12             0.10

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000256.0 e-6; = (1/var)*||X-X_r||^2 =  1000255.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963783.6 e-6; = (1/var)*||X-X_r||^2 =  963783.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999819.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962982.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36472.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36472.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000176.2 e-6; = (1/var)*||X-X_r||^2 =  1000176.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963253.9 e-6; = (1/var)*||X-X_r||^2 =  963253.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999596.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962901.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36922.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36922.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1000539.8 e-6; = (1/var)*||X-X_r||^2 =  1000539.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  966603.9 e-6; = (1/var)*||X-X_r||^2 =  966603.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999556.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962866.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -33935.9 e-6; = (1/var)*||X-X_r||^2 val-train = -33935.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999711.5 e-6; = (1/var)*||X-X_r||^2 =  999711.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963116.7 e-6; = (1/var)*||X-X_r||^2 =  963116.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999391.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962866.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36594.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36594.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999846.2 e-6; = (1/var)*||X-X_r||^2 =  999846.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963206.4 e-6; = (1/var)*||X-X_r||^2 =  963206.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999347.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962854.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36639.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36639.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999466.3 e-6; = (1/var)*||X-X_r||^2 =  999466.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962906.7 e-6; = (1/var)*||X-X_r||^2 =  962906.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962853.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36559.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36559.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999597.8 e-6; = (1/var)*||X-X_r||^2 =  999597.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962934.3 e-6; = (1/var)*||X-X_r||^2 =  962934.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999273.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962851.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36663.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36663.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999394.3 e-6; = (1/var)*||X-X_r||^2 =  999394.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962904.0 e-6; = (1/var)*||X-X_r||^2 =  962904.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999266.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962851.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36490.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36490.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999335.7 e-6; = (1/var)*||X-X_r||^2 =  999335.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962948.1 e-6; = (1/var)*||X-X_r||^2 =  962948.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999235.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962851.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36387.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36387.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999329.2 e-6; = (1/var)*||X-X_r||^2 =  999329.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963038.7 e-6; = (1/var)*||X-X_r||^2 =  963038.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999230.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962851.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36290.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36290.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999231.5 e-6; = (1/var)*||X-X_r||^2 =  999231.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962870.3 e-6; = (1/var)*||X-X_r||^2 =  962870.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999207.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36361.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36361.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999480.2 e-6; = (1/var)*||X-X_r||^2 =  999480.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962976.8 e-6; = (1/var)*||X-X_r||^2 =  962976.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999150.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36503.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36503.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999364.2 e-6; = (1/var)*||X-X_r||^2 =  999364.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  965048.4 e-6; = (1/var)*||X-X_r||^2 =  965048.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999150.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34315.7 e-6; = (1/var)*||X-X_r||^2 val-train = -34315.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:36:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999172.5 e-6; = (1/var)*||X-X_r||^2 =  999172.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962974.2 e-6; = (1/var)*||X-X_r||^2 =  962974.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999150.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36198.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36198.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:43:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999456.2 e-6; = (1/var)*||X-X_r||^2 =  999456.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963290.8 e-6; = (1/var)*||X-X_r||^2 =  963290.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999150.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36165.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36165.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999206.1 e-6; = (1/var)*||X-X_r||^2 =  999206.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962958.9 e-6; = (1/var)*||X-X_r||^2 =  962958.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999150.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36247.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36247.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999391.5 e-6; = (1/var)*||X-X_r||^2 =  999391.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964759.2 e-6; = (1/var)*||X-X_r||^2 =  964759.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999139.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -34632.4 e-6; = (1/var)*||X-X_r||^2 val-train = -34632.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999270.8 e-6; = (1/var)*||X-X_r||^2 =  999270.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963162.8 e-6; = (1/var)*||X-X_r||^2 =  963162.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999139.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36108.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36108.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999316.9 e-6; = (1/var)*||X-X_r||^2 =  999316.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963354.6 e-6; = (1/var)*||X-X_r||^2 =  963354.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999139.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35962.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35962.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.39%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:17:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999208.9 e-6; = (1/var)*||X-X_r||^2 =  999208.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963017.7 e-6; = (1/var)*||X-X_r||^2 =  963017.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  999139.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962836.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36191.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36191.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

Finished [22:39:26 04.01.2023] 264) Finished running for K = 256 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 8) change_channel_size_across_layers = False:
Total training time is = 0:0:3 h/m/s. 

--------------------------------------------------- 

Started [22:39:26 04.01.2023] 265) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 448) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 724 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.14
1                           encoder.sequential_convs.conv2d_2.weight                        32             4.42
2                           encoder.sequential_convs.conv2d_3.weight                       131            18.09
3                                  encoder.pre_residual_stack.weight                       147            20.30
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.97
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.97
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
8                              encoder.channel_adjusting_conv.weight                         8             1.10
9                                                        VQ.E.weight                         8             1.10
10                             decoder.channel_adjusting_conv.weight                        73            10.08
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.97
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.55
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.97
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.55
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.09
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.42
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.14

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.57; perplexity/K = 11.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.70; perplexity/K = 11.48%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  293939.9 e-6; = (1/var)*||X-X_r||^2 =  197646.3 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  96293.6 e-6 = 32.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  289659.8 e-6; = (1/var)*||X-X_r||^2 =  199254.0 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  90405.8 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  293939.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  289659.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4280.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1607.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5887.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.07; perplexity/K = 13.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.97; perplexity/K = 14.04%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  117814.9 e-6; = (1/var)*||X-X_r||^2 =  93552.3 e-6 = 79.4 %; (1+beta)*||Z_e-Z_q||^2 =  24262.6 e-6 = 20.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  123288.7 e-6; = (1/var)*||X-X_r||^2 =  100328.0 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  22960.8 e-6 = 18.6 %)
Min.  Avg. Train Loss across Mini-Batch =  117814.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  121542.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5473.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6775.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1301.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.86; perplexity/K = 11.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.03; perplexity/K = 10.96%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  82466.4 e-6; = (1/var)*||X-X_r||^2 =  62065.3 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  20401.1 e-6 = 24.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  84820.1 e-6; = (1/var)*||X-X_r||^2 =  65615.7 e-6 = 77.4 %; (1+beta)*||Z_e-Z_q||^2 =  19204.4 e-6 = 22.6 %)
Min.  Avg. Train Loss across Mini-Batch =  78129.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  81825.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2353.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3550.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1196.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.60; perplexity/K = 12.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.18; perplexity/K = 12.64%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42794.1 e-6; = (1/var)*||X-X_r||^2 =  31438.7 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  11355.4 e-6 = 26.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  45141.1 e-6; = (1/var)*||X-X_r||^2 =  33895.1 e-6 = 75.1 %; (1+beta)*||Z_e-Z_q||^2 =  11246.0 e-6 = 24.9 %)
Min.  Avg. Train Loss across Mini-Batch =  42794.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45141.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2347.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2456.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -109.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.80; perplexity/K = 10.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.53; perplexity/K = 10.57%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46331.0 e-6; = (1/var)*||X-X_r||^2 =  29844.8 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  16486.2 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  25832.2 e-6; = (1/var)*||X-X_r||^2 =  16990.6 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  8841.6 e-6 = 34.2 %)
Min.  Avg. Train Loss across Mini-Batch =  22260.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24421.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20498.8 e-6; = (1/var)*||X-X_r||^2 val-train = -12854.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7644.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.22; perplexity/K = 11.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.44; perplexity/K = 12.07%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12437.8 e-6; = (1/var)*||X-X_r||^2 =  6232.0 e-6 = 50.1 %; (1+beta)*||Z_e-Z_q||^2 =  6205.8 e-6 = 49.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  14029.2 e-6; = (1/var)*||X-X_r||^2 =  7829.5 e-6 = 55.8 %; (1+beta)*||Z_e-Z_q||^2 =  6199.7 e-6 = 44.2 %)
Min.  Avg. Train Loss across Mini-Batch =  12437.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14029.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1591.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1597.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.96; perplexity/K = 9.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.43; perplexity/K = 8.93%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  174301.3 e-6; = (1/var)*||X-X_r||^2 =  137390.4 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  36910.9 e-6 = 21.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  99269.2 e-6; = (1/var)*||X-X_r||^2 =  78350.9 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  20918.3 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5753.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6852.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -75032.1 e-6; = (1/var)*||X-X_r||^2 val-train = -59039.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15992.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.91; perplexity/K = 8.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.19; perplexity/K = 8.74%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5041.0 e-6; = (1/var)*||X-X_r||^2 =  2356.1 e-6 = 46.7 %; (1+beta)*||Z_e-Z_q||^2 =  2684.9 e-6 = 53.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5084.6 e-6; = (1/var)*||X-X_r||^2 =  2532.2 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  2552.4 e-6 = 50.2 %)
Min.  Avg. Train Loss across Mini-Batch =  4242.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5084.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43.6 e-6; = (1/var)*||X-X_r||^2 val-train = 176.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -132.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.22; perplexity/K = 12.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.34; perplexity/K = 11.98%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8589.1 e-6; = (1/var)*||X-X_r||^2 =  4991.3 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  3597.8 e-6 = 41.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  9940.6 e-6; = (1/var)*||X-X_r||^2 =  5882.6 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  4058.0 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3738.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4548.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1351.5 e-6; = (1/var)*||X-X_r||^2 val-train = 891.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 460.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.39; perplexity/K = 13.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.17; perplexity/K = 13.41%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6491.5 e-6; = (1/var)*||X-X_r||^2 =  3576.9 e-6 = 55.1 %; (1+beta)*||Z_e-Z_q||^2 =  2914.7 e-6 = 44.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  6929.6 e-6; = (1/var)*||X-X_r||^2 =  4169.9 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  2759.8 e-6 = 39.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3738.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4548.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   438.1 e-6; = (1/var)*||X-X_r||^2 val-train = 593.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -154.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.54; perplexity/K = 16.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.51; perplexity/K = 14.46%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  89079.6 e-6; = (1/var)*||X-X_r||^2 =  34012.8 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  55066.8 e-6 = 61.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  17060.0 e-6; = (1/var)*||X-X_r||^2 =  9346.5 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  7713.6 e-6 = 45.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3738.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4548.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -72019.5 e-6; = (1/var)*||X-X_r||^2 val-train = -24666.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -47353.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.55; perplexity/K = 13.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.74; perplexity/K = 13.86%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3412.3 e-6; = (1/var)*||X-X_r||^2 =  1984.4 e-6 = 58.2 %; (1+beta)*||Z_e-Z_q||^2 =  1427.9 e-6 = 41.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4250.2 e-6; = (1/var)*||X-X_r||^2 =  2676.8 e-6 = 63.0 %; (1+beta)*||Z_e-Z_q||^2 =  1573.4 e-6 = 37.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3396.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4222.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   837.9 e-6; = (1/var)*||X-X_r||^2 val-train = 692.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 145.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.01; perplexity/K = 14.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.40; perplexity/K = 15.16%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2970.3 e-6; = (1/var)*||X-X_r||^2 =  1625.2 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  1345.2 e-6 = 45.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3985.1 e-6; = (1/var)*||X-X_r||^2 =  2554.8 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  1430.3 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2905.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3689.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1014.8 e-6; = (1/var)*||X-X_r||^2 val-train = 929.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 85.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.39; perplexity/K = 12.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.11; perplexity/K = 11.02%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2500.5 e-6; = (1/var)*||X-X_r||^2 =  1527.7 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  972.8 e-6 = 38.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3470.0 e-6; = (1/var)*||X-X_r||^2 =  2386.8 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  1083.2 e-6 = 31.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2337.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3055.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   969.5 e-6; = (1/var)*||X-X_r||^2 val-train = 859.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 110.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.13; perplexity/K = 14.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.80; perplexity/K = 14.69%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2194.9 e-6; = (1/var)*||X-X_r||^2 =  1159.2 e-6 = 52.8 %; (1+beta)*||Z_e-Z_q||^2 =  1035.7 e-6 = 47.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2993.9 e-6; = (1/var)*||X-X_r||^2 =  1877.9 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  1116.0 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  2083.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2843.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   799.0 e-6; = (1/var)*||X-X_r||^2 val-train = 718.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 80.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.75; perplexity/K = 16.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.87; perplexity/K = 16.30%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  20350.3 e-6; = (1/var)*||X-X_r||^2 =  13572.2 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  6778.0 e-6 = 33.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  22044.7 e-6; = (1/var)*||X-X_r||^2 =  15001.9 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  7042.8 e-6 = 31.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2799.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1694.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1429.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 264.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.84; perplexity/K = 16.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.73; perplexity/K = 15.42%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6570.9 e-6; = (1/var)*||X-X_r||^2 =  4728.5 e-6 = 72.0 %; (1+beta)*||Z_e-Z_q||^2 =  1842.4 e-6 = 28.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  7478.4 e-6; = (1/var)*||X-X_r||^2 =  5593.1 e-6 = 74.8 %; (1+beta)*||Z_e-Z_q||^2 =  1885.3 e-6 = 25.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2799.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   907.6 e-6; = (1/var)*||X-X_r||^2 val-train = 864.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 43.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.90; perplexity/K = 17.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.80; perplexity/K = 16.25%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5488.3 e-6; = (1/var)*||X-X_r||^2 =  3327.9 e-6 = 60.6 %; (1+beta)*||Z_e-Z_q||^2 =  2160.4 e-6 = 39.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  6344.9 e-6; = (1/var)*||X-X_r||^2 =  4229.9 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  2115.1 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2799.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   856.6 e-6; = (1/var)*||X-X_r||^2 val-train = 902.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -45.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.72; perplexity/K = 13.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.23; perplexity/K = 12.68%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5006.5 e-6; = (1/var)*||X-X_r||^2 =  2958.8 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  2047.7 e-6 = 40.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  6058.5 e-6; = (1/var)*||X-X_r||^2 =  3995.7 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  2062.8 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2799.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1052.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1036.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.49; perplexity/K = 16.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.27; perplexity/K = 15.84%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4607.2 e-6; = (1/var)*||X-X_r||^2 =  2375.3 e-6 = 51.6 %; (1+beta)*||Z_e-Z_q||^2 =  2231.9 e-6 = 48.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  5464.2 e-6; = (1/var)*||X-X_r||^2 =  3296.7 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  2167.6 e-6 = 39.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2799.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   857.0 e-6; = (1/var)*||X-X_r||^2 val-train = 921.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -64.4 e-6 

----------------------------------------------------------------------------------

Finished [23:28:31 04.01.2023] 265) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 448) change_channel_size_across_layers = True:
Total training time is = 0:7:5 h/m/s. 

--------------------------------------------------- 

Started [00:43:54 05.01.2023] 266) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 448) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2400 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.46
2                           encoder.sequential_convs.conv2d_3.weight                       524            21.83
3                                  encoder.pre_residual_stack.weight                       589            24.54
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.04
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.04
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
8                              encoder.channel_adjusting_conv.weight                        16             0.67
9                                                        VQ.E.weight                         8             0.33
10                             decoder.channel_adjusting_conv.weight                       147             6.12
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.04
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.04
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.83
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.46
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.96; perplexity/K = 7.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.15; perplexity/K = 7.15%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  298570.9 e-6; = (1/var)*||X-X_r||^2 =  169616.3 e-6 = 56.8 %; (1+beta)*||Z_e-Z_q||^2 =  128954.6 e-6 = 43.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  368794.1 e-6; = (1/var)*||X-X_r||^2 =  201179.5 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  167614.5 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  298570.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  294403.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   70223.2 e-6; = (1/var)*||X-X_r||^2 val-train = 31563.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38659.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.89; perplexity/K = 10.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.64; perplexity/K = 10.66%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  277074.5 e-6; = (1/var)*||X-X_r||^2 =  188368.3 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  88706.2 e-6 = 32.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  263082.5 e-6; = (1/var)*||X-X_r||^2 =  187353.4 e-6 = 71.2 %; (1+beta)*||Z_e-Z_q||^2 =  75729.1 e-6 = 28.8 %)
Min.  Avg. Train Loss across Mini-Batch =  72252.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  76262.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -13992.0 e-6; = (1/var)*||X-X_r||^2 val-train = -1014.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12977.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.38; perplexity/K = 10.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.49; perplexity/K = 9.76%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47144.4 e-6; = (1/var)*||X-X_r||^2 =  32192.3 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  14952.1 e-6 = 31.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  50572.1 e-6; = (1/var)*||X-X_r||^2 =  35375.2 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  15196.9 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  47144.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  50572.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3427.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3182.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 244.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.42; perplexity/K = 10.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.68; perplexity/K = 10.69%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22934.1 e-6; = (1/var)*||X-X_r||^2 =  13800.1 e-6 = 60.2 %; (1+beta)*||Z_e-Z_q||^2 =  9134.0 e-6 = 39.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  26157.1 e-6; = (1/var)*||X-X_r||^2 =  16306.4 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  9850.7 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  22934.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26152.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3223.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2506.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 716.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.15; perplexity/K = 11.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.44; perplexity/K = 11.28%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12531.1 e-6; = (1/var)*||X-X_r||^2 =  6114.3 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  6416.8 e-6 = 51.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  14752.2 e-6; = (1/var)*||X-X_r||^2 =  7976.3 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  6776.0 e-6 = 45.9 %)
Min.  Avg. Train Loss across Mini-Batch =  12531.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14695.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2221.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1861.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 359.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.45; perplexity/K = 10.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.55; perplexity/K = 9.80%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8773.3 e-6; = (1/var)*||X-X_r||^2 =  3931.4 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  4841.8 e-6 = 55.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  11181.0 e-6; = (1/var)*||X-X_r||^2 =  5784.1 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  5396.8 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  8335.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9935.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2407.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1852.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 555.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.40; perplexity/K = 10.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.07; perplexity/K = 11.77%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9876.1 e-6; = (1/var)*||X-X_r||^2 =  5627.6 e-6 = 57.0 %; (1+beta)*||Z_e-Z_q||^2 =  4248.5 e-6 = 43.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  11263.9 e-6; = (1/var)*||X-X_r||^2 =  6987.5 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  4276.4 e-6 = 38.0 %)
Min.  Avg. Train Loss across Mini-Batch =  8335.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9935.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1387.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1359.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.32; perplexity/K = 15.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.33; perplexity/K = 16.66%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11416.3 e-6; = (1/var)*||X-X_r||^2 =  6115.3 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  5301.0 e-6 = 46.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  12540.3 e-6; = (1/var)*||X-X_r||^2 =  7280.9 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  5259.4 e-6 = 41.9 %)
Min.  Avg. Train Loss across Mini-Batch =  7797.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9389.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1124.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1165.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -41.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.22; perplexity/K = 15.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.84; perplexity/K = 14.72%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4982.0 e-6; = (1/var)*||X-X_r||^2 =  2781.5 e-6 = 55.8 %; (1+beta)*||Z_e-Z_q||^2 =  2200.5 e-6 = 44.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  6049.2 e-6; = (1/var)*||X-X_r||^2 =  3885.2 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  2164.0 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4628.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5642.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1067.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1103.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -36.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.37; perplexity/K = 14.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.24; perplexity/K = 13.47%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3637.0 e-6; = (1/var)*||X-X_r||^2 =  1797.8 e-6 = 49.4 %; (1+beta)*||Z_e-Z_q||^2 =  1839.2 e-6 = 50.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  4479.8 e-6; = (1/var)*||X-X_r||^2 =  2646.3 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  1833.5 e-6 = 40.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3244.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4147.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   842.8 e-6; = (1/var)*||X-X_r||^2 val-train = 848.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.41; perplexity/K = 12.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.46; perplexity/K = 12.08%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4483.3 e-6; = (1/var)*||X-X_r||^2 =  2466.5 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  2016.8 e-6 = 45.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3936.7 e-6; = (1/var)*||X-X_r||^2 =  2463.0 e-6 = 62.6 %; (1+beta)*||Z_e-Z_q||^2 =  1473.7 e-6 = 37.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2376.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3156.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -546.6 e-6; = (1/var)*||X-X_r||^2 val-train = -3.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -543.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.06; perplexity/K = 10.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.90; perplexity/K = 9.30%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2380.3 e-6; = (1/var)*||X-X_r||^2 =  1121.6 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  1258.6 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  2909.7 e-6; = (1/var)*||X-X_r||^2 =  1703.8 e-6 = 58.6 %; (1+beta)*||Z_e-Z_q||^2 =  1205.8 e-6 = 41.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2030.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2765.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   529.4 e-6; = (1/var)*||X-X_r||^2 val-train = 582.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -52.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.46; perplexity/K = 12.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.94; perplexity/K = 12.46%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1759.3 e-6; = (1/var)*||X-X_r||^2 =  847.1 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  912.1 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2496.8 e-6; = (1/var)*||X-X_r||^2 =  1500.5 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  996.2 e-6 = 39.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1759.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2491.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   737.5 e-6; = (1/var)*||X-X_r||^2 val-train = 653.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 84.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.94; perplexity/K = 10.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.61; perplexity/K = 9.85%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1719.1 e-6; = (1/var)*||X-X_r||^2 =  756.7 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  962.3 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2314.7 e-6; = (1/var)*||X-X_r||^2 =  1342.8 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  971.9 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1654.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2314.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   595.6 e-6; = (1/var)*||X-X_r||^2 val-train = 586.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.52; perplexity/K = 11.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.14; perplexity/K = 10.26%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3900.2 e-6; = (1/var)*||X-X_r||^2 =  2203.5 e-6 = 56.5 %; (1+beta)*||Z_e-Z_q||^2 =  1696.7 e-6 = 43.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2931.8 e-6; = (1/var)*||X-X_r||^2 =  1702.1 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  1229.7 e-6 = 41.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1442.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2123.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -968.4 e-6; = (1/var)*||X-X_r||^2 val-train = -501.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -467.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.37; perplexity/K = 11.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.91; perplexity/K = 10.87%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1916.5 e-6; = (1/var)*||X-X_r||^2 =  713.6 e-6 = 37.2 %; (1+beta)*||Z_e-Z_q||^2 =  1203.0 e-6 = 62.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2517.8 e-6; = (1/var)*||X-X_r||^2 =  1338.6 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  1179.2 e-6 = 46.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1402.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2075.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   601.3 e-6; = (1/var)*||X-X_r||^2 val-train = 625.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -23.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.15; perplexity/K = 11.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.17; perplexity/K = 11.07%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1373.9 e-6; = (1/var)*||X-X_r||^2 =  532.6 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  841.3 e-6 = 61.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2231.4 e-6; = (1/var)*||X-X_r||^2 =  1299.3 e-6 = 58.2 %; (1+beta)*||Z_e-Z_q||^2 =  932.0 e-6 = 41.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1352.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2003.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   857.5 e-6; = (1/var)*||X-X_r||^2 val-train = 766.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 90.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.78; perplexity/K = 13.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.41; perplexity/K = 12.04%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1316.2 e-6; = (1/var)*||X-X_r||^2 =  486.6 e-6 = 37.0 %; (1+beta)*||Z_e-Z_q||^2 =  829.7 e-6 = 63.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1988.5 e-6; = (1/var)*||X-X_r||^2 =  1097.5 e-6 = 55.2 %; (1+beta)*||Z_e-Z_q||^2 =  891.0 e-6 = 44.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1277.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1891.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   672.3 e-6; = (1/var)*||X-X_r||^2 val-train = 610.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 61.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.82; perplexity/K = 10.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.71; perplexity/K = 9.93%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1492.9 e-6; = (1/var)*||X-X_r||^2 =  572.6 e-6 = 38.4 %; (1+beta)*||Z_e-Z_q||^2 =  920.3 e-6 = 61.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2433.8 e-6; = (1/var)*||X-X_r||^2 =  1363.8 e-6 = 56.0 %; (1+beta)*||Z_e-Z_q||^2 =  1070.0 e-6 = 44.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1173.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1762.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   940.9 e-6; = (1/var)*||X-X_r||^2 val-train = 791.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 149.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.30; perplexity/K = 9.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.70; perplexity/K = 9.14%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1388.5 e-6; = (1/var)*||X-X_r||^2 =  466.4 e-6 = 33.6 %; (1+beta)*||Z_e-Z_q||^2 =  922.1 e-6 = 66.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1953.3 e-6; = (1/var)*||X-X_r||^2 =  1010.4 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  942.9 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1173.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1762.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   564.8 e-6; = (1/var)*||X-X_r||^2 val-train = 544.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.9 e-6 

----------------------------------------------------------------------------------

Finished [01:50:23 05.01.2023] 266) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 448) change_channel_size_across_layers = True:
Total training time is = 0:7:29 h/m/s. 

--------------------------------------------------- 

Started [01:50:23 05.01.2023] 267) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 448) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1456 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.41
1                           encoder.sequential_convs.conv2d_2.weight                       262            17.99
2                           encoder.sequential_convs.conv2d_3.weight                       262            17.99
3                                  encoder.pre_residual_stack.weight                       147            10.10
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.47
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.47
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
8                              encoder.channel_adjusting_conv.weight                         8             0.55
9                                                        VQ.E.weight                         8             0.55
10                             decoder.channel_adjusting_conv.weight                        73             5.01
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.47
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.27
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.47
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.27
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            17.99
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            17.99
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.41

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.63; perplexity/K = 11.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.53; perplexity/K = 10.57%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  203865.8 e-6; = (1/var)*||X-X_r||^2 =  119800.4 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  84065.4 e-6 = 41.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  203053.3 e-6; = (1/var)*||X-X_r||^2 =  125160.4 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  77892.9 e-6 = 38.4 %)
Min.  Avg. Train Loss across Mini-Batch =  203865.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  203053.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -812.5 e-6; = (1/var)*||X-X_r||^2 val-train = 5360.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6172.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.32; perplexity/K = 11.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.44; perplexity/K = 11.28%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72723.4 e-6; = (1/var)*||X-X_r||^2 =  44027.7 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  28695.8 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  75550.0 e-6; = (1/var)*||X-X_r||^2 =  48240.8 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  27309.1 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  72723.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  75550.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2826.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4213.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1386.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.54; perplexity/K = 9.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.18; perplexity/K = 9.52%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23169.1 e-6; = (1/var)*||X-X_r||^2 =  12315.2 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  10853.9 e-6 = 46.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  26087.6 e-6; = (1/var)*||X-X_r||^2 =  15042.6 e-6 = 57.7 %; (1+beta)*||Z_e-Z_q||^2 =  11045.0 e-6 = 42.3 %)
Min.  Avg. Train Loss across Mini-Batch =  23169.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26087.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2918.5 e-6; = (1/var)*||X-X_r||^2 val-train = 2727.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 191.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.24; perplexity/K = 9.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.94; perplexity/K = 9.33%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11720.3 e-6; = (1/var)*||X-X_r||^2 =  5606.9 e-6 = 47.8 %; (1+beta)*||Z_e-Z_q||^2 =  6113.5 e-6 = 52.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  14032.3 e-6; = (1/var)*||X-X_r||^2 =  7535.1 e-6 = 53.7 %; (1+beta)*||Z_e-Z_q||^2 =  6497.1 e-6 = 46.3 %)
Min.  Avg. Train Loss across Mini-Batch =  11720.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14023.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2311.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1928.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 383.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.12; perplexity/K = 7.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.35; perplexity/K = 7.30%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7733.3 e-6; = (1/var)*||X-X_r||^2 =  3240.9 e-6 = 41.9 %; (1+beta)*||Z_e-Z_q||^2 =  4492.4 e-6 = 58.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  9429.7 e-6; = (1/var)*||X-X_r||^2 =  4844.5 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  4585.1 e-6 = 48.6 %)
Min.  Avg. Train Loss across Mini-Batch =  7725.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9429.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1696.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1603.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 92.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.82; perplexity/K = 6.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.45; perplexity/K = 6.60%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5529.5 e-6; = (1/var)*||X-X_r||^2 =  2294.0 e-6 = 41.5 %; (1+beta)*||Z_e-Z_q||^2 =  3235.5 e-6 = 58.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  7325.6 e-6; = (1/var)*||X-X_r||^2 =  3816.9 e-6 = 52.1 %; (1+beta)*||Z_e-Z_q||^2 =  3508.7 e-6 = 47.9 %)
Min.  Avg. Train Loss across Mini-Batch =  5529.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7129.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1796.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1522.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 273.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.03; perplexity/K = 6.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.00; perplexity/K = 6.25%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7308.0 e-6; = (1/var)*||X-X_r||^2 =  2821.4 e-6 = 38.6 %; (1+beta)*||Z_e-Z_q||^2 =  4486.6 e-6 = 61.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  8267.7 e-6; = (1/var)*||X-X_r||^2 =  3914.4 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  4353.4 e-6 = 52.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4441.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6016.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   959.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1093.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -133.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.74; perplexity/K = 6.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.90; perplexity/K = 6.17%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3789.0 e-6; = (1/var)*||X-X_r||^2 =  1554.2 e-6 = 41.0 %; (1+beta)*||Z_e-Z_q||^2 =  2234.8 e-6 = 59.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4947.9 e-6; = (1/var)*||X-X_r||^2 =  2713.0 e-6 = 54.8 %; (1+beta)*||Z_e-Z_q||^2 =  2234.9 e-6 = 45.2 %)
Min.  Avg. Train Loss across Mini-Batch =  3596.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4856.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1159.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1158.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.87; perplexity/K = 5.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.65; perplexity/K = 5.19%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3680.1 e-6; = (1/var)*||X-X_r||^2 =  1348.0 e-6 = 36.6 %; (1+beta)*||Z_e-Z_q||^2 =  2332.1 e-6 = 63.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  4633.1 e-6; = (1/var)*||X-X_r||^2 =  2300.9 e-6 = 49.7 %; (1+beta)*||Z_e-Z_q||^2 =  2332.2 e-6 = 50.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3255.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4376.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   953.0 e-6; = (1/var)*||X-X_r||^2 val-train = 952.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.64; perplexity/K = 5.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.95; perplexity/K = 4.65%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2981.0 e-6; = (1/var)*||X-X_r||^2 =  1031.7 e-6 = 34.6 %; (1+beta)*||Z_e-Z_q||^2 =  1949.3 e-6 = 65.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3955.2 e-6; = (1/var)*||X-X_r||^2 =  1948.6 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  2006.6 e-6 = 50.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2809.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3853.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   974.2 e-6; = (1/var)*||X-X_r||^2 val-train = 916.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 57.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.59; perplexity/K = 5.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.13; perplexity/K = 4.79%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2853.1 e-6; = (1/var)*||X-X_r||^2 =  945.4 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  1907.7 e-6 = 66.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3861.9 e-6; = (1/var)*||X-X_r||^2 =  1815.5 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  2046.4 e-6 = 53.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2476.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3446.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1008.8 e-6; = (1/var)*||X-X_r||^2 val-train = 870.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 138.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.76; perplexity/K = 5.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.61; perplexity/K = 5.16%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2778.1 e-6; = (1/var)*||X-X_r||^2 =  860.9 e-6 = 31.0 %; (1+beta)*||Z_e-Z_q||^2 =  1917.2 e-6 = 69.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3650.4 e-6; = (1/var)*||X-X_r||^2 =  1718.6 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  1931.8 e-6 = 52.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2098.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3003.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   872.2 e-6; = (1/var)*||X-X_r||^2 val-train = 857.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.07; perplexity/K = 5.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.24; perplexity/K = 5.66%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2251.2 e-6; = (1/var)*||X-X_r||^2 =  670.7 e-6 = 29.8 %; (1+beta)*||Z_e-Z_q||^2 =  1580.5 e-6 = 70.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3173.3 e-6; = (1/var)*||X-X_r||^2 =  1529.6 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  1643.6 e-6 = 51.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1884.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2713.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   922.1 e-6; = (1/var)*||X-X_r||^2 val-train = 858.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.82; perplexity/K = 5.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.98; perplexity/K = 4.67%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2137.3 e-6; = (1/var)*||X-X_r||^2 =  713.5 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  1423.8 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2739.5 e-6; = (1/var)*||X-X_r||^2 =  1345.9 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  1393.6 e-6 = 50.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1623.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2327.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   602.2 e-6; = (1/var)*||X-X_r||^2 val-train = 632.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -30.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.18; perplexity/K = 6.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.98; perplexity/K = 5.45%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2839.7 e-6; = (1/var)*||X-X_r||^2 =  1253.8 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  1586.0 e-6 = 55.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  3282.8 e-6; = (1/var)*||X-X_r||^2 =  1589.7 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  1693.1 e-6 = 51.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1424.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2033.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   443.1 e-6; = (1/var)*||X-X_r||^2 val-train = 336.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 107.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.49; perplexity/K = 5.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.60; perplexity/K = 5.94%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1365.2 e-6; = (1/var)*||X-X_r||^2 =  474.1 e-6 = 34.7 %; (1+beta)*||Z_e-Z_q||^2 =  891.1 e-6 = 65.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1989.3 e-6; = (1/var)*||X-X_r||^2 =  1027.7 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  961.6 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1256.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1864.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   624.1 e-6; = (1/var)*||X-X_r||^2 val-train = 553.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 70.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.33; perplexity/K = 5.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.86; perplexity/K = 5.36%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1088.1 e-6; = (1/var)*||X-X_r||^2 =  385.8 e-6 = 35.5 %; (1+beta)*||Z_e-Z_q||^2 =  702.3 e-6 = 64.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1674.6 e-6; = (1/var)*||X-X_r||^2 =  978.7 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  695.9 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1088.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1669.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   586.5 e-6; = (1/var)*||X-X_r||^2 val-train = 592.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.97; perplexity/K = 6.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.29; perplexity/K = 6.47%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1327.8 e-6; = (1/var)*||X-X_r||^2 =  383.6 e-6 = 28.9 %; (1+beta)*||Z_e-Z_q||^2 =  944.2 e-6 = 71.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1983.8 e-6; = (1/var)*||X-X_r||^2 =  987.3 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  996.6 e-6 = 50.2 %)
Min.  Avg. Train Loss across Mini-Batch =  916.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1471.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   656.1 e-6; = (1/var)*||X-X_r||^2 val-train = 603.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 52.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.76; perplexity/K = 5.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.97; perplexity/K = 5.44%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3514.2 e-6; = (1/var)*||X-X_r||^2 =  1749.2 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  1765.0 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2304.3 e-6; = (1/var)*||X-X_r||^2 =  1277.0 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  1027.3 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  916.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1437.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1209.9 e-6; = (1/var)*||X-X_r||^2 val-train = -472.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -737.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.39; perplexity/K = 5.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.16; perplexity/K = 5.59%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  848.1 e-6; = (1/var)*||X-X_r||^2 =  309.7 e-6 = 36.5 %; (1+beta)*||Z_e-Z_q||^2 =  538.4 e-6 = 63.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1259.6 e-6; = (1/var)*||X-X_r||^2 =  752.4 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  507.2 e-6 = 40.3 %)
Min.  Avg. Train Loss across Mini-Batch =  803.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1221.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   411.5 e-6; = (1/var)*||X-X_r||^2 val-train = 442.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -31.2 e-6 

----------------------------------------------------------------------------------

Finished [02:57:10 05.01.2023] 267) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 448) change_channel_size_across_layers = False:
Total training time is = 0:7:46 h/m/s. 

--------------------------------------------------- 

Started [02:57:10 05.01.2023] 268) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 448) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5300 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.23
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.77
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.77
3                                  encoder.pre_residual_stack.weight                       589            11.11
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                         8             0.15
10                             decoder.channel_adjusting_conv.weight                       147             2.77
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.77
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.77
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.36; perplexity/K = 14.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.42; perplexity/K = 13.61%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  134676.6 e-6; = (1/var)*||X-X_r||^2 =  58644.0 e-6 = 43.5 %; (1+beta)*||Z_e-Z_q||^2 =  76032.6 e-6 = 56.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  138100.6 e-6; = (1/var)*||X-X_r||^2 =  66673.3 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  71427.4 e-6 = 51.7 %)
Min.  Avg. Train Loss across Mini-Batch =  134676.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  138100.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3424.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8029.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4605.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.75; perplexity/K = 9.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.92; perplexity/K = 8.53%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24029.1 e-6; = (1/var)*||X-X_r||^2 =  10143.4 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  13885.7 e-6 = 57.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  27818.2 e-6; = (1/var)*||X-X_r||^2 =  13904.6 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  13913.5 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  24029.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  27818.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3789.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3761.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.81; perplexity/K = 10.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.28; perplexity/K = 11.16%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9156.3 e-6; = (1/var)*||X-X_r||^2 =  3623.1 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  5533.2 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  10702.6 e-6; = (1/var)*||X-X_r||^2 =  5276.6 e-6 = 49.3 %; (1+beta)*||Z_e-Z_q||^2 =  5426.0 e-6 = 50.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7766.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10046.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1546.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1653.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -107.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.50; perplexity/K = 10.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.81; perplexity/K = 10.79%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3533.1 e-6; = (1/var)*||X-X_r||^2 =  1467.0 e-6 = 41.5 %; (1+beta)*||Z_e-Z_q||^2 =  2066.1 e-6 = 58.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  4962.6 e-6; = (1/var)*||X-X_r||^2 =  2843.1 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  2119.4 e-6 = 42.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3516.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4933.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1429.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1376.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 53.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.89; perplexity/K = 15.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.36; perplexity/K = 14.35%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3401.9 e-6; = (1/var)*||X-X_r||^2 =  2069.4 e-6 = 60.8 %; (1+beta)*||Z_e-Z_q||^2 =  1332.4 e-6 = 39.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  24273.8 e-6; = (1/var)*||X-X_r||^2 =  19915.3 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  4358.5 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1988.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2985.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20871.9 e-6; = (1/var)*||X-X_r||^2 val-train = 17845.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3026.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.86; perplexity/K = 10.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.64; perplexity/K = 9.09%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2053.8 e-6; = (1/var)*||X-X_r||^2 =  1148.6 e-6 = 55.9 %; (1+beta)*||Z_e-Z_q||^2 =  905.2 e-6 = 44.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2532.7 e-6; = (1/var)*||X-X_r||^2 =  1617.6 e-6 = 63.9 %; (1+beta)*||Z_e-Z_q||^2 =  915.1 e-6 = 36.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1408.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2222.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   478.9 e-6; = (1/var)*||X-X_r||^2 val-train = 469.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.66; perplexity/K = 11.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.11; perplexity/K = 11.02%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2092.3 e-6; = (1/var)*||X-X_r||^2 =  779.2 e-6 = 37.2 %; (1+beta)*||Z_e-Z_q||^2 =  1313.2 e-6 = 62.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2776.3 e-6; = (1/var)*||X-X_r||^2 =  1537.9 e-6 = 55.4 %; (1+beta)*||Z_e-Z_q||^2 =  1238.4 e-6 = 44.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1172.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1924.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   684.0 e-6; = (1/var)*||X-X_r||^2 val-train = 758.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -74.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.42; perplexity/K = 11.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 10.68%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2484.7 e-6; = (1/var)*||X-X_r||^2 =  943.9 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  1540.8 e-6 = 62.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3233.8 e-6; = (1/var)*||X-X_r||^2 =  1732.1 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  1501.7 e-6 = 46.4 %)
Min.  Avg. Train Loss across Mini-Batch =  756.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1367.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   749.1 e-6; = (1/var)*||X-X_r||^2 val-train = 788.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -39.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.93; perplexity/K = 12.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.50; perplexity/K = 12.11%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6342.7 e-6; = (1/var)*||X-X_r||^2 =  2458.4 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  3884.3 e-6 = 61.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  7632.4 e-6; = (1/var)*||X-X_r||^2 =  3912.6 e-6 = 51.3 %; (1+beta)*||Z_e-Z_q||^2 =  3719.8 e-6 = 48.7 %)
Min.  Avg. Train Loss across Mini-Batch =  756.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1337.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1289.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1454.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -164.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.76; perplexity/K = 10.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.15; perplexity/K = 10.28%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  964.4 e-6; = (1/var)*||X-X_r||^2 =  443.0 e-6 = 45.9 %; (1+beta)*||Z_e-Z_q||^2 =  521.4 e-6 = 54.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1473.5 e-6; = (1/var)*||X-X_r||^2 =  962.8 e-6 = 65.3 %; (1+beta)*||Z_e-Z_q||^2 =  510.7 e-6 = 34.7 %)
Min.  Avg. Train Loss across Mini-Batch =  756.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1337.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   509.1 e-6; = (1/var)*||X-X_r||^2 val-train = 519.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.41; perplexity/K = 8.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.85; perplexity/K = 9.26%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  631.6 e-6; = (1/var)*||X-X_r||^2 =  268.9 e-6 = 42.6 %; (1+beta)*||Z_e-Z_q||^2 =  362.7 e-6 = 57.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1048.6 e-6; = (1/var)*||X-X_r||^2 =  682.1 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  366.5 e-6 = 35.0 %)
Min.  Avg. Train Loss across Mini-Batch =  477.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  838.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   417.0 e-6; = (1/var)*||X-X_r||^2 val-train = 413.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.49; perplexity/K = 11.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.18; perplexity/K = 11.08%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  447.8 e-6; = (1/var)*||X-X_r||^2 =  190.6 e-6 = 42.6 %; (1+beta)*||Z_e-Z_q||^2 =  257.3 e-6 = 57.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  758.7 e-6; = (1/var)*||X-X_r||^2 =  499.6 e-6 = 65.9 %; (1+beta)*||Z_e-Z_q||^2 =  259.0 e-6 = 34.1 %)
Min.  Avg. Train Loss across Mini-Batch =  307.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  628.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   310.8 e-6; = (1/var)*||X-X_r||^2 val-train = 309.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.53; perplexity/K = 9.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.62; perplexity/K = 9.08%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  258.0 e-6; = (1/var)*||X-X_r||^2 =  126.6 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  131.4 e-6 = 50.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  587.4 e-6; = (1/var)*||X-X_r||^2 =  437.6 e-6 = 74.5 %; (1+beta)*||Z_e-Z_q||^2 =  149.8 e-6 = 25.5 %)
Min.  Avg. Train Loss across Mini-Batch =  242.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  520.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   329.4 e-6; = (1/var)*||X-X_r||^2 val-train = 311.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.21; perplexity/K = 9.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.42; perplexity/K = 8.92%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  152.2 e-6; = (1/var)*||X-X_r||^2 =  87.7 e-6 = 57.6 %; (1+beta)*||Z_e-Z_q||^2 =  64.5 e-6 = 42.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  388.3 e-6; = (1/var)*||X-X_r||^2 =  318.5 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  69.7 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  152.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  371.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   236.0 e-6; = (1/var)*||X-X_r||^2 val-train = 230.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.85; perplexity/K = 10.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.01; perplexity/K = 10.94%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  494.8 e-6; = (1/var)*||X-X_r||^2 =  209.6 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  285.3 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  661.8 e-6; = (1/var)*||X-X_r||^2 =  438.1 e-6 = 66.2 %; (1+beta)*||Z_e-Z_q||^2 =  223.7 e-6 = 33.8 %)
Min.  Avg. Train Loss across Mini-Batch =  138.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  360.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   167.0 e-6; = (1/var)*||X-X_r||^2 val-train = 228.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -61.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.28; perplexity/K = 9.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.65; perplexity/K = 9.10%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  200.5 e-6; = (1/var)*||X-X_r||^2 =  88.4 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  112.1 e-6 = 55.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  396.9 e-6; = (1/var)*||X-X_r||^2 =  283.7 e-6 = 71.5 %; (1+beta)*||Z_e-Z_q||^2 =  113.2 e-6 = 28.5 %)
Min.  Avg. Train Loss across Mini-Batch =  134.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  351.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   196.5 e-6; = (1/var)*||X-X_r||^2 val-train = 195.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.93; perplexity/K = 9.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.00; perplexity/K = 10.16%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  248.1 e-6; = (1/var)*||X-X_r||^2 =  111.4 e-6 = 44.9 %; (1+beta)*||Z_e-Z_q||^2 =  136.7 e-6 = 55.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  385.8 e-6; = (1/var)*||X-X_r||^2 =  257.5 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  128.3 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  105.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  273.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137.7 e-6; = (1/var)*||X-X_r||^2 val-train = 146.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.96; perplexity/K = 7.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.06; perplexity/K = 7.86%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  95.2 e-6; = (1/var)*||X-X_r||^2 =  51.4 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  43.8 e-6 = 46.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  257.0 e-6; = (1/var)*||X-X_r||^2 =  210.6 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  46.4 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  93.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  247.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   161.9 e-6; = (1/var)*||X-X_r||^2 val-train = 159.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.64; perplexity/K = 8.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.15; perplexity/K = 7.93%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  103.7 e-6; = (1/var)*||X-X_r||^2 =  80.1 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  23.6 e-6 = 22.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  214.2 e-6; = (1/var)*||X-X_r||^2 =  189.7 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  24.5 e-6 = 11.5 %)
Min.  Avg. Train Loss across Mini-Batch =  67.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   110.5 e-6; = (1/var)*||X-X_r||^2 val-train = 109.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.25; perplexity/K = 7.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.75; perplexity/K = 6.84%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  90.0 e-6; = (1/var)*||X-X_r||^2 =  58.0 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  32.0 e-6 = 35.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  203.0 e-6; = (1/var)*||X-X_r||^2 =  171.7 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  31.3 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  67.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  182.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   113.0 e-6; = (1/var)*||X-X_r||^2 val-train = 113.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

Finished [05:14:28 05.01.2023] 268) Finished running for K = 128 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 448) change_channel_size_across_layers = False:
Total training time is = 0:7:18 h/m/s. 

--------------------------------------------------- 

Started [05:14:28 05.01.2023] 269) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 112) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 738 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.08
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.34
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.75
4                                  encoder.pre_residual_stack.weight                       147            19.92
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
9                              encoder.channel_adjusting_conv.weight                         8             1.08
10                                                       VQ.E.weight                         8             1.08
11                             decoder.channel_adjusting_conv.weight                        73             9.89
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.75
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.34
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.08
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.18; perplexity/K = 5.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.93; perplexity/K = 5.42%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1686994.0 e-6; = (1/var)*||X-X_r||^2 =  581780.7 e-6 = 34.5 %; (1+beta)*||Z_e-Z_q||^2 =  1105213.2 e-6 = 65.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2198155.7 e-6; = (1/var)*||X-X_r||^2 =  575441.3 e-6 = 26.2 %; (1+beta)*||Z_e-Z_q||^2 =  1622714.4 e-6 = 73.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1542492.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1153964.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   511161.8 e-6; = (1/var)*||X-X_r||^2 val-train = -6339.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 517501.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.64; perplexity/K = 9.87%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.67; perplexity/K = 9.90%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  207019.1 e-6; = (1/var)*||X-X_r||^2 =  179297.0 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  27722.0 e-6 = 13.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  207636.7 e-6; = (1/var)*||X-X_r||^2 =  182960.4 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  24676.3 e-6 = 11.9 %)
Min.  Avg. Train Loss across Mini-Batch =  207019.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  207636.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   617.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3663.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3045.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 10.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.81; perplexity/K = 10.01%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  121483.8 e-6; = (1/var)*||X-X_r||^2 =  101745.0 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  19738.8 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  130414.5 e-6; = (1/var)*||X-X_r||^2 =  109181.9 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  21232.6 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  119618.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  123729.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8930.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7436.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1493.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.37; perplexity/K = 13.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.95; perplexity/K = 14.02%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  85010.4 e-6; = (1/var)*||X-X_r||^2 =  72103.1 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  12907.2 e-6 = 15.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  90288.4 e-6; = (1/var)*||X-X_r||^2 =  77591.1 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  12697.4 e-6 = 14.1 %)
Min.  Avg. Train Loss across Mini-Batch =  85010.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  90288.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5278.1 e-6; = (1/var)*||X-X_r||^2 val-train = 5487.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -209.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.99; perplexity/K = 15.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.66; perplexity/K = 15.36%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  65345.8 e-6; = (1/var)*||X-X_r||^2 =  56938.0 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  8407.8 e-6 = 12.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  71263.6 e-6; = (1/var)*||X-X_r||^2 =  62040.1 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  9223.5 e-6 = 12.9 %)
Min.  Avg. Train Loss across Mini-Batch =  65345.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71263.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5917.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5102.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 815.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.95; perplexity/K = 17.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.47; perplexity/K = 17.55%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55213.7 e-6; = (1/var)*||X-X_r||^2 =  49328.8 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  5884.9 e-6 = 10.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  60935.2 e-6; = (1/var)*||X-X_r||^2 =  54486.0 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  6449.2 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  55213.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  60692.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5721.6 e-6; = (1/var)*||X-X_r||^2 val-train = 5157.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 564.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.47; perplexity/K = 16.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.56; perplexity/K = 16.85%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  51744.3 e-6; = (1/var)*||X-X_r||^2 =  46307.5 e-6 = 89.5 %; (1+beta)*||Z_e-Z_q||^2 =  5436.8 e-6 = 10.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  56159.9 e-6; = (1/var)*||X-X_r||^2 =  50354.1 e-6 = 89.7 %; (1+beta)*||Z_e-Z_q||^2 =  5805.8 e-6 = 10.3 %)
Min.  Avg. Train Loss across Mini-Batch =  51541.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  56029.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4415.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4046.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 369.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.83; perplexity/K = 18.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.45; perplexity/K = 19.10%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47039.5 e-6; = (1/var)*||X-X_r||^2 =  43013.8 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  4025.7 e-6 = 8.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  52069.8 e-6; = (1/var)*||X-X_r||^2 =  47483.5 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  4586.3 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  47039.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51923.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5030.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4469.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 560.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.76; perplexity/K = 18.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.03; perplexity/K = 17.99%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  44351.6 e-6; = (1/var)*||X-X_r||^2 =  40802.1 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  3549.4 e-6 = 8.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  49202.1 e-6; = (1/var)*||X-X_r||^2 =  45011.6 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  4190.5 e-6 = 8.5 %)
Min.  Avg. Train Loss across Mini-Batch =  44351.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49202.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4850.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4209.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 641.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.54; perplexity/K = 19.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.24; perplexity/K = 20.50%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42233.9 e-6; = (1/var)*||X-X_r||^2 =  38980.2 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  3253.7 e-6 = 7.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  47180.9 e-6; = (1/var)*||X-X_r||^2 =  43344.9 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  3836.0 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  42233.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  47104.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4947.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4364.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 582.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.41; perplexity/K = 19.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.32; perplexity/K = 20.56%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40828.8 e-6; = (1/var)*||X-X_r||^2 =  38114.8 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  2714.0 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  45901.4 e-6; = (1/var)*||X-X_r||^2 =  42616.8 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  3284.6 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  40769.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45482.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5072.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4502.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 570.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.50; perplexity/K = 19.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.08; perplexity/K = 19.59%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  52603.3 e-6; = (1/var)*||X-X_r||^2 =  47886.4 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  4716.9 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  56203.5 e-6; = (1/var)*||X-X_r||^2 =  51421.5 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  4782.0 e-6 = 8.5 %)
Min.  Avg. Train Loss across Mini-Batch =  40000.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44855.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3600.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3535.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 65.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.02; perplexity/K = 20.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.85; perplexity/K = 20.98%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45714.8 e-6; = (1/var)*||X-X_r||^2 =  42128.4 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  3586.4 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  49633.7 e-6; = (1/var)*||X-X_r||^2 =  45799.1 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  3834.6 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  40000.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44855.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3918.9 e-6; = (1/var)*||X-X_r||^2 val-train = 3670.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 248.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.52; perplexity/K = 21.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.01; perplexity/K = 21.10%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43152.5 e-6; = (1/var)*||X-X_r||^2 =  40001.9 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  3150.6 e-6 = 7.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  47089.1 e-6; = (1/var)*||X-X_r||^2 =  43591.4 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  3497.7 e-6 = 7.4 %)
Min.  Avg. Train Loss across Mini-Batch =  40000.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44855.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3936.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3589.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 347.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.05; perplexity/K = 22.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.68; perplexity/K = 22.40%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42789.8 e-6; = (1/var)*||X-X_r||^2 =  39881.6 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  2908.2 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  49115.7 e-6; = (1/var)*||X-X_r||^2 =  45402.4 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  3713.3 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  40000.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44855.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6325.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5520.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 805.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.04; perplexity/K = 23.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.75; perplexity/K = 22.46%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41285.6 e-6; = (1/var)*||X-X_r||^2 =  38385.0 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  2900.6 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  45352.6 e-6; = (1/var)*||X-X_r||^2 =  42147.3 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  3205.4 e-6 = 7.1 %)
Min.  Avg. Train Loss across Mini-Batch =  40000.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44855.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4067.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3762.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 304.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.79; perplexity/K = 22.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.32; perplexity/K = 22.91%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  40008.7 e-6; = (1/var)*||X-X_r||^2 =  37208.7 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  2800.0 e-6 = 7.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  44233.9 e-6; = (1/var)*||X-X_r||^2 =  41064.3 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  3169.5 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  39898.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43796.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4225.2 e-6; = (1/var)*||X-X_r||^2 val-train = 3855.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 369.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.76; perplexity/K = 24.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.44; perplexity/K = 23.78%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41023.0 e-6; = (1/var)*||X-X_r||^2 =  37888.0 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  3135.1 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  46043.0 e-6; = (1/var)*||X-X_r||^2 =  42419.2 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  3623.8 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  38900.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  43359.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5019.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4531.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 488.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.60; perplexity/K = 24.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.96; perplexity/K = 24.97%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38573.7 e-6; = (1/var)*||X-X_r||^2 =  35963.6 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  2610.1 e-6 = 6.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  43001.2 e-6; = (1/var)*||X-X_r||^2 =  39984.7 e-6 = 93.0 %; (1+beta)*||Z_e-Z_q||^2 =  3016.6 e-6 = 7.0 %)
Min.  Avg. Train Loss across Mini-Batch =  38541.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42606.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4427.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4021.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 406.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.24; perplexity/K = 25.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.54; perplexity/K = 25.42%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  38148.6 e-6; = (1/var)*||X-X_r||^2 =  35646.5 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  2502.1 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  42136.3 e-6; = (1/var)*||X-X_r||^2 =  39285.2 e-6 = 93.2 %; (1+beta)*||Z_e-Z_q||^2 =  2851.1 e-6 = 6.8 %)
Min.  Avg. Train Loss across Mini-Batch =  38020.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42082.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3987.7 e-6; = (1/var)*||X-X_r||^2 val-train = 3638.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 349.1 e-6 

----------------------------------------------------------------------------------

Finished [06:03:08 05.01.2023] 269) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 112) change_channel_size_across_layers = True:
Total training time is = 0:3:39 h/m/s. 

--------------------------------------------------- 

Started [06:03:08 05.01.2023] 270) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 112) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2460 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.30
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.33
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.30
4                                  encoder.pre_residual_stack.weight                       589            23.94
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         8             0.33
11                             decoder.channel_adjusting_conv.weight                       147             5.98
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.30
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.33
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.89; perplexity/K = 10.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.66; perplexity/K = 9.89%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  318133.8 e-6; = (1/var)*||X-X_r||^2 =  236236.2 e-6 = 74.3 %; (1+beta)*||Z_e-Z_q||^2 =  81897.6 e-6 = 25.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  317108.9 e-6; = (1/var)*||X-X_r||^2 =  242519.3 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  74589.6 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  318133.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  317108.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1025.0 e-6; = (1/var)*||X-X_r||^2 val-train = 6283.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -7308.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.54; perplexity/K = 19.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.99; perplexity/K = 20.31%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  85028.6 e-6; = (1/var)*||X-X_r||^2 =  74339.1 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  10689.5 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  90959.5 e-6; = (1/var)*||X-X_r||^2 =  80650.1 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  10309.5 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  85028.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  90959.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5930.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6310.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -380.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.32; perplexity/K = 22.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.46; perplexity/K = 22.23%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43280.7 e-6; = (1/var)*||X-X_r||^2 =  38222.9 e-6 = 88.3 %; (1+beta)*||Z_e-Z_q||^2 =  5057.8 e-6 = 11.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  46938.3 e-6; = (1/var)*||X-X_r||^2 =  41911.1 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  5027.2 e-6 = 10.7 %)
Min.  Avg. Train Loss across Mini-Batch =  43280.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46938.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3657.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3688.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -30.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.53; perplexity/K = 26.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.94; perplexity/K = 27.30%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26513.5 e-6; = (1/var)*||X-X_r||^2 =  23262.8 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  3250.8 e-6 = 12.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  28242.2 e-6; = (1/var)*||X-X_r||^2 =  25103.2 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  3139.0 e-6 = 11.1 %)
Min.  Avg. Train Loss across Mini-Batch =  26513.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28242.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1728.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1840.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -111.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.27; perplexity/K = 29.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.21; perplexity/K = 29.07%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8828.6 e-6; = (1/var)*||X-X_r||^2 =  6867.7 e-6 = 77.8 %; (1+beta)*||Z_e-Z_q||^2 =  1960.9 e-6 = 22.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  10603.7 e-6; = (1/var)*||X-X_r||^2 =  8532.4 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  2071.3 e-6 = 19.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8828.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10603.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1775.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1664.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 110.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.87; perplexity/K = 28.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.37; perplexity/K = 28.41%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3999.9 e-6; = (1/var)*||X-X_r||^2 =  2790.9 e-6 = 69.8 %; (1+beta)*||Z_e-Z_q||^2 =  1209.0 e-6 = 30.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  5323.7 e-6; = (1/var)*||X-X_r||^2 =  4027.3 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  1296.4 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3999.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5323.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1323.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1236.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 87.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.24; perplexity/K = 31.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.36; perplexity/K = 31.53%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2829.0 e-6; = (1/var)*||X-X_r||^2 =  2012.5 e-6 = 71.1 %; (1+beta)*||Z_e-Z_q||^2 =  816.6 e-6 = 28.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3864.7 e-6; = (1/var)*||X-X_r||^2 =  2987.3 e-6 = 77.3 %; (1+beta)*||Z_e-Z_q||^2 =  877.4 e-6 = 22.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2635.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3730.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1035.7 e-6; = (1/var)*||X-X_r||^2 val-train = 974.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 60.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.78; perplexity/K = 31.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.93; perplexity/K = 30.41%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2403.1 e-6; = (1/var)*||X-X_r||^2 =  1441.6 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  961.4 e-6 = 40.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3237.5 e-6; = (1/var)*||X-X_r||^2 =  2220.4 e-6 = 68.6 %; (1+beta)*||Z_e-Z_q||^2 =  1017.2 e-6 = 31.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2115.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2978.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   834.4 e-6; = (1/var)*||X-X_r||^2 val-train = 778.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 55.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.62; perplexity/K = 31.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.25; perplexity/K = 30.66%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1527.6 e-6; = (1/var)*||X-X_r||^2 =  951.1 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  576.5 e-6 = 37.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2257.3 e-6; = (1/var)*||X-X_r||^2 =  1638.5 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  618.8 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1527.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2228.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   729.7 e-6; = (1/var)*||X-X_r||^2 val-train = 687.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 42.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.43; perplexity/K = 30.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.32; perplexity/K = 30.72%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1292.1 e-6; = (1/var)*||X-X_r||^2 =  783.1 e-6 = 60.6 %; (1+beta)*||Z_e-Z_q||^2 =  509.0 e-6 = 39.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2047.1 e-6; = (1/var)*||X-X_r||^2 =  1495.4 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  551.8 e-6 = 27.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1254.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1955.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   755.0 e-6; = (1/var)*||X-X_r||^2 val-train = 712.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 42.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.78; perplexity/K = 31.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.74; perplexity/K = 30.27%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1159.9 e-6; = (1/var)*||X-X_r||^2 =  692.0 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  467.8 e-6 = 40.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1766.6 e-6; = (1/var)*||X-X_r||^2 =  1284.4 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  482.2 e-6 = 27.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1158.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1766.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   606.7 e-6; = (1/var)*||X-X_r||^2 val-train = 592.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.02; perplexity/K = 29.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.41; perplexity/K = 28.45%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21648.4 e-6; = (1/var)*||X-X_r||^2 =  19538.0 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  2110.4 e-6 = 9.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  48910.5 e-6; = (1/var)*||X-X_r||^2 =  44361.5 e-6 = 90.7 %; (1+beta)*||Z_e-Z_q||^2 =  4549.0 e-6 = 9.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1002.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1549.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27262.1 e-6; = (1/var)*||X-X_r||^2 val-train = 24823.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2438.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.90; perplexity/K = 31.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.11; perplexity/K = 29.77%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  814.1 e-6; = (1/var)*||X-X_r||^2 =  501.2 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  312.9 e-6 = 38.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1428.5 e-6; = (1/var)*||X-X_r||^2 =  1077.0 e-6 = 75.4 %; (1+beta)*||Z_e-Z_q||^2 =  351.5 e-6 = 24.6 %)
Min.  Avg. Train Loss across Mini-Batch =  810.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1342.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   614.4 e-6; = (1/var)*||X-X_r||^2 val-train = 575.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.33; perplexity/K = 30.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.48; perplexity/K = 30.85%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  906.2 e-6; = (1/var)*||X-X_r||^2 =  590.5 e-6 = 65.2 %; (1+beta)*||Z_e-Z_q||^2 =  315.7 e-6 = 34.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1397.1 e-6; = (1/var)*||X-X_r||^2 =  1056.2 e-6 = 75.6 %; (1+beta)*||Z_e-Z_q||^2 =  340.8 e-6 = 24.4 %)
Min.  Avg. Train Loss across Mini-Batch =  738.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1248.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   490.9 e-6; = (1/var)*||X-X_r||^2 val-train = 465.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.71; perplexity/K = 32.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.03; perplexity/K = 31.27%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  685.4 e-6; = (1/var)*||X-X_r||^2 =  360.6 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  324.8 e-6 = 47.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1219.6 e-6; = (1/var)*||X-X_r||^2 =  883.8 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  335.7 e-6 = 27.5 %)
Min.  Avg. Train Loss across Mini-Batch =  647.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1164.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   534.2 e-6; = (1/var)*||X-X_r||^2 val-train = 523.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.68; perplexity/K = 31.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.85; perplexity/K = 31.91%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1291.8 e-6; = (1/var)*||X-X_r||^2 =  781.5 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  510.3 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1824.9 e-6; = (1/var)*||X-X_r||^2 =  1325.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  499.9 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  599.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1077.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   533.1 e-6; = (1/var)*||X-X_r||^2 val-train = 543.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.03; perplexity/K = 31.27%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.14; perplexity/K = 30.58%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  712.2 e-6; = (1/var)*||X-X_r||^2 =  478.4 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  233.8 e-6 = 32.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1163.7 e-6; = (1/var)*||X-X_r||^2 =  912.5 e-6 = 78.4 %; (1+beta)*||Z_e-Z_q||^2 =  251.2 e-6 = 21.6 %)
Min.  Avg. Train Loss across Mini-Batch =  566.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  996.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   451.6 e-6; = (1/var)*||X-X_r||^2 val-train = 434.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.24; perplexity/K = 32.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.71; perplexity/K = 31.80%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  565.0 e-6; = (1/var)*||X-X_r||^2 =  290.3 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  274.7 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1033.0 e-6; = (1/var)*||X-X_r||^2 =  754.6 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  278.4 e-6 = 27.0 %)
Min.  Avg. Train Loss across Mini-Batch =  522.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  938.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   468.0 e-6; = (1/var)*||X-X_r||^2 val-train = 464.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.92; perplexity/K = 31.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.60; perplexity/K = 31.72%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  505.1 e-6; = (1/var)*||X-X_r||^2 =  313.0 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  192.1 e-6 = 38.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  999.9 e-6; = (1/var)*||X-X_r||^2 =  787.0 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  212.9 e-6 = 21.3 %)
Min.  Avg. Train Loss across Mini-Batch =  460.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  885.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   494.9 e-6; = (1/var)*||X-X_r||^2 val-train = 474.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.40; perplexity/K = 31.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.12; perplexity/K = 30.56%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  503.2 e-6; = (1/var)*||X-X_r||^2 =  250.5 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  252.7 e-6 = 50.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  908.4 e-6; = (1/var)*||X-X_r||^2 =  653.1 e-6 = 71.9 %; (1+beta)*||Z_e-Z_q||^2 =  255.3 e-6 = 28.1 %)
Min.  Avg. Train Loss across Mini-Batch =  444.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  853.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   405.3 e-6; = (1/var)*||X-X_r||^2 val-train = 402.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.6 e-6 

----------------------------------------------------------------------------------

Finished [06:53:06 05.01.2023] 270) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 112) change_channel_size_across_layers = True:
Total training time is = 0:3:57 h/m/s. 

--------------------------------------------------- 

Started [10:56:09 05.01.2023] 271) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 112) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1980 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.23
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.23
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.23
4                                  encoder.pre_residual_stack.weight                       147             7.42
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.40
10                                                       VQ.E.weight                         8             0.40
11                             decoder.channel_adjusting_conv.weight                        73             3.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.23
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.23
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.23
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.80; perplexity/K = 15.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.63; perplexity/K = 15.34%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  237389.5 e-6; = (1/var)*||X-X_r||^2 =  173361.1 e-6 = 73.0 %; (1+beta)*||Z_e-Z_q||^2 =  64028.3 e-6 = 27.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  247487.2 e-6; = (1/var)*||X-X_r||^2 =  184948.5 e-6 = 74.7 %; (1+beta)*||Z_e-Z_q||^2 =  62538.7 e-6 = 25.3 %)
Min.  Avg. Train Loss across Mini-Batch =  237389.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  247487.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10097.7 e-6; = (1/var)*||X-X_r||^2 val-train = 11587.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1489.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.97; perplexity/K = 24.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.27; perplexity/K = 24.43%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53438.0 e-6; = (1/var)*||X-X_r||^2 =  44308.0 e-6 = 82.9 %; (1+beta)*||Z_e-Z_q||^2 =  9130.0 e-6 = 17.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  68344.7 e-6; = (1/var)*||X-X_r||^2 =  58924.8 e-6 = 86.2 %; (1+beta)*||Z_e-Z_q||^2 =  9420.0 e-6 = 13.8 %)
Min.  Avg. Train Loss across Mini-Batch =  53438.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  66659.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14906.8 e-6; = (1/var)*||X-X_r||^2 val-train = 14616.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 290.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.08; perplexity/K = 31.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.23; perplexity/K = 31.43%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18927.8 e-6; = (1/var)*||X-X_r||^2 =  16254.3 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  2673.5 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  24055.7 e-6; = (1/var)*||X-X_r||^2 =  21309.6 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  2746.1 e-6 = 11.4 %)
Min.  Avg. Train Loss across Mini-Batch =  18294.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24055.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5127.9 e-6; = (1/var)*||X-X_r||^2 val-train = 5055.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 72.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.64; perplexity/K = 38.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.41; perplexity/K = 37.04%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8234.2 e-6; = (1/var)*||X-X_r||^2 =  7004.8 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  1229.4 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  13105.7 e-6; = (1/var)*||X-X_r||^2 =  11702.1 e-6 = 89.3 %; (1+beta)*||Z_e-Z_q||^2 =  1403.6 e-6 = 10.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7905.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12296.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4871.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4697.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 174.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.10; perplexity/K = 40.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 51.86; perplexity/K = 40.52%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5009.2 e-6; = (1/var)*||X-X_r||^2 =  4270.6 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  738.6 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  8560.0 e-6; = (1/var)*||X-X_r||^2 =  7753.6 e-6 = 90.6 %; (1+beta)*||Z_e-Z_q||^2 =  806.4 e-6 = 9.4 %)
Min.  Avg. Train Loss across Mini-Batch =  4659.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7939.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3550.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3483.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 67.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.30; perplexity/K = 40.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.05; perplexity/K = 40.66%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3145.3 e-6; = (1/var)*||X-X_r||^2 =  2649.9 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  495.4 e-6 = 15.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  5581.5 e-6; = (1/var)*||X-X_r||^2 =  5075.5 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  506.0 e-6 = 9.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3050.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5549.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2436.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2425.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.45; perplexity/K = 44.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.28; perplexity/K = 43.97%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2483.0 e-6; = (1/var)*||X-X_r||^2 =  2114.1 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  368.9 e-6 = 14.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  4719.1 e-6; = (1/var)*||X-X_r||^2 =  4339.0 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  380.2 e-6 = 8.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2464.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4563.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2236.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2224.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.29; perplexity/K = 44.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.07; perplexity/K = 43.80%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2160.2 e-6; = (1/var)*||X-X_r||^2 =  1853.4 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  306.8 e-6 = 14.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4269.3 e-6; = (1/var)*||X-X_r||^2 =  3938.6 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  330.6 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1886.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3781.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2109.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2085.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.86; perplexity/K = 44.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.31; perplexity/K = 43.21%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2775.0 e-6; = (1/var)*||X-X_r||^2 =  2244.4 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  530.5 e-6 = 19.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  4099.6 e-6; = (1/var)*||X-X_r||^2 =  3609.0 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  490.6 e-6 = 12.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1538.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3091.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1324.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1364.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -40.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.48; perplexity/K = 44.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.19; perplexity/K = 44.68%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1344.3 e-6; = (1/var)*||X-X_r||^2 =  1111.9 e-6 = 82.7 %; (1+beta)*||Z_e-Z_q||^2 =  232.4 e-6 = 17.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2708.0 e-6; = (1/var)*||X-X_r||^2 =  2470.2 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  237.7 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1300.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2702.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1363.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1358.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.44; perplexity/K = 45.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 55.12; perplexity/K = 43.06%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1550.8 e-6; = (1/var)*||X-X_r||^2 =  1357.0 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  193.8 e-6 = 12.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  3189.8 e-6; = (1/var)*||X-X_r||^2 =  2991.5 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  198.2 e-6 = 6.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1126.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2381.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1639.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1634.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.26; perplexity/K = 44.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.62; perplexity/K = 45.80%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  956.9 e-6; = (1/var)*||X-X_r||^2 =  770.6 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  186.3 e-6 = 19.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2176.2 e-6; = (1/var)*||X-X_r||^2 =  1987.3 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  188.9 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  923.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2095.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1219.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1216.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.46; perplexity/K = 44.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.69; perplexity/K = 45.07%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1175.2 e-6; = (1/var)*||X-X_r||^2 =  1020.9 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  154.3 e-6 = 13.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2141.0 e-6; = (1/var)*||X-X_r||^2 =  1975.7 e-6 = 92.3 %; (1+beta)*||Z_e-Z_q||^2 =  165.4 e-6 = 7.7 %)
Min.  Avg. Train Loss across Mini-Batch =  815.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1899.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   965.9 e-6; = (1/var)*||X-X_r||^2 val-train = 954.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.23; perplexity/K = 44.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 56.62; perplexity/K = 44.24%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  805.8 e-6; = (1/var)*||X-X_r||^2 =  629.8 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  176.0 e-6 = 21.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1846.7 e-6; = (1/var)*||X-X_r||^2 =  1667.6 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  179.0 e-6 = 9.7 %)
Min.  Avg. Train Loss across Mini-Batch =  732.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1731.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1040.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1037.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 59.71; perplexity/K = 46.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.64; perplexity/K = 47.37%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  696.0 e-6; = (1/var)*||X-X_r||^2 =  571.0 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  125.0 e-6 = 18.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  1631.9 e-6; = (1/var)*||X-X_r||^2 =  1503.2 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  128.7 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  668.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1586.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   935.9 e-6; = (1/var)*||X-X_r||^2 val-train = 932.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.83; perplexity/K = 45.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.33; perplexity/K = 45.57%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  651.7 e-6; = (1/var)*||X-X_r||^2 =  515.9 e-6 = 79.2 %; (1+beta)*||Z_e-Z_q||^2 =  135.8 e-6 = 20.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1527.3 e-6; = (1/var)*||X-X_r||^2 =  1388.4 e-6 = 90.9 %; (1+beta)*||Z_e-Z_q||^2 =  138.9 e-6 = 9.1 %)
Min.  Avg. Train Loss across Mini-Batch =  633.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1521.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   875.6 e-6; = (1/var)*||X-X_r||^2 val-train = 872.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 57.96; perplexity/K = 45.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.48; perplexity/K = 42.56%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  867.7 e-6; = (1/var)*||X-X_r||^2 =  653.8 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  213.9 e-6 = 24.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1718.7 e-6; = (1/var)*||X-X_r||^2 =  1513.0 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  205.8 e-6 = 12.0 %)
Min.  Avg. Train Loss across Mini-Batch =  572.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1392.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   851.0 e-6; = (1/var)*||X-X_r||^2 val-train = 859.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.22; perplexity/K = 51.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 66.30; perplexity/K = 51.79%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  584.0 e-6; = (1/var)*||X-X_r||^2 =  493.1 e-6 = 84.4 %; (1+beta)*||Z_e-Z_q||^2 =  90.8 e-6 = 15.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1286.8 e-6; = (1/var)*||X-X_r||^2 =  1190.8 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  96.0 e-6 = 7.5 %)
Min.  Avg. Train Loss across Mini-Batch =  499.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1218.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   702.8 e-6; = (1/var)*||X-X_r||^2 val-train = 697.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.48; perplexity/K = 49.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 63.43; perplexity/K = 49.56%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  476.2 e-6; = (1/var)*||X-X_r||^2 =  377.8 e-6 = 79.3 %; (1+beta)*||Z_e-Z_q||^2 =  98.4 e-6 = 20.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1156.2 e-6; = (1/var)*||X-X_r||^2 =  1054.3 e-6 = 91.2 %; (1+beta)*||Z_e-Z_q||^2 =  101.9 e-6 = 8.8 %)
Min.  Avg. Train Loss across Mini-Batch =  465.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1144.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   680.0 e-6; = (1/var)*||X-X_r||^2 val-train = 676.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 60.11; perplexity/K = 46.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 58.35; perplexity/K = 45.59%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  466.8 e-6; = (1/var)*||X-X_r||^2 =  381.0 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  85.8 e-6 = 18.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1195.1 e-6; = (1/var)*||X-X_r||^2 =  1108.2 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  86.8 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  450.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1094.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   728.3 e-6; = (1/var)*||X-X_r||^2 val-train = 727.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.0 e-6 

----------------------------------------------------------------------------------

Finished [12:01:29 05.01.2023] 271) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 112) change_channel_size_across_layers = False:
Total training time is = 0:3:19 h/m/s. 

--------------------------------------------------- 

Started [12:01:29 05.01.2023] 272) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 112) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7396 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.17
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.17
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.17
4                                  encoder.pre_residual_stack.weight                       589             7.96
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         8             0.11
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.17
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.17
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.17
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.75; perplexity/K = 27.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.49; perplexity/K = 25.38%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  149618.2 e-6; = (1/var)*||X-X_r||^2 =  71894.0 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  77724.1 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  151180.9 e-6; = (1/var)*||X-X_r||^2 =  81599.5 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  69581.4 e-6 = 46.0 %)
Min.  Avg. Train Loss across Mini-Batch =  147650.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151180.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1562.8 e-6; = (1/var)*||X-X_r||^2 val-train = 9705.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8142.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.90; perplexity/K = 35.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.84; perplexity/K = 32.69%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25518.5 e-6; = (1/var)*||X-X_r||^2 =  10624.4 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  14894.2 e-6 = 58.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  30733.2 e-6; = (1/var)*||X-X_r||^2 =  16133.8 e-6 = 52.5 %; (1+beta)*||Z_e-Z_q||^2 =  14599.4 e-6 = 47.5 %)
Min.  Avg. Train Loss across Mini-Batch =  25320.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30165.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5214.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5509.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -294.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.18; perplexity/K = 40.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 50.67; perplexity/K = 39.58%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22573.8 e-6; = (1/var)*||X-X_r||^2 =  9054.3 e-6 = 40.1 %; (1+beta)*||Z_e-Z_q||^2 =  13519.5 e-6 = 59.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  27695.0 e-6; = (1/var)*||X-X_r||^2 =  14853.8 e-6 = 53.6 %; (1+beta)*||Z_e-Z_q||^2 =  12841.2 e-6 = 46.4 %)
Min.  Avg. Train Loss across Mini-Batch =  12704.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  15840.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5121.2 e-6; = (1/var)*||X-X_r||^2 val-train = 5799.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -678.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.79; perplexity/K = 32.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.42; perplexity/K = 34.70%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  60625.0 e-6; = (1/var)*||X-X_r||^2 =  32731.0 e-6 = 54.0 %; (1+beta)*||Z_e-Z_q||^2 =  27894.1 e-6 = 46.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  38419.3 e-6; = (1/var)*||X-X_r||^2 =  20572.6 e-6 = 53.5 %; (1+beta)*||Z_e-Z_q||^2 =  17846.7 e-6 = 46.5 %)
Min.  Avg. Train Loss across Mini-Batch =  6671.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9002.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -22205.7 e-6; = (1/var)*||X-X_r||^2 val-train = -12158.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -10047.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.06; perplexity/K = 33.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.49; perplexity/K = 34.76%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2944.0 e-6; = (1/var)*||X-X_r||^2 =  993.1 e-6 = 33.7 %; (1+beta)*||Z_e-Z_q||^2 =  1950.9 e-6 = 66.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  4019.4 e-6; = (1/var)*||X-X_r||^2 =  2068.4 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  1951.0 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2944.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4019.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1075.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1075.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.26; perplexity/K = 32.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.23; perplexity/K = 32.21%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2630.8 e-6; = (1/var)*||X-X_r||^2 =  842.8 e-6 = 32.0 %; (1+beta)*||Z_e-Z_q||^2 =  1788.0 e-6 = 68.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3997.2 e-6; = (1/var)*||X-X_r||^2 =  2103.7 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  1893.5 e-6 = 47.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2229.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3228.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1366.4 e-6; = (1/var)*||X-X_r||^2 val-train = 1260.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 105.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.36; perplexity/K = 36.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.95; perplexity/K = 35.90%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3304.2 e-6; = (1/var)*||X-X_r||^2 =  1099.4 e-6 = 33.3 %; (1+beta)*||Z_e-Z_q||^2 =  2204.7 e-6 = 66.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  3748.1 e-6; = (1/var)*||X-X_r||^2 =  1802.9 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  1945.1 e-6 = 51.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1638.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2602.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   443.9 e-6; = (1/var)*||X-X_r||^2 val-train = 703.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -259.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.54; perplexity/K = 36.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.23; perplexity/K = 33.78%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2774.2 e-6; = (1/var)*||X-X_r||^2 =  686.7 e-6 = 24.8 %; (1+beta)*||Z_e-Z_q||^2 =  2087.5 e-6 = 75.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3686.2 e-6; = (1/var)*||X-X_r||^2 =  1729.1 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  1957.1 e-6 = 53.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1304.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2132.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   912.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1042.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -130.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.64; perplexity/K = 29.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.93; perplexity/K = 31.20%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1073.5 e-6; = (1/var)*||X-X_r||^2 =  382.0 e-6 = 35.6 %; (1+beta)*||Z_e-Z_q||^2 =  691.5 e-6 = 64.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1883.6 e-6; = (1/var)*||X-X_r||^2 =  1169.3 e-6 = 62.1 %; (1+beta)*||Z_e-Z_q||^2 =  714.3 e-6 = 37.9 %)
Min.  Avg. Train Loss across Mini-Batch =  965.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1697.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   810.1 e-6; = (1/var)*||X-X_r||^2 val-train = 787.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.26; perplexity/K = 37.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.78; perplexity/K = 36.55%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1805.1 e-6; = (1/var)*||X-X_r||^2 =  443.3 e-6 = 24.6 %; (1+beta)*||Z_e-Z_q||^2 =  1361.8 e-6 = 75.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2579.3 e-6; = (1/var)*||X-X_r||^2 =  1250.5 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  1328.8 e-6 = 51.5 %)
Min.  Avg. Train Loss across Mini-Batch =  887.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1443.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   774.2 e-6; = (1/var)*||X-X_r||^2 val-train = 807.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -33.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.69; perplexity/K = 33.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.64; perplexity/K = 30.97%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  917.5 e-6; = (1/var)*||X-X_r||^2 =  270.7 e-6 = 29.5 %; (1+beta)*||Z_e-Z_q||^2 =  646.8 e-6 = 70.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1567.1 e-6; = (1/var)*||X-X_r||^2 =  888.8 e-6 = 56.7 %; (1+beta)*||Z_e-Z_q||^2 =  678.3 e-6 = 43.3 %)
Min.  Avg. Train Loss across Mini-Batch =  659.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1332.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   649.5 e-6; = (1/var)*||X-X_r||^2 val-train = 618.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 31.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.25; perplexity/K = 34.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.35; perplexity/K = 33.87%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1774.6 e-6; = (1/var)*||X-X_r||^2 =  372.3 e-6 = 21.0 %; (1+beta)*||Z_e-Z_q||^2 =  1402.3 e-6 = 79.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2482.0 e-6; = (1/var)*||X-X_r||^2 =  1129.6 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  1352.4 e-6 = 54.5 %)
Min.  Avg. Train Loss across Mini-Batch =  659.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1243.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   707.3 e-6; = (1/var)*||X-X_r||^2 val-train = 757.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -50.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.97; perplexity/K = 31.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.09; perplexity/K = 31.32%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1035.7 e-6; = (1/var)*||X-X_r||^2 =  234.3 e-6 = 22.6 %; (1+beta)*||Z_e-Z_q||^2 =  801.4 e-6 = 77.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1855.3 e-6; = (1/var)*||X-X_r||^2 =  1055.8 e-6 = 56.9 %; (1+beta)*||Z_e-Z_q||^2 =  799.4 e-6 = 43.1 %)
Min.  Avg. Train Loss across Mini-Batch =  659.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1242.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   819.5 e-6; = (1/var)*||X-X_r||^2 val-train = 821.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.77; perplexity/K = 32.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.67; perplexity/K = 30.99%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  618.0 e-6; = (1/var)*||X-X_r||^2 =  200.0 e-6 = 32.4 %; (1+beta)*||Z_e-Z_q||^2 =  417.9 e-6 = 67.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1047.4 e-6; = (1/var)*||X-X_r||^2 =  619.7 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  427.7 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  571.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1047.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   429.5 e-6; = (1/var)*||X-X_r||^2 val-train = 419.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.05; perplexity/K = 33.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.94; perplexity/K = 34.33%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  479.9 e-6; = (1/var)*||X-X_r||^2 =  185.1 e-6 = 38.6 %; (1+beta)*||Z_e-Z_q||^2 =  294.7 e-6 = 61.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  923.3 e-6; = (1/var)*||X-X_r||^2 =  615.9 e-6 = 66.7 %; (1+beta)*||Z_e-Z_q||^2 =  307.4 e-6 = 33.3 %)
Min.  Avg. Train Loss across Mini-Batch =  396.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  848.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   443.4 e-6; = (1/var)*||X-X_r||^2 val-train = 430.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.83; perplexity/K = 33.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.81; perplexity/K = 31.88%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  725.2 e-6; = (1/var)*||X-X_r||^2 =  180.5 e-6 = 24.9 %; (1+beta)*||Z_e-Z_q||^2 =  544.7 e-6 = 75.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1309.3 e-6; = (1/var)*||X-X_r||^2 =  750.4 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  558.9 e-6 = 42.7 %)
Min.  Avg. Train Loss across Mini-Batch =  381.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  848.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   584.2 e-6; = (1/var)*||X-X_r||^2 val-train = 569.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.77; perplexity/K = 34.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.29; perplexity/K = 34.60%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1002.4 e-6; = (1/var)*||X-X_r||^2 =  219.7 e-6 = 21.9 %; (1+beta)*||Z_e-Z_q||^2 =  782.7 e-6 = 78.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1560.5 e-6; = (1/var)*||X-X_r||^2 =  778.6 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  781.9 e-6 = 50.1 %)
Min.  Avg. Train Loss across Mini-Batch =  354.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  759.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   558.2 e-6; = (1/var)*||X-X_r||^2 val-train = 559.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.91; perplexity/K = 34.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.27; perplexity/K = 32.24%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  809.0 e-6; = (1/var)*||X-X_r||^2 =  171.6 e-6 = 21.2 %; (1+beta)*||Z_e-Z_q||^2 =  637.4 e-6 = 78.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1422.0 e-6; = (1/var)*||X-X_r||^2 =  775.7 e-6 = 54.6 %; (1+beta)*||Z_e-Z_q||^2 =  646.3 e-6 = 45.4 %)
Min.  Avg. Train Loss across Mini-Batch =  354.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  759.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   613.0 e-6; = (1/var)*||X-X_r||^2 val-train = 604.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.22; perplexity/K = 35.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.18; perplexity/K = 36.86%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1410.6 e-6; = (1/var)*||X-X_r||^2 =  445.0 e-6 = 31.5 %; (1+beta)*||Z_e-Z_q||^2 =  965.6 e-6 = 68.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1750.2 e-6; = (1/var)*||X-X_r||^2 =  911.4 e-6 = 52.1 %; (1+beta)*||Z_e-Z_q||^2 =  838.8 e-6 = 47.9 %)
Min.  Avg. Train Loss across Mini-Batch =  353.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  759.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   339.6 e-6; = (1/var)*||X-X_r||^2 val-train = 466.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -126.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 52.60; perplexity/K = 41.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 54.86; perplexity/K = 42.86%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:15:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9605.1 e-6; = (1/var)*||X-X_r||^2 =  5773.3 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  3831.8 e-6 = 39.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5606.9 e-6; = (1/var)*||X-X_r||^2 =  3441.5 e-6 = 61.4 %; (1+beta)*||Z_e-Z_q||^2 =  2165.4 e-6 = 38.6 %)
Min.  Avg. Train Loss across Mini-Batch =  353.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  759.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3998.2 e-6; = (1/var)*||X-X_r||^2 val-train = -2331.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1666.4 e-6 

----------------------------------------------------------------------------------

Finished [14:17:30 05.01.2023] 272) Finished running for K = 128 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 112) change_channel_size_across_layers = False:
Total training time is = 0:3:1 h/m/s. 

--------------------------------------------------- 

Started [14:17:30 05.01.2023] 273) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 28) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 742 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.27
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.08
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.31
4                           encoder.sequential_convs.conv2d_5.weight                       131            17.65
5                                  encoder.pre_residual_stack.weight                       147            19.81
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.85
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.85
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
10                             encoder.channel_adjusting_conv.weight                         8             1.08
11                                                       VQ.E.weight                         8             1.08
12                             decoder.channel_adjusting_conv.weight                        73             9.84
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.85
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.85
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.65
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.31
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.08
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.34; perplexity/K = 6.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.98; perplexity/K = 6.23%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1207732.6 e-6; = (1/var)*||X-X_r||^2 =  635175.4 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  572557.1 e-6 = 47.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  845812.7 e-6; = (1/var)*||X-X_r||^2 =  625041.1 e-6 = 73.9 %; (1+beta)*||Z_e-Z_q||^2 =  220771.6 e-6 = 26.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1074647.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  708864.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -361919.9 e-6; = (1/var)*||X-X_r||^2 val-train = -10134.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -351785.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.44; perplexity/K = 10.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.74; perplexity/K = 11.51%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  567368.8 e-6; = (1/var)*||X-X_r||^2 =  513162.0 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  54206.8 e-6 = 9.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  574333.7 e-6; = (1/var)*||X-X_r||^2 =  514871.8 e-6 = 89.6 %; (1+beta)*||Z_e-Z_q||^2 =  59461.9 e-6 = 10.4 %)
Min.  Avg. Train Loss across Mini-Batch =  556191.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  545827.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6964.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1709.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5255.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.44; perplexity/K = 16.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.90; perplexity/K = 15.55%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  480034.5 e-6; = (1/var)*||X-X_r||^2 =  451981.6 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  28052.9 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  478469.9 e-6; = (1/var)*||X-X_r||^2 =  447282.2 e-6 = 93.5 %; (1+beta)*||Z_e-Z_q||^2 =  31187.7 e-6 = 6.5 %)
Min.  Avg. Train Loss across Mini-Batch =  479359.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  472371.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1564.6 e-6; = (1/var)*||X-X_r||^2 val-train = -4699.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3134.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.22; perplexity/K = 19.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.93; perplexity/K = 18.70%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  439900.9 e-6; = (1/var)*||X-X_r||^2 =  416208.4 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  23692.5 e-6 = 5.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  437904.3 e-6; = (1/var)*||X-X_r||^2 =  414138.2 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  23766.1 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  439900.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  437904.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1996.7 e-6; = (1/var)*||X-X_r||^2 val-train = -2070.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 73.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.79; perplexity/K = 19.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.89; perplexity/K = 19.45%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  426775.0 e-6; = (1/var)*||X-X_r||^2 =  405245.1 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  21529.8 e-6 = 5.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  429678.5 e-6; = (1/var)*||X-X_r||^2 =  408428.3 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  21250.2 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  422706.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426868.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2903.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3183.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -279.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.51; perplexity/K = 22.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.54; perplexity/K = 21.51%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  411185.5 e-6; = (1/var)*||X-X_r||^2 =  394596.6 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16588.9 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  421903.9 e-6; = (1/var)*||X-X_r||^2 =  402588.6 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  19315.3 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  411185.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  419148.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10718.4 e-6; = (1/var)*||X-X_r||^2 val-train = 7992.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2726.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.28; perplexity/K = 19.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.98; perplexity/K = 21.08%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405120.2 e-6; = (1/var)*||X-X_r||^2 =  389879.2 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15241.0 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  412683.9 e-6; = (1/var)*||X-X_r||^2 =  395193.7 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  17490.2 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  403710.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  412683.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7563.7 e-6; = (1/var)*||X-X_r||^2 val-train = 5314.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2249.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.14; perplexity/K = 21.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.70; perplexity/K = 22.43%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  399758.4 e-6; = (1/var)*||X-X_r||^2 =  386095.0 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  13663.4 e-6 = 3.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  413074.0 e-6; = (1/var)*||X-X_r||^2 =  396565.5 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  16508.4 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  397985.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  409704.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   13315.6 e-6; = (1/var)*||X-X_r||^2 val-train = 10470.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2845.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.81; perplexity/K = 21.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.31; perplexity/K = 21.33%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  392513.3 e-6; = (1/var)*||X-X_r||^2 =  381084.8 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11428.5 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  407996.3 e-6; = (1/var)*||X-X_r||^2 =  392649.7 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15346.6 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  392513.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  406443.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15483.0 e-6; = (1/var)*||X-X_r||^2 val-train = 11564.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3918.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.90; perplexity/K = 23.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.89; perplexity/K = 21.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390560.3 e-6; = (1/var)*||X-X_r||^2 =  379530.3 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  11030.0 e-6 = 2.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  406433.2 e-6; = (1/var)*||X-X_r||^2 =  392071.0 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  14362.2 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  390560.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  405488.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15872.9 e-6; = (1/var)*||X-X_r||^2 val-train = 12540.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3332.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.66; perplexity/K = 22.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.97; perplexity/K = 21.85%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387271.4 e-6; = (1/var)*||X-X_r||^2 =  377361.7 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  9909.7 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  403900.7 e-6; = (1/var)*||X-X_r||^2 =  390088.4 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  13812.3 e-6 = 3.4 %)
Min.  Avg. Train Loss across Mini-Batch =  386704.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  403168.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16629.3 e-6; = (1/var)*||X-X_r||^2 val-train = 12726.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3902.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.51; perplexity/K = 21.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.80; perplexity/K = 23.28%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  387479.6 e-6; = (1/var)*||X-X_r||^2 =  377307.8 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  10171.9 e-6 = 2.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  406399.7 e-6; = (1/var)*||X-X_r||^2 =  390812.6 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  15587.1 e-6 = 3.8 %)
Min.  Avg. Train Loss across Mini-Batch =  384763.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  401793.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18920.1 e-6; = (1/var)*||X-X_r||^2 val-train = 13504.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5415.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.78; perplexity/K = 20.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.19; perplexity/K = 22.02%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  382424.8 e-6; = (1/var)*||X-X_r||^2 =  374164.1 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  8260.7 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  401346.9 e-6; = (1/var)*||X-X_r||^2 =  389182.9 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  12164.0 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  382424.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  400840.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18922.1 e-6; = (1/var)*||X-X_r||^2 val-train = 15018.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3903.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.51; perplexity/K = 21.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.79; perplexity/K = 21.71%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  383505.0 e-6; = (1/var)*||X-X_r||^2 =  374660.4 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8844.7 e-6 = 2.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  405852.1 e-6; = (1/var)*||X-X_r||^2 =  392685.6 e-6 = 96.8 %; (1+beta)*||Z_e-Z_q||^2 =  13166.4 e-6 = 3.2 %)
Min.  Avg. Train Loss across Mini-Batch =  381236.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  399949.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22347.0 e-6; = (1/var)*||X-X_r||^2 val-train = 18025.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4321.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.93; perplexity/K = 21.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.49; perplexity/K = 21.48%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  383142.6 e-6; = (1/var)*||X-X_r||^2 =  374091.2 e-6 = 97.6 %; (1+beta)*||Z_e-Z_q||^2 =  9051.4 e-6 = 2.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  402777.6 e-6; = (1/var)*||X-X_r||^2 =  390140.0 e-6 = 96.9 %; (1+beta)*||Z_e-Z_q||^2 =  12637.6 e-6 = 3.1 %)
Min.  Avg. Train Loss across Mini-Batch =  379644.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  399449.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   19635.0 e-6; = (1/var)*||X-X_r||^2 val-train = 16048.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3586.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.54; perplexity/K = 21.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.04; perplexity/K = 21.12%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  379976.5 e-6; = (1/var)*||X-X_r||^2 =  372029.0 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7947.5 e-6 = 2.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  400990.3 e-6; = (1/var)*||X-X_r||^2 =  389029.9 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11960.4 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  377747.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  398183.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   21013.9 e-6; = (1/var)*||X-X_r||^2 val-train = 17001.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4012.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.91; perplexity/K = 21.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.18; perplexity/K = 22.02%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  380204.9 e-6; = (1/var)*||X-X_r||^2 =  371890.6 e-6 = 97.8 %; (1+beta)*||Z_e-Z_q||^2 =  8314.4 e-6 = 2.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  400372.2 e-6; = (1/var)*||X-X_r||^2 =  388906.2 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11466.0 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  377339.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  398070.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   20167.3 e-6; = (1/var)*||X-X_r||^2 val-train = 17015.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3151.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.52; perplexity/K = 22.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.82; perplexity/K = 21.73%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  379388.0 e-6; = (1/var)*||X-X_r||^2 =  371399.5 e-6 = 97.9 %; (1+beta)*||Z_e-Z_q||^2 =  7988.5 e-6 = 2.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  401470.7 e-6; = (1/var)*||X-X_r||^2 =  390004.4 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11466.3 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  375508.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  398070.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   22082.7 e-6; = (1/var)*||X-X_r||^2 val-train = 18604.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3477.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.97; perplexity/K = 21.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.00; perplexity/K = 21.09%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  374997.0 e-6; = (1/var)*||X-X_r||^2 =  368581.8 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6415.2 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  398663.5 e-6; = (1/var)*||X-X_r||^2 =  388489.2 e-6 = 97.4 %; (1+beta)*||Z_e-Z_q||^2 =  10174.3 e-6 = 2.6 %)
Min.  Avg. Train Loss across Mini-Batch =  374791.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  397232.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   23666.5 e-6; = (1/var)*||X-X_r||^2 val-train = 19907.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3759.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.15; perplexity/K = 20.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.18; perplexity/K = 22.79%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  373854.3 e-6; = (1/var)*||X-X_r||^2 =  367623.4 e-6 = 98.3 %; (1+beta)*||Z_e-Z_q||^2 =  6230.9 e-6 = 1.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  400863.0 e-6; = (1/var)*||X-X_r||^2 =  389422.4 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  11440.5 e-6 = 2.9 %)
Min.  Avg. Train Loss across Mini-Batch =  373756.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  396263.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27008.6 e-6; = (1/var)*||X-X_r||^2 val-train = 21799.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5209.7 e-6 

----------------------------------------------------------------------------------

Finished [15:08:57 05.01.2023] 273) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 28) change_channel_size_across_layers = True:
Total training time is = 0:1:27 h/m/s. 

--------------------------------------------------- 

Started [15:08:57 05.01.2023] 274) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 28) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2474 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.32
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.29
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.30
4                           encoder.sequential_convs.conv2d_5.weight                       524            21.18
5                                  encoder.pre_residual_stack.weight                       589            23.81
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
10                             encoder.channel_adjusting_conv.weight                        16             0.65
11                                                       VQ.E.weight                         8             0.32
12                             decoder.channel_adjusting_conv.weight                       147             5.94
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.18
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.30
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.57; perplexity/K = 9.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.71; perplexity/K = 9.15%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  802752.2 e-6; = (1/var)*||X-X_r||^2 =  592131.1 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  210621.1 e-6 = 26.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  682852.4 e-6; = (1/var)*||X-X_r||^2 =  579095.2 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  103757.1 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  768451.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  682852.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -119899.9 e-6; = (1/var)*||X-X_r||^2 val-train = -13035.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -106863.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.79; perplexity/K = 17.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.35; perplexity/K = 18.24%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  557900.2 e-6; = (1/var)*||X-X_r||^2 =  489833.2 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  68067.0 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  551633.0 e-6; = (1/var)*||X-X_r||^2 =  484398.3 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  67234.7 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  554475.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  549533.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6267.2 e-6; = (1/var)*||X-X_r||^2 val-train = -5434.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -832.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.62; perplexity/K = 21.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.50; perplexity/K = 20.70%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  487305.8 e-6; = (1/var)*||X-X_r||^2 =  427744.3 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  59561.6 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  495149.9 e-6; = (1/var)*||X-X_r||^2 =  435844.4 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  59305.4 e-6 = 12.0 %)
Min.  Avg. Train Loss across Mini-Batch =  482655.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  488683.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7844.0 e-6; = (1/var)*||X-X_r||^2 val-train = 8100.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -256.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.12; perplexity/K = 23.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.44; perplexity/K = 23.00%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  438642.2 e-6; = (1/var)*||X-X_r||^2 =  394566.0 e-6 = 90.0 %; (1+beta)*||Z_e-Z_q||^2 =  44076.2 e-6 = 10.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  456841.6 e-6; = (1/var)*||X-X_r||^2 =  407448.2 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  49393.4 e-6 = 10.8 %)
Min.  Avg. Train Loss across Mini-Batch =  438642.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  453628.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18199.4 e-6; = (1/var)*||X-X_r||^2 val-train = 12882.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5317.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.59; perplexity/K = 24.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.40; perplexity/K = 23.75%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  437597.8 e-6; = (1/var)*||X-X_r||^2 =  390340.0 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  47257.9 e-6 = 10.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  462397.5 e-6; = (1/var)*||X-X_r||^2 =  403427.8 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  58969.7 e-6 = 12.8 %)
Min.  Avg. Train Loss across Mini-Batch =  415664.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  436464.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   24799.7 e-6; = (1/var)*||X-X_r||^2 val-train = 13087.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11711.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.52; perplexity/K = 25.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.61; perplexity/K = 25.48%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  400634.7 e-6; = (1/var)*||X-X_r||^2 =  368391.6 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  32243.1 e-6 = 8.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  428156.9 e-6; = (1/var)*||X-X_r||^2 =  390846.7 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  37310.2 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  393938.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  420254.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27522.2 e-6; = (1/var)*||X-X_r||^2 val-train = 22455.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5067.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.31; perplexity/K = 26.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.60; perplexity/K = 25.47%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390081.6 e-6; = (1/var)*||X-X_r||^2 =  359763.9 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  30317.7 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  417731.5 e-6; = (1/var)*||X-X_r||^2 =  384576.9 e-6 = 92.1 %; (1+beta)*||Z_e-Z_q||^2 =  33154.5 e-6 = 7.9 %)
Min.  Avg. Train Loss across Mini-Batch =  379296.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  411084.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27649.8 e-6; = (1/var)*||X-X_r||^2 val-train = 24813.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2836.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.93; perplexity/K = 24.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.41; perplexity/K = 25.32%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  373697.9 e-6; = (1/var)*||X-X_r||^2 =  351801.9 e-6 = 94.1 %; (1+beta)*||Z_e-Z_q||^2 =  21896.1 e-6 = 5.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  407450.8 e-6; = (1/var)*||X-X_r||^2 =  381519.7 e-6 = 93.6 %; (1+beta)*||Z_e-Z_q||^2 =  25931.1 e-6 = 6.4 %)
Min.  Avg. Train Loss across Mini-Batch =  370206.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  403941.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   33752.9 e-6; = (1/var)*||X-X_r||^2 val-train = 29717.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4035.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.73; perplexity/K = 26.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.46; perplexity/K = 24.58%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  363237.7 e-6; = (1/var)*||X-X_r||^2 =  347747.2 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  15490.5 e-6 = 4.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  398921.2 e-6; = (1/var)*||X-X_r||^2 =  378389.1 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  20532.1 e-6 = 5.1 %)
Min.  Avg. Train Loss across Mini-Batch =  363237.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  398921.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35683.6 e-6; = (1/var)*||X-X_r||^2 val-train = 30642.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5041.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.51; perplexity/K = 26.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.39; perplexity/K = 25.31%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  357833.1 e-6; = (1/var)*||X-X_r||^2 =  346034.3 e-6 = 96.7 %; (1+beta)*||Z_e-Z_q||^2 =  11798.8 e-6 = 3.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  395136.5 e-6; = (1/var)*||X-X_r||^2 =  378363.1 e-6 = 95.8 %; (1+beta)*||Z_e-Z_q||^2 =  16773.4 e-6 = 4.2 %)
Min.  Avg. Train Loss across Mini-Batch =  357833.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  394254.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37303.4 e-6; = (1/var)*||X-X_r||^2 val-train = 32328.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4974.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.95; perplexity/K = 25.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.23; perplexity/K = 25.96%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  355999.6 e-6; = (1/var)*||X-X_r||^2 =  345231.7 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  10768.0 e-6 = 3.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  393563.9 e-6; = (1/var)*||X-X_r||^2 =  377690.1 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  15873.8 e-6 = 4.0 %)
Min.  Avg. Train Loss across Mini-Batch =  355621.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  392995.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37564.3 e-6; = (1/var)*||X-X_r||^2 val-train = 32458.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5105.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.96; perplexity/K = 24.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.41; perplexity/K = 25.32%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  355004.4 e-6; = (1/var)*||X-X_r||^2 =  344587.1 e-6 = 97.1 %; (1+beta)*||Z_e-Z_q||^2 =  10417.3 e-6 = 2.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  391401.2 e-6; = (1/var)*||X-X_r||^2 =  376136.7 e-6 = 96.1 %; (1+beta)*||Z_e-Z_q||^2 =  15264.4 e-6 = 3.9 %)
Min.  Avg. Train Loss across Mini-Batch =  353333.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  389033.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36396.7 e-6; = (1/var)*||X-X_r||^2 val-train = 31549.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4847.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.25; perplexity/K = 26.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.89; perplexity/K = 24.13%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  362016.7 e-6; = (1/var)*||X-X_r||^2 =  347353.0 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  14663.7 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  394882.9 e-6; = (1/var)*||X-X_r||^2 =  376803.8 e-6 = 95.4 %; (1+beta)*||Z_e-Z_q||^2 =  18079.1 e-6 = 4.6 %)
Min.  Avg. Train Loss across Mini-Batch =  351170.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  388000.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32866.2 e-6; = (1/var)*||X-X_r||^2 val-train = 29450.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3415.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.96; perplexity/K = 27.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.53; perplexity/K = 24.63%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350134.5 e-6; = (1/var)*||X-X_r||^2 =  343003.9 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  7130.6 e-6 = 2.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  389430.5 e-6; = (1/var)*||X-X_r||^2 =  377652.7 e-6 = 97.0 %; (1+beta)*||Z_e-Z_q||^2 =  11777.8 e-6 = 3.0 %)
Min.  Avg. Train Loss across Mini-Batch =  350134.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  386952.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39296.0 e-6; = (1/var)*||X-X_r||^2 val-train = 34648.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4647.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.78; perplexity/K = 24.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.63; perplexity/K = 26.27%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  353575.1 e-6; = (1/var)*||X-X_r||^2 =  343940.1 e-6 = 97.3 %; (1+beta)*||Z_e-Z_q||^2 =  9635.0 e-6 = 2.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  389289.7 e-6; = (1/var)*||X-X_r||^2 =  375652.5 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  13637.1 e-6 = 3.5 %)
Min.  Avg. Train Loss across Mini-Batch =  349162.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384912.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35714.6 e-6; = (1/var)*||X-X_r||^2 val-train = 31712.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4002.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.48; perplexity/K = 25.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.37; perplexity/K = 24.51%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  348508.4 e-6; = (1/var)*||X-X_r||^2 =  342340.8 e-6 = 98.2 %; (1+beta)*||Z_e-Z_q||^2 =  6167.6 e-6 = 1.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  385037.0 e-6; = (1/var)*||X-X_r||^2 =  374435.5 e-6 = 97.2 %; (1+beta)*||Z_e-Z_q||^2 =  10601.5 e-6 = 2.8 %)
Min.  Avg. Train Loss across Mini-Batch =  348508.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384912.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   36528.6 e-6; = (1/var)*||X-X_r||^2 val-train = 32094.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4433.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.91; perplexity/K = 24.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.58; perplexity/K = 26.23%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347671.6 e-6; = (1/var)*||X-X_r||^2 =  342182.8 e-6 = 98.4 %; (1+beta)*||Z_e-Z_q||^2 =  5488.8 e-6 = 1.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  385484.6 e-6; = (1/var)*||X-X_r||^2 =  375878.5 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  9606.1 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  347548.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384213.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37812.9 e-6; = (1/var)*||X-X_r||^2 val-train = 33695.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4117.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.48; perplexity/K = 25.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.87; perplexity/K = 27.24%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  373885.8 e-6; = (1/var)*||X-X_r||^2 =  358956.5 e-6 = 96.0 %; (1+beta)*||Z_e-Z_q||^2 =  14929.4 e-6 = 4.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  409149.0 e-6; = (1/var)*||X-X_r||^2 =  391497.6 e-6 = 95.7 %; (1+beta)*||Z_e-Z_q||^2 =  17651.4 e-6 = 4.3 %)
Min.  Avg. Train Loss across Mini-Batch =  346964.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  384213.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35263.1 e-6; = (1/var)*||X-X_r||^2 val-train = 32541.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2722.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.20; perplexity/K = 25.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.57; perplexity/K = 25.45%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  347182.3 e-6; = (1/var)*||X-X_r||^2 =  341853.6 e-6 = 98.5 %; (1+beta)*||Z_e-Z_q||^2 =  5328.7 e-6 = 1.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  385197.6 e-6; = (1/var)*||X-X_r||^2 =  375748.9 e-6 = 97.5 %; (1+beta)*||Z_e-Z_q||^2 =  9448.7 e-6 = 2.5 %)
Min.  Avg. Train Loss across Mini-Batch =  346334.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383112.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38015.4 e-6; = (1/var)*||X-X_r||^2 val-train = 33895.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4120.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.48; perplexity/K = 25.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.87; perplexity/K = 25.68%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  346290.3 e-6; = (1/var)*||X-X_r||^2 =  341508.1 e-6 = 98.6 %; (1+beta)*||Z_e-Z_q||^2 =  4782.2 e-6 = 1.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  386193.9 e-6; = (1/var)*||X-X_r||^2 =  377233.3 e-6 = 97.7 %; (1+beta)*||Z_e-Z_q||^2 =  8960.5 e-6 = 2.3 %)
Min.  Avg. Train Loss across Mini-Batch =  345940.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  383112.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39903.6 e-6; = (1/var)*||X-X_r||^2 val-train = 35725.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4178.3 e-6 

----------------------------------------------------------------------------------

Finished [16:00:17 05.01.2023] 274) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 28) change_channel_size_across_layers = True:
Total training time is = 0:1:19 h/m/s. 

--------------------------------------------------- 

Started [16:00:17 05.01.2023] 275) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 28) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2504 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.24
1                           encoder.sequential_convs.conv2d_2.weight                       262            10.46
2                           encoder.sequential_convs.conv2d_3.weight                       262            10.46
3                           encoder.sequential_convs.conv2d_4.weight                       262            10.46
4                           encoder.sequential_convs.conv2d_5.weight                       262            10.46
5                                  encoder.pre_residual_stack.weight                       147             5.87
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.44
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.44
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
10                             encoder.channel_adjusting_conv.weight                         8             0.32
11                                                       VQ.E.weight                         8             0.32
12                             decoder.channel_adjusting_conv.weight                        73             2.92
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.44
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.44
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262            10.46
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262            10.46
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262            10.46
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262            10.46
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.24

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.45; perplexity/K = 9.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.30; perplexity/K = 10.39%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  661926.9 e-6; = (1/var)*||X-X_r||^2 =  580575.6 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  81351.3 e-6 = 12.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  635204.1 e-6; = (1/var)*||X-X_r||^2 =  573983.0 e-6 = 90.4 %; (1+beta)*||Z_e-Z_q||^2 =  61221.1 e-6 = 9.6 %)
Min.  Avg. Train Loss across Mini-Batch =  661926.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  633888.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -26722.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6592.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -20130.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.92; perplexity/K = 13.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.92; perplexity/K = 14.00%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  354333.3 e-6; = (1/var)*||X-X_r||^2 =  261806.9 e-6 = 73.9 %; (1+beta)*||Z_e-Z_q||^2 =  92526.4 e-6 = 26.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  398799.3 e-6; = (1/var)*||X-X_r||^2 =  289515.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  109284.3 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  354333.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  362563.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   44466.1 e-6; = (1/var)*||X-X_r||^2 val-train = 27708.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16757.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 27.60; perplexity/K = 21.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.11; perplexity/K = 20.40%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  254278.4 e-6; = (1/var)*||X-X_r||^2 =  157059.4 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  97219.1 e-6 = 38.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  289350.6 e-6; = (1/var)*||X-X_r||^2 =  184668.8 e-6 = 63.8 %; (1+beta)*||Z_e-Z_q||^2 =  104681.8 e-6 = 36.2 %)
Min.  Avg. Train Loss across Mini-Batch =  236345.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  274508.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   35072.1 e-6; = (1/var)*||X-X_r||^2 val-train = 27609.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7462.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.98; perplexity/K = 24.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.30; perplexity/K = 23.67%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  163782.5 e-6; = (1/var)*||X-X_r||^2 =  91141.1 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  72641.4 e-6 = 44.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  229243.9 e-6; = (1/var)*||X-X_r||^2 =  144720.6 e-6 = 63.1 %; (1+beta)*||Z_e-Z_q||^2 =  84523.3 e-6 = 36.9 %)
Min.  Avg. Train Loss across Mini-Batch =  153299.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  211403.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   65461.4 e-6; = (1/var)*||X-X_r||^2 val-train = 53579.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11882.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.59; perplexity/K = 25.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.18; perplexity/K = 25.14%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  116791.7 e-6; = (1/var)*||X-X_r||^2 =  59689.5 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  57102.2 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  192361.8 e-6; = (1/var)*||X-X_r||^2 =  122333.8 e-6 = 63.6 %; (1+beta)*||Z_e-Z_q||^2 =  70028.1 e-6 = 36.4 %)
Min.  Avg. Train Loss across Mini-Batch =  104244.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  185365.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   75570.1 e-6; = (1/var)*||X-X_r||^2 val-train = 62644.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12925.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.06; perplexity/K = 25.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.43; perplexity/K = 26.11%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  70812.3 e-6; = (1/var)*||X-X_r||^2 =  34241.9 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  36570.4 e-6 = 51.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  173547.4 e-6; = (1/var)*||X-X_r||^2 =  116374.4 e-6 = 67.1 %; (1+beta)*||Z_e-Z_q||^2 =  57172.9 e-6 = 32.9 %)
Min.  Avg. Train Loss across Mini-Batch =  69807.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  171660.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   102735.0 e-6; = (1/var)*||X-X_r||^2 val-train = 82132.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20602.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.28; perplexity/K = 27.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.81; perplexity/K = 24.85%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  55004.6 e-6; = (1/var)*||X-X_r||^2 =  25819.3 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  29185.3 e-6 = 53.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  172952.0 e-6; = (1/var)*||X-X_r||^2 =  121682.3 e-6 = 70.4 %; (1+beta)*||Z_e-Z_q||^2 =  51269.7 e-6 = 29.6 %)
Min.  Avg. Train Loss across Mini-Batch =  51315.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  165529.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   117947.5 e-6; = (1/var)*||X-X_r||^2 val-train = 95863.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22084.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.60; perplexity/K = 25.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.58; perplexity/K = 27.02%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  195733.1 e-6; = (1/var)*||X-X_r||^2 =  112135.9 e-6 = 57.3 %; (1+beta)*||Z_e-Z_q||^2 =  83597.2 e-6 = 42.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  276603.5 e-6; = (1/var)*||X-X_r||^2 =  175566.7 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  101036.8 e-6 = 36.5 %)
Min.  Avg. Train Loss across Mini-Batch =  36647.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157244.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   80870.4 e-6; = (1/var)*||X-X_r||^2 val-train = 63430.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17439.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.74; perplexity/K = 26.36%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.74; perplexity/K = 27.14%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27996.7 e-6; = (1/var)*||X-X_r||^2 =  12136.7 e-6 = 43.4 %; (1+beta)*||Z_e-Z_q||^2 =  15860.0 e-6 = 56.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  156062.1 e-6; = (1/var)*||X-X_r||^2 =  113173.3 e-6 = 72.5 %; (1+beta)*||Z_e-Z_q||^2 =  42888.8 e-6 = 27.5 %)
Min.  Avg. Train Loss across Mini-Batch =  27996.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  154711.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   128065.4 e-6; = (1/var)*||X-X_r||^2 val-train = 101036.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27028.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.30; perplexity/K = 26.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.80; perplexity/K = 26.41%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22258.9 e-6; = (1/var)*||X-X_r||^2 =  9564.0 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  12694.9 e-6 = 57.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  151705.0 e-6; = (1/var)*||X-X_r||^2 =  112184.3 e-6 = 73.9 %; (1+beta)*||Z_e-Z_q||^2 =  39520.7 e-6 = 26.1 %)
Min.  Avg. Train Loss across Mini-Batch =  21144.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  151705.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   129446.1 e-6; = (1/var)*||X-X_r||^2 val-train = 102620.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26825.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.39; perplexity/K = 25.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.37; perplexity/K = 26.07%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27107.2 e-6; = (1/var)*||X-X_r||^2 =  10011.7 e-6 = 36.9 %; (1+beta)*||Z_e-Z_q||^2 =  17095.5 e-6 = 63.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  153335.0 e-6; = (1/var)*||X-X_r||^2 =  112335.1 e-6 = 73.3 %; (1+beta)*||Z_e-Z_q||^2 =  40999.8 e-6 = 26.7 %)
Min.  Avg. Train Loss across Mini-Batch =  16429.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  150560.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   126227.8 e-6; = (1/var)*||X-X_r||^2 val-train = 102323.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23904.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.20; perplexity/K = 25.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.70; perplexity/K = 26.33%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13517.4 e-6; = (1/var)*||X-X_r||^2 =  6569.1 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  6948.3 e-6 = 51.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  151414.4 e-6; = (1/var)*||X-X_r||^2 =  116828.9 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  34585.5 e-6 = 22.8 %)
Min.  Avg. Train Loss across Mini-Batch =  13098.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147074.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137897.0 e-6; = (1/var)*||X-X_r||^2 val-train = 110259.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27637.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.43; perplexity/K = 25.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.71; perplexity/K = 26.34%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  137573.3 e-6; = (1/var)*||X-X_r||^2 =  70755.3 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  66818.0 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  213144.5 e-6; = (1/var)*||X-X_r||^2 =  138099.9 e-6 = 64.8 %; (1+beta)*||Z_e-Z_q||^2 =  75044.6 e-6 = 35.2 %)
Min.  Avg. Train Loss across Mini-Batch =  10995.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  147074.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   75571.2 e-6; = (1/var)*||X-X_r||^2 val-train = 67344.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8226.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.86; perplexity/K = 24.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.46; perplexity/K = 25.36%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10226.9 e-6; = (1/var)*||X-X_r||^2 =  5027.0 e-6 = 49.2 %; (1+beta)*||Z_e-Z_q||^2 =  5199.9 e-6 = 50.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  150296.6 e-6; = (1/var)*||X-X_r||^2 =  118041.1 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  32255.5 e-6 = 21.5 %)
Min.  Avg. Train Loss across Mini-Batch =  9708.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  146898.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   140069.8 e-6; = (1/var)*||X-X_r||^2 val-train = 113014.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27055.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.41; perplexity/K = 26.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.30; perplexity/K = 24.45%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12235.5 e-6; = (1/var)*||X-X_r||^2 =  4841.4 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  7394.1 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  152105.9 e-6; = (1/var)*||X-X_r||^2 =  119386.6 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  32719.3 e-6 = 21.5 %)
Min.  Avg. Train Loss across Mini-Batch =  8686.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  146898.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   139870.4 e-6; = (1/var)*||X-X_r||^2 val-train = 114545.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25325.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.83; perplexity/K = 25.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.22; perplexity/K = 24.39%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7603.1 e-6; = (1/var)*||X-X_r||^2 =  3882.4 e-6 = 51.1 %; (1+beta)*||Z_e-Z_q||^2 =  3720.6 e-6 = 48.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  145461.2 e-6; = (1/var)*||X-X_r||^2 =  114649.0 e-6 = 78.8 %; (1+beta)*||Z_e-Z_q||^2 =  30812.2 e-6 = 21.2 %)
Min.  Avg. Train Loss across Mini-Batch =  7249.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  145461.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   137858.1 e-6; = (1/var)*||X-X_r||^2 val-train = 110766.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27091.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.28; perplexity/K = 27.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.77; perplexity/K = 25.60%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  42622.0 e-6; = (1/var)*||X-X_r||^2 =  15184.4 e-6 = 35.6 %; (1+beta)*||Z_e-Z_q||^2 =  27437.6 e-6 = 64.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  156930.7 e-6; = (1/var)*||X-X_r||^2 =  115276.9 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  41653.9 e-6 = 26.5 %)
Min.  Avg. Train Loss across Mini-Batch =  6701.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143880.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   114308.7 e-6; = (1/var)*||X-X_r||^2 val-train = 100092.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14216.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.01; perplexity/K = 25.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.60; perplexity/K = 26.25%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18920.9 e-6; = (1/var)*||X-X_r||^2 =  6013.2 e-6 = 31.8 %; (1+beta)*||Z_e-Z_q||^2 =  12907.6 e-6 = 68.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  146461.3 e-6; = (1/var)*||X-X_r||^2 =  112346.2 e-6 = 76.7 %; (1+beta)*||Z_e-Z_q||^2 =  34115.0 e-6 = 23.3 %)
Min.  Avg. Train Loss across Mini-Batch =  6325.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143712.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   127540.4 e-6; = (1/var)*||X-X_r||^2 val-train = 106333.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21207.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.30; perplexity/K = 25.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.12; perplexity/K = 26.66%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6296.7 e-6; = (1/var)*||X-X_r||^2 =  3087.2 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  3209.6 e-6 = 51.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  145607.2 e-6; = (1/var)*||X-X_r||^2 =  116259.7 e-6 = 79.8 %; (1+beta)*||Z_e-Z_q||^2 =  29347.6 e-6 = 20.2 %)
Min.  Avg. Train Loss across Mini-Batch =  5973.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141811.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   139310.5 e-6; = (1/var)*||X-X_r||^2 val-train = 113172.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26138.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.45; perplexity/K = 26.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.90; perplexity/K = 26.48%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6144.6 e-6; = (1/var)*||X-X_r||^2 =  3136.2 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  3008.4 e-6 = 49.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  151862.2 e-6; = (1/var)*||X-X_r||^2 =  122892.9 e-6 = 80.9 %; (1+beta)*||Z_e-Z_q||^2 =  28969.4 e-6 = 19.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5215.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141811.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145717.6 e-6; = (1/var)*||X-X_r||^2 val-train = 119756.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25960.9 e-6 

----------------------------------------------------------------------------------

Finished [17:06:36 05.01.2023] 275) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 28) change_channel_size_across_layers = False:
Total training time is = 0:1:19 h/m/s. 

--------------------------------------------------- 

Started [17:06:36 05.01.2023] 276) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 28) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9492 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.13
1                           encoder.sequential_convs.conv2d_2.weight                      1048            11.04
2                           encoder.sequential_convs.conv2d_3.weight                      1048            11.04
3                           encoder.sequential_convs.conv2d_4.weight                      1048            11.04
4                           encoder.sequential_convs.conv2d_5.weight                      1048            11.04
5                                  encoder.pre_residual_stack.weight                       589             6.21
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                         8             0.08
12                             decoder.channel_adjusting_conv.weight                       147             1.55
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            11.04
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            11.04
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            11.04
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            11.04
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.32; perplexity/K = 18.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.66; perplexity/K = 18.48%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  631139.6 e-6; = (1/var)*||X-X_r||^2 =  291944.4 e-6 = 46.3 %; (1+beta)*||Z_e-Z_q||^2 =  339195.2 e-6 = 53.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  545583.8 e-6; = (1/var)*||X-X_r||^2 =  291579.0 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  254004.8 e-6 = 46.6 %)
Min.  Avg. Train Loss across Mini-Batch =  539523.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  511160.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -85555.8 e-6; = (1/var)*||X-X_r||^2 val-train = -365.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -85190.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.24; perplexity/K = 26.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.95; perplexity/K = 27.30%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  304918.5 e-6; = (1/var)*||X-X_r||^2 =  127537.6 e-6 = 41.8 %; (1+beta)*||Z_e-Z_q||^2 =  177380.9 e-6 = 58.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  371497.8 e-6; = (1/var)*||X-X_r||^2 =  174502.8 e-6 = 47.0 %; (1+beta)*||Z_e-Z_q||^2 =  196995.0 e-6 = 53.0 %)
Min.  Avg. Train Loss across Mini-Batch =  304918.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  357058.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   66579.3 e-6; = (1/var)*||X-X_r||^2 val-train = 46965.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19614.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.80; perplexity/K = 29.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.01; perplexity/K = 30.47%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  158203.6 e-6; = (1/var)*||X-X_r||^2 =  49270.3 e-6 = 31.1 %; (1+beta)*||Z_e-Z_q||^2 =  108933.2 e-6 = 68.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  274895.1 e-6; = (1/var)*||X-X_r||^2 =  143579.6 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  131315.5 e-6 = 47.8 %)
Min.  Avg. Train Loss across Mini-Batch =  158203.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  267784.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   116691.6 e-6; = (1/var)*||X-X_r||^2 val-train = 94309.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22382.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.59; perplexity/K = 30.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.34; perplexity/K = 33.86%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  87929.2 e-6; = (1/var)*||X-X_r||^2 =  26668.8 e-6 = 30.3 %; (1+beta)*||Z_e-Z_q||^2 =  61260.4 e-6 = 69.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  215847.2 e-6; = (1/var)*||X-X_r||^2 =  135322.7 e-6 = 62.7 %; (1+beta)*||Z_e-Z_q||^2 =  80524.4 e-6 = 37.3 %)
Min.  Avg. Train Loss across Mini-Batch =  70851.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  199940.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   127917.9 e-6; = (1/var)*||X-X_r||^2 val-train = 108653.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19264.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.76; perplexity/K = 31.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.56; perplexity/K = 31.69%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26504.9 e-6; = (1/var)*||X-X_r||^2 =  6852.2 e-6 = 25.9 %; (1+beta)*||Z_e-Z_q||^2 =  19652.8 e-6 = 74.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  170316.6 e-6; = (1/var)*||X-X_r||^2 =  125610.1 e-6 = 73.8 %; (1+beta)*||Z_e-Z_q||^2 =  44706.5 e-6 = 26.2 %)
Min.  Avg. Train Loss across Mini-Batch =  26504.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  170316.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   143811.7 e-6; = (1/var)*||X-X_r||^2 val-train = 118758.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25053.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.10; perplexity/K = 30.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.53; perplexity/K = 30.88%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15543.1 e-6; = (1/var)*||X-X_r||^2 =  4908.4 e-6 = 31.6 %; (1+beta)*||Z_e-Z_q||^2 =  10634.7 e-6 = 68.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  158374.5 e-6; = (1/var)*||X-X_r||^2 =  124326.2 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  34048.3 e-6 = 21.5 %)
Min.  Avg. Train Loss across Mini-Batch =  14877.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  157149.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   142831.4 e-6; = (1/var)*||X-X_r||^2 val-train = 119417.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23413.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.71; perplexity/K = 31.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.65; perplexity/K = 30.98%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13578.3 e-6; = (1/var)*||X-X_r||^2 =  3500.1 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  10078.2 e-6 = 74.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  151966.7 e-6; = (1/var)*||X-X_r||^2 =  121353.3 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  30613.5 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  9389.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  149792.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   138388.4 e-6; = (1/var)*||X-X_r||^2 val-train = 117853.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20535.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.97; perplexity/K = 32.79%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.66; perplexity/K = 32.55%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6943.6 e-6; = (1/var)*||X-X_r||^2 =  2621.8 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  4321.8 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  153697.7 e-6; = (1/var)*||X-X_r||^2 =  129494.1 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  24203.6 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  6943.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  145897.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   146754.1 e-6; = (1/var)*||X-X_r||^2 val-train = 126872.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19881.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.30; perplexity/K = 30.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.54; perplexity/K = 31.67%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7131.2 e-6; = (1/var)*||X-X_r||^2 =  2205.3 e-6 = 30.9 %; (1+beta)*||Z_e-Z_q||^2 =  4925.9 e-6 = 69.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  155970.9 e-6; = (1/var)*||X-X_r||^2 =  132401.2 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  23569.6 e-6 = 15.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5645.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  145897.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   148839.7 e-6; = (1/var)*||X-X_r||^2 val-train = 130195.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18643.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.54; perplexity/K = 30.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.76; perplexity/K = 31.84%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14566.8 e-6; = (1/var)*||X-X_r||^2 =  3393.4 e-6 = 23.3 %; (1+beta)*||Z_e-Z_q||^2 =  11173.4 e-6 = 76.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  162212.9 e-6; = (1/var)*||X-X_r||^2 =  131897.7 e-6 = 81.3 %; (1+beta)*||Z_e-Z_q||^2 =  30315.2 e-6 = 18.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4739.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  145897.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   147646.1 e-6; = (1/var)*||X-X_r||^2 val-train = 128504.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19141.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.61; perplexity/K = 31.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.10; perplexity/K = 30.54%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3755.5 e-6; = (1/var)*||X-X_r||^2 =  1554.2 e-6 = 41.4 %; (1+beta)*||Z_e-Z_q||^2 =  2201.3 e-6 = 58.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  144030.2 e-6; = (1/var)*||X-X_r||^2 =  126956.9 e-6 = 88.1 %; (1+beta)*||Z_e-Z_q||^2 =  17073.3 e-6 = 11.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3680.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143687.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   140274.7 e-6; = (1/var)*||X-X_r||^2 val-train = 125402.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14872.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.39; perplexity/K = 33.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.42; perplexity/K = 31.58%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:22:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6153.2 e-6; = (1/var)*||X-X_r||^2 =  1723.7 e-6 = 28.0 %; (1+beta)*||Z_e-Z_q||^2 =  4429.5 e-6 = 72.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  151496.9 e-6; = (1/var)*||X-X_r||^2 =  129975.2 e-6 = 85.8 %; (1+beta)*||Z_e-Z_q||^2 =  21521.7 e-6 = 14.2 %)
Min.  Avg. Train Loss across Mini-Batch =  2721.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143395.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145343.7 e-6; = (1/var)*||X-X_r||^2 val-train = 128251.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17092.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.36; perplexity/K = 29.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.69; perplexity/K = 31.79%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:29:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28113.4 e-6; = (1/var)*||X-X_r||^2 =  8989.5 e-6 = 32.0 %; (1+beta)*||Z_e-Z_q||^2 =  19123.9 e-6 = 68.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  160640.4 e-6; = (1/var)*||X-X_r||^2 =  130063.5 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  30576.9 e-6 = 19.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2721.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  143395.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   132527.0 e-6; = (1/var)*||X-X_r||^2 val-train = 121074.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11453.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.50; perplexity/K = 31.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.87; perplexity/K = 30.37%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3105.7 e-6; = (1/var)*||X-X_r||^2 =  1028.9 e-6 = 33.1 %; (1+beta)*||Z_e-Z_q||^2 =  2076.8 e-6 = 66.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  145136.2 e-6; = (1/var)*||X-X_r||^2 =  128246.8 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  16889.4 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  2721.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   142030.5 e-6; = (1/var)*||X-X_r||^2 val-train = 127217.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14812.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.25; perplexity/K = 32.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.59; perplexity/K = 32.50%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3055.0 e-6; = (1/var)*||X-X_r||^2 =  1021.1 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  2033.9 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  149646.4 e-6; = (1/var)*||X-X_r||^2 =  132489.8 e-6 = 88.5 %; (1+beta)*||Z_e-Z_q||^2 =  17156.6 e-6 = 11.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2232.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   146591.4 e-6; = (1/var)*||X-X_r||^2 val-train = 131468.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15122.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.71; perplexity/K = 31.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.23; perplexity/K = 32.21%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1922.1 e-6; = (1/var)*||X-X_r||^2 =  754.6 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  1167.4 e-6 = 60.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  151096.6 e-6; = (1/var)*||X-X_r||^2 =  134055.4 e-6 = 88.7 %; (1+beta)*||Z_e-Z_q||^2 =  17041.2 e-6 = 11.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1626.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   149174.6 e-6; = (1/var)*||X-X_r||^2 val-train = 133300.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15873.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.02; perplexity/K = 33.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.90; perplexity/K = 32.74%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1315.4 e-6; = (1/var)*||X-X_r||^2 =  696.6 e-6 = 53.0 %; (1+beta)*||Z_e-Z_q||^2 =  618.8 e-6 = 47.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  147147.5 e-6; = (1/var)*||X-X_r||^2 =  130102.4 e-6 = 88.4 %; (1+beta)*||Z_e-Z_q||^2 =  17045.2 e-6 = 11.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1302.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145832.1 e-6; = (1/var)*||X-X_r||^2 val-train = 129405.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16426.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.46; perplexity/K = 33.17%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.26; perplexity/K = 32.24%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:3:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8673.5 e-6; = (1/var)*||X-X_r||^2 =  2051.6 e-6 = 23.7 %; (1+beta)*||Z_e-Z_q||^2 =  6621.9 e-6 = 76.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  151977.4 e-6; = (1/var)*||X-X_r||^2 =  131370.8 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  20606.6 e-6 = 13.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1281.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   143303.9 e-6; = (1/var)*||X-X_r||^2 val-train = 129319.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13984.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.59; perplexity/K = 31.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.55; perplexity/K = 32.46%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:10:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4180.8 e-6; = (1/var)*||X-X_r||^2 =  886.2 e-6 = 21.2 %; (1+beta)*||Z_e-Z_q||^2 =  3294.7 e-6 = 78.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  150156.0 e-6; = (1/var)*||X-X_r||^2 =  130386.6 e-6 = 86.8 %; (1+beta)*||Z_e-Z_q||^2 =  19769.3 e-6 = 13.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1217.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145975.1 e-6; = (1/var)*||X-X_r||^2 val-train = 129500.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16474.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.14; perplexity/K = 32.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.96; perplexity/K = 31.22%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  975.2 e-6; = (1/var)*||X-X_r||^2 =  625.6 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  349.6 e-6 = 35.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  149963.7 e-6; = (1/var)*||X-X_r||^2 =  134009.3 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  15954.4 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  956.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  141869.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   148988.5 e-6; = (1/var)*||X-X_r||^2 val-train = 133383.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15604.8 e-6 

----------------------------------------------------------------------------------

Finished [19:24:11 05.01.2023] 276) Finished running for K = 128 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 28) change_channel_size_across_layers = False:
Total training time is = 0:1:34 h/m/s. 

--------------------------------------------------- 

Started [19:24:11 05.01.2023] 277) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 7) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 742 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.27
3                           encoder.sequential_convs.conv2d_4.weight                         8             1.08
4                           encoder.sequential_convs.conv2d_5.weight                        32             4.31
5                           encoder.sequential_convs.conv2d_6.weight                       131            17.65
6                                  encoder.pre_residual_stack.weight                       147            19.81
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.85
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.85
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
11                             encoder.channel_adjusting_conv.weight                         8             1.08
12                                                       VQ.E.weight                         8             1.08
13                             decoder.channel_adjusting_conv.weight                        73             9.84
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.85
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.85
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.65
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.31
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.08
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.32; perplexity/K = 5.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.72; perplexity/K = 5.25%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2740227.8 e-6; = (1/var)*||X-X_r||^2 =  900770.3 e-6 = 32.9 %; (1+beta)*||Z_e-Z_q||^2 =  1839457.5 e-6 = 67.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1775019.4 e-6; = (1/var)*||X-X_r||^2 =  865720.6 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  909298.8 e-6 = 51.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1078526.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  935447.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -965208.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35049.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -930158.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.77; perplexity/K = 4.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.20; perplexity/K = 4.85%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1456553.6 e-6; = (1/var)*||X-X_r||^2 =  778468.5 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  678085.2 e-6 = 46.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1095365.5 e-6; = (1/var)*||X-X_r||^2 =  774520.5 e-6 = 70.7 %; (1+beta)*||Z_e-Z_q||^2 =  320845.0 e-6 = 29.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1078526.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  929917.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -361188.1 e-6; = (1/var)*||X-X_r||^2 val-train = -3948.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -357240.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.45; perplexity/K = 4.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.24; perplexity/K = 5.65%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  989641.7 e-6; = (1/var)*||X-X_r||^2 =  680901.2 e-6 = 68.8 %; (1+beta)*||Z_e-Z_q||^2 =  308740.5 e-6 = 31.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1003969.8 e-6; = (1/var)*||X-X_r||^2 =  671558.7 e-6 = 66.9 %; (1+beta)*||Z_e-Z_q||^2 =  332411.0 e-6 = 33.1 %)
Min.  Avg. Train Loss across Mini-Batch =  985474.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  908874.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14328.0 e-6; = (1/var)*||X-X_r||^2 val-train = -9342.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23670.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.80; perplexity/K = 2.97%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.42; perplexity/K = 3.46%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  881648.5 e-6; = (1/var)*||X-X_r||^2 =  683315.1 e-6 = 77.5 %; (1+beta)*||Z_e-Z_q||^2 =  198333.5 e-6 = 22.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  883135.9 e-6; = (1/var)*||X-X_r||^2 =  669478.3 e-6 = 75.8 %; (1+beta)*||Z_e-Z_q||^2 =  213657.6 e-6 = 24.2 %)
Min.  Avg. Train Loss across Mini-Batch =  881648.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  831948.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1487.4 e-6; = (1/var)*||X-X_r||^2 val-train = -13836.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15324.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.69; perplexity/K = 2.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.49; perplexity/K = 3.50%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  891744.5 e-6; = (1/var)*||X-X_r||^2 =  712938.9 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  178805.7 e-6 = 20.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  899811.1 e-6; = (1/var)*||X-X_r||^2 =  696369.4 e-6 = 77.4 %; (1+beta)*||Z_e-Z_q||^2 =  203441.7 e-6 = 22.6 %)
Min.  Avg. Train Loss across Mini-Batch =  869890.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  823776.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8066.6 e-6; = (1/var)*||X-X_r||^2 val-train = -16569.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24636.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.74; perplexity/K = 6.04%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.00; perplexity/K = 5.47%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  937142.1 e-6; = (1/var)*||X-X_r||^2 =  673936.5 e-6 = 71.9 %; (1+beta)*||Z_e-Z_q||^2 =  263205.6 e-6 = 28.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  906163.3 e-6; = (1/var)*||X-X_r||^2 =  654108.7 e-6 = 72.2 %; (1+beta)*||Z_e-Z_q||^2 =  252054.5 e-6 = 27.8 %)
Min.  Avg. Train Loss across Mini-Batch =  869890.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  823776.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30978.9 e-6; = (1/var)*||X-X_r||^2 val-train = -19827.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11151.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.68; perplexity/K = 2.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.13; perplexity/K = 2.44%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  866817.7 e-6; = (1/var)*||X-X_r||^2 =  698194.5 e-6 = 80.5 %; (1+beta)*||Z_e-Z_q||^2 =  168623.2 e-6 = 19.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  835200.0 e-6; = (1/var)*||X-X_r||^2 =  674761.2 e-6 = 80.8 %; (1+beta)*||Z_e-Z_q||^2 =  160438.8 e-6 = 19.2 %)
Min.  Avg. Train Loss across Mini-Batch =  847170.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  819113.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31617.7 e-6; = (1/var)*||X-X_r||^2 val-train = -23433.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8184.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.79; perplexity/K = 1.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.76; perplexity/K = 1.38%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  860505.7 e-6; = (1/var)*||X-X_r||^2 =  740967.8 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  119537.9 e-6 = 13.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  839715.5 e-6; = (1/var)*||X-X_r||^2 =  733512.8 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  106202.7 e-6 = 12.6 %)
Min.  Avg. Train Loss across Mini-Batch =  847170.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  819113.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20790.2 e-6; = (1/var)*||X-X_r||^2 val-train = -7455.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13335.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.47; perplexity/K = 5.06%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.07; perplexity/K = 3.96%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  890782.2 e-6; = (1/var)*||X-X_r||^2 =  682564.5 e-6 = 76.6 %; (1+beta)*||Z_e-Z_q||^2 =  208217.7 e-6 = 23.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  868874.3 e-6; = (1/var)*||X-X_r||^2 =  670382.9 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  198491.4 e-6 = 22.8 %)
Min.  Avg. Train Loss across Mini-Batch =  847170.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  819113.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21907.9 e-6; = (1/var)*||X-X_r||^2 val-train = -12181.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9726.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.76; perplexity/K = 1.37%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.23; perplexity/K = 1.74%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  832637.7 e-6; = (1/var)*||X-X_r||^2 =  698160.2 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  134477.5 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  811171.3 e-6; = (1/var)*||X-X_r||^2 =  678644.0 e-6 = 83.7 %; (1+beta)*||Z_e-Z_q||^2 =  132527.3 e-6 = 16.3 %)
Min.  Avg. Train Loss across Mini-Batch =  830714.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  798056.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -21466.3 e-6; = (1/var)*||X-X_r||^2 val-train = -19516.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1950.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.24; perplexity/K = 2.53%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.94; perplexity/K = 2.30%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  864279.3 e-6; = (1/var)*||X-X_r||^2 =  720446.9 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  143832.4 e-6 = 16.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  824703.0 e-6; = (1/var)*||X-X_r||^2 =  703283.1 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  121419.9 e-6 = 14.7 %)
Min.  Avg. Train Loss across Mini-Batch =  824390.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  789546.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -39576.3 e-6; = (1/var)*||X-X_r||^2 val-train = -17163.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -22412.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.89; perplexity/K = 5.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.36; perplexity/K = 5.75%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  819564.7 e-6; = (1/var)*||X-X_r||^2 =  664241.5 e-6 = 81.0 %; (1+beta)*||Z_e-Z_q||^2 =  155323.2 e-6 = 19.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  780877.7 e-6; = (1/var)*||X-X_r||^2 =  646716.5 e-6 = 82.8 %; (1+beta)*||Z_e-Z_q||^2 =  134161.2 e-6 = 17.2 %)
Min.  Avg. Train Loss across Mini-Batch =  808909.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  780877.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -38687.0 e-6; = (1/var)*||X-X_r||^2 val-train = -17525.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -21161.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.44; perplexity/K = 5.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.82; perplexity/K = 3.77%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  835639.4 e-6; = (1/var)*||X-X_r||^2 =  691476.3 e-6 = 82.7 %; (1+beta)*||Z_e-Z_q||^2 =  144163.1 e-6 = 17.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  823236.5 e-6; = (1/var)*||X-X_r||^2 =  672972.5 e-6 = 81.7 %; (1+beta)*||Z_e-Z_q||^2 =  150264.0 e-6 = 18.3 %)
Min.  Avg. Train Loss across Mini-Batch =  806601.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  777656.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12403.0 e-6; = (1/var)*||X-X_r||^2 val-train = -18503.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6100.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.07; perplexity/K = 6.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.49; perplexity/K = 5.85%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  780559.3 e-6; = (1/var)*||X-X_r||^2 =  650258.5 e-6 = 83.3 %; (1+beta)*||Z_e-Z_q||^2 =  130300.8 e-6 = 16.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  763470.2 e-6; = (1/var)*||X-X_r||^2 =  634625.4 e-6 = 83.1 %; (1+beta)*||Z_e-Z_q||^2 =  128844.8 e-6 = 16.9 %)
Min.  Avg. Train Loss across Mini-Batch =  777495.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  762794.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17089.1 e-6; = (1/var)*||X-X_r||^2 val-train = -15633.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1456.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.22; perplexity/K = 7.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.62; perplexity/K = 6.73%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  823213.5 e-6; = (1/var)*||X-X_r||^2 =  646393.2 e-6 = 78.5 %; (1+beta)*||Z_e-Z_q||^2 =  176820.2 e-6 = 21.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  775888.1 e-6; = (1/var)*||X-X_r||^2 =  632590.2 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  143297.9 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  772148.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  745534.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -47325.4 e-6; = (1/var)*||X-X_r||^2 val-train = -13803.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -33522.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.74; perplexity/K = 6.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.41; perplexity/K = 5.79%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  796883.2 e-6; = (1/var)*||X-X_r||^2 =  660104.2 e-6 = 82.8 %; (1+beta)*||Z_e-Z_q||^2 =  136779.0 e-6 = 17.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  785897.4 e-6; = (1/var)*||X-X_r||^2 =  648173.5 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  137723.8 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  772148.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  745534.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10985.8 e-6; = (1/var)*||X-X_r||^2 val-train = -11930.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 944.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.61; perplexity/K = 14.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.27; perplexity/K = 9.58%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  821786.8 e-6; = (1/var)*||X-X_r||^2 =  646637.5 e-6 = 78.7 %; (1+beta)*||Z_e-Z_q||^2 =  175149.3 e-6 = 21.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  797516.9 e-6; = (1/var)*||X-X_r||^2 =  641042.0 e-6 = 80.4 %; (1+beta)*||Z_e-Z_q||^2 =  156474.9 e-6 = 19.6 %)
Min.  Avg. Train Loss across Mini-Batch =  759162.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  738600.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -24269.9 e-6; = (1/var)*||X-X_r||^2 val-train = -5595.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18674.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.87; perplexity/K = 7.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.78; perplexity/K = 8.42%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  775611.8 e-6; = (1/var)*||X-X_r||^2 =  649746.5 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  125865.3 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  746831.5 e-6; = (1/var)*||X-X_r||^2 =  633432.8 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  113398.7 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  759162.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  733983.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -28780.3 e-6; = (1/var)*||X-X_r||^2 val-train = -16313.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12466.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.67; perplexity/K = 10.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.54; perplexity/K = 9.02%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  790117.2 e-6; = (1/var)*||X-X_r||^2 =  644605.7 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  145511.5 e-6 = 18.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  820315.2 e-6; = (1/var)*||X-X_r||^2 =  642112.0 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  178203.2 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  750686.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  720286.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30198.1 e-6; = (1/var)*||X-X_r||^2 val-train = -2493.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32691.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.26; perplexity/K = 8.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.33; perplexity/K = 6.51%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  763400.3 e-6; = (1/var)*||X-X_r||^2 =  633755.3 e-6 = 83.0 %; (1+beta)*||Z_e-Z_q||^2 =  129645.0 e-6 = 17.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  777715.2 e-6; = (1/var)*||X-X_r||^2 =  620217.0 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  157498.2 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  734586.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  717127.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14314.9 e-6; = (1/var)*||X-X_r||^2 val-train = -13538.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27853.2 e-6 

----------------------------------------------------------------------------------

Finished [20:16:45 05.01.2023] 277) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 7) change_channel_size_across_layers = True:
Total training time is = 0:0:34 h/m/s. 

--------------------------------------------------- 

Started [20:16:45 05.01.2023] 278) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 7) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2478 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.32
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.29
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.29
5                           encoder.sequential_convs.conv2d_6.weight                       524            21.15
6                                  encoder.pre_residual_stack.weight                       589            23.77
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
11                             encoder.channel_adjusting_conv.weight                        16             0.65
12                                                       VQ.E.weight                         8             0.32
13                             decoder.channel_adjusting_conv.weight                       147             5.93
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.15
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.29
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.86; perplexity/K = 3.80%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.59; perplexity/K = 3.58%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1405319.3 e-6; = (1/var)*||X-X_r||^2 =  907702.8 e-6 = 64.6 %; (1+beta)*||Z_e-Z_q||^2 =  497616.5 e-6 = 35.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  956796.3 e-6; = (1/var)*||X-X_r||^2 =  871969.8 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  84826.5 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  974989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  937755.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -448523.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35733.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -412790.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.10; perplexity/K = 2.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.05; perplexity/K = 2.38%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1278161.5 e-6; = (1/var)*||X-X_r||^2 =  859597.6 e-6 = 67.3 %; (1+beta)*||Z_e-Z_q||^2 =  418563.8 e-6 = 32.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1233699.7 e-6; = (1/var)*||X-X_r||^2 =  842381.2 e-6 = 68.3 %; (1+beta)*||Z_e-Z_q||^2 =  391318.5 e-6 = 31.7 %)
Min.  Avg. Train Loss across Mini-Batch =  974989.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -44461.8 e-6; = (1/var)*||X-X_r||^2 val-train = -17216.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -27245.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.28; perplexity/K = 1.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.17; perplexity/K = 0.91%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1019095.3 e-6; = (1/var)*||X-X_r||^2 =  902641.7 e-6 = 88.6 %; (1+beta)*||Z_e-Z_q||^2 =  116453.7 e-6 = 11.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  964550.3 e-6; = (1/var)*||X-X_r||^2 =  887377.3 e-6 = 92.0 %; (1+beta)*||Z_e-Z_q||^2 =  77172.9 e-6 = 8.0 %)
Min.  Avg. Train Loss across Mini-Batch =  969530.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -54545.1 e-6; = (1/var)*||X-X_r||^2 val-train = -15264.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -39280.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  974702.0 e-6; = (1/var)*||X-X_r||^2 =  971782.5 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  2919.5 e-6 = 0.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  950820.1 e-6; = (1/var)*||X-X_r||^2 =  948782.6 e-6 = 99.8 %; (1+beta)*||Z_e-Z_q||^2 =  2037.5 e-6 = 0.2 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23881.9 e-6; = (1/var)*||X-X_r||^2 val-train = -22999.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -882.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999753.2 e-6; = (1/var)*||X-X_r||^2 =  999686.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  66.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963110.3 e-6; = (1/var)*||X-X_r||^2 =  963070.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  39.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36642.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36616.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -26.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999477.2 e-6; = (1/var)*||X-X_r||^2 =  999472.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.7 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963463.1 e-6; = (1/var)*||X-X_r||^2 =  963459.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36014.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36013.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999356.9 e-6; = (1/var)*||X-X_r||^2 =  999355.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962985.8 e-6; = (1/var)*||X-X_r||^2 =  962984.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36371.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36371.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999496.1 e-6; = (1/var)*||X-X_r||^2 =  999495.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963911.1 e-6; = (1/var)*||X-X_r||^2 =  963910.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35585.0 e-6; = (1/var)*||X-X_r||^2 val-train = -35584.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999282.7 e-6; = (1/var)*||X-X_r||^2 =  999282.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962929.5 e-6; = (1/var)*||X-X_r||^2 =  962929.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36353.2 e-6; = (1/var)*||X-X_r||^2 val-train = -36353.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999355.9 e-6; = (1/var)*||X-X_r||^2 =  999355.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962941.1 e-6; = (1/var)*||X-X_r||^2 =  962940.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36414.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36414.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999206.8 e-6; = (1/var)*||X-X_r||^2 =  999206.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962907.1 e-6; = (1/var)*||X-X_r||^2 =  962906.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36299.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36299.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999212.1 e-6; = (1/var)*||X-X_r||^2 =  999212.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962991.1 e-6; = (1/var)*||X-X_r||^2 =  962991.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36220.9 e-6; = (1/var)*||X-X_r||^2 val-train = -36220.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999427.3 e-6; = (1/var)*||X-X_r||^2 =  999427.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962920.6 e-6; = (1/var)*||X-X_r||^2 =  962920.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36506.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36506.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999233.8 e-6; = (1/var)*||X-X_r||^2 =  999233.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962993.0 e-6; = (1/var)*||X-X_r||^2 =  962992.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36240.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36240.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999331.4 e-6; = (1/var)*||X-X_r||^2 =  999331.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963132.7 e-6; = (1/var)*||X-X_r||^2 =  963132.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36198.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36198.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999172.5 e-6; = (1/var)*||X-X_r||^2 =  999172.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963093.0 e-6; = (1/var)*||X-X_r||^2 =  963092.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36079.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36079.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999228.6 e-6; = (1/var)*||X-X_r||^2 =  999228.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962897.2 e-6; = (1/var)*||X-X_r||^2 =  962897.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36331.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36331.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999200.7 e-6; = (1/var)*||X-X_r||^2 =  999200.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963040.7 e-6; = (1/var)*||X-X_r||^2 =  963040.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36160.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36160.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999210.9 e-6; = (1/var)*||X-X_r||^2 =  999210.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962947.0 e-6; = (1/var)*||X-X_r||^2 =  962946.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36264.0 e-6; = (1/var)*||X-X_r||^2 val-train = -36264.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999245.1 e-6; = (1/var)*||X-X_r||^2 =  999245.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963650.0 e-6; = (1/var)*||X-X_r||^2 =  963650.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945515.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  904374.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35595.1 e-6; = (1/var)*||X-X_r||^2 val-train = -35595.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [21:09:25 05.01.2023] 278) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 7) change_channel_size_across_layers = True:
Total training time is = 0:0:40 h/m/s. 

--------------------------------------------------- 

Started [21:09:25 05.01.2023] 279) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 7) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 3028 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.20
1                           encoder.sequential_convs.conv2d_2.weight                       262             8.65
2                           encoder.sequential_convs.conv2d_3.weight                       262             8.65
3                           encoder.sequential_convs.conv2d_4.weight                       262             8.65
4                           encoder.sequential_convs.conv2d_5.weight                       262             8.65
5                           encoder.sequential_convs.conv2d_6.weight                       262             8.65
6                                  encoder.pre_residual_stack.weight                       147             4.85
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.19
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.19
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
11                             encoder.channel_adjusting_conv.weight                         8             0.26
12                                                       VQ.E.weight                         8             0.26
13                             decoder.channel_adjusting_conv.weight                        73             2.41
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.19
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.13
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.19
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.13
18                    decoder.sequential_trans_convs.conv2d_1.weight                       262             8.65
19                    decoder.sequential_trans_convs.conv2d_2.weight                       262             8.65
20                    decoder.sequential_trans_convs.conv2d_3.weight                       262             8.65
21                    decoder.sequential_trans_convs.conv2d_4.weight                       262             8.65
22                    decoder.sequential_trans_convs.conv2d_5.weight                       262             8.65
23                    decoder.sequential_trans_convs.conv2d_6.weight                         6             0.20

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1025673.7 e-6; = (1/var)*||X-X_r||^2 =  989857.7 e-6 = 96.5 %; (1+beta)*||Z_e-Z_q||^2 =  35816.1 e-6 = 3.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  961196.7 e-6; = (1/var)*||X-X_r||^2 =  960278.1 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  918.6 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  989961.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  946101.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -64477.1 e-6; = (1/var)*||X-X_r||^2 val-train = -29579.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -34897.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.12; perplexity/K = 6.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.64; perplexity/K = 5.19%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1293045.4 e-6; = (1/var)*||X-X_r||^2 =  780211.7 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  512833.7 e-6 = 39.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1121657.5 e-6; = (1/var)*||X-X_r||^2 =  774154.3 e-6 = 69.0 %; (1+beta)*||Z_e-Z_q||^2 =  347503.1 e-6 = 31.0 %)
Min.  Avg. Train Loss across Mini-Batch =  989961.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  946101.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -171387.9 e-6; = (1/var)*||X-X_r||^2 val-train = -6057.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -165330.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.87; perplexity/K = 1.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.53; perplexity/K = 1.19%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  926935.5 e-6; = (1/var)*||X-X_r||^2 =  856475.1 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  70460.3 e-6 = 7.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  903854.8 e-6; = (1/var)*||X-X_r||^2 =  858849.5 e-6 = 95.0 %; (1+beta)*||Z_e-Z_q||^2 =  45005.4 e-6 = 5.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23080.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2374.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25455.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  986351.4 e-6; = (1/var)*||X-X_r||^2 =  986166.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  185.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  958811.2 e-6; = (1/var)*||X-X_r||^2 =  958586.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  224.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27540.2 e-6; = (1/var)*||X-X_r||^2 val-train = -27579.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999573.7 e-6; = (1/var)*||X-X_r||^2 =  999560.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  13.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962982.2 e-6; = (1/var)*||X-X_r||^2 =  962970.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  12.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36591.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36590.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999938.4 e-6; = (1/var)*||X-X_r||^2 =  999935.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963028.8 e-6; = (1/var)*||X-X_r||^2 =  963025.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36909.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36910.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999479.1 e-6; = (1/var)*||X-X_r||^2 =  999478.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963699.7 e-6; = (1/var)*||X-X_r||^2 =  963698.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35779.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35779.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999300.8 e-6; = (1/var)*||X-X_r||^2 =  999299.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962874.7 e-6; = (1/var)*||X-X_r||^2 =  962871.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  3.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36426.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36428.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999319.9 e-6; = (1/var)*||X-X_r||^2 =  999319.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.6 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963100.1 e-6; = (1/var)*||X-X_r||^2 =  963099.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36219.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36219.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999346.0 e-6; = (1/var)*||X-X_r||^2 =  999345.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.9 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962942.3 e-6; = (1/var)*||X-X_r||^2 =  962942.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36403.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36403.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999368.3 e-6; = (1/var)*||X-X_r||^2 =  999368.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963020.6 e-6; = (1/var)*||X-X_r||^2 =  963019.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.6 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36347.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36349.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999333.5 e-6; = (1/var)*||X-X_r||^2 =  999333.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962905.2 e-6; = (1/var)*||X-X_r||^2 =  962905.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36428.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36428.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999302.4 e-6; = (1/var)*||X-X_r||^2 =  999302.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962895.8 e-6; = (1/var)*||X-X_r||^2 =  962895.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36406.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36406.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999277.2 e-6; = (1/var)*||X-X_r||^2 =  999277.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963232.5 e-6; = (1/var)*||X-X_r||^2 =  963232.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36044.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36044.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999222.8 e-6; = (1/var)*||X-X_r||^2 =  999222.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962913.2 e-6; = (1/var)*||X-X_r||^2 =  962913.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36309.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36309.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999218.5 e-6; = (1/var)*||X-X_r||^2 =  999218.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963136.7 e-6; = (1/var)*||X-X_r||^2 =  963136.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36081.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36081.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999208.7 e-6; = (1/var)*||X-X_r||^2 =  999208.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962948.6 e-6; = (1/var)*||X-X_r||^2 =  962948.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36260.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36260.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999232.7 e-6; = (1/var)*||X-X_r||^2 =  999232.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963069.9 e-6; = (1/var)*||X-X_r||^2 =  963069.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36162.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36162.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999402.6 e-6; = (1/var)*||X-X_r||^2 =  999402.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962966.9 e-6; = (1/var)*||X-X_r||^2 =  962966.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36435.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36435.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999205.6 e-6; = (1/var)*||X-X_r||^2 =  999205.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962908.2 e-6; = (1/var)*||X-X_r||^2 =  962908.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  922899.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  877959.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36297.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36297.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [22:16:24 05.01.2023] 279) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 7) change_channel_size_across_layers = False:
Total training time is = 0:0:58 h/m/s. 

--------------------------------------------------- 

Started [22:16:24 05.01.2023] 280) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 7) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(128, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 11588 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.10
1                           encoder.sequential_convs.conv2d_2.weight                      1048             9.04
2                           encoder.sequential_convs.conv2d_3.weight                      1048             9.04
3                           encoder.sequential_convs.conv2d_4.weight                      1048             9.04
4                           encoder.sequential_convs.conv2d_5.weight                      1048             9.04
5                           encoder.sequential_convs.conv2d_6.weight                      1048             9.04
6                                  encoder.pre_residual_stack.weight                       589             5.08
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
11                             encoder.channel_adjusting_conv.weight                        16             0.14
12                                                       VQ.E.weight                         8             0.07
13                             decoder.channel_adjusting_conv.weight                       147             1.27
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.63
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.07
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.63
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.07
18                    decoder.sequential_trans_convs.conv2d_1.weight                      1048             9.04
19                    decoder.sequential_trans_convs.conv2d_2.weight                      1048             9.04
20                    decoder.sequential_trans_convs.conv2d_3.weight                      1048             9.04
21                    decoder.sequential_trans_convs.conv2d_4.weight                      1048             9.04
22                    decoder.sequential_trans_convs.conv2d_5.weight                      1048             9.04
23                    decoder.sequential_trans_convs.conv2d_6.weight                        12             0.10

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1035665.8 e-6; = (1/var)*||X-X_r||^2 =  941961.7 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  93704.1 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962080.8 e-6; = (1/var)*||X-X_r||^2 =  949393.7 e-6 = 98.7 %; (1+beta)*||Z_e-Z_q||^2 =  12687.1 e-6 = 1.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1027154.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962080.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -73585.0 e-6; = (1/var)*||X-X_r||^2 val-train = 7432.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -81017.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.59; perplexity/K = 1.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.26; perplexity/K = 0.99%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  988569.0 e-6; = (1/var)*||X-X_r||^2 =  918563.1 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  70005.9 e-6 = 7.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  945410.2 e-6; = (1/var)*||X-X_r||^2 =  913426.4 e-6 = 96.6 %; (1+beta)*||Z_e-Z_q||^2 =  31983.8 e-6 = 3.4 %)
Min.  Avg. Train Loss across Mini-Batch =  970023.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -43158.8 e-6; = (1/var)*||X-X_r||^2 val-train = -5136.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -38022.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.05; perplexity/K = 0.82%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  967281.3 e-6; = (1/var)*||X-X_r||^2 =  966861.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  419.5 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  939372.8 e-6; = (1/var)*||X-X_r||^2 =  939052.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  320.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -27908.5 e-6; = (1/var)*||X-X_r||^2 val-train = -27809.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -99.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999593.1 e-6; = (1/var)*||X-X_r||^2 =  999586.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  6.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963329.7 e-6; = (1/var)*||X-X_r||^2 =  963325.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36263.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36261.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999647.8 e-6; = (1/var)*||X-X_r||^2 =  999643.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.8 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963560.1 e-6; = (1/var)*||X-X_r||^2 =  963556.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  4.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36087.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36087.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999868.1 e-6; = (1/var)*||X-X_r||^2 =  999867.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962875.7 e-6; = (1/var)*||X-X_r||^2 =  962874.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36992.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36992.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999509.6 e-6; = (1/var)*||X-X_r||^2 =  999509.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963492.0 e-6; = (1/var)*||X-X_r||^2 =  963491.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36017.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36017.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999668.0 e-6; = (1/var)*||X-X_r||^2 =  999668.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962898.3 e-6; = (1/var)*||X-X_r||^2 =  962898.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36769.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36769.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999363.5 e-6; = (1/var)*||X-X_r||^2 =  999363.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963187.3 e-6; = (1/var)*||X-X_r||^2 =  963187.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36176.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36176.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:9:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999255.5 e-6; = (1/var)*||X-X_r||^2 =  999255.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962951.1 e-6; = (1/var)*||X-X_r||^2 =  962951.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36304.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36304.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:16:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999276.5 e-6; = (1/var)*||X-X_r||^2 =  999276.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963257.9 e-6; = (1/var)*||X-X_r||^2 =  963257.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36018.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36018.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:23:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999263.2 e-6; = (1/var)*||X-X_r||^2 =  999263.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963548.8 e-6; = (1/var)*||X-X_r||^2 =  963548.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35714.4 e-6; = (1/var)*||X-X_r||^2 val-train = -35714.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:30:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999241.9 e-6; = (1/var)*||X-X_r||^2 =  999241.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963045.1 e-6; = (1/var)*||X-X_r||^2 =  963045.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36196.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36196.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:36:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999417.3 e-6; = (1/var)*||X-X_r||^2 =  999417.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962906.0 e-6; = (1/var)*||X-X_r||^2 =  962906.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36511.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36511.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:43:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999331.3 e-6; = (1/var)*||X-X_r||^2 =  999331.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962919.9 e-6; = (1/var)*||X-X_r||^2 =  962919.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36411.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36411.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:50:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999209.9 e-6; = (1/var)*||X-X_r||^2 =  999209.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963388.7 e-6; = (1/var)*||X-X_r||^2 =  963388.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35821.2 e-6; = (1/var)*||X-X_r||^2 val-train = -35821.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:57:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999217.9 e-6; = (1/var)*||X-X_r||^2 =  999217.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962886.2 e-6; = (1/var)*||X-X_r||^2 =  962886.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36331.7 e-6; = (1/var)*||X-X_r||^2 val-train = -36331.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:4:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999345.0 e-6; = (1/var)*||X-X_r||^2 =  999345.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963283.1 e-6; = (1/var)*||X-X_r||^2 =  963283.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36061.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36061.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:11:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999282.2 e-6; = (1/var)*||X-X_r||^2 =  999282.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963227.0 e-6; = (1/var)*||X-X_r||^2 =  963227.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36055.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36055.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 0.78%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:18:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999294.9 e-6; = (1/var)*||X-X_r||^2 =  999294.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962940.6 e-6; = (1/var)*||X-X_r||^2 =  962940.6 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  945909.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  903450.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36354.3 e-6; = (1/var)*||X-X_r||^2 val-train = -36354.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

Finished [00:35:31 06.01.2023] 280) Finished running for K = 128 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 7) change_channel_size_across_layers = False:
Total training time is = 0:0:7 h/m/s. 

--------------------------------------------------- 

Started [00:35:31 06.01.2023] 281) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 384) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 720 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.14
1                           encoder.sequential_convs.conv2d_2.weight                        32             4.44
2                           encoder.sequential_convs.conv2d_3.weight                       131            18.19
3                                  encoder.pre_residual_stack.weight                       147            20.42
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.00
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.56
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.00
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.56
8                              encoder.channel_adjusting_conv.weight                         8             1.11
9                                                        VQ.E.weight                         4             0.56
10                             decoder.channel_adjusting_conv.weight                        73            10.14
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             5.00
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.56
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             5.00
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.56
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            18.19
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.44
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.14

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.24; perplexity/K = 22.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.26; perplexity/K = 22.29%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  569785.3 e-6; = (1/var)*||X-X_r||^2 =  262466.9 e-6 = 46.1 %; (1+beta)*||Z_e-Z_q||^2 =  307318.4 e-6 = 53.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  560150.8 e-6; = (1/var)*||X-X_r||^2 =  262493.9 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  297656.8 e-6 = 53.1 %)
Min.  Avg. Train Loss across Mini-Batch =  569785.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  548001.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9634.6 e-6; = (1/var)*||X-X_r||^2 val-train = 27.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9661.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.42; perplexity/K = 25.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.27; perplexity/K = 25.42%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  135565.9 e-6; = (1/var)*||X-X_r||^2 =  100508.5 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  35057.4 e-6 = 25.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  135936.8 e-6; = (1/var)*||X-X_r||^2 =  102495.7 e-6 = 75.4 %; (1+beta)*||Z_e-Z_q||^2 =  33441.1 e-6 = 24.6 %)
Min.  Avg. Train Loss across Mini-Batch =  135565.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  135936.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   370.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1987.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1616.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.74; perplexity/K = 15.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.73; perplexity/K = 15.20%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  74509.2 e-6; = (1/var)*||X-X_r||^2 =  53707.1 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  20802.2 e-6 = 27.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  78090.2 e-6; = (1/var)*||X-X_r||^2 =  57967.8 e-6 = 74.2 %; (1+beta)*||Z_e-Z_q||^2 =  20122.4 e-6 = 25.8 %)
Min.  Avg. Train Loss across Mini-Batch =  74298.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78090.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3580.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4260.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -679.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.74; perplexity/K = 18.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.48; perplexity/K = 17.93%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  56407.5 e-6; = (1/var)*||X-X_r||^2 =  37162.5 e-6 = 65.9 %; (1+beta)*||Z_e-Z_q||^2 =  19245.0 e-6 = 34.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  61070.4 e-6; = (1/var)*||X-X_r||^2 =  41301.2 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  19769.2 e-6 = 32.4 %)
Min.  Avg. Train Loss across Mini-Batch =  56017.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  59969.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4662.9 e-6; = (1/var)*||X-X_r||^2 val-train = 4138.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 524.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.90; perplexity/K = 18.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.75; perplexity/K = 16.80%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50528.2 e-6; = (1/var)*||X-X_r||^2 =  31197.1 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  19331.1 e-6 = 38.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  55622.5 e-6; = (1/var)*||X-X_r||^2 =  34551.0 e-6 = 62.1 %; (1+beta)*||Z_e-Z_q||^2 =  21071.5 e-6 = 37.9 %)
Min.  Avg. Train Loss across Mini-Batch =  50528.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  54557.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5094.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3353.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1740.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.84; perplexity/K = 16.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.51; perplexity/K = 16.43%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  45246.6 e-6; = (1/var)*||X-X_r||^2 =  26886.8 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  18359.8 e-6 = 40.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  49414.4 e-6; = (1/var)*||X-X_r||^2 =  30505.8 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  18908.6 e-6 = 38.3 %)
Min.  Avg. Train Loss across Mini-Batch =  45246.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  49022.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4167.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3619.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 548.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.43; perplexity/K = 17.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.56; perplexity/K = 18.06%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43373.1 e-6; = (1/var)*||X-X_r||^2 =  25843.9 e-6 = 59.6 %; (1+beta)*||Z_e-Z_q||^2 =  17529.2 e-6 = 40.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  49274.1 e-6; = (1/var)*||X-X_r||^2 =  29442.7 e-6 = 59.8 %; (1+beta)*||Z_e-Z_q||^2 =  19831.4 e-6 = 40.2 %)
Min.  Avg. Train Loss across Mini-Batch =  42676.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  46090.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5901.0 e-6; = (1/var)*||X-X_r||^2 val-train = 3598.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2302.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.56; perplexity/K = 14.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.33; perplexity/K = 17.70%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  39818.4 e-6; = (1/var)*||X-X_r||^2 =  23284.5 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  16534.0 e-6 = 41.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  44367.0 e-6; = (1/var)*||X-X_r||^2 =  26827.4 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  17539.6 e-6 = 39.5 %)
Min.  Avg. Train Loss across Mini-Batch =  39818.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42829.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4548.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3543.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1005.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.51; perplexity/K = 14.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.37; perplexity/K = 14.64%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36486.0 e-6; = (1/var)*||X-X_r||^2 =  18753.7 e-6 = 51.4 %; (1+beta)*||Z_e-Z_q||^2 =  17732.3 e-6 = 48.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  40407.4 e-6; = (1/var)*||X-X_r||^2 =  21861.1 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  18546.4 e-6 = 45.9 %)
Min.  Avg. Train Loss across Mini-Batch =  36399.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  39888.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3921.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3107.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 814.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.93; perplexity/K = 13.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.50; perplexity/K = 13.28%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33494.8 e-6; = (1/var)*||X-X_r||^2 =  16215.4 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  17279.4 e-6 = 51.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  37685.9 e-6; = (1/var)*||X-X_r||^2 =  19389.7 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  18296.2 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  33171.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  37326.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4191.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3174.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1016.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.49; perplexity/K = 17.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.90; perplexity/K = 18.59%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32109.0 e-6; = (1/var)*||X-X_r||^2 =  16188.9 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  15920.1 e-6 = 49.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  36206.1 e-6; = (1/var)*||X-X_r||^2 =  18273.8 e-6 = 50.5 %; (1+beta)*||Z_e-Z_q||^2 =  17932.3 e-6 = 49.5 %)
Min.  Avg. Train Loss across Mini-Batch =  31118.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35258.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4097.1 e-6; = (1/var)*||X-X_r||^2 val-train = 2084.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2012.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.68; perplexity/K = 16.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.68; perplexity/K = 15.13%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30198.9 e-6; = (1/var)*||X-X_r||^2 =  14482.3 e-6 = 48.0 %; (1+beta)*||Z_e-Z_q||^2 =  15716.6 e-6 = 52.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  37476.4 e-6; = (1/var)*||X-X_r||^2 =  18808.3 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  18668.1 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  29460.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33858.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7277.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4326.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2951.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.62; perplexity/K = 15.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.03; perplexity/K = 15.68%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29216.0 e-6; = (1/var)*||X-X_r||^2 =  14171.9 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  15044.1 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  33002.8 e-6; = (1/var)*||X-X_r||^2 =  16759.6 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  16243.2 e-6 = 49.2 %)
Min.  Avg. Train Loss across Mini-Batch =  28027.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32286.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3786.8 e-6; = (1/var)*||X-X_r||^2 val-train = 2587.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1199.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.99; perplexity/K = 15.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.16; perplexity/K = 15.87%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27144.9 e-6; = (1/var)*||X-X_r||^2 =  13261.4 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  13883.5 e-6 = 51.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  32792.3 e-6; = (1/var)*||X-X_r||^2 =  16336.8 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  16455.5 e-6 = 50.2 %)
Min.  Avg. Train Loss across Mini-Batch =  26987.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5647.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3075.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2572.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.51; perplexity/K = 19.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.98; perplexity/K = 17.16%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28374.4 e-6; = (1/var)*||X-X_r||^2 =  13873.8 e-6 = 48.9 %; (1+beta)*||Z_e-Z_q||^2 =  14500.6 e-6 = 51.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  35008.8 e-6; = (1/var)*||X-X_r||^2 =  17363.1 e-6 = 49.6 %; (1+beta)*||Z_e-Z_q||^2 =  17645.7 e-6 = 50.4 %)
Min.  Avg. Train Loss across Mini-Batch =  26425.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30586.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6634.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3489.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3145.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.57; perplexity/K = 18.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.24; perplexity/K = 17.57%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25961.8 e-6; = (1/var)*||X-X_r||^2 =  12516.9 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  13444.9 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  30907.2 e-6; = (1/var)*||X-X_r||^2 =  15523.3 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  15383.9 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  25687.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30132.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4945.4 e-6; = (1/var)*||X-X_r||^2 val-train = 3006.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1939.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 20.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.91; perplexity/K = 20.17%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25279.8 e-6; = (1/var)*||X-X_r||^2 =  12042.2 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  13237.6 e-6 = 52.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  29785.3 e-6; = (1/var)*||X-X_r||^2 =  14900.8 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  14884.6 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  25267.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29496.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4505.6 e-6; = (1/var)*||X-X_r||^2 val-train = 2858.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1647.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.95; perplexity/K = 17.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.92; perplexity/K = 17.06%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24760.1 e-6; = (1/var)*||X-X_r||^2 =  11909.4 e-6 = 48.1 %; (1+beta)*||Z_e-Z_q||^2 =  12850.7 e-6 = 51.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  29042.1 e-6; = (1/var)*||X-X_r||^2 =  14814.5 e-6 = 51.0 %; (1+beta)*||Z_e-Z_q||^2 =  14227.6 e-6 = 49.0 %)
Min.  Avg. Train Loss across Mini-Batch =  24760.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29042.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4282.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2905.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1376.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.77; perplexity/K = 18.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 20.43%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24648.2 e-6; = (1/var)*||X-X_r||^2 =  11619.0 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  13029.2 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  29642.4 e-6; = (1/var)*||X-X_r||^2 =  14947.4 e-6 = 50.4 %; (1+beta)*||Z_e-Z_q||^2 =  14695.0 e-6 = 49.6 %)
Min.  Avg. Train Loss across Mini-Batch =  24465.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28642.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4994.1 e-6; = (1/var)*||X-X_r||^2 val-train = 3328.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1665.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.00; perplexity/K = 20.31%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.30; perplexity/K = 20.79%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24566.4 e-6; = (1/var)*||X-X_r||^2 =  11901.2 e-6 = 48.4 %; (1+beta)*||Z_e-Z_q||^2 =  12665.2 e-6 = 51.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  29369.1 e-6; = (1/var)*||X-X_r||^2 =  14431.7 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  14937.4 e-6 = 50.9 %)
Min.  Avg. Train Loss across Mini-Batch =  24465.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28109.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4802.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2530.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2272.2 e-6 

----------------------------------------------------------------------------------

Finished [01:24:23 06.01.2023] 281) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 384) change_channel_size_across_layers = True:
Total training time is = 0:7:51 h/m/s. 

--------------------------------------------------- 

Started [01:24:23 06.01.2023] 282) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 384) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2396 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.13
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.47
2                           encoder.sequential_convs.conv2d_3.weight                       524            21.87
3                                  encoder.pre_residual_stack.weight                       589            24.58
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.05
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.05
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
8                              encoder.channel_adjusting_conv.weight                        16             0.67
9                                                        VQ.E.weight                         4             0.17
10                             decoder.channel_adjusting_conv.weight                       147             6.14
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             3.05
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             3.05
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.87
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.47
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.98; perplexity/K = 17.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.69; perplexity/K = 16.71%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  536357.7 e-6; = (1/var)*||X-X_r||^2 =  218802.9 e-6 = 40.8 %; (1+beta)*||Z_e-Z_q||^2 =  317554.9 e-6 = 59.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  512852.7 e-6; = (1/var)*||X-X_r||^2 =  219145.3 e-6 = 42.7 %; (1+beta)*||Z_e-Z_q||^2 =  293707.4 e-6 = 57.3 %)
Min.  Avg. Train Loss across Mini-Batch =  536357.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  512852.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -23505.1 e-6; = (1/var)*||X-X_r||^2 val-train = 342.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -23847.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.36; perplexity/K = 24.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.22; perplexity/K = 23.77%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  102126.1 e-6; = (1/var)*||X-X_r||^2 =  59183.4 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  42942.7 e-6 = 42.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  107355.4 e-6; = (1/var)*||X-X_r||^2 =  65000.0 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  42355.4 e-6 = 39.5 %)
Min.  Avg. Train Loss across Mini-Batch =  102126.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  107315.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5229.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5816.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -587.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.26; perplexity/K = 26.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.51; perplexity/K = 25.80%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  41968.7 e-6; = (1/var)*||X-X_r||^2 =  20545.8 e-6 = 49.0 %; (1+beta)*||Z_e-Z_q||^2 =  21422.9 e-6 = 51.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  45684.6 e-6; = (1/var)*||X-X_r||^2 =  24513.8 e-6 = 53.7 %; (1+beta)*||Z_e-Z_q||^2 =  21170.7 e-6 = 46.3 %)
Min.  Avg. Train Loss across Mini-Batch =  41968.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  45684.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3715.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3968.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -252.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.60; perplexity/K = 21.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.87; perplexity/K = 20.12%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  31973.3 e-6; = (1/var)*||X-X_r||^2 =  12616.8 e-6 = 39.5 %; (1+beta)*||Z_e-Z_q||^2 =  19356.5 e-6 = 60.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  34651.2 e-6; = (1/var)*||X-X_r||^2 =  15190.0 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  19461.2 e-6 = 56.2 %)
Min.  Avg. Train Loss across Mini-Batch =  23824.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26700.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2677.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2573.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 104.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.05; perplexity/K = 18.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.82; perplexity/K = 18.48%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12247.7 e-6; = (1/var)*||X-X_r||^2 =  3550.6 e-6 = 29.0 %; (1+beta)*||Z_e-Z_q||^2 =  8697.1 e-6 = 71.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  14364.2 e-6; = (1/var)*||X-X_r||^2 =  5452.3 e-6 = 38.0 %; (1+beta)*||Z_e-Z_q||^2 =  8911.9 e-6 = 62.0 %)
Min.  Avg. Train Loss across Mini-Batch =  12247.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  14364.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2116.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1901.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 214.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.28; perplexity/K = 8.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.80; perplexity/K = 7.51%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1312931.3 e-6; = (1/var)*||X-X_r||^2 =  418585.7 e-6 = 31.9 %; (1+beta)*||Z_e-Z_q||^2 =  894345.7 e-6 = 68.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  375369.6 e-6; = (1/var)*||X-X_r||^2 =  282551.9 e-6 = 75.3 %; (1+beta)*||Z_e-Z_q||^2 =  92817.6 e-6 = 24.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7106.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8745.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -937561.8 e-6; = (1/var)*||X-X_r||^2 val-train = -136033.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -801528.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.41; perplexity/K = 24.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.32; perplexity/K = 22.38%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34548.7 e-6; = (1/var)*||X-X_r||^2 =  12307.2 e-6 = 35.6 %; (1+beta)*||Z_e-Z_q||^2 =  22241.5 e-6 = 64.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  14496.9 e-6; = (1/var)*||X-X_r||^2 =  5749.9 e-6 = 39.7 %; (1+beta)*||Z_e-Z_q||^2 =  8747.0 e-6 = 60.3 %)
Min.  Avg. Train Loss across Mini-Batch =  7106.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8745.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -20051.9 e-6; = (1/var)*||X-X_r||^2 val-train = -6557.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13494.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.92; perplexity/K = 24.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.70; perplexity/K = 24.53%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16406.9 e-6; = (1/var)*||X-X_r||^2 =  7162.8 e-6 = 43.7 %; (1+beta)*||Z_e-Z_q||^2 =  9244.1 e-6 = 56.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  17644.4 e-6; = (1/var)*||X-X_r||^2 =  8344.2 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  9300.3 e-6 = 52.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4033.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4634.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1237.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1181.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 56.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 19.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.48; perplexity/K = 17.94%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3516.4 e-6; = (1/var)*||X-X_r||^2 =  1135.9 e-6 = 32.3 %; (1+beta)*||Z_e-Z_q||^2 =  2380.5 e-6 = 67.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4352.2 e-6; = (1/var)*||X-X_r||^2 =  1926.7 e-6 = 44.3 %; (1+beta)*||Z_e-Z_q||^2 =  2425.5 e-6 = 55.7 %)
Min.  Avg. Train Loss across Mini-Batch =  3516.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4352.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   835.8 e-6; = (1/var)*||X-X_r||^2 val-train = 790.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 45.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.15; perplexity/K = 18.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.27; perplexity/K = 19.17%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2347.0 e-6; = (1/var)*||X-X_r||^2 =  653.5 e-6 = 27.8 %; (1+beta)*||Z_e-Z_q||^2 =  1693.5 e-6 = 72.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3020.4 e-6; = (1/var)*||X-X_r||^2 =  1300.7 e-6 = 43.1 %; (1+beta)*||Z_e-Z_q||^2 =  1719.7 e-6 = 56.9 %)
Min.  Avg. Train Loss across Mini-Batch =  2131.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2899.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   673.4 e-6; = (1/var)*||X-X_r||^2 val-train = 647.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.65; perplexity/K = 18.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.54; perplexity/K = 16.46%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1512.9 e-6; = (1/var)*||X-X_r||^2 =  435.6 e-6 = 28.8 %; (1+beta)*||Z_e-Z_q||^2 =  1077.4 e-6 = 71.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2772.3 e-6; = (1/var)*||X-X_r||^2 =  1345.7 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  1426.7 e-6 = 51.5 %)
Min.  Avg. Train Loss across Mini-Batch =  1430.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1975.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1259.4 e-6; = (1/var)*||X-X_r||^2 val-train = 910.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 349.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.73; perplexity/K = 15.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.97; perplexity/K = 12.46%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1384.5 e-6; = (1/var)*||X-X_r||^2 =  325.3 e-6 = 23.5 %; (1+beta)*||Z_e-Z_q||^2 =  1059.2 e-6 = 76.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1897.1 e-6; = (1/var)*||X-X_r||^2 =  735.3 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  1161.8 e-6 = 61.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1350.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1823.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   512.6 e-6; = (1/var)*||X-X_r||^2 val-train = 410.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.35; perplexity/K = 14.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.24; perplexity/K = 14.44%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1853.7 e-6; = (1/var)*||X-X_r||^2 =  365.3 e-6 = 19.7 %; (1+beta)*||Z_e-Z_q||^2 =  1488.4 e-6 = 80.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2359.5 e-6; = (1/var)*||X-X_r||^2 =  848.8 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  1510.7 e-6 = 64.0 %)
Min.  Avg. Train Loss across Mini-Batch =  979.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1446.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   505.8 e-6; = (1/var)*||X-X_r||^2 val-train = 483.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.27; perplexity/K = 19.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.43; perplexity/K = 17.85%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1286.1 e-6; = (1/var)*||X-X_r||^2 =  289.6 e-6 = 22.5 %; (1+beta)*||Z_e-Z_q||^2 =  996.5 e-6 = 77.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1931.8 e-6; = (1/var)*||X-X_r||^2 =  804.4 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  1127.4 e-6 = 58.4 %)
Min.  Avg. Train Loss across Mini-Batch =  979.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1446.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   645.7 e-6; = (1/var)*||X-X_r||^2 val-train = 514.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 130.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.81; perplexity/K = 18.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.72; perplexity/K = 16.75%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6257.8 e-6; = (1/var)*||X-X_r||^2 =  1674.3 e-6 = 26.8 %; (1+beta)*||Z_e-Z_q||^2 =  4583.5 e-6 = 73.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  6895.3 e-6; = (1/var)*||X-X_r||^2 =  2519.9 e-6 = 36.5 %; (1+beta)*||Z_e-Z_q||^2 =  4375.4 e-6 = 63.5 %)
Min.  Avg. Train Loss across Mini-Batch =  781.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1139.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   637.5 e-6; = (1/var)*||X-X_r||^2 val-train = 845.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -208.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.20; perplexity/K = 17.50%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.89; perplexity/K = 17.02%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1197.3 e-6; = (1/var)*||X-X_r||^2 =  221.1 e-6 = 18.5 %; (1+beta)*||Z_e-Z_q||^2 =  976.2 e-6 = 81.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1641.4 e-6; = (1/var)*||X-X_r||^2 =  650.5 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  990.9 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  781.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1139.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   444.2 e-6; = (1/var)*||X-X_r||^2 val-train = 429.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.45; perplexity/K = 14.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.50; perplexity/K = 13.28%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1147.3 e-6; = (1/var)*||X-X_r||^2 =  178.9 e-6 = 15.6 %; (1+beta)*||Z_e-Z_q||^2 =  968.5 e-6 = 84.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1586.2 e-6; = (1/var)*||X-X_r||^2 =  634.9 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  951.3 e-6 = 60.0 %)
Min.  Avg. Train Loss across Mini-Batch =  664.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  992.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   438.9 e-6; = (1/var)*||X-X_r||^2 val-train = 456.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.55; perplexity/K = 14.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.17; perplexity/K = 15.89%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  920.1 e-6; = (1/var)*||X-X_r||^2 =  200.5 e-6 = 21.8 %; (1+beta)*||Z_e-Z_q||^2 =  719.6 e-6 = 78.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  1190.9 e-6; = (1/var)*||X-X_r||^2 =  487.2 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  703.8 e-6 = 59.1 %)
Min.  Avg. Train Loss across Mini-Batch =  605.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  931.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   270.8 e-6; = (1/var)*||X-X_r||^2 val-train = 286.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.50; perplexity/K = 13.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.39; perplexity/K = 13.11%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  845.1 e-6; = (1/var)*||X-X_r||^2 =  131.4 e-6 = 15.5 %; (1+beta)*||Z_e-Z_q||^2 =  713.7 e-6 = 84.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1216.8 e-6; = (1/var)*||X-X_r||^2 =  481.8 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  735.0 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  528.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  836.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   371.8 e-6; = (1/var)*||X-X_r||^2 val-train = 350.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.26; perplexity/K = 12.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.68; perplexity/K = 12.01%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  532.5 e-6; = (1/var)*||X-X_r||^2 =  98.7 e-6 = 18.5 %; (1+beta)*||Z_e-Z_q||^2 =  433.8 e-6 = 81.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  813.0 e-6; = (1/var)*||X-X_r||^2 =  361.3 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  451.7 e-6 = 55.6 %)
Min.  Avg. Train Loss across Mini-Batch =  528.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  813.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   280.4 e-6; = (1/var)*||X-X_r||^2 val-train = 262.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.8 e-6 

----------------------------------------------------------------------------------

Finished [02:30:29 06.01.2023] 282) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 384) change_channel_size_across_layers = True:
Total training time is = 0:7:6 h/m/s. 

--------------------------------------------------- 

Started [02:30:29 06.01.2023] 283) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 384) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1452 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.41
1                           encoder.sequential_convs.conv2d_2.weight                       262            18.04
2                           encoder.sequential_convs.conv2d_3.weight                       262            18.04
3                                  encoder.pre_residual_stack.weight                       147            10.12
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.48
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.28
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.48
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.28
8                              encoder.channel_adjusting_conv.weight                         8             0.55
9                                                        VQ.E.weight                         4             0.28
10                             decoder.channel_adjusting_conv.weight                        73             5.03
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             2.48
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.28
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             2.48
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.28
15                    decoder.sequential_trans_convs.conv2d_1.weight                       262            18.04
16                    decoder.sequential_trans_convs.conv2d_2.weight                       262            18.04
17                    decoder.sequential_trans_convs.conv2d_3.weight                         6             0.41

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.40; perplexity/K = 27.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.24; perplexity/K = 26.94%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  490918.7 e-6; = (1/var)*||X-X_r||^2 =  254193.6 e-6 = 51.8 %; (1+beta)*||Z_e-Z_q||^2 =  236725.1 e-6 = 48.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  459717.9 e-6; = (1/var)*||X-X_r||^2 =  247520.9 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  212196.9 e-6 = 46.2 %)
Min.  Avg. Train Loss across Mini-Batch =  458823.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  457409.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31200.8 e-6; = (1/var)*||X-X_r||^2 val-train = -6672.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -24528.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.77; perplexity/K = 37.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.70; perplexity/K = 35.47%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  72804.5 e-6; = (1/var)*||X-X_r||^2 =  49187.8 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  23616.7 e-6 = 32.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  78306.2 e-6; = (1/var)*||X-X_r||^2 =  55875.8 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  22430.4 e-6 = 28.6 %)
Min.  Avg. Train Loss across Mini-Batch =  72804.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  78306.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5501.7 e-6; = (1/var)*||X-X_r||^2 val-train = 6688.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1186.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.74; perplexity/K = 26.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.36; perplexity/K = 24.00%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  144174.3 e-6; = (1/var)*||X-X_r||^2 =  112429.5 e-6 = 78.0 %; (1+beta)*||Z_e-Z_q||^2 =  31744.8 e-6 = 22.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  142912.8 e-6; = (1/var)*||X-X_r||^2 =  113110.6 e-6 = 79.1 %; (1+beta)*||Z_e-Z_q||^2 =  29802.3 e-6 = 20.9 %)
Min.  Avg. Train Loss across Mini-Batch =  30495.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  34888.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1261.5 e-6; = (1/var)*||X-X_r||^2 val-train = 681.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1942.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.88; perplexity/K = 24.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.41; perplexity/K = 24.08%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28089.5 e-6; = (1/var)*||X-X_r||^2 =  18954.0 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  9135.5 e-6 = 32.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  31753.0 e-6; = (1/var)*||X-X_r||^2 =  22214.2 e-6 = 70.0 %; (1+beta)*||Z_e-Z_q||^2 =  9538.7 e-6 = 30.0 %)
Min.  Avg. Train Loss across Mini-Batch =  28089.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  31594.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3663.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3260.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 403.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.33; perplexity/K = 23.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.87; perplexity/K = 21.67%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14773.4 e-6; = (1/var)*||X-X_r||^2 =  9021.1 e-6 = 61.1 %; (1+beta)*||Z_e-Z_q||^2 =  5752.4 e-6 = 38.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  17279.8 e-6; = (1/var)*||X-X_r||^2 =  11395.3 e-6 = 65.9 %; (1+beta)*||Z_e-Z_q||^2 =  5884.5 e-6 = 34.1 %)
Min.  Avg. Train Loss across Mini-Batch =  14773.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  17279.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2506.4 e-6; = (1/var)*||X-X_r||^2 val-train = 2374.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 132.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.45; perplexity/K = 21.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.37; perplexity/K = 17.77%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10479.5 e-6; = (1/var)*||X-X_r||^2 =  5963.8 e-6 = 56.9 %; (1+beta)*||Z_e-Z_q||^2 =  4515.7 e-6 = 43.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  12976.4 e-6; = (1/var)*||X-X_r||^2 =  8008.5 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  4967.8 e-6 = 38.3 %)
Min.  Avg. Train Loss across Mini-Batch =  10075.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  12235.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2496.9 e-6; = (1/var)*||X-X_r||^2 val-train = 2044.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 452.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.46; perplexity/K = 19.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.28; perplexity/K = 19.18%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7964.7 e-6; = (1/var)*||X-X_r||^2 =  4431.9 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  3532.9 e-6 = 44.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  9878.2 e-6; = (1/var)*||X-X_r||^2 =  6252.2 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  3626.1 e-6 = 36.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7906.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  9830.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1913.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1820.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 93.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.52; perplexity/K = 24.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.19; perplexity/K = 23.73%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  85196.6 e-6; = (1/var)*||X-X_r||^2 =  37546.3 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  47650.3 e-6 = 55.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  40443.7 e-6; = (1/var)*||X-X_r||^2 =  21596.8 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  18846.9 e-6 = 46.6 %)
Min.  Avg. Train Loss across Mini-Batch =  6063.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7742.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -44752.9 e-6; = (1/var)*||X-X_r||^2 val-train = -15949.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -28803.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.34; perplexity/K = 22.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.94; perplexity/K = 21.78%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9719.8 e-6; = (1/var)*||X-X_r||^2 =  4591.9 e-6 = 47.2 %; (1+beta)*||Z_e-Z_q||^2 =  5128.0 e-6 = 52.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  11104.6 e-6; = (1/var)*||X-X_r||^2 =  6100.7 e-6 = 54.9 %; (1+beta)*||Z_e-Z_q||^2 =  5003.9 e-6 = 45.1 %)
Min.  Avg. Train Loss across Mini-Batch =  5225.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6661.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1384.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1508.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -124.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.90; perplexity/K = 20.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.67; perplexity/K = 18.24%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4086.0 e-6; = (1/var)*||X-X_r||^2 =  2131.4 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  1954.6 e-6 = 47.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  5313.5 e-6; = (1/var)*||X-X_r||^2 =  3309.2 e-6 = 62.3 %; (1+beta)*||Z_e-Z_q||^2 =  2004.3 e-6 = 37.7 %)
Min.  Avg. Train Loss across Mini-Batch =  4086.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5313.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1227.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1177.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 49.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.06; perplexity/K = 15.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.51; perplexity/K = 16.42%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5533.2 e-6; = (1/var)*||X-X_r||^2 =  2681.4 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  2851.8 e-6 = 51.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  6863.8 e-6; = (1/var)*||X-X_r||^2 =  4012.7 e-6 = 58.5 %; (1+beta)*||Z_e-Z_q||^2 =  2851.1 e-6 = 41.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3046.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4195.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1330.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1331.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.03; perplexity/K = 17.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.54; perplexity/K = 16.47%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3005.8 e-6; = (1/var)*||X-X_r||^2 =  1422.2 e-6 = 47.3 %; (1+beta)*||Z_e-Z_q||^2 =  1583.6 e-6 = 52.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  4407.0 e-6; = (1/var)*||X-X_r||^2 =  2598.5 e-6 = 59.0 %; (1+beta)*||Z_e-Z_q||^2 =  1808.5 e-6 = 41.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2935.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3987.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1401.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1176.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 224.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.56; perplexity/K = 21.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.11; perplexity/K = 20.48%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3132.7 e-6; = (1/var)*||X-X_r||^2 =  1333.3 e-6 = 42.6 %; (1+beta)*||Z_e-Z_q||^2 =  1799.3 e-6 = 57.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3908.3 e-6; = (1/var)*||X-X_r||^2 =  2160.6 e-6 = 55.3 %; (1+beta)*||Z_e-Z_q||^2 =  1747.7 e-6 = 44.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2496.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3382.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   775.6 e-6; = (1/var)*||X-X_r||^2 val-train = 827.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -51.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.86; perplexity/K = 12.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.80; perplexity/K = 12.19%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2605.7 e-6; = (1/var)*||X-X_r||^2 =  1146.5 e-6 = 44.0 %; (1+beta)*||Z_e-Z_q||^2 =  1459.2 e-6 = 56.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3277.7 e-6; = (1/var)*||X-X_r||^2 =  1900.2 e-6 = 58.0 %; (1+beta)*||Z_e-Z_q||^2 =  1377.5 e-6 = 42.0 %)
Min.  Avg. Train Loss across Mini-Batch =  2395.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3245.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   672.0 e-6; = (1/var)*||X-X_r||^2 val-train = 753.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -81.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.17; perplexity/K = 14.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.83; perplexity/K = 13.80%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2085.7 e-6; = (1/var)*||X-X_r||^2 =  948.9 e-6 = 45.5 %; (1+beta)*||Z_e-Z_q||^2 =  1136.8 e-6 = 54.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2955.4 e-6; = (1/var)*||X-X_r||^2 =  1716.1 e-6 = 58.1 %; (1+beta)*||Z_e-Z_q||^2 =  1239.3 e-6 = 41.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1918.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2714.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   869.7 e-6; = (1/var)*||X-X_r||^2 val-train = 767.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.02; perplexity/K = 14.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.59; perplexity/K = 14.98%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1817.6 e-6; = (1/var)*||X-X_r||^2 =  864.4 e-6 = 47.6 %; (1+beta)*||Z_e-Z_q||^2 =  953.2 e-6 = 52.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2673.4 e-6; = (1/var)*||X-X_r||^2 =  1573.2 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  1100.1 e-6 = 41.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1700.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2370.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   855.7 e-6; = (1/var)*||X-X_r||^2 val-train = 708.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 146.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.77; perplexity/K = 15.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.33; perplexity/K = 16.13%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2362.8 e-6; = (1/var)*||X-X_r||^2 =  966.9 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  1395.9 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3023.2 e-6; = (1/var)*||X-X_r||^2 =  1666.7 e-6 = 55.1 %; (1+beta)*||Z_e-Z_q||^2 =  1356.5 e-6 = 44.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1468.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2186.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   660.4 e-6; = (1/var)*||X-X_r||^2 val-train = 699.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -39.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.40; perplexity/K = 16.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.64; perplexity/K = 16.62%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:59:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1798.5 e-6; = (1/var)*||X-X_r||^2 =  697.0 e-6 = 38.8 %; (1+beta)*||Z_e-Z_q||^2 =  1101.5 e-6 = 61.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  2480.7 e-6; = (1/var)*||X-X_r||^2 =  1381.7 e-6 = 55.7 %; (1+beta)*||Z_e-Z_q||^2 =  1099.0 e-6 = 44.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1362.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2023.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   682.2 e-6; = (1/var)*||X-X_r||^2 val-train = 684.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.38; perplexity/K = 16.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.94; perplexity/K = 15.53%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1579.3 e-6; = (1/var)*||X-X_r||^2 =  631.8 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  947.5 e-6 = 60.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  2201.4 e-6; = (1/var)*||X-X_r||^2 =  1226.8 e-6 = 55.7 %; (1+beta)*||Z_e-Z_q||^2 =  974.5 e-6 = 44.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1362.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2023.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   622.1 e-6; = (1/var)*||X-X_r||^2 val-train = 595.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.35; perplexity/K = 16.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.39; perplexity/K = 16.24%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4217.1 e-6; = (1/var)*||X-X_r||^2 =  1849.0 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  2368.1 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4705.9 e-6; = (1/var)*||X-X_r||^2 =  2544.7 e-6 = 54.1 %; (1+beta)*||Z_e-Z_q||^2 =  2161.2 e-6 = 45.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1207.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1747.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   488.7 e-6; = (1/var)*||X-X_r||^2 val-train = 695.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -206.9 e-6 

----------------------------------------------------------------------------------

Finished [03:36:52 06.01.2023] 283) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 384) change_channel_size_across_layers = False:
Total training time is = 0:7:22 h/m/s. 

--------------------------------------------------- 

Started [03:36:52 06.01.2023] 284) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 384) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 5296 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.23
1                           encoder.sequential_convs.conv2d_2.weight                      1048            19.79
2                           encoder.sequential_convs.conv2d_3.weight                      1048            19.79
3                                  encoder.pre_residual_stack.weight                       589            11.12
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
8                              encoder.channel_adjusting_conv.weight                        16             0.30
9                                                        VQ.E.weight                         4             0.08
10                             decoder.channel_adjusting_conv.weight                       147             2.78
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             1.38
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.15
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             1.38
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.15
15                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            19.79
16                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            19.79
17                    decoder.sequential_trans_convs.conv2d_3.weight                        12             0.23

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.73; perplexity/K = 32.40%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.04; perplexity/K = 31.32%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  231322.6 e-6; = (1/var)*||X-X_r||^2 =  99494.8 e-6 = 43.0 %; (1+beta)*||Z_e-Z_q||^2 =  131827.8 e-6 = 57.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  230327.3 e-6; = (1/var)*||X-X_r||^2 =  104271.4 e-6 = 45.3 %; (1+beta)*||Z_e-Z_q||^2 =  126055.9 e-6 = 54.7 %)
Min.  Avg. Train Loss across Mini-Batch =  231322.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  230327.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -995.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4776.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5772.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.33; perplexity/K = 25.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.13; perplexity/K = 26.77%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  94066.5 e-6; = (1/var)*||X-X_r||^2 =  45439.5 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  48627.0 e-6 = 51.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  92713.9 e-6; = (1/var)*||X-X_r||^2 =  46044.1 e-6 = 49.7 %; (1+beta)*||Z_e-Z_q||^2 =  46669.9 e-6 = 50.3 %)
Min.  Avg. Train Loss across Mini-Batch =  39754.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  44892.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1352.6 e-6; = (1/var)*||X-X_r||^2 val-train = 604.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1957.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.97; perplexity/K = 28.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.43; perplexity/K = 24.11%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9439.8 e-6; = (1/var)*||X-X_r||^2 =  4477.9 e-6 = 47.4 %; (1+beta)*||Z_e-Z_q||^2 =  4961.9 e-6 = 52.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  11920.1 e-6; = (1/var)*||X-X_r||^2 =  6817.4 e-6 = 57.2 %; (1+beta)*||Z_e-Z_q||^2 =  5102.6 e-6 = 42.8 %)
Min.  Avg. Train Loss across Mini-Batch =  9258.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  11541.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2480.2 e-6; = (1/var)*||X-X_r||^2 val-train = 2339.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 140.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.64; perplexity/K = 27.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.40; perplexity/K = 25.63%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5400.8 e-6; = (1/var)*||X-X_r||^2 =  2512.3 e-6 = 46.5 %; (1+beta)*||Z_e-Z_q||^2 =  2888.5 e-6 = 53.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  7846.1 e-6; = (1/var)*||X-X_r||^2 =  4704.6 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  3141.5 e-6 = 40.0 %)
Min.  Avg. Train Loss across Mini-Batch =  4155.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5546.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2445.3 e-6; = (1/var)*||X-X_r||^2 val-train = 2192.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 253.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.74; perplexity/K = 24.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.69; perplexity/K = 26.08%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2488.0 e-6; = (1/var)*||X-X_r||^2 =  1054.6 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  1433.4 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  3617.4 e-6; = (1/var)*||X-X_r||^2 =  2081.7 e-6 = 57.5 %; (1+beta)*||Z_e-Z_q||^2 =  1535.6 e-6 = 42.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2444.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3544.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1129.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1027.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 102.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.37; perplexity/K = 25.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.19; perplexity/K = 25.30%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4068.3 e-6; = (1/var)*||X-X_r||^2 =  1334.1 e-6 = 32.8 %; (1+beta)*||Z_e-Z_q||^2 =  2734.2 e-6 = 67.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  5057.6 e-6; = (1/var)*||X-X_r||^2 =  2135.0 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  2922.6 e-6 = 57.8 %)
Min.  Avg. Train Loss across Mini-Batch =  2385.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3544.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   989.2 e-6; = (1/var)*||X-X_r||^2 val-train = 800.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 188.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.48; perplexity/K = 30.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.81; perplexity/K = 29.39%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49656.9 e-6; = (1/var)*||X-X_r||^2 =  18191.2 e-6 = 36.6 %; (1+beta)*||Z_e-Z_q||^2 =  31465.7 e-6 = 63.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  53017.7 e-6; = (1/var)*||X-X_r||^2 =  21521.5 e-6 = 40.6 %; (1+beta)*||Z_e-Z_q||^2 =  31496.2 e-6 = 59.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1812.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2511.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3360.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3330.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 30.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.48; perplexity/K = 14.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.14; perplexity/K = 15.84%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47561.0 e-6; = (1/var)*||X-X_r||^2 =  14327.7 e-6 = 30.1 %; (1+beta)*||Z_e-Z_q||^2 =  33233.3 e-6 = 69.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  43412.0 e-6; = (1/var)*||X-X_r||^2 =  14505.4 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  28906.6 e-6 = 66.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1812.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2511.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4149.0 e-6; = (1/var)*||X-X_r||^2 val-train = 177.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4326.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.66; perplexity/K = 15.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.86; perplexity/K = 15.40%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2166.9 e-6; = (1/var)*||X-X_r||^2 =  681.6 e-6 = 31.5 %; (1+beta)*||Z_e-Z_q||^2 =  1485.3 e-6 = 68.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2918.1 e-6; = (1/var)*||X-X_r||^2 =  1453.3 e-6 = 49.8 %; (1+beta)*||Z_e-Z_q||^2 =  1464.9 e-6 = 50.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1812.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2511.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   751.2 e-6; = (1/var)*||X-X_r||^2 val-train = 771.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -20.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.38; perplexity/K = 20.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.80; perplexity/K = 20.00%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:6 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1245.5 e-6; = (1/var)*||X-X_r||^2 =  415.7 e-6 = 33.4 %; (1+beta)*||Z_e-Z_q||^2 =  829.8 e-6 = 66.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2031.8 e-6; = (1/var)*||X-X_r||^2 =  1195.3 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  836.5 e-6 = 41.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1245.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1871.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   786.3 e-6; = (1/var)*||X-X_r||^2 val-train = 779.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.54; perplexity/K = 21.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.05; perplexity/K = 21.95%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  957.4 e-6; = (1/var)*||X-X_r||^2 =  283.1 e-6 = 29.6 %; (1+beta)*||Z_e-Z_q||^2 =  674.2 e-6 = 70.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1410.6 e-6; = (1/var)*||X-X_r||^2 =  736.6 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  674.1 e-6 = 47.8 %)
Min.  Avg. Train Loss across Mini-Batch =  737.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1267.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   453.3 e-6; = (1/var)*||X-X_r||^2 val-train = 453.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.17; perplexity/K = 17.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.87; perplexity/K = 15.42%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  759.1 e-6; = (1/var)*||X-X_r||^2 =  227.3 e-6 = 29.9 %; (1+beta)*||Z_e-Z_q||^2 =  531.7 e-6 = 70.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1277.3 e-6; = (1/var)*||X-X_r||^2 =  745.7 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  531.6 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  435.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  917.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   518.2 e-6; = (1/var)*||X-X_r||^2 val-train = 518.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.59; perplexity/K = 18.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.52; perplexity/K = 18.00%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  529.5 e-6; = (1/var)*||X-X_r||^2 =  262.3 e-6 = 49.5 %; (1+beta)*||Z_e-Z_q||^2 =  267.3 e-6 = 50.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  877.7 e-6; = (1/var)*||X-X_r||^2 =  613.3 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  264.4 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  398.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  797.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   348.1 e-6; = (1/var)*||X-X_r||^2 val-train = 351.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.67; perplexity/K = 24.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.88; perplexity/K = 23.26%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2864.9 e-6; = (1/var)*||X-X_r||^2 =  822.9 e-6 = 28.7 %; (1+beta)*||Z_e-Z_q||^2 =  2041.9 e-6 = 71.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3560.3 e-6; = (1/var)*||X-X_r||^2 =  1538.9 e-6 = 43.2 %; (1+beta)*||Z_e-Z_q||^2 =  2021.4 e-6 = 56.8 %)
Min.  Avg. Train Loss across Mini-Batch =  255.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  624.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   695.5 e-6; = (1/var)*||X-X_r||^2 val-train = 716.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -20.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.86; perplexity/K = 23.22%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.78; perplexity/K = 21.53%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  565.6 e-6; = (1/var)*||X-X_r||^2 =  173.7 e-6 = 30.7 %; (1+beta)*||Z_e-Z_q||^2 =  391.9 e-6 = 69.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  931.4 e-6; = (1/var)*||X-X_r||^2 =  538.2 e-6 = 57.8 %; (1+beta)*||Z_e-Z_q||^2 =  393.2 e-6 = 42.2 %)
Min.  Avg. Train Loss across Mini-Batch =  255.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  624.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   365.9 e-6; = (1/var)*||X-X_r||^2 val-train = 364.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.73; perplexity/K = 21.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.90; perplexity/K = 21.72%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:48:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2970.8 e-6; = (1/var)*||X-X_r||^2 =  863.0 e-6 = 29.1 %; (1+beta)*||Z_e-Z_q||^2 =  2107.8 e-6 = 70.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  3050.8 e-6; = (1/var)*||X-X_r||^2 =  1246.2 e-6 = 40.8 %; (1+beta)*||Z_e-Z_q||^2 =  1804.7 e-6 = 59.2 %)
Min.  Avg. Train Loss across Mini-Batch =  255.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  624.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   80.0 e-6; = (1/var)*||X-X_r||^2 val-train = 383.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -303.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.15; perplexity/K = 17.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.80; perplexity/K = 15.31%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:55:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  403.6 e-6; = (1/var)*||X-X_r||^2 =  114.6 e-6 = 28.4 %; (1+beta)*||Z_e-Z_q||^2 =  289.0 e-6 = 71.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  699.8 e-6; = (1/var)*||X-X_r||^2 =  408.5 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  291.3 e-6 = 41.6 %)
Min.  Avg. Train Loss across Mini-Batch =  255.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  624.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   296.2 e-6; = (1/var)*||X-X_r||^2 val-train = 293.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.80; perplexity/K = 12.19%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.52; perplexity/K = 11.75%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  313.3 e-6; = (1/var)*||X-X_r||^2 =  91.1 e-6 = 29.1 %; (1+beta)*||Z_e-Z_q||^2 =  222.3 e-6 = 70.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  545.1 e-6; = (1/var)*||X-X_r||^2 =  322.2 e-6 = 59.1 %; (1+beta)*||Z_e-Z_q||^2 =  222.8 e-6 = 40.9 %)
Min.  Avg. Train Loss across Mini-Batch =  255.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  545.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   231.7 e-6; = (1/var)*||X-X_r||^2 val-train = 231.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.04; perplexity/K = 12.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.51; perplexity/K = 11.73%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  370.5 e-6; = (1/var)*||X-X_r||^2 =  111.1 e-6 = 30.0 %; (1+beta)*||Z_e-Z_q||^2 =  259.4 e-6 = 70.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  613.0 e-6; = (1/var)*||X-X_r||^2 =  336.2 e-6 = 54.9 %; (1+beta)*||Z_e-Z_q||^2 =  276.7 e-6 = 45.1 %)
Min.  Avg. Train Loss across Mini-Batch =  190.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  388.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   242.5 e-6; = (1/var)*||X-X_r||^2 val-train = 225.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 18.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.36; perplexity/K = 17.75%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  274.0 e-6; = (1/var)*||X-X_r||^2 =  106.8 e-6 = 39.0 %; (1+beta)*||Z_e-Z_q||^2 =  167.2 e-6 = 61.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  418.9 e-6; = (1/var)*||X-X_r||^2 =  268.7 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  150.1 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  149.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  329.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   144.8 e-6; = (1/var)*||X-X_r||^2 val-train = 161.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.1 e-6 

----------------------------------------------------------------------------------

Finished [05:53:40 06.01.2023] 284) Finished running for K = 64 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 384) change_channel_size_across_layers = False:
Total training time is = 0:7:48 h/m/s. 

--------------------------------------------------- 

Started [05:53:40 06.01.2023] 285) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 734 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             1.09
2                           encoder.sequential_convs.conv2d_3.weight                        32             4.36
3                           encoder.sequential_convs.conv2d_4.weight                       131            17.85
4                                  encoder.pre_residual_stack.weight                       147            20.03
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.90
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.90
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
9                              encoder.channel_adjusting_conv.weight                         8             1.09
10                                                       VQ.E.weight                         4             0.54
11                             decoder.channel_adjusting_conv.weight                        73             9.95
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.90
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.90
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
16                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.85
17                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.36
18                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.09
19                    decoder.sequential_trans_convs.conv2d_4.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.60; perplexity/K = 38.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.68; perplexity/K = 37.00%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  403905.2 e-6; = (1/var)*||X-X_r||^2 =  287288.1 e-6 = 71.1 %; (1+beta)*||Z_e-Z_q||^2 =  116617.1 e-6 = 28.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  402423.0 e-6; = (1/var)*||X-X_r||^2 =  292120.3 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  110302.7 e-6 = 27.4 %)
Min.  Avg. Train Loss across Mini-Batch =  403905.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  402062.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1482.1 e-6; = (1/var)*||X-X_r||^2 val-train = 4832.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -6314.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.55; perplexity/K = 57.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.42; perplexity/K = 55.34%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  199175.4 e-6; = (1/var)*||X-X_r||^2 =  173658.6 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  25516.8 e-6 = 12.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  282196.6 e-6; = (1/var)*||X-X_r||^2 =  237266.2 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  44930.4 e-6 = 15.9 %)
Min.  Avg. Train Loss across Mini-Batch =  170139.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  174421.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   83021.2 e-6; = (1/var)*||X-X_r||^2 val-train = 63607.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19413.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.11; perplexity/K = 62.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.91; perplexity/K = 62.36%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  108649.3 e-6; = (1/var)*||X-X_r||^2 =  98608.9 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  10040.5 e-6 = 9.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  113297.5 e-6; = (1/var)*||X-X_r||^2 =  103582.1 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  9715.4 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  108384.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  112364.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4648.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4973.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -325.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.99; perplexity/K = 60.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.18; perplexity/K = 59.66%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  82575.6 e-6; = (1/var)*||X-X_r||^2 =  73973.7 e-6 = 89.6 %; (1+beta)*||Z_e-Z_q||^2 =  8601.9 e-6 = 10.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  87394.0 e-6; = (1/var)*||X-X_r||^2 =  78791.1 e-6 = 90.2 %; (1+beta)*||Z_e-Z_q||^2 =  8602.8 e-6 = 9.8 %)
Min.  Avg. Train Loss across Mini-Batch =  82524.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  87255.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4818.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4817.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.80; perplexity/K = 63.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.47; perplexity/K = 58.54%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:11:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  70121.0 e-6; = (1/var)*||X-X_r||^2 =  62560.0 e-6 = 89.2 %; (1+beta)*||Z_e-Z_q||^2 =  7561.0 e-6 = 10.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  74953.7 e-6; = (1/var)*||X-X_r||^2 =  67302.7 e-6 = 89.8 %; (1+beta)*||Z_e-Z_q||^2 =  7651.0 e-6 = 10.2 %)
Min.  Avg. Train Loss across Mini-Batch =  69882.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  74953.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4832.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4742.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 90.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.09; perplexity/K = 61.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.77; perplexity/K = 63.71%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  62652.4 e-6; = (1/var)*||X-X_r||^2 =  55691.7 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  6960.7 e-6 = 11.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  66515.2 e-6; = (1/var)*||X-X_r||^2 =  59491.1 e-6 = 89.4 %; (1+beta)*||Z_e-Z_q||^2 =  7024.1 e-6 = 10.6 %)
Min.  Avg. Train Loss across Mini-Batch =  62326.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  66515.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3862.8 e-6; = (1/var)*||X-X_r||^2 val-train = 3799.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.54; perplexity/K = 63.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.58; perplexity/K = 64.97%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  50389.3 e-6; = (1/var)*||X-X_r||^2 =  44028.6 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  6360.7 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  56302.1 e-6; = (1/var)*||X-X_r||^2 =  49115.5 e-6 = 87.2 %; (1+beta)*||Z_e-Z_q||^2 =  7186.6 e-6 = 12.8 %)
Min.  Avg. Train Loss across Mini-Batch =  50389.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  55892.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5912.8 e-6; = (1/var)*||X-X_r||^2 val-train = 5086.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 825.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.08; perplexity/K = 64.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.16; perplexity/K = 67.44%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43467.4 e-6; = (1/var)*||X-X_r||^2 =  37634.7 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  5832.7 e-6 = 13.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  49116.4 e-6; = (1/var)*||X-X_r||^2 =  42659.6 e-6 = 86.9 %; (1+beta)*||Z_e-Z_q||^2 =  6456.8 e-6 = 13.1 %)
Min.  Avg. Train Loss across Mini-Batch =  43403.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  48439.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5649.0 e-6; = (1/var)*||X-X_r||^2 val-train = 5024.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 624.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.18; perplexity/K = 64.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.22; perplexity/K = 64.40%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36918.0 e-6; = (1/var)*||X-X_r||^2 =  31455.0 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  5463.0 e-6 = 14.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  42634.5 e-6; = (1/var)*||X-X_r||^2 =  36373.2 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  6261.4 e-6 = 14.7 %)
Min.  Avg. Train Loss across Mini-Batch =  36918.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  42634.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5716.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4918.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 798.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.77; perplexity/K = 65.26%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.39; perplexity/K = 63.11%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  33217.9 e-6; = (1/var)*||X-X_r||^2 =  28343.8 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  4874.1 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  39624.2 e-6; = (1/var)*||X-X_r||^2 =  33742.5 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  5881.7 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  33114.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  38514.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6406.3 e-6; = (1/var)*||X-X_r||^2 val-train = 5398.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1007.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.02; perplexity/K = 65.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.65; perplexity/K = 65.07%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30769.1 e-6; = (1/var)*||X-X_r||^2 =  26369.5 e-6 = 85.7 %; (1+beta)*||Z_e-Z_q||^2 =  4399.6 e-6 = 14.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  35972.6 e-6; = (1/var)*||X-X_r||^2 =  30954.9 e-6 = 86.1 %; (1+beta)*||Z_e-Z_q||^2 =  5017.7 e-6 = 13.9 %)
Min.  Avg. Train Loss across Mini-Batch =  30678.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  35555.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5203.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4585.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 618.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.30; perplexity/K = 64.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.05; perplexity/K = 64.14%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28558.1 e-6; = (1/var)*||X-X_r||^2 =  24681.1 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  3877.0 e-6 = 13.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  33505.9 e-6; = (1/var)*||X-X_r||^2 =  29000.2 e-6 = 86.6 %; (1+beta)*||Z_e-Z_q||^2 =  4505.7 e-6 = 13.4 %)
Min.  Avg. Train Loss across Mini-Batch =  28196.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  33251.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4947.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4319.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 628.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.01; perplexity/K = 64.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.64; perplexity/K = 63.51%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  27627.9 e-6; = (1/var)*||X-X_r||^2 =  23896.4 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  3731.5 e-6 = 13.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  32773.3 e-6; = (1/var)*||X-X_r||^2 =  28341.9 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  4431.4 e-6 = 13.5 %)
Min.  Avg. Train Loss across Mini-Batch =  27528.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  32275.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5145.5 e-6; = (1/var)*||X-X_r||^2 val-train = 4445.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 699.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.79; perplexity/K = 65.29%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.88; perplexity/K = 63.88%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26742.7 e-6; = (1/var)*||X-X_r||^2 =  23183.4 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  3559.3 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  31426.0 e-6; = (1/var)*||X-X_r||^2 =  27185.1 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  4240.9 e-6 = 13.5 %)
Min.  Avg. Train Loss across Mini-Batch =  26182.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30970.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4683.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4001.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 681.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.53; perplexity/K = 61.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.68; perplexity/K = 63.56%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25254.8 e-6; = (1/var)*||X-X_r||^2 =  22079.0 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  3175.8 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  30142.2 e-6; = (1/var)*||X-X_r||^2 =  26344.8 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  3797.4 e-6 = 12.6 %)
Min.  Avg. Train Loss across Mini-Batch =  25191.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29978.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4887.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4265.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 621.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.67; perplexity/K = 66.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.47; perplexity/K = 64.79%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25327.9 e-6; = (1/var)*||X-X_r||^2 =  22134.7 e-6 = 87.4 %; (1+beta)*||Z_e-Z_q||^2 =  3193.3 e-6 = 12.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  30641.7 e-6; = (1/var)*||X-X_r||^2 =  26875.0 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  3766.6 e-6 = 12.3 %)
Min.  Avg. Train Loss across Mini-Batch =  24686.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29697.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5313.7 e-6; = (1/var)*||X-X_r||^2 val-train = 4740.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 573.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.90; perplexity/K = 62.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.38; perplexity/K = 63.09%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24451.9 e-6; = (1/var)*||X-X_r||^2 =  21410.9 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  3041.0 e-6 = 12.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  29685.3 e-6; = (1/var)*||X-X_r||^2 =  26074.6 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  3610.8 e-6 = 12.2 %)
Min.  Avg. Train Loss across Mini-Batch =  24233.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29445.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5233.4 e-6; = (1/var)*||X-X_r||^2 val-train = 4663.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 569.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.03; perplexity/K = 60.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.66; perplexity/K = 61.97%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24187.7 e-6; = (1/var)*||X-X_r||^2 =  21225.7 e-6 = 87.8 %; (1+beta)*||Z_e-Z_q||^2 =  2962.0 e-6 = 12.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  29569.3 e-6; = (1/var)*||X-X_r||^2 =  25908.3 e-6 = 87.6 %; (1+beta)*||Z_e-Z_q||^2 =  3661.0 e-6 = 12.4 %)
Min.  Avg. Train Loss across Mini-Batch =  23973.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  29122.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5381.6 e-6; = (1/var)*||X-X_r||^2 val-train = 4682.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 699.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.85; perplexity/K = 63.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.27; perplexity/K = 61.36%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  24713.8 e-6; = (1/var)*||X-X_r||^2 =  21224.5 e-6 = 85.9 %; (1+beta)*||Z_e-Z_q||^2 =  3489.3 e-6 = 14.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  28793.4 e-6; = (1/var)*||X-X_r||^2 =  25085.2 e-6 = 87.1 %; (1+beta)*||Z_e-Z_q||^2 =  3708.2 e-6 = 12.9 %)
Min.  Avg. Train Loss across Mini-Batch =  23611.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28362.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4079.6 e-6; = (1/var)*||X-X_r||^2 val-train = 3860.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 219.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.23; perplexity/K = 61.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.86; perplexity/K = 60.72%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  23415.6 e-6; = (1/var)*||X-X_r||^2 =  20616.9 e-6 = 88.0 %; (1+beta)*||Z_e-Z_q||^2 =  2798.7 e-6 = 12.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  28866.6 e-6; = (1/var)*||X-X_r||^2 =  25382.3 e-6 = 87.9 %; (1+beta)*||Z_e-Z_q||^2 =  3484.3 e-6 = 12.1 %)
Min.  Avg. Train Loss across Mini-Batch =  23230.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28131.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5451.0 e-6; = (1/var)*||X-X_r||^2 val-train = 4765.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 685.6 e-6 

----------------------------------------------------------------------------------

Finished [06:42:07 06.01.2023] 285) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = True:
Total training time is = 0:3:26 h/m/s. 

--------------------------------------------------- 

Started [06:42:07 06.01.2023] 286) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 96) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2456 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.04
1                           encoder.sequential_convs.conv2d_2.weight                        32             1.30
2                           encoder.sequential_convs.conv2d_3.weight                       131             5.33
3                           encoder.sequential_convs.conv2d_4.weight                       524            21.34
4                                  encoder.pre_residual_stack.weight                       589            23.98
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
9                              encoder.channel_adjusting_conv.weight                        16             0.65
10                                                       VQ.E.weight                         4             0.16
11                             decoder.channel_adjusting_conv.weight                       147             5.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.97
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.33
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.97
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.33
16                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.34
17                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.33
18                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
19                    decoder.sequential_trans_convs.conv2d_4.weight                         1             0.04

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.62; perplexity/K = 30.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.74; perplexity/K = 29.28%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  379258.3 e-6; = (1/var)*||X-X_r||^2 =  281236.6 e-6 = 74.2 %; (1+beta)*||Z_e-Z_q||^2 =  98021.7 e-6 = 25.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  391279.4 e-6; = (1/var)*||X-X_r||^2 =  293134.5 e-6 = 74.9 %; (1+beta)*||Z_e-Z_q||^2 =  98144.9 e-6 = 25.1 %)
Min.  Avg. Train Loss across Mini-Batch =  370542.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  372850.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12021.1 e-6; = (1/var)*||X-X_r||^2 val-train = 11897.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 123.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.04; perplexity/K = 40.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.00; perplexity/K = 40.63%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  116964.0 e-6; = (1/var)*||X-X_r||^2 =  98818.3 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  18145.7 e-6 = 15.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  127109.2 e-6; = (1/var)*||X-X_r||^2 =  108296.3 e-6 = 85.2 %; (1+beta)*||Z_e-Z_q||^2 =  18813.0 e-6 = 14.8 %)
Min.  Avg. Train Loss across Mini-Batch =  116964.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  125555.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   10145.2 e-6; = (1/var)*||X-X_r||^2 val-train = 9478.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 667.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.99; perplexity/K = 46.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.33; perplexity/K = 45.82%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  46667.5 e-6; = (1/var)*||X-X_r||^2 =  39614.5 e-6 = 84.9 %; (1+beta)*||Z_e-Z_q||^2 =  7053.0 e-6 = 15.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  51826.9 e-6; = (1/var)*||X-X_r||^2 =  44807.7 e-6 = 86.5 %; (1+beta)*||Z_e-Z_q||^2 =  7019.2 e-6 = 13.5 %)
Min.  Avg. Train Loss across Mini-Batch =  46667.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  51826.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5159.4 e-6; = (1/var)*||X-X_r||^2 val-train = 5193.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -33.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.69; perplexity/K = 57.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.96; perplexity/K = 54.63%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  47407.3 e-6; = (1/var)*||X-X_r||^2 =  40449.6 e-6 = 85.3 %; (1+beta)*||Z_e-Z_q||^2 =  6957.7 e-6 = 14.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  34643.2 e-6; = (1/var)*||X-X_r||^2 =  29220.5 e-6 = 84.3 %; (1+beta)*||Z_e-Z_q||^2 =  5422.8 e-6 = 15.7 %)
Min.  Avg. Train Loss across Mini-Batch =  20813.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  24559.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -12764.1 e-6; = (1/var)*||X-X_r||^2 val-train = -11229.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1534.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.08; perplexity/K = 57.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.43; perplexity/K = 56.92%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8103.5 e-6; = (1/var)*||X-X_r||^2 =  5724.5 e-6 = 70.6 %; (1+beta)*||Z_e-Z_q||^2 =  2379.0 e-6 = 29.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  10662.5 e-6; = (1/var)*||X-X_r||^2 =  8110.9 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  2551.6 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  8103.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10662.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2559.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2386.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 172.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.54; perplexity/K = 57.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.98; perplexity/K = 56.21%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5185.6 e-6; = (1/var)*||X-X_r||^2 =  3531.3 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  1654.3 e-6 = 31.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  7210.6 e-6; = (1/var)*||X-X_r||^2 =  5419.7 e-6 = 75.2 %; (1+beta)*||Z_e-Z_q||^2 =  1790.9 e-6 = 24.8 %)
Min.  Avg. Train Loss across Mini-Batch =  4905.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  6864.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2025.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1888.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 136.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.68; perplexity/K = 60.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.96; perplexity/K = 57.75%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3799.2 e-6; = (1/var)*||X-X_r||^2 =  2218.1 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  1581.1 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5553.8 e-6; = (1/var)*||X-X_r||^2 =  3897.7 e-6 = 70.2 %; (1+beta)*||Z_e-Z_q||^2 =  1656.1 e-6 = 29.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3509.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4993.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1754.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1679.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 75.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.49; perplexity/K = 64.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.88; perplexity/K = 65.44%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2854.7 e-6; = (1/var)*||X-X_r||^2 =  1714.1 e-6 = 60.0 %; (1+beta)*||Z_e-Z_q||^2 =  1140.6 e-6 = 40.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4097.6 e-6; = (1/var)*||X-X_r||^2 =  2933.7 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  1164.0 e-6 = 28.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2602.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4055.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1243.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1219.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.37; perplexity/K = 64.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.25; perplexity/K = 64.45%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2001.8 e-6; = (1/var)*||X-X_r||^2 =  1338.5 e-6 = 66.9 %; (1+beta)*||Z_e-Z_q||^2 =  663.3 e-6 = 33.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  3205.9 e-6; = (1/var)*||X-X_r||^2 =  2478.2 e-6 = 77.3 %; (1+beta)*||Z_e-Z_q||^2 =  727.7 e-6 = 22.7 %)
Min.  Avg. Train Loss across Mini-Batch =  1936.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3076.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1204.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1139.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 64.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.88; perplexity/K = 65.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.50; perplexity/K = 64.84%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2364.0 e-6; = (1/var)*||X-X_r||^2 =  1407.9 e-6 = 59.6 %; (1+beta)*||Z_e-Z_q||^2 =  956.1 e-6 = 40.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3466.9 e-6; = (1/var)*||X-X_r||^2 =  2527.8 e-6 = 72.9 %; (1+beta)*||Z_e-Z_q||^2 =  939.1 e-6 = 27.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1602.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2712.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1102.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1119.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -17.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.59; perplexity/K = 68.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.24; perplexity/K = 67.57%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1385.0 e-6; = (1/var)*||X-X_r||^2 =  813.0 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  572.1 e-6 = 41.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  2416.7 e-6; = (1/var)*||X-X_r||^2 =  1839.4 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  577.3 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1323.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2318.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1031.7 e-6; = (1/var)*||X-X_r||^2 val-train = 1026.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.62; perplexity/K = 69.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.14; perplexity/K = 70.53%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1269.1 e-6; = (1/var)*||X-X_r||^2 =  747.2 e-6 = 58.9 %; (1+beta)*||Z_e-Z_q||^2 =  521.9 e-6 = 41.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  2245.5 e-6; = (1/var)*||X-X_r||^2 =  1709.4 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  536.0 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1230.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2230.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   976.4 e-6; = (1/var)*||X-X_r||^2 val-train = 962.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.13; perplexity/K = 70.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.18; perplexity/K = 70.60%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:53 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1069.4 e-6; = (1/var)*||X-X_r||^2 =  677.2 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  392.2 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2158.6 e-6; = (1/var)*||X-X_r||^2 =  1752.9 e-6 = 81.2 %; (1+beta)*||Z_e-Z_q||^2 =  405.7 e-6 = 18.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1022.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1936.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1089.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1075.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.39; perplexity/K = 70.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.80; perplexity/K = 71.57%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  882.7 e-6; = (1/var)*||X-X_r||^2 =  515.7 e-6 = 58.4 %; (1+beta)*||Z_e-Z_q||^2 =  367.0 e-6 = 41.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1905.7 e-6; = (1/var)*||X-X_r||^2 =  1524.6 e-6 = 80.0 %; (1+beta)*||Z_e-Z_q||^2 =  381.1 e-6 = 20.0 %)
Min.  Avg. Train Loss across Mini-Batch =  876.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1736.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1022.9 e-6; = (1/var)*||X-X_r||^2 val-train = 1008.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.80; perplexity/K = 68.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.34; perplexity/K = 67.72%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  844.6 e-6; = (1/var)*||X-X_r||^2 =  501.5 e-6 = 59.4 %; (1+beta)*||Z_e-Z_q||^2 =  343.0 e-6 = 40.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1666.3 e-6; = (1/var)*||X-X_r||^2 =  1310.3 e-6 = 78.6 %; (1+beta)*||Z_e-Z_q||^2 =  355.9 e-6 = 21.4 %)
Min.  Avg. Train Loss across Mini-Batch =  740.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1641.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   821.7 e-6; = (1/var)*||X-X_r||^2 val-train = 808.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.77; perplexity/K = 68.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.61; perplexity/K = 68.15%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  726.4 e-6; = (1/var)*||X-X_r||^2 =  504.7 e-6 = 69.5 %; (1+beta)*||Z_e-Z_q||^2 =  221.7 e-6 = 30.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1493.8 e-6; = (1/var)*||X-X_r||^2 =  1242.8 e-6 = 83.2 %; (1+beta)*||Z_e-Z_q||^2 =  251.0 e-6 = 16.8 %)
Min.  Avg. Train Loss across Mini-Batch =  650.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1389.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   767.4 e-6; = (1/var)*||X-X_r||^2 val-train = 738.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.10; perplexity/K = 70.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.92; perplexity/K = 68.63%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  792.8 e-6; = (1/var)*||X-X_r||^2 =  520.6 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  272.1 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1446.8 e-6; = (1/var)*||X-X_r||^2 =  1178.6 e-6 = 81.5 %; (1+beta)*||Z_e-Z_q||^2 =  268.2 e-6 = 18.5 %)
Min.  Avg. Train Loss across Mini-Batch =  638.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1370.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   654.0 e-6; = (1/var)*||X-X_r||^2 val-train = 657.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.84; perplexity/K = 66.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.01; perplexity/K = 68.76%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:44:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  749.6 e-6; = (1/var)*||X-X_r||^2 =  396.8 e-6 = 52.9 %; (1+beta)*||Z_e-Z_q||^2 =  352.8 e-6 = 47.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1480.9 e-6; = (1/var)*||X-X_r||^2 =  1126.4 e-6 = 76.1 %; (1+beta)*||Z_e-Z_q||^2 =  354.5 e-6 = 23.9 %)
Min.  Avg. Train Loss across Mini-Batch =  572.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1315.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   731.3 e-6; = (1/var)*||X-X_r||^2 val-train = 729.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.09; perplexity/K = 67.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.45; perplexity/K = 71.02%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  510.1 e-6; = (1/var)*||X-X_r||^2 =  280.2 e-6 = 54.9 %; (1+beta)*||Z_e-Z_q||^2 =  230.0 e-6 = 45.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1167.4 e-6; = (1/var)*||X-X_r||^2 =  932.5 e-6 = 79.9 %; (1+beta)*||Z_e-Z_q||^2 =  234.9 e-6 = 20.1 %)
Min.  Avg. Train Loss across Mini-Batch =  510.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1167.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   657.2 e-6; = (1/var)*||X-X_r||^2 val-train = 652.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 47.17; perplexity/K = 73.70%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 45.81; perplexity/K = 71.58%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12208.9 e-6; = (1/var)*||X-X_r||^2 =  10275.5 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  1933.3 e-6 = 15.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4931.2 e-6; = (1/var)*||X-X_r||^2 =  3917.7 e-6 = 79.4 %; (1+beta)*||Z_e-Z_q||^2 =  1013.5 e-6 = 20.6 %)
Min.  Avg. Train Loss across Mini-Batch =  448.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1065.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -7277.7 e-6; = (1/var)*||X-X_r||^2 val-train = -6357.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -919.9 e-6 

----------------------------------------------------------------------------------

Finished [07:31:49 06.01.2023] 286) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 96) change_channel_size_across_layers = True:
Total training time is = 0:3:42 h/m/s. 

--------------------------------------------------- 

Started [07:31:49 06.01.2023] 287) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 1976 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.30
1                           encoder.sequential_convs.conv2d_2.weight                       262            13.26
2                           encoder.sequential_convs.conv2d_3.weight                       262            13.26
3                           encoder.sequential_convs.conv2d_4.weight                       262            13.26
4                                  encoder.pre_residual_stack.weight                       147             7.44
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
9                              encoder.channel_adjusting_conv.weight                         8             0.40
10                                                       VQ.E.weight                         4             0.20
11                             decoder.channel_adjusting_conv.weight                        73             3.69
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.82
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.20
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.82
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.20
16                    decoder.sequential_trans_convs.conv2d_1.weight                       262            13.26
17                    decoder.sequential_trans_convs.conv2d_2.weight                       262            13.26
18                    decoder.sequential_trans_convs.conv2d_3.weight                       262            13.26
19                    decoder.sequential_trans_convs.conv2d_4.weight                         6             0.30

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.23; perplexity/K = 45.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 28.10; perplexity/K = 43.91%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  365453.9 e-6; = (1/var)*||X-X_r||^2 =  152491.0 e-6 = 41.7 %; (1+beta)*||Z_e-Z_q||^2 =  212963.0 e-6 = 58.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  359541.2 e-6; = (1/var)*||X-X_r||^2 =  158670.7 e-6 = 44.1 %; (1+beta)*||Z_e-Z_q||^2 =  200870.5 e-6 = 55.9 %)
Min.  Avg. Train Loss across Mini-Batch =  363326.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  359541.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5912.8 e-6; = (1/var)*||X-X_r||^2 val-train = 6179.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12092.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.37; perplexity/K = 59.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.17; perplexity/K = 58.08%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  110419.6 e-6; = (1/var)*||X-X_r||^2 =  61659.1 e-6 = 55.8 %; (1+beta)*||Z_e-Z_q||^2 =  48760.5 e-6 = 44.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  100400.4 e-6; = (1/var)*||X-X_r||^2 =  60518.7 e-6 = 60.3 %; (1+beta)*||Z_e-Z_q||^2 =  39881.7 e-6 = 39.7 %)
Min.  Avg. Train Loss across Mini-Batch =  60454.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  65372.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10019.2 e-6; = (1/var)*||X-X_r||^2 val-train = -1140.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8878.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.35; perplexity/K = 61.48%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.16; perplexity/K = 62.75%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  17808.0 e-6; = (1/var)*||X-X_r||^2 =  7102.6 e-6 = 39.9 %; (1+beta)*||Z_e-Z_q||^2 =  10705.4 e-6 = 60.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  32668.4 e-6; = (1/var)*||X-X_r||^2 =  19350.5 e-6 = 59.2 %; (1+beta)*||Z_e-Z_q||^2 =  13317.9 e-6 = 40.8 %)
Min.  Avg. Train Loss across Mini-Batch =  16105.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  18916.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   14860.4 e-6; = (1/var)*||X-X_r||^2 val-train = 12247.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2612.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.67; perplexity/K = 60.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.39; perplexity/K = 56.86%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7169.9 e-6; = (1/var)*||X-X_r||^2 =  2167.6 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  5002.3 e-6 = 69.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  8677.4 e-6; = (1/var)*||X-X_r||^2 =  3671.8 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  5005.6 e-6 = 57.7 %)
Min.  Avg. Train Loss across Mini-Batch =  7169.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8671.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1507.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1504.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.32; perplexity/K = 61.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.61; perplexity/K = 60.33%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9300.5 e-6; = (1/var)*||X-X_r||^2 =  2550.4 e-6 = 27.4 %; (1+beta)*||Z_e-Z_q||^2 =  6750.0 e-6 = 72.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  9902.2 e-6; = (1/var)*||X-X_r||^2 =  3533.1 e-6 = 35.7 %; (1+beta)*||Z_e-Z_q||^2 =  6369.0 e-6 = 64.3 %)
Min.  Avg. Train Loss across Mini-Batch =  3732.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5158.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   601.7 e-6; = (1/var)*||X-X_r||^2 val-train = 982.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -381.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.53; perplexity/K = 52.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.21; perplexity/K = 53.46%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2702.9 e-6; = (1/var)*||X-X_r||^2 =  698.6 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  2004.3 e-6 = 74.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3675.3 e-6; = (1/var)*||X-X_r||^2 =  1638.1 e-6 = 44.6 %; (1+beta)*||Z_e-Z_q||^2 =  2037.2 e-6 = 55.4 %)
Min.  Avg. Train Loss across Mini-Batch =  2702.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3675.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   972.5 e-6; = (1/var)*||X-X_r||^2 val-train = 939.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.19; perplexity/K = 59.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.57; perplexity/K = 57.15%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5839.6 e-6; = (1/var)*||X-X_r||^2 =  1123.9 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  4715.7 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  6276.6 e-6; = (1/var)*||X-X_r||^2 =  1913.9 e-6 = 30.5 %; (1+beta)*||Z_e-Z_q||^2 =  4362.7 e-6 = 69.5 %)
Min.  Avg. Train Loss across Mini-Batch =  2317.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2996.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   437.0 e-6; = (1/var)*||X-X_r||^2 val-train = 790.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -353.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.01; perplexity/K = 65.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 44.23; perplexity/K = 69.11%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3646.4 e-6; = (1/var)*||X-X_r||^2 =  1436.2 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  2210.1 e-6 = 60.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  16377.2 e-6; = (1/var)*||X-X_r||^2 =  10948.0 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  5429.2 e-6 = 33.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1908.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2453.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12730.8 e-6; = (1/var)*||X-X_r||^2 val-train = 9511.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3219.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.06; perplexity/K = 56.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.19; perplexity/K = 54.98%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3258.1 e-6; = (1/var)*||X-X_r||^2 =  611.0 e-6 = 18.8 %; (1+beta)*||Z_e-Z_q||^2 =  2647.1 e-6 = 81.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3535.8 e-6; = (1/var)*||X-X_r||^2 =  1051.9 e-6 = 29.8 %; (1+beta)*||Z_e-Z_q||^2 =  2483.9 e-6 = 70.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1889.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2301.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   277.7 e-6; = (1/var)*||X-X_r||^2 val-train = 441.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -163.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.35; perplexity/K = 63.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.62; perplexity/K = 58.78%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3938.8 e-6; = (1/var)*||X-X_r||^2 =  664.8 e-6 = 16.9 %; (1+beta)*||Z_e-Z_q||^2 =  3274.0 e-6 = 83.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  4038.1 e-6; = (1/var)*||X-X_r||^2 =  1064.1 e-6 = 26.4 %; (1+beta)*||Z_e-Z_q||^2 =  2973.9 e-6 = 73.6 %)
Min.  Avg. Train Loss across Mini-Batch =  1125.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   99.2 e-6; = (1/var)*||X-X_r||^2 val-train = 399.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -300.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.60; perplexity/K = 58.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.10; perplexity/K = 54.85%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1898.4 e-6; = (1/var)*||X-X_r||^2 =  311.8 e-6 = 16.4 %; (1+beta)*||Z_e-Z_q||^2 =  1586.6 e-6 = 83.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  2424.8 e-6; = (1/var)*||X-X_r||^2 =  874.1 e-6 = 36.0 %; (1+beta)*||Z_e-Z_q||^2 =  1550.7 e-6 = 64.0 %)
Min.  Avg. Train Loss across Mini-Batch =  1125.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   526.4 e-6; = (1/var)*||X-X_r||^2 val-train = 562.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -35.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.41; perplexity/K = 75.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 49.28; perplexity/K = 77.00%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  12085.7 e-6; = (1/var)*||X-X_r||^2 =  2148.4 e-6 = 17.8 %; (1+beta)*||Z_e-Z_q||^2 =  9937.3 e-6 = 82.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  13804.7 e-6; = (1/var)*||X-X_r||^2 =  2735.4 e-6 = 19.8 %; (1+beta)*||Z_e-Z_q||^2 =  11069.3 e-6 = 80.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1105.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1719.0 e-6; = (1/var)*||X-X_r||^2 val-train = 587.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1132.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.10; perplexity/K = 61.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.18; perplexity/K = 58.10%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3840.0 e-6; = (1/var)*||X-X_r||^2 =  676.0 e-6 = 17.6 %; (1+beta)*||Z_e-Z_q||^2 =  3164.0 e-6 = 82.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  3597.3 e-6; = (1/var)*||X-X_r||^2 =  1001.7 e-6 = 27.8 %; (1+beta)*||Z_e-Z_q||^2 =  2595.6 e-6 = 72.2 %)
Min.  Avg. Train Loss across Mini-Batch =  979.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -242.7 e-6; = (1/var)*||X-X_r||^2 val-train = 325.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -568.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.21; perplexity/K = 50.33%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.98; perplexity/K = 49.96%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1193.8 e-6; = (1/var)*||X-X_r||^2 =  186.8 e-6 = 15.6 %; (1+beta)*||Z_e-Z_q||^2 =  1007.0 e-6 = 84.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1883.2 e-6; = (1/var)*||X-X_r||^2 =  860.2 e-6 = 45.7 %; (1+beta)*||Z_e-Z_q||^2 =  1023.0 e-6 = 54.3 %)
Min.  Avg. Train Loss across Mini-Batch =  979.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1481.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   689.4 e-6; = (1/var)*||X-X_r||^2 val-train = 673.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 15.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.46; perplexity/K = 55.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.07; perplexity/K = 57.92%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2128.7 e-6; = (1/var)*||X-X_r||^2 =  521.9 e-6 = 24.5 %; (1+beta)*||Z_e-Z_q||^2 =  1606.8 e-6 = 75.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2001.7 e-6; = (1/var)*||X-X_r||^2 =  438.1 e-6 = 21.9 %; (1+beta)*||Z_e-Z_q||^2 =  1563.6 e-6 = 78.1 %)
Min.  Avg. Train Loss across Mini-Batch =  769.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1269.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -127.0 e-6; = (1/var)*||X-X_r||^2 val-train = -83.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -43.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.36; perplexity/K = 72.43%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 43.60; perplexity/K = 68.12%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6558.7 e-6; = (1/var)*||X-X_r||^2 =  1492.0 e-6 = 22.7 %; (1+beta)*||Z_e-Z_q||^2 =  5066.7 e-6 = 77.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  6666.3 e-6; = (1/var)*||X-X_r||^2 =  2192.3 e-6 = 32.9 %; (1+beta)*||Z_e-Z_q||^2 =  4474.0 e-6 = 67.1 %)
Min.  Avg. Train Loss across Mini-Batch =  769.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1269.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   107.6 e-6; = (1/var)*||X-X_r||^2 val-train = 700.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -592.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.78; perplexity/K = 62.16%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.79; perplexity/K = 57.49%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2156.9 e-6; = (1/var)*||X-X_r||^2 =  186.5 e-6 = 8.6 %; (1+beta)*||Z_e-Z_q||^2 =  1970.3 e-6 = 91.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2766.8 e-6; = (1/var)*||X-X_r||^2 =  896.6 e-6 = 32.4 %; (1+beta)*||Z_e-Z_q||^2 =  1870.3 e-6 = 67.6 %)
Min.  Avg. Train Loss across Mini-Batch =  769.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1269.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   610.0 e-6; = (1/var)*||X-X_r||^2 val-train = 710.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -100.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.49; perplexity/K = 72.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 48.09; perplexity/K = 75.13%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5211.1 e-6; = (1/var)*||X-X_r||^2 =  1975.1 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  3235.9 e-6 = 62.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8491.7 e-6; = (1/var)*||X-X_r||^2 =  3218.0 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  5273.6 e-6 = 62.1 %)
Min.  Avg. Train Loss across Mini-Batch =  574.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  879.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   3280.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1242.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2037.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 40.79; perplexity/K = 63.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.09; perplexity/K = 64.20%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5140.1 e-6; = (1/var)*||X-X_r||^2 =  1739.2 e-6 = 33.8 %; (1+beta)*||Z_e-Z_q||^2 =  3400.9 e-6 = 66.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  3594.4 e-6; = (1/var)*||X-X_r||^2 =  947.1 e-6 = 26.3 %; (1+beta)*||Z_e-Z_q||^2 =  2647.3 e-6 = 73.7 %)
Min.  Avg. Train Loss across Mini-Batch =  574.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  879.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1545.7 e-6; = (1/var)*||X-X_r||^2 val-train = -792.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -753.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.70; perplexity/K = 52.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.72; perplexity/K = 52.68%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:4:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1017.0 e-6; = (1/var)*||X-X_r||^2 =  84.3 e-6 = 8.3 %; (1+beta)*||Z_e-Z_q||^2 =  932.6 e-6 = 91.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1361.7 e-6; = (1/var)*||X-X_r||^2 =  441.8 e-6 = 32.4 %; (1+beta)*||Z_e-Z_q||^2 =  919.9 e-6 = 67.6 %)
Min.  Avg. Train Loss across Mini-Batch =  526.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  879.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   344.7 e-6; = (1/var)*||X-X_r||^2 val-train = 357.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12.7 e-6 

----------------------------------------------------------------------------------

Finished [08:36:57 06.01.2023] 287) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 96) change_channel_size_across_layers = False:
Total training time is = 0:3:7 h/m/s. 

--------------------------------------------------- 

Started [08:36:57 06.01.2023] 288) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 96) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 7392 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.16
1                           encoder.sequential_convs.conv2d_2.weight                      1048            14.18
2                           encoder.sequential_convs.conv2d_3.weight                      1048            14.18
3                           encoder.sequential_convs.conv2d_4.weight                      1048            14.18
4                                  encoder.pre_residual_stack.weight                       589             7.97
5   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
6   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
7   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
8   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
9                              encoder.channel_adjusting_conv.weight                        16             0.22
10                                                       VQ.E.weight                         4             0.05
11                             decoder.channel_adjusting_conv.weight                       147             1.99
12  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.99
13  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.11
14  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.99
15  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.11
16                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            14.18
17                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            14.18
18                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            14.18
19                    decoder.sequential_trans_convs.conv2d_4.weight                        12             0.16

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.22; perplexity/K = 51.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.17; perplexity/K = 53.39%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  139839.7 e-6; = (1/var)*||X-X_r||^2 =  59324.2 e-6 = 42.4 %; (1+beta)*||Z_e-Z_q||^2 =  80515.5 e-6 = 57.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  137163.0 e-6; = (1/var)*||X-X_r||^2 =  66534.6 e-6 = 48.5 %; (1+beta)*||Z_e-Z_q||^2 =  70628.4 e-6 = 51.5 %)
Min.  Avg. Train Loss across Mini-Batch =  127143.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  137163.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2676.7 e-6; = (1/var)*||X-X_r||^2 val-train = 7210.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9887.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.76; perplexity/K = 65.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.21; perplexity/K = 61.27%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  22483.9 e-6; = (1/var)*||X-X_r||^2 =  7412.1 e-6 = 33.0 %; (1+beta)*||Z_e-Z_q||^2 =  15071.8 e-6 = 67.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  26520.7 e-6; = (1/var)*||X-X_r||^2 =  11730.4 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  14790.3 e-6 = 55.8 %)
Min.  Avg. Train Loss across Mini-Batch =  22483.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  26520.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4036.8 e-6; = (1/var)*||X-X_r||^2 val-train = 4318.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -281.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.07; perplexity/K = 59.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.65; perplexity/K = 60.40%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7029.1 e-6; = (1/var)*||X-X_r||^2 =  2121.8 e-6 = 30.2 %; (1+beta)*||Z_e-Z_q||^2 =  4907.3 e-6 = 69.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  8325.3 e-6; = (1/var)*||X-X_r||^2 =  3509.1 e-6 = 42.1 %; (1+beta)*||Z_e-Z_q||^2 =  4816.3 e-6 = 57.9 %)
Min.  Avg. Train Loss across Mini-Batch =  7029.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  8325.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1296.2 e-6; = (1/var)*||X-X_r||^2 val-train = 1387.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -91.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.38; perplexity/K = 55.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.81; perplexity/K = 55.95%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4236.0 e-6; = (1/var)*||X-X_r||^2 =  1003.5 e-6 = 23.7 %; (1+beta)*||Z_e-Z_q||^2 =  3232.5 e-6 = 76.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  5227.1 e-6; = (1/var)*||X-X_r||^2 =  2058.2 e-6 = 39.4 %; (1+beta)*||Z_e-Z_q||^2 =  3168.9 e-6 = 60.6 %)
Min.  Avg. Train Loss across Mini-Batch =  4027.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5227.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   991.0 e-6; = (1/var)*||X-X_r||^2 val-train = 1054.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -63.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.19; perplexity/K = 58.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.89; perplexity/K = 62.33%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7302.3 e-6; = (1/var)*||X-X_r||^2 =  1585.1 e-6 = 21.7 %; (1+beta)*||Z_e-Z_q||^2 =  5717.1 e-6 = 78.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  7653.8 e-6; = (1/var)*||X-X_r||^2 =  2549.3 e-6 = 33.3 %; (1+beta)*||Z_e-Z_q||^2 =  5104.5 e-6 = 66.7 %)
Min.  Avg. Train Loss across Mini-Batch =  2686.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  3448.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   351.6 e-6; = (1/var)*||X-X_r||^2 val-train = 964.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -612.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.24; perplexity/K = 47.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.93; perplexity/K = 49.90%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2244.7 e-6; = (1/var)*||X-X_r||^2 =  464.4 e-6 = 20.7 %; (1+beta)*||Z_e-Z_q||^2 =  1780.4 e-6 = 79.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3097.8 e-6; = (1/var)*||X-X_r||^2 =  1232.8 e-6 = 39.8 %; (1+beta)*||Z_e-Z_q||^2 =  1865.0 e-6 = 60.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1659.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2336.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   853.1 e-6; = (1/var)*||X-X_r||^2 val-train = 768.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 84.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.29; perplexity/K = 50.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.04; perplexity/K = 48.49%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3229.6 e-6; = (1/var)*||X-X_r||^2 =  441.6 e-6 = 13.7 %; (1+beta)*||Z_e-Z_q||^2 =  2788.0 e-6 = 86.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  3777.3 e-6; = (1/var)*||X-X_r||^2 =  1129.8 e-6 = 29.9 %; (1+beta)*||Z_e-Z_q||^2 =  2647.4 e-6 = 70.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1659.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2336.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   547.7 e-6; = (1/var)*||X-X_r||^2 val-train = 688.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -140.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.37; perplexity/K = 64.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 42.97; perplexity/K = 67.15%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:58 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6165.8 e-6; = (1/var)*||X-X_r||^2 =  1487.1 e-6 = 24.1 %; (1+beta)*||Z_e-Z_q||^2 =  4678.7 e-6 = 75.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  6618.8 e-6; = (1/var)*||X-X_r||^2 =  1965.6 e-6 = 29.7 %; (1+beta)*||Z_e-Z_q||^2 =  4653.1 e-6 = 70.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1120.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1732.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   453.0 e-6; = (1/var)*||X-X_r||^2 val-train = 478.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.33; perplexity/K = 48.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 29.22; perplexity/K = 45.66%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1333.3 e-6; = (1/var)*||X-X_r||^2 =  202.5 e-6 = 15.2 %; (1+beta)*||Z_e-Z_q||^2 =  1130.8 e-6 = 84.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1887.4 e-6; = (1/var)*||X-X_r||^2 =  772.2 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  1115.2 e-6 = 59.1 %)
Min.  Avg. Train Loss across Mini-Batch =  924.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1561.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   554.1 e-6; = (1/var)*||X-X_r||^2 val-train = 569.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -15.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.07; perplexity/K = 50.11%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.08; perplexity/K = 48.56%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:7:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1435.6 e-6; = (1/var)*||X-X_r||^2 =  174.6 e-6 = 12.2 %; (1+beta)*||Z_e-Z_q||^2 =  1261.0 e-6 = 87.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2040.9 e-6; = (1/var)*||X-X_r||^2 =  801.8 e-6 = 39.3 %; (1+beta)*||Z_e-Z_q||^2 =  1239.2 e-6 = 60.7 %)
Min.  Avg. Train Loss across Mini-Batch =  900.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1448.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   605.4 e-6; = (1/var)*||X-X_r||^2 val-train = 627.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -21.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 46.34; perplexity/K = 72.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 41.57; perplexity/K = 64.96%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:14:12 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5403.4 e-6; = (1/var)*||X-X_r||^2 =  2693.8 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  2709.7 e-6 = 50.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  12794.0 e-6; = (1/var)*||X-X_r||^2 =  5942.9 e-6 = 46.5 %; (1+beta)*||Z_e-Z_q||^2 =  6851.1 e-6 = 53.5 %)
Min.  Avg. Train Loss across Mini-Batch =  609.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1131.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7390.5 e-6; = (1/var)*||X-X_r||^2 val-train = 3249.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4141.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.34; perplexity/K = 58.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.92; perplexity/K = 57.69%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:20:56 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1478.1 e-6; = (1/var)*||X-X_r||^2 =  206.0 e-6 = 13.9 %; (1+beta)*||Z_e-Z_q||^2 =  1272.1 e-6 = 86.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1790.8 e-6; = (1/var)*||X-X_r||^2 =  595.9 e-6 = 33.3 %; (1+beta)*||Z_e-Z_q||^2 =  1194.9 e-6 = 66.7 %)
Min.  Avg. Train Loss across Mini-Batch =  609.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1131.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   312.7 e-6; = (1/var)*||X-X_r||^2 val-train = 389.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -77.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.01; perplexity/K = 53.15%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.40; perplexity/K = 53.75%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:27:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1292.2 e-6; = (1/var)*||X-X_r||^2 =  122.6 e-6 = 9.5 %; (1+beta)*||Z_e-Z_q||^2 =  1169.6 e-6 = 90.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  2032.9 e-6; = (1/var)*||X-X_r||^2 =  889.1 e-6 = 43.7 %; (1+beta)*||Z_e-Z_q||^2 =  1143.8 e-6 = 56.3 %)
Min.  Avg. Train Loss across Mini-Batch =  609.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  984.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   740.8 e-6; = (1/var)*||X-X_r||^2 val-train = 766.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.53; perplexity/K = 57.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.78; perplexity/K = 51.21%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:34:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  911.5 e-6; = (1/var)*||X-X_r||^2 =  102.2 e-6 = 11.2 %; (1+beta)*||Z_e-Z_q||^2 =  809.3 e-6 = 88.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1612.5 e-6; = (1/var)*||X-X_r||^2 =  783.3 e-6 = 48.6 %; (1+beta)*||Z_e-Z_q||^2 =  829.2 e-6 = 51.4 %)
Min.  Avg. Train Loss across Mini-Batch =  609.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  912.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   701.0 e-6; = (1/var)*||X-X_r||^2 val-train = 681.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 19.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.24; perplexity/K = 56.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.74; perplexity/K = 54.28%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:41:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1093.7 e-6; = (1/var)*||X-X_r||^2 =  114.6 e-6 = 10.5 %; (1+beta)*||Z_e-Z_q||^2 =  979.1 e-6 = 89.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1627.8 e-6; = (1/var)*||X-X_r||^2 =  696.2 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  931.6 e-6 = 57.2 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  912.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   534.0 e-6; = (1/var)*||X-X_r||^2 val-train = 581.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -47.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.08; perplexity/K = 57.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.16; perplexity/K = 61.18%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:47:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1843.7 e-6; = (1/var)*||X-X_r||^2 =  187.3 e-6 = 10.2 %; (1+beta)*||Z_e-Z_q||^2 =  1656.4 e-6 = 89.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  2176.9 e-6; = (1/var)*||X-X_r||^2 =  630.6 e-6 = 29.0 %; (1+beta)*||Z_e-Z_q||^2 =  1546.4 e-6 = 71.0 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   333.2 e-6; = (1/var)*||X-X_r||^2 val-train = 443.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -110.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.20; perplexity/K = 56.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.50; perplexity/K = 52.34%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:54:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1560.4 e-6; = (1/var)*||X-X_r||^2 =  135.4 e-6 = 8.7 %; (1+beta)*||Z_e-Z_q||^2 =  1425.0 e-6 = 91.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  1970.8 e-6; = (1/var)*||X-X_r||^2 =  585.3 e-6 = 29.7 %; (1+beta)*||Z_e-Z_q||^2 =  1385.5 e-6 = 70.3 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   410.3 e-6; = (1/var)*||X-X_r||^2 val-train = 449.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -39.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.91; perplexity/K = 56.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.49; perplexity/K = 58.59%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:1:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  832.1 e-6; = (1/var)*||X-X_r||^2 =  57.8 e-6 = 6.9 %; (1+beta)*||Z_e-Z_q||^2 =  774.2 e-6 = 93.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1519.1 e-6; = (1/var)*||X-X_r||^2 =  728.6 e-6 = 48.0 %; (1+beta)*||Z_e-Z_q||^2 =  790.4 e-6 = 52.0 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   687.0 e-6; = (1/var)*||X-X_r||^2 val-train = 670.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.09; perplexity/K = 54.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.78; perplexity/K = 57.48%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:8:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  733.2 e-6; = (1/var)*||X-X_r||^2 =  75.8 e-6 = 10.3 %; (1+beta)*||Z_e-Z_q||^2 =  657.4 e-6 = 89.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  1203.7 e-6; = (1/var)*||X-X_r||^2 =  536.4 e-6 = 44.6 %; (1+beta)*||Z_e-Z_q||^2 =  667.3 e-6 = 55.4 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   470.5 e-6; = (1/var)*||X-X_r||^2 val-train = 460.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.86; perplexity/K = 60.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 39.81; perplexity/K = 62.21%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:14:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2522.3 e-6; = (1/var)*||X-X_r||^2 =  999.0 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  1523.2 e-6 = 60.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  2276.9 e-6; = (1/var)*||X-X_r||^2 =  1111.1 e-6 = 48.8 %; (1+beta)*||Z_e-Z_q||^2 =  1165.8 e-6 = 51.2 %)
Min.  Avg. Train Loss across Mini-Batch =  445.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  898.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -245.4 e-6; = (1/var)*||X-X_r||^2 val-train = 112.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -357.5 e-6 

----------------------------------------------------------------------------------

Finished [10:52:27 06.01.2023] 288) Finished running for K = 64 & D = 64 & M = 3 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 96) change_channel_size_across_layers = False:
Total training time is = 0:3:30 h/m/s. 

--------------------------------------------------- 

Started [10:52:27 06.01.2023] 289) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 738 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.27
2                           encoder.sequential_convs.conv2d_3.weight                         8             1.08
3                           encoder.sequential_convs.conv2d_4.weight                        32             4.34
4                           encoder.sequential_convs.conv2d_5.weight                       131            17.75
5                                  encoder.pre_residual_stack.weight                       147            19.92
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
10                             encoder.channel_adjusting_conv.weight                         8             1.08
11                                                       VQ.E.weight                         4             0.54
12                             decoder.channel_adjusting_conv.weight                        73             9.89
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
17                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.75
18                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.34
19                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.08
20                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.44; perplexity/K = 3.81%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.54; perplexity/K = 3.96%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  973002.5 e-6; = (1/var)*||X-X_r||^2 =  967130.4 e-6 = 99.4 %; (1+beta)*||Z_e-Z_q||^2 =  5872.1 e-6 = 0.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  936553.1 e-6; = (1/var)*||X-X_r||^2 =  935173.0 e-6 = 99.9 %; (1+beta)*||Z_e-Z_q||^2 =  1380.1 e-6 = 0.1 %)
Min.  Avg. Train Loss across Mini-Batch =  965317.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  932332.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36449.4 e-6; = (1/var)*||X-X_r||^2 val-train = -31957.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4492.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.21; perplexity/K = 17.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.19; perplexity/K = 15.92%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  633496.1 e-6; = (1/var)*||X-X_r||^2 =  572056.1 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  61440.0 e-6 = 9.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  629024.1 e-6; = (1/var)*||X-X_r||^2 =  559151.7 e-6 = 88.9 %; (1+beta)*||Z_e-Z_q||^2 =  69872.4 e-6 = 11.1 %)
Min.  Avg. Train Loss across Mini-Batch =  613312.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  596058.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4472.0 e-6; = (1/var)*||X-X_r||^2 val-train = -12904.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8432.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.37; perplexity/K = 22.46%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.70; perplexity/K = 22.96%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  524995.3 e-6; = (1/var)*||X-X_r||^2 =  494744.1 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  30251.1 e-6 = 5.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  521187.9 e-6; = (1/var)*||X-X_r||^2 =  488640.8 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  32547.2 e-6 = 6.2 %)
Min.  Avg. Train Loss across Mini-Batch =  524995.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  518219.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -3807.3 e-6; = (1/var)*||X-X_r||^2 val-train = -6103.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2296.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.43; perplexity/K = 22.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.96; perplexity/K = 21.82%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  489434.8 e-6; = (1/var)*||X-X_r||^2 =  449952.8 e-6 = 91.9 %; (1+beta)*||Z_e-Z_q||^2 =  39482.0 e-6 = 8.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  487115.6 e-6; = (1/var)*||X-X_r||^2 =  450071.2 e-6 = 92.4 %; (1+beta)*||Z_e-Z_q||^2 =  37044.4 e-6 = 7.6 %)
Min.  Avg. Train Loss across Mini-Batch =  487324.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  487115.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -2319.3 e-6; = (1/var)*||X-X_r||^2 val-train = 118.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2437.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.87; perplexity/K = 23.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.34; perplexity/K = 22.41%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  464903.1 e-6; = (1/var)*||X-X_r||^2 =  422930.7 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  41972.4 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  473438.0 e-6; = (1/var)*||X-X_r||^2 =  429687.4 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  43750.6 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  462144.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  470821.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   8534.9 e-6; = (1/var)*||X-X_r||^2 val-train = 6756.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1778.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.46; perplexity/K = 22.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.73; perplexity/K = 23.02%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  455516.8 e-6; = (1/var)*||X-X_r||^2 =  414390.5 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  41126.3 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  471261.2 e-6; = (1/var)*||X-X_r||^2 =  425337.7 e-6 = 90.3 %; (1+beta)*||Z_e-Z_q||^2 =  45923.6 e-6 = 9.7 %)
Min.  Avg. Train Loss across Mini-Batch =  447485.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  462631.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   15744.4 e-6; = (1/var)*||X-X_r||^2 val-train = 10947.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4797.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.78; perplexity/K = 23.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.66; perplexity/K = 22.90%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  443444.1 e-6; = (1/var)*||X-X_r||^2 =  406107.5 e-6 = 91.6 %; (1+beta)*||Z_e-Z_q||^2 =  37336.6 e-6 = 8.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  459477.7 e-6; = (1/var)*||X-X_r||^2 =  418676.0 e-6 = 91.1 %; (1+beta)*||Z_e-Z_q||^2 =  40801.7 e-6 = 8.9 %)
Min.  Avg. Train Loss across Mini-Batch =  437091.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  455763.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   16033.6 e-6; = (1/var)*||X-X_r||^2 val-train = 12568.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 3465.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.68; perplexity/K = 22.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.91; perplexity/K = 23.30%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  429035.1 e-6; = (1/var)*||X-X_r||^2 =  397035.0 e-6 = 92.5 %; (1+beta)*||Z_e-Z_q||^2 =  32000.0 e-6 = 7.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  458232.5 e-6; = (1/var)*||X-X_r||^2 =  418698.4 e-6 = 91.4 %; (1+beta)*||Z_e-Z_q||^2 =  39534.1 e-6 = 8.6 %)
Min.  Avg. Train Loss across Mini-Batch =  428805.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  453284.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   29197.5 e-6; = (1/var)*||X-X_r||^2 val-train = 21663.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7534.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.94; perplexity/K = 23.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.50; perplexity/K = 22.66%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  423346.0 e-6; = (1/var)*||X-X_r||^2 =  392933.5 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  30412.5 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  454714.4 e-6; = (1/var)*||X-X_r||^2 =  416139.2 e-6 = 91.5 %; (1+beta)*||Z_e-Z_q||^2 =  38575.1 e-6 = 8.5 %)
Min.  Avg. Train Loss across Mini-Batch =  422385.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  450825.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31368.4 e-6; = (1/var)*||X-X_r||^2 val-train = 23205.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8162.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.83; perplexity/K = 26.30%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.56; perplexity/K = 25.87%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  420680.9 e-6; = (1/var)*||X-X_r||^2 =  392709.1 e-6 = 93.4 %; (1+beta)*||Z_e-Z_q||^2 =  27971.8 e-6 = 6.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  448004.7 e-6; = (1/var)*||X-X_r||^2 =  415179.2 e-6 = 92.7 %; (1+beta)*||Z_e-Z_q||^2 =  32825.5 e-6 = 7.3 %)
Min.  Avg. Train Loss across Mini-Batch =  414320.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  441763.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   27323.8 e-6; = (1/var)*||X-X_r||^2 val-train = 22470.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4853.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.44; perplexity/K = 27.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.53; perplexity/K = 27.40%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  408233.1 e-6; = (1/var)*||X-X_r||^2 =  385105.4 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  23127.6 e-6 = 5.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  438871.4 e-6; = (1/var)*||X-X_r||^2 =  408513.5 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  30357.9 e-6 = 6.9 %)
Min.  Avg. Train Loss across Mini-Batch =  407800.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  435912.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   30638.3 e-6; = (1/var)*||X-X_r||^2 val-train = 23408.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7230.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.04; perplexity/K = 26.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.03; perplexity/K = 26.60%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  405066.7 e-6; = (1/var)*||X-X_r||^2 =  382839.5 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  22227.2 e-6 = 5.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  442728.6 e-6; = (1/var)*||X-X_r||^2 =  411171.7 e-6 = 92.9 %; (1+beta)*||Z_e-Z_q||^2 =  31556.9 e-6 = 7.1 %)
Min.  Avg. Train Loss across Mini-Batch =  403284.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  435912.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   37661.9 e-6; = (1/var)*||X-X_r||^2 val-train = 28332.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9329.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.53; perplexity/K = 25.83%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.91; perplexity/K = 24.86%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  427951.3 e-6; = (1/var)*||X-X_r||^2 =  397078.1 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  30873.2 e-6 = 7.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  446040.3 e-6; = (1/var)*||X-X_r||^2 =  414108.8 e-6 = 92.8 %; (1+beta)*||Z_e-Z_q||^2 =  31931.5 e-6 = 7.2 %)
Min.  Avg. Train Loss across Mini-Batch =  399796.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  434177.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   18089.0 e-6; = (1/var)*||X-X_r||^2 val-train = 17030.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1058.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.70; perplexity/K = 26.10%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.07; perplexity/K = 26.68%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:17 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  402497.6 e-6; = (1/var)*||X-X_r||^2 =  381253.2 e-6 = 94.7 %; (1+beta)*||Z_e-Z_q||^2 =  21244.4 e-6 = 5.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  436934.3 e-6; = (1/var)*||X-X_r||^2 =  409599.2 e-6 = 93.7 %; (1+beta)*||Z_e-Z_q||^2 =  27335.1 e-6 = 6.3 %)
Min.  Avg. Train Loss across Mini-Batch =  396512.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  432752.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   34436.7 e-6; = (1/var)*||X-X_r||^2 val-train = 28346.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6090.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.52; perplexity/K = 27.38%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.78; perplexity/K = 26.22%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  395569.1 e-6; = (1/var)*||X-X_r||^2 =  377676.9 e-6 = 95.5 %; (1+beta)*||Z_e-Z_q||^2 =  17892.2 e-6 = 4.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  435163.2 e-6; = (1/var)*||X-X_r||^2 =  409943.6 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  25219.6 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  393613.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  431642.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39594.1 e-6; = (1/var)*||X-X_r||^2 val-train = 32266.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7327.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.28; perplexity/K = 27.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.37; perplexity/K = 27.14%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  393823.6 e-6; = (1/var)*||X-X_r||^2 =  376541.9 e-6 = 95.6 %; (1+beta)*||Z_e-Z_q||^2 =  17281.8 e-6 = 4.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  433028.8 e-6; = (1/var)*||X-X_r||^2 =  408601.2 e-6 = 94.4 %; (1+beta)*||Z_e-Z_q||^2 =  24427.6 e-6 = 5.6 %)
Min.  Avg. Train Loss across Mini-Batch =  392638.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  429460.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   39205.1 e-6; = (1/var)*||X-X_r||^2 val-train = 32059.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7145.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.74; perplexity/K = 27.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.77; perplexity/K = 26.20%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  390425.8 e-6; = (1/var)*||X-X_r||^2 =  374570.7 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  15855.1 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  432480.2 e-6; = (1/var)*||X-X_r||^2 =  409222.6 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  23257.6 e-6 = 5.4 %)
Min.  Avg. Train Loss across Mini-Batch =  389664.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  429399.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   42054.4 e-6; = (1/var)*||X-X_r||^2 val-train = 34651.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7402.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.23; perplexity/K = 26.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.12; perplexity/K = 26.75%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  391153.4 e-6; = (1/var)*||X-X_r||^2 =  375086.3 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  16067.1 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  434379.5 e-6; = (1/var)*||X-X_r||^2 =  410566.9 e-6 = 94.5 %; (1+beta)*||Z_e-Z_q||^2 =  23812.7 e-6 = 5.5 %)
Min.  Avg. Train Loss across Mini-Batch =  388409.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  428833.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   43226.1 e-6; = (1/var)*||X-X_r||^2 val-train = 35480.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7745.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.86; perplexity/K = 26.34%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.74; perplexity/K = 26.16%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  388727.1 e-6; = (1/var)*||X-X_r||^2 =  373800.8 e-6 = 96.2 %; (1+beta)*||Z_e-Z_q||^2 =  14926.3 e-6 = 3.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  430068.0 e-6; = (1/var)*||X-X_r||^2 =  408340.6 e-6 = 94.9 %; (1+beta)*||Z_e-Z_q||^2 =  21727.4 e-6 = 5.1 %)
Min.  Avg. Train Loss across Mini-Batch =  386189.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426136.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   41340.9 e-6; = (1/var)*||X-X_r||^2 val-train = 34539.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6801.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.02; perplexity/K = 26.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.73; perplexity/K = 26.13%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:25 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  389792.3 e-6; = (1/var)*||X-X_r||^2 =  373898.7 e-6 = 95.9 %; (1+beta)*||Z_e-Z_q||^2 =  15893.6 e-6 = 4.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  428174.4 e-6; = (1/var)*||X-X_r||^2 =  407106.4 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  21068.0 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  385166.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  426136.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   38382.1 e-6; = (1/var)*||X-X_r||^2 val-train = 33207.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 5174.5 e-6 

----------------------------------------------------------------------------------

Finished [11:43:29 06.01.2023] 289) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = True:
Total training time is = 0:1:1 h/m/s. 

--------------------------------------------------- 

Started [11:43:29 06.01.2023] 290) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 24) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2470 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         8             0.32
2                           encoder.sequential_convs.conv2d_3.weight                        32             1.30
3                           encoder.sequential_convs.conv2d_4.weight                       131             5.30
4                           encoder.sequential_convs.conv2d_5.weight                       524            21.21
5                                  encoder.pre_residual_stack.weight                       589            23.85
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.96
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.96
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
10                             encoder.channel_adjusting_conv.weight                        16             0.65
11                                                       VQ.E.weight                         4             0.16
12                             decoder.channel_adjusting_conv.weight                       147             5.95
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.96
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.96
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
17                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.21
18                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.30
19                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.30
20                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
21                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.88; perplexity/K = 2.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.01; perplexity/K = 3.14%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1289936.4 e-6; = (1/var)*||X-X_r||^2 =  969871.2 e-6 = 75.2 %; (1+beta)*||Z_e-Z_q||^2 =  320065.2 e-6 = 24.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  993801.4 e-6; = (1/var)*||X-X_r||^2 =  936151.0 e-6 = 94.2 %; (1+beta)*||Z_e-Z_q||^2 =  57650.4 e-6 = 5.8 %)
Min.  Avg. Train Loss across Mini-Batch =  976384.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  942173.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -296135.0 e-6; = (1/var)*||X-X_r||^2 val-train = -33720.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -262414.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.04; perplexity/K = 4.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.82; perplexity/K = 4.40%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  964817.6 e-6; = (1/var)*||X-X_r||^2 =  961129.0 e-6 = 99.6 %; (1+beta)*||Z_e-Z_q||^2 =  3688.6 e-6 = 0.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  932933.6 e-6; = (1/var)*||X-X_r||^2 =  929702.1 e-6 = 99.7 %; (1+beta)*||Z_e-Z_q||^2 =  3231.5 e-6 = 0.3 %)
Min.  Avg. Train Loss across Mini-Batch =  963288.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  930793.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -31883.9 e-6; = (1/var)*||X-X_r||^2 val-train = -31426.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -457.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.29; perplexity/K = 16.08%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.95; perplexity/K = 15.54%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:33 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  583532.7 e-6; = (1/var)*||X-X_r||^2 =  530991.9 e-6 = 91.0 %; (1+beta)*||Z_e-Z_q||^2 =  52540.8 e-6 = 9.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  551470.5 e-6; = (1/var)*||X-X_r||^2 =  517363.0 e-6 = 93.8 %; (1+beta)*||Z_e-Z_q||^2 =  34107.6 e-6 = 6.2 %)
Min.  Avg. Train Loss across Mini-Batch =  576052.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  545990.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -32062.2 e-6; = (1/var)*||X-X_r||^2 val-train = -13629.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -18433.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.17; perplexity/K = 17.45%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.62; perplexity/K = 16.60%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  551374.0 e-6; = (1/var)*||X-X_r||^2 =  494514.9 e-6 = 89.7 %; (1+beta)*||Z_e-Z_q||^2 =  56859.0 e-6 = 10.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  558995.4 e-6; = (1/var)*||X-X_r||^2 =  502523.0 e-6 = 89.9 %; (1+beta)*||Z_e-Z_q||^2 =  56472.4 e-6 = 10.1 %)
Min.  Avg. Train Loss across Mini-Batch =  547762.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  542307.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   7621.4 e-6; = (1/var)*||X-X_r||^2 val-train = 8008.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -386.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.75; perplexity/K = 18.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.09; perplexity/K = 17.32%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  452191.9 e-6; = (1/var)*||X-X_r||^2 =  371355.0 e-6 = 82.1 %; (1+beta)*||Z_e-Z_q||^2 =  80836.9 e-6 = 17.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  483989.5 e-6; = (1/var)*||X-X_r||^2 =  380227.0 e-6 = 78.6 %; (1+beta)*||Z_e-Z_q||^2 =  103762.5 e-6 = 21.4 %)
Min.  Avg. Train Loss across Mini-Batch =  436317.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  446417.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   31797.6 e-6; = (1/var)*||X-X_r||^2 val-train = 8872.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22925.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.28; perplexity/K = 20.75%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.25; perplexity/K = 20.71%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:5 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  410912.7 e-6; = (1/var)*||X-X_r||^2 =  335385.5 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  75527.1 e-6 = 18.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  423303.7 e-6; = (1/var)*||X-X_r||^2 =  347096.0 e-6 = 82.0 %; (1+beta)*||Z_e-Z_q||^2 =  76207.7 e-6 = 18.0 %)
Min.  Avg. Train Loss across Mini-Batch =  402583.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  423161.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12391.0 e-6; = (1/var)*||X-X_r||^2 val-train = 11710.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 680.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.07; perplexity/K = 20.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.79; perplexity/K = 19.99%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  315284.1 e-6; = (1/var)*||X-X_r||^2 =  205049.8 e-6 = 65.0 %; (1+beta)*||Z_e-Z_q||^2 =  110234.3 e-6 = 35.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  374380.6 e-6; = (1/var)*||X-X_r||^2 =  252898.1 e-6 = 67.6 %; (1+beta)*||Z_e-Z_q||^2 =  121482.5 e-6 = 32.4 %)
Min.  Avg. Train Loss across Mini-Batch =  312635.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  351611.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   59096.5 e-6; = (1/var)*||X-X_r||^2 val-train = 47848.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11248.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.18; perplexity/K = 23.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.07; perplexity/K = 21.99%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:7 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  316046.5 e-6; = (1/var)*||X-X_r||^2 =  198588.5 e-6 = 62.8 %; (1+beta)*||Z_e-Z_q||^2 =  117458.0 e-6 = 37.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  370447.5 e-6; = (1/var)*||X-X_r||^2 =  243589.2 e-6 = 65.8 %; (1+beta)*||Z_e-Z_q||^2 =  126858.3 e-6 = 34.2 %)
Min.  Avg. Train Loss across Mini-Batch =  287706.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  345524.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   54401.0 e-6; = (1/var)*||X-X_r||^2 val-train = 45000.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 9400.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.18; perplexity/K = 20.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.80; perplexity/K = 23.13%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:38 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  291630.6 e-6; = (1/var)*||X-X_r||^2 =  180098.9 e-6 = 61.8 %; (1+beta)*||Z_e-Z_q||^2 =  111531.7 e-6 = 38.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  353336.4 e-6; = (1/var)*||X-X_r||^2 =  237297.7 e-6 = 67.2 %; (1+beta)*||Z_e-Z_q||^2 =  116038.7 e-6 = 32.8 %)
Min.  Avg. Train Loss across Mini-Batch =  266429.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  334834.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   61705.8 e-6; = (1/var)*||X-X_r||^2 val-train = 57198.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4507.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.08; perplexity/K = 22.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.81; perplexity/K = 23.15%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  253507.9 e-6; = (1/var)*||X-X_r||^2 =  153504.7 e-6 = 60.6 %; (1+beta)*||Z_e-Z_q||^2 =  100003.2 e-6 = 39.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  333132.8 e-6; = (1/var)*||X-X_r||^2 =  220754.9 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  112377.9 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  253507.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  327081.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   79624.9 e-6; = (1/var)*||X-X_r||^2 val-train = 67250.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 12374.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.07; perplexity/K = 21.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.50; perplexity/K = 22.65%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  344499.4 e-6; = (1/var)*||X-X_r||^2 =  212399.7 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  132099.7 e-6 = 38.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  417154.0 e-6; = (1/var)*||X-X_r||^2 =  266254.8 e-6 = 63.8 %; (1+beta)*||Z_e-Z_q||^2 =  150899.2 e-6 = 36.2 %)
Min.  Avg. Train Loss across Mini-Batch =  244852.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  323179.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   72654.6 e-6; = (1/var)*||X-X_r||^2 val-train = 53855.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18799.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.99; perplexity/K = 26.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.28; perplexity/K = 25.43%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:11 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  275101.3 e-6; = (1/var)*||X-X_r||^2 =  165357.0 e-6 = 60.1 %; (1+beta)*||Z_e-Z_q||^2 =  109744.3 e-6 = 39.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  336123.6 e-6; = (1/var)*||X-X_r||^2 =  215878.2 e-6 = 64.2 %; (1+beta)*||Z_e-Z_q||^2 =  120245.5 e-6 = 35.8 %)
Min.  Avg. Train Loss across Mini-Batch =  231561.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  321265.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   61022.4 e-6; = (1/var)*||X-X_r||^2 val-train = 50521.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10501.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.00; perplexity/K = 28.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.72; perplexity/K = 24.56%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  345977.4 e-6; = (1/var)*||X-X_r||^2 =  213588.7 e-6 = 61.7 %; (1+beta)*||Z_e-Z_q||^2 =  132388.7 e-6 = 38.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  405821.1 e-6; = (1/var)*||X-X_r||^2 =  259970.4 e-6 = 64.1 %; (1+beta)*||Z_e-Z_q||^2 =  145850.8 e-6 = 35.9 %)
Min.  Avg. Train Loss across Mini-Batch =  223750.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  313673.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   59843.8 e-6; = (1/var)*||X-X_r||^2 val-train = 46381.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 13462.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.43; perplexity/K = 27.24%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.09; perplexity/K = 28.26%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  234344.4 e-6; = (1/var)*||X-X_r||^2 =  141734.8 e-6 = 60.5 %; (1+beta)*||Z_e-Z_q||^2 =  92609.6 e-6 = 39.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  319836.0 e-6; = (1/var)*||X-X_r||^2 =  211151.6 e-6 = 66.0 %; (1+beta)*||Z_e-Z_q||^2 =  108684.4 e-6 = 34.0 %)
Min.  Avg. Train Loss across Mini-Batch =  208593.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  293494.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   85491.6 e-6; = (1/var)*||X-X_r||^2 val-train = 69416.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16074.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.78; perplexity/K = 27.78%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.00; perplexity/K = 26.56%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:37:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  248530.8 e-6; = (1/var)*||X-X_r||^2 =  152314.4 e-6 = 61.3 %; (1+beta)*||Z_e-Z_q||^2 =  96216.4 e-6 = 38.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  315439.4 e-6; = (1/var)*||X-X_r||^2 =  209181.5 e-6 = 66.3 %; (1+beta)*||Z_e-Z_q||^2 =  106257.9 e-6 = 33.7 %)
Min.  Avg. Train Loss across Mini-Batch =  195553.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  284095.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   66908.6 e-6; = (1/var)*||X-X_r||^2 val-train = 56867.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10041.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.66; perplexity/K = 27.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.74; perplexity/K = 26.15%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  184809.0 e-6; = (1/var)*||X-X_r||^2 =  108630.2 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  76178.8 e-6 = 41.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  290550.0 e-6; = (1/var)*||X-X_r||^2 =  197523.5 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  93026.6 e-6 = 32.0 %)
Min.  Avg. Train Loss across Mini-Batch =  182467.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  281279.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   105741.1 e-6; = (1/var)*||X-X_r||^2 val-train = 88893.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 16847.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.32; perplexity/K = 28.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.26; perplexity/K = 26.97%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  190429.4 e-6; = (1/var)*||X-X_r||^2 =  111868.1 e-6 = 58.7 %; (1+beta)*||Z_e-Z_q||^2 =  78561.3 e-6 = 41.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  292942.9 e-6; = (1/var)*||X-X_r||^2 =  199396.1 e-6 = 68.1 %; (1+beta)*||Z_e-Z_q||^2 =  93546.7 e-6 = 31.9 %)
Min.  Avg. Train Loss across Mini-Batch =  173408.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  281279.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   102513.4 e-6; = (1/var)*||X-X_r||^2 val-train = 87528.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14985.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.28; perplexity/K = 27.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.60; perplexity/K = 29.06%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  167756.9 e-6; = (1/var)*||X-X_r||^2 =  100233.2 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  67523.8 e-6 = 40.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  281035.3 e-6; = (1/var)*||X-X_r||^2 =  195280.4 e-6 = 69.5 %; (1+beta)*||Z_e-Z_q||^2 =  85755.0 e-6 = 30.5 %)
Min.  Avg. Train Loss across Mini-Batch =  166516.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  276160.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   113278.4 e-6; = (1/var)*||X-X_r||^2 val-train = 95047.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 18231.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.99; perplexity/K = 29.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.36; perplexity/K = 27.13%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  156487.6 e-6; = (1/var)*||X-X_r||^2 =  94920.5 e-6 = 60.7 %; (1+beta)*||Z_e-Z_q||^2 =  61567.0 e-6 = 39.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  276665.6 e-6; = (1/var)*||X-X_r||^2 =  194530.9 e-6 = 70.3 %; (1+beta)*||Z_e-Z_q||^2 =  82134.7 e-6 = 29.7 %)
Min.  Avg. Train Loss across Mini-Batch =  156487.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  275823.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   120178.0 e-6; = (1/var)*||X-X_r||^2 val-train = 99610.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20567.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.63; perplexity/K = 25.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.54; perplexity/K = 25.84%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  245468.8 e-6; = (1/var)*||X-X_r||^2 =  155429.1 e-6 = 63.3 %; (1+beta)*||Z_e-Z_q||^2 =  90039.7 e-6 = 36.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  375122.2 e-6; = (1/var)*||X-X_r||^2 =  262197.8 e-6 = 69.9 %; (1+beta)*||Z_e-Z_q||^2 =  112924.5 e-6 = 30.1 %)
Min.  Avg. Train Loss across Mini-Batch =  152446.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  272103.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   129653.4 e-6; = (1/var)*||X-X_r||^2 val-train = 106768.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22884.8 e-6 

----------------------------------------------------------------------------------

Finished [12:34:27 06.01.2023] 290) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 24) change_channel_size_across_layers = True:
Total training time is = 0:1:58 h/m/s. 

--------------------------------------------------- 

Started [12:34:27 06.01.2023] 291) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(128, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2500 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         6             0.24
1                           encoder.sequential_convs.conv2d_2.weight                       262            10.48
2                           encoder.sequential_convs.conv2d_3.weight                       262            10.48
3                           encoder.sequential_convs.conv2d_4.weight                       262            10.48
4                           encoder.sequential_convs.conv2d_5.weight                       262            10.48
5                                  encoder.pre_residual_stack.weight                       147             5.88
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.44
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.44
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
10                             encoder.channel_adjusting_conv.weight                         8             0.32
11                                                       VQ.E.weight                         4             0.16
12                             decoder.channel_adjusting_conv.weight                        73             2.92
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             1.44
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.16
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             1.44
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.16
17                    decoder.sequential_trans_convs.conv2d_1.weight                       262            10.48
18                    decoder.sequential_trans_convs.conv2d_2.weight                       262            10.48
19                    decoder.sequential_trans_convs.conv2d_3.weight                       262            10.48
20                    decoder.sequential_trans_convs.conv2d_4.weight                       262            10.48
21                    decoder.sequential_trans_convs.conv2d_5.weight                         6             0.24

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.34; perplexity/K = 17.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.00; perplexity/K = 17.18%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  699897.0 e-6; = (1/var)*||X-X_r||^2 =  573551.7 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  126345.3 e-6 = 18.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  662855.5 e-6; = (1/var)*||X-X_r||^2 =  561920.2 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  100935.4 e-6 = 15.2 %)
Min.  Avg. Train Loss across Mini-Batch =  699897.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  661668.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -37041.5 e-6; = (1/var)*||X-X_r||^2 val-train = -11631.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25410.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.17; perplexity/K = 31.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.67; perplexity/K = 30.74%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  380219.5 e-6; = (1/var)*||X-X_r||^2 =  231439.5 e-6 = 60.9 %; (1+beta)*||Z_e-Z_q||^2 =  148780.0 e-6 = 39.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  420983.2 e-6; = (1/var)*||X-X_r||^2 =  267562.0 e-6 = 63.6 %; (1+beta)*||Z_e-Z_q||^2 =  153421.1 e-6 = 36.4 %)
Min.  Avg. Train Loss across Mini-Batch =  380219.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  393980.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   40763.7 e-6; = (1/var)*||X-X_r||^2 val-train = 36122.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4641.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.62; perplexity/K = 36.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.79; perplexity/K = 35.61%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  266564.4 e-6; = (1/var)*||X-X_r||^2 =  130972.9 e-6 = 49.1 %; (1+beta)*||Z_e-Z_q||^2 =  135591.5 e-6 = 50.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  317027.3 e-6; = (1/var)*||X-X_r||^2 =  173281.7 e-6 = 54.7 %; (1+beta)*||Z_e-Z_q||^2 =  143745.5 e-6 = 45.3 %)
Min.  Avg. Train Loss across Mini-Batch =  262087.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  317027.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   50462.8 e-6; = (1/var)*||X-X_r||^2 val-train = 42308.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8154.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.45; perplexity/K = 38.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.33; perplexity/K = 38.01%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  199898.5 e-6; = (1/var)*||X-X_r||^2 =  87322.4 e-6 = 43.7 %; (1+beta)*||Z_e-Z_q||^2 =  112576.2 e-6 = 56.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  288864.1 e-6; = (1/var)*||X-X_r||^2 =  158910.9 e-6 = 55.0 %; (1+beta)*||Z_e-Z_q||^2 =  129953.2 e-6 = 45.0 %)
Min.  Avg. Train Loss across Mini-Batch =  173964.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  263452.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   88965.6 e-6; = (1/var)*||X-X_r||^2 val-train = 71588.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 17377.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.10; perplexity/K = 39.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.63; perplexity/K = 38.49%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  124363.7 e-6; = (1/var)*||X-X_r||^2 =  52610.5 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  71753.2 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  246311.1 e-6; = (1/var)*||X-X_r||^2 =  147102.2 e-6 = 59.7 %; (1+beta)*||Z_e-Z_q||^2 =  99209.0 e-6 = 40.3 %)
Min.  Avg. Train Loss across Mini-Batch =  111641.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  235268.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   121947.4 e-6; = (1/var)*||X-X_r||^2 val-train = 94491.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 27455.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.12; perplexity/K = 39.25%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.84; perplexity/K = 37.26%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  74638.7 e-6; = (1/var)*||X-X_r||^2 =  29759.6 e-6 = 39.9 %; (1+beta)*||Z_e-Z_q||^2 =  44879.2 e-6 = 60.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  214215.6 e-6; = (1/var)*||X-X_r||^2 =  136009.7 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  78205.9 e-6 = 36.5 %)
Min.  Avg. Train Loss across Mini-Batch =  74638.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  214215.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   139576.8 e-6; = (1/var)*||X-X_r||^2 val-train = 106250.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 33326.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.55; perplexity/K = 39.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.68; perplexity/K = 38.56%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:22:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  53469.6 e-6; = (1/var)*||X-X_r||^2 =  22876.6 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  30593.0 e-6 = 57.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  209239.8 e-6; = (1/var)*||X-X_r||^2 =  143401.9 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  65837.9 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  51437.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  202716.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   155770.2 e-6; = (1/var)*||X-X_r||^2 val-train = 120525.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 35244.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.50; perplexity/K = 38.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.37; perplexity/K = 38.08%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  36232.7 e-6; = (1/var)*||X-X_r||^2 =  15909.3 e-6 = 43.9 %; (1+beta)*||Z_e-Z_q||^2 =  20323.4 e-6 = 56.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  204136.1 e-6; = (1/var)*||X-X_r||^2 =  145762.8 e-6 = 71.4 %; (1+beta)*||Z_e-Z_q||^2 =  58373.3 e-6 = 28.6 %)
Min.  Avg. Train Loss across Mini-Batch =  36232.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  197725.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   167903.5 e-6; = (1/var)*||X-X_r||^2 val-train = 129853.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38049.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.32; perplexity/K = 38.00%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.23; perplexity/K = 37.86%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  28042.3 e-6; = (1/var)*||X-X_r||^2 =  13216.4 e-6 = 47.1 %; (1+beta)*||Z_e-Z_q||^2 =  14825.9 e-6 = 52.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  201157.4 e-6; = (1/var)*||X-X_r||^2 =  145069.7 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  56087.7 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  28042.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  197275.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   173115.0 e-6; = (1/var)*||X-X_r||^2 val-train = 131853.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 41261.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.25; perplexity/K = 37.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.47; perplexity/K = 38.23%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:32:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  25998.5 e-6; = (1/var)*||X-X_r||^2 =  11396.5 e-6 = 43.8 %; (1+beta)*||Z_e-Z_q||^2 =  14602.0 e-6 = 56.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  194612.4 e-6; = (1/var)*||X-X_r||^2 =  141390.0 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  53222.4 e-6 = 27.3 %)
Min.  Avg. Train Loss across Mini-Batch =  23142.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  189790.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   168613.9 e-6; = (1/var)*||X-X_r||^2 val-train = 129993.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38620.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.57; perplexity/K = 38.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.97; perplexity/K = 39.02%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:35:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  19939.6 e-6; = (1/var)*||X-X_r||^2 =  9707.3 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  10232.3 e-6 = 51.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  190596.7 e-6; = (1/var)*||X-X_r||^2 =  141152.0 e-6 = 74.1 %; (1+beta)*||Z_e-Z_q||^2 =  49444.8 e-6 = 25.9 %)
Min.  Avg. Train Loss across Mini-Batch =  18833.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  189107.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   170657.1 e-6; = (1/var)*||X-X_r||^2 val-train = 131444.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 39212.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.53; perplexity/K = 39.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.16; perplexity/K = 39.31%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  15994.1 e-6; = (1/var)*||X-X_r||^2 =  7936.4 e-6 = 49.6 %; (1+beta)*||Z_e-Z_q||^2 =  8057.7 e-6 = 50.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  193767.6 e-6; = (1/var)*||X-X_r||^2 =  146997.4 e-6 = 75.9 %; (1+beta)*||Z_e-Z_q||^2 =  46770.2 e-6 = 24.1 %)
Min.  Avg. Train Loss across Mini-Batch =  15994.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  189107.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   177773.5 e-6; = (1/var)*||X-X_r||^2 val-train = 139061.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38712.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.95; perplexity/K = 37.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.73; perplexity/K = 38.64%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:42:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  138281.6 e-6; = (1/var)*||X-X_r||^2 =  72692.0 e-6 = 52.6 %; (1+beta)*||Z_e-Z_q||^2 =  65589.7 e-6 = 47.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  282139.8 e-6; = (1/var)*||X-X_r||^2 =  182543.9 e-6 = 64.7 %; (1+beta)*||Z_e-Z_q||^2 =  99596.0 e-6 = 35.3 %)
Min.  Avg. Train Loss across Mini-Batch =  13125.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  189107.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   143858.2 e-6; = (1/var)*||X-X_r||^2 val-train = 109851.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34006.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.43; perplexity/K = 39.73%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.40; perplexity/K = 38.12%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:45:31 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13487.9 e-6; = (1/var)*||X-X_r||^2 =  6500.1 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  6987.8 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  192626.3 e-6; = (1/var)*||X-X_r||^2 =  145482.1 e-6 = 75.5 %; (1+beta)*||Z_e-Z_q||^2 =  47144.2 e-6 = 24.5 %)
Min.  Avg. Train Loss across Mini-Batch =  12953.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  189107.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   179138.4 e-6; = (1/var)*||X-X_r||^2 val-train = 138982.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 40156.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.90; perplexity/K = 38.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.00; perplexity/K = 37.49%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:46 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  18343.4 e-6; = (1/var)*||X-X_r||^2 =  7461.2 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  10882.1 e-6 = 59.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  197527.1 e-6; = (1/var)*||X-X_r||^2 =  147689.4 e-6 = 74.8 %; (1+beta)*||Z_e-Z_q||^2 =  49837.7 e-6 = 25.2 %)
Min.  Avg. Train Loss across Mini-Batch =  11604.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  187590.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   179183.7 e-6; = (1/var)*||X-X_r||^2 val-train = 140228.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38955.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.02; perplexity/K = 39.09%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 25.90; perplexity/K = 40.46%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:52:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  29866.6 e-6; = (1/var)*||X-X_r||^2 =  9352.4 e-6 = 31.3 %; (1+beta)*||Z_e-Z_q||^2 =  20514.2 e-6 = 68.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  192950.9 e-6; = (1/var)*||X-X_r||^2 =  140347.4 e-6 = 72.7 %; (1+beta)*||Z_e-Z_q||^2 =  52603.4 e-6 = 27.3 %)
Min.  Avg. Train Loss across Mini-Batch =  10869.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  187590.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   163084.3 e-6; = (1/var)*||X-X_r||^2 val-train = 130995.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 32089.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.88; perplexity/K = 38.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.05; perplexity/K = 37.57%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:55:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10696.1 e-6; = (1/var)*||X-X_r||^2 =  5514.0 e-6 = 51.6 %; (1+beta)*||Z_e-Z_q||^2 =  5182.0 e-6 = 48.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  192640.3 e-6; = (1/var)*||X-X_r||^2 =  148663.0 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  43977.2 e-6 = 22.8 %)
Min.  Avg. Train Loss across Mini-Batch =  9824.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  187590.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   181944.2 e-6; = (1/var)*||X-X_r||^2 val-train = 143149.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 38795.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.75; perplexity/K = 38.66%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.35; perplexity/K = 38.05%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:58:32 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16926.3 e-6; = (1/var)*||X-X_r||^2 =  6403.1 e-6 = 37.8 %; (1+beta)*||Z_e-Z_q||^2 =  10523.2 e-6 = 62.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  192966.0 e-6; = (1/var)*||X-X_r||^2 =  147673.1 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  45292.9 e-6 = 23.5 %)
Min.  Avg. Train Loss across Mini-Batch =  9260.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  187579.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   176039.7 e-6; = (1/var)*||X-X_r||^2 val-train = 141270.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 34769.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.71; perplexity/K = 38.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.49; perplexity/K = 38.27%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:47 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  10465.3 e-6; = (1/var)*||X-X_r||^2 =  5047.1 e-6 = 48.2 %; (1+beta)*||Z_e-Z_q||^2 =  5418.2 e-6 = 51.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  190437.3 e-6; = (1/var)*||X-X_r||^2 =  147847.4 e-6 = 77.6 %; (1+beta)*||Z_e-Z_q||^2 =  42589.9 e-6 = 22.4 %)
Min.  Avg. Train Loss across Mini-Batch =  8977.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186582.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   179972.0 e-6; = (1/var)*||X-X_r||^2 val-train = 142800.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 37171.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.39; perplexity/K = 38.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 24.16; perplexity/K = 37.76%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:5:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  32035.6 e-6; = (1/var)*||X-X_r||^2 =  10027.8 e-6 = 31.3 %; (1+beta)*||Z_e-Z_q||^2 =  22007.8 e-6 = 68.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  200150.3 e-6; = (1/var)*||X-X_r||^2 =  148709.2 e-6 = 74.3 %; (1+beta)*||Z_e-Z_q||^2 =  51441.1 e-6 = 25.7 %)
Min.  Avg. Train Loss across Mini-Batch =  8776.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  186582.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   168114.7 e-6; = (1/var)*||X-X_r||^2 val-train = 138681.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 29433.4 e-6 

----------------------------------------------------------------------------------

Finished [13:40:07 06.01.2023] 291) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 24) change_channel_size_across_layers = False:
Total training time is = 0:1:39 h/m/s. 

--------------------------------------------------- 

Started [13:40:07 06.01.2023] 292) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 24) change_channel_size_across_layers = False:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(256, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(256, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 9488 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                        12             0.13
1                           encoder.sequential_convs.conv2d_2.weight                      1048            11.05
2                           encoder.sequential_convs.conv2d_3.weight                      1048            11.05
3                           encoder.sequential_convs.conv2d_4.weight                      1048            11.05
4                           encoder.sequential_convs.conv2d_5.weight                      1048            11.05
5                                  encoder.pre_residual_stack.weight                       589             6.21
6   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
7   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
8   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
9   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
10                             encoder.channel_adjusting_conv.weight                        16             0.17
11                                                       VQ.E.weight                         4             0.04
12                             decoder.channel_adjusting_conv.weight                       147             1.55
13  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             0.77
14  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.08
15  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             0.77
16  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.08
17                    decoder.sequential_trans_convs.conv2d_1.weight                      1048            11.05
18                    decoder.sequential_trans_convs.conv2d_2.weight                      1048            11.05
19                    decoder.sequential_trans_convs.conv2d_3.weight                      1048            11.05
20                    decoder.sequential_trans_convs.conv2d_4.weight                      1048            11.05
21                    decoder.sequential_trans_convs.conv2d_5.weight                        12             0.13

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.08; perplexity/K = 25.13%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.37; perplexity/K = 28.70%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  756772.0 e-6; = (1/var)*||X-X_r||^2 =  339220.5 e-6 = 44.8 %; (1+beta)*||Z_e-Z_q||^2 =  417551.5 e-6 = 55.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  645984.8 e-6; = (1/var)*||X-X_r||^2 =  343362.4 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  302622.3 e-6 = 46.8 %)
Min.  Avg. Train Loss across Mini-Batch =  602319.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  555293.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -110787.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4142.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -114929.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.61; perplexity/K = 41.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 26.16; perplexity/K = 40.87%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  302376.6 e-6; = (1/var)*||X-X_r||^2 =  117094.7 e-6 = 38.7 %; (1+beta)*||Z_e-Z_q||^2 =  185281.9 e-6 = 61.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  370576.0 e-6; = (1/var)*||X-X_r||^2 =  178977.8 e-6 = 48.3 %; (1+beta)*||Z_e-Z_q||^2 =  191598.2 e-6 = 51.7 %)
Min.  Avg. Train Loss across Mini-Batch =  297216.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  365284.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   68199.4 e-6; = (1/var)*||X-X_r||^2 val-train = 61883.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 6316.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 32.21; perplexity/K = 50.32%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 30.89; perplexity/K = 48.27%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  155402.5 e-6; = (1/var)*||X-X_r||^2 =  43743.3 e-6 = 28.1 %; (1+beta)*||Z_e-Z_q||^2 =  111659.2 e-6 = 71.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  257208.5 e-6; = (1/var)*||X-X_r||^2 =  130624.5 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  126584.0 e-6 = 49.2 %)
Min.  Avg. Train Loss across Mini-Batch =  138155.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  254812.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   101806.1 e-6; = (1/var)*||X-X_r||^2 val-train = 86881.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 14924.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.77; perplexity/K = 52.76%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 31.83; perplexity/K = 49.73%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:27:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  49217.2 e-6; = (1/var)*||X-X_r||^2 =  10461.3 e-6 = 21.3 %; (1+beta)*||Z_e-Z_q||^2 =  38755.8 e-6 = 78.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  187589.0 e-6; = (1/var)*||X-X_r||^2 =  123228.3 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  64360.7 e-6 = 34.3 %)
Min.  Avg. Train Loss across Mini-Batch =  35441.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  174138.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   138371.8 e-6; = (1/var)*||X-X_r||^2 val-train = 112766.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25604.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.74; perplexity/K = 54.28%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.29; perplexity/K = 55.14%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  21416.8 e-6; = (1/var)*||X-X_r||^2 =  5686.4 e-6 = 26.6 %; (1+beta)*||Z_e-Z_q||^2 =  15730.4 e-6 = 73.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  160242.4 e-6; = (1/var)*||X-X_r||^2 =  116010.5 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  44231.9 e-6 = 27.6 %)
Min.  Avg. Train Loss across Mini-Batch =  17761.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  158690.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   138825.6 e-6; = (1/var)*||X-X_r||^2 val-train = 110324.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28501.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.57; perplexity/K = 55.58%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.77; perplexity/K = 55.88%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9579.4 e-6; = (1/var)*||X-X_r||^2 =  2786.2 e-6 = 29.1 %; (1+beta)*||Z_e-Z_q||^2 =  6793.3 e-6 = 70.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  152112.6 e-6; = (1/var)*||X-X_r||^2 =  117302.6 e-6 = 77.1 %; (1+beta)*||Z_e-Z_q||^2 =  34810.0 e-6 = 22.9 %)
Min.  Avg. Train Loss across Mini-Batch =  9579.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   142533.2 e-6; = (1/var)*||X-X_r||^2 val-train = 114516.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28016.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.09; perplexity/K = 56.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.39; perplexity/K = 56.85%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:47:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9357.1 e-6; = (1/var)*||X-X_r||^2 =  2912.0 e-6 = 31.1 %; (1+beta)*||Z_e-Z_q||^2 =  6445.1 e-6 = 68.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  147975.7 e-6; = (1/var)*||X-X_r||^2 =  116681.7 e-6 = 78.9 %; (1+beta)*||Z_e-Z_q||^2 =  31294.0 e-6 = 21.1 %)
Min.  Avg. Train Loss across Mini-Batch =  7675.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   138618.6 e-6; = (1/var)*||X-X_r||^2 val-train = 113769.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24848.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.13; perplexity/K = 58.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.47; perplexity/K = 56.99%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:54:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59342.2 e-6; = (1/var)*||X-X_r||^2 =  24289.1 e-6 = 40.9 %; (1+beta)*||Z_e-Z_q||^2 =  35053.0 e-6 = 59.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  206481.2 e-6; = (1/var)*||X-X_r||^2 =  145801.1 e-6 = 70.6 %; (1+beta)*||Z_e-Z_q||^2 =  60680.1 e-6 = 29.4 %)
Min.  Avg. Train Loss across Mini-Batch =  5984.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   147139.0 e-6; = (1/var)*||X-X_r||^2 val-train = 121512.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25627.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.22; perplexity/K = 56.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.76; perplexity/K = 55.87%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:1:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11944.8 e-6; = (1/var)*||X-X_r||^2 =  2282.2 e-6 = 19.1 %; (1+beta)*||Z_e-Z_q||^2 =  9662.6 e-6 = 80.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  156846.1 e-6; = (1/var)*||X-X_r||^2 =  124704.1 e-6 = 79.5 %; (1+beta)*||Z_e-Z_q||^2 =  32142.1 e-6 = 20.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4243.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   144901.3 e-6; = (1/var)*||X-X_r||^2 val-train = 122421.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22479.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.33; perplexity/K = 53.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.85; perplexity/K = 57.58%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:8:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2722.2 e-6; = (1/var)*||X-X_r||^2 =  969.9 e-6 = 35.6 %; (1+beta)*||Z_e-Z_q||^2 =  1752.3 e-6 = 64.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  156773.9 e-6; = (1/var)*||X-X_r||^2 =  128334.3 e-6 = 81.9 %; (1+beta)*||Z_e-Z_q||^2 =  28439.7 e-6 = 18.1 %)
Min.  Avg. Train Loss across Mini-Batch =  2411.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   154051.7 e-6; = (1/var)*||X-X_r||^2 val-train = 127364.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26687.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.79; perplexity/K = 55.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.10; perplexity/K = 57.96%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:15:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8195.9 e-6; = (1/var)*||X-X_r||^2 =  2504.1 e-6 = 30.6 %; (1+beta)*||Z_e-Z_q||^2 =  5691.8 e-6 = 69.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  166703.4 e-6; = (1/var)*||X-X_r||^2 =  135140.4 e-6 = 81.1 %; (1+beta)*||Z_e-Z_q||^2 =  31563.1 e-6 = 18.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1733.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   158507.5 e-6; = (1/var)*||X-X_r||^2 val-train = 132636.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 25871.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.50; perplexity/K = 55.47%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.77; perplexity/K = 59.01%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:21:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4963.9 e-6; = (1/var)*||X-X_r||^2 =  910.6 e-6 = 18.3 %; (1+beta)*||Z_e-Z_q||^2 =  4053.3 e-6 = 81.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  160388.7 e-6; = (1/var)*||X-X_r||^2 =  130084.4 e-6 = 81.1 %; (1+beta)*||Z_e-Z_q||^2 =  30304.2 e-6 = 18.9 %)
Min.  Avg. Train Loss across Mini-Batch =  1472.9 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   155424.7 e-6; = (1/var)*||X-X_r||^2 val-train = 129173.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26250.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.81; perplexity/K = 55.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.07; perplexity/K = 57.92%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:28:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1540.4 e-6; = (1/var)*||X-X_r||^2 =  658.9 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  881.5 e-6 = 57.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  160513.9 e-6; = (1/var)*||X-X_r||^2 =  131092.9 e-6 = 81.7 %; (1+beta)*||Z_e-Z_q||^2 =  29421.0 e-6 = 18.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1435.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   158973.5 e-6; = (1/var)*||X-X_r||^2 val-train = 130433.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 28539.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.39; perplexity/K = 56.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.97; perplexity/K = 57.76%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:35:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  16034.5 e-6; = (1/var)*||X-X_r||^2 =  3049.3 e-6 = 19.0 %; (1+beta)*||Z_e-Z_q||^2 =  12985.2 e-6 = 81.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  161275.6 e-6; = (1/var)*||X-X_r||^2 =  124450.2 e-6 = 77.2 %; (1+beta)*||Z_e-Z_q||^2 =  36825.4 e-6 = 22.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1429.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   145241.2 e-6; = (1/var)*||X-X_r||^2 val-train = 121400.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 23840.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.78; perplexity/K = 54.35%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.54; perplexity/K = 53.97%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:42:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  13287.8 e-6; = (1/var)*||X-X_r||^2 =  1783.4 e-6 = 13.4 %; (1+beta)*||Z_e-Z_q||^2 =  11504.4 e-6 = 86.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  160296.5 e-6; = (1/var)*||X-X_r||^2 =  127577.9 e-6 = 79.6 %; (1+beta)*||Z_e-Z_q||^2 =  32718.6 e-6 = 20.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1013.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   147008.7 e-6; = (1/var)*||X-X_r||^2 val-train = 125794.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 21214.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 33.93; perplexity/K = 53.01%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.32; perplexity/K = 58.31%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:49:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3158.5 e-6; = (1/var)*||X-X_r||^2 =  646.2 e-6 = 20.5 %; (1+beta)*||Z_e-Z_q||^2 =  2512.3 e-6 = 79.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  157455.2 e-6; = (1/var)*||X-X_r||^2 =  129960.9 e-6 = 82.5 %; (1+beta)*||Z_e-Z_q||^2 =  27494.3 e-6 = 17.5 %)
Min.  Avg. Train Loss across Mini-Batch =  912.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   154296.7 e-6; = (1/var)*||X-X_r||^2 val-train = 129314.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24982.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 34.41; perplexity/K = 53.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.93; perplexity/K = 59.27%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:56:8 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6071.3 e-6; = (1/var)*||X-X_r||^2 =  925.9 e-6 = 15.2 %; (1+beta)*||Z_e-Z_q||^2 =  5145.4 e-6 = 84.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  152827.7 e-6; = (1/var)*||X-X_r||^2 =  122794.6 e-6 = 80.3 %; (1+beta)*||Z_e-Z_q||^2 =  30033.1 e-6 = 19.7 %)
Min.  Avg. Train Loss across Mini-Batch =  777.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   146756.4 e-6; = (1/var)*||X-X_r||^2 val-train = 121868.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 24887.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 38.32; perplexity/K = 59.88%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 37.42; perplexity/K = 58.47%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:2:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4701.2 e-6; = (1/var)*||X-X_r||^2 =  903.4 e-6 = 19.2 %; (1+beta)*||Z_e-Z_q||^2 =  3797.8 e-6 = 80.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  164153.4 e-6; = (1/var)*||X-X_r||^2 =  133934.3 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  30219.1 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  736.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159452.2 e-6; = (1/var)*||X-X_r||^2 val-train = 133030.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26421.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.60; perplexity/K = 57.18%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.74; perplexity/K = 55.84%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:9:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1026.0 e-6; = (1/var)*||X-X_r||^2 =  536.0 e-6 = 52.2 %; (1+beta)*||Z_e-Z_q||^2 =  490.1 e-6 = 47.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  160260.1 e-6; = (1/var)*||X-X_r||^2 =  133711.0 e-6 = 83.4 %; (1+beta)*||Z_e-Z_q||^2 =  26549.0 e-6 = 16.6 %)
Min.  Avg. Train Loss across Mini-Batch =  736.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   159234.0 e-6; = (1/var)*||X-X_r||^2 val-train = 133175.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26059.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 36.12; perplexity/K = 56.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 35.39; perplexity/K = 55.30%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:3 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 2:16:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2392.5 e-6; = (1/var)*||X-X_r||^2 =  617.6 e-6 = 25.8 %; (1+beta)*||Z_e-Z_q||^2 =  1774.9 e-6 = 74.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  151597.1 e-6; = (1/var)*||X-X_r||^2 =  123644.0 e-6 = 81.6 %; (1+beta)*||Z_e-Z_q||^2 =  27953.0 e-6 = 18.4 %)
Min.  Avg. Train Loss across Mini-Batch =  736.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  142719.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   149204.6 e-6; = (1/var)*||X-X_r||^2 val-train = 123026.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26178.2 e-6 

----------------------------------------------------------------------------------

Finished [15:57:22 06.01.2023] 292) Finished running for K = 64 & D = 64 & M = 1 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 24) change_channel_size_across_layers = False:
Total training time is = 0:1:14 h/m/s. 

--------------------------------------------------- 

Started [15:57:22 06.01.2023] 293) Finished running for K = 64 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 6) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(4, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(8, 4, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(4, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 738 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         0             0.00
2                           encoder.sequential_convs.conv2d_3.weight                         2             0.27
3                           encoder.sequential_convs.conv2d_4.weight                         8             1.08
4                           encoder.sequential_convs.conv2d_5.weight                        32             4.34
5                           encoder.sequential_convs.conv2d_6.weight                       131            17.75
6                                  encoder.pre_residual_stack.weight                       147            19.92
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
11                             encoder.channel_adjusting_conv.weight                         8             1.08
12                                                       VQ.E.weight                         4             0.54
13                             decoder.channel_adjusting_conv.weight                        73             9.89
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.88
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.54
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.88
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.54
18                    decoder.sequential_trans_convs.conv2d_1.weight                       131            17.75
19                    decoder.sequential_trans_convs.conv2d_2.weight                        32             4.34
20                    decoder.sequential_trans_convs.conv2d_3.weight                         8             1.08
21                    decoder.sequential_trans_convs.conv2d_4.weight                         2             0.27
22                    decoder.sequential_trans_convs.conv2d_5.weight                         0             0.00
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.17; perplexity/K = 8.07%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.66; perplexity/K = 7.29%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2069684.1 e-6; = (1/var)*||X-X_r||^2 =  875573.5 e-6 = 42.3 %; (1+beta)*||Z_e-Z_q||^2 =  1194110.6 e-6 = 57.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2102116.2 e-6; = (1/var)*||X-X_r||^2 =  856443.1 e-6 = 40.7 %; (1+beta)*||Z_e-Z_q||^2 =  1245673.1 e-6 = 59.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1016133.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962168.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   32432.1 e-6; = (1/var)*||X-X_r||^2 val-train = -19130.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 51562.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.67; perplexity/K = 11.98%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.51; perplexity/K = 13.29%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1392586.7 e-6; = (1/var)*||X-X_r||^2 =  702580.5 e-6 = 50.5 %; (1+beta)*||Z_e-Z_q||^2 =  690006.1 e-6 = 49.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1267136.1 e-6; = (1/var)*||X-X_r||^2 =  681282.0 e-6 = 53.8 %; (1+beta)*||Z_e-Z_q||^2 =  585854.1 e-6 = 46.2 %)
Min.  Avg. Train Loss across Mini-Batch =  1016133.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  962168.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -125450.5 e-6; = (1/var)*||X-X_r||^2 val-train = -21298.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -104152.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.27; perplexity/K = 5.12%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.62; perplexity/K = 4.09%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:45 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  958406.1 e-6; = (1/var)*||X-X_r||^2 =  704193.6 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  254212.5 e-6 = 26.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  884959.9 e-6; = (1/var)*||X-X_r||^2 =  687306.0 e-6 = 77.7 %; (1+beta)*||Z_e-Z_q||^2 =  197653.9 e-6 = 22.3 %)
Min.  Avg. Train Loss across Mini-Batch =  948688.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  860234.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -73446.2 e-6; = (1/var)*||X-X_r||^2 val-train = -16887.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -56558.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.61; perplexity/K = 2.52%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.03; perplexity/K = 3.18%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  894009.9 e-6; = (1/var)*||X-X_r||^2 =  738691.0 e-6 = 82.6 %; (1+beta)*||Z_e-Z_q||^2 =  155318.9 e-6 = 17.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  850203.2 e-6; = (1/var)*||X-X_r||^2 =  719274.3 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  130928.9 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  886134.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  842806.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -43806.7 e-6; = (1/var)*||X-X_r||^2 val-train = -19416.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -24390.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.41; perplexity/K = 2.20%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.20; perplexity/K = 1.88%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:54 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  906173.4 e-6; = (1/var)*||X-X_r||^2 =  839478.9 e-6 = 92.6 %; (1+beta)*||Z_e-Z_q||^2 =  66694.5 e-6 = 7.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  896462.0 e-6; = (1/var)*||X-X_r||^2 =  818853.0 e-6 = 91.3 %; (1+beta)*||Z_e-Z_q||^2 =  77609.0 e-6 = 8.7 %)
Min.  Avg. Train Loss across Mini-Batch =  878354.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  842806.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -9711.4 e-6; = (1/var)*||X-X_r||^2 val-train = -20626.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 10914.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.05; perplexity/K = 3.21%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.43; perplexity/K = 3.80%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:29 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  892077.3 e-6; = (1/var)*||X-X_r||^2 =  753485.2 e-6 = 84.5 %; (1+beta)*||Z_e-Z_q||^2 =  138592.1 e-6 = 15.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  887542.1 e-6; = (1/var)*||X-X_r||^2 =  750897.5 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  136644.6 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  878354.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  842806.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -4535.2 e-6; = (1/var)*||X-X_r||^2 val-train = -2587.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1947.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.97; perplexity/K = 4.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.84; perplexity/K = 4.44%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:4 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  872105.1 e-6; = (1/var)*||X-X_r||^2 =  732267.9 e-6 = 84.0 %; (1+beta)*||Z_e-Z_q||^2 =  139837.2 e-6 = 16.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  835162.5 e-6; = (1/var)*||X-X_r||^2 =  714529.4 e-6 = 85.6 %; (1+beta)*||Z_e-Z_q||^2 =  120633.1 e-6 = 14.4 %)
Min.  Avg. Train Loss across Mini-Batch =  858640.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  828365.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36942.6 e-6; = (1/var)*||X-X_r||^2 val-train = -17738.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -19204.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.33; perplexity/K = 3.63%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.71; perplexity/K = 4.23%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:39 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  881543.7 e-6; = (1/var)*||X-X_r||^2 =  774651.4 e-6 = 87.9 %; (1+beta)*||Z_e-Z_q||^2 =  106892.3 e-6 = 12.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  886892.1 e-6; = (1/var)*||X-X_r||^2 =  782128.6 e-6 = 88.2 %; (1+beta)*||Z_e-Z_q||^2 =  104763.4 e-6 = 11.8 %)
Min.  Avg. Train Loss across Mini-Batch =  852666.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  819092.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   5348.3 e-6; = (1/var)*||X-X_r||^2 val-train = 7477.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -2128.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.42; perplexity/K = 11.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.71; perplexity/K = 10.49%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  849711.2 e-6; = (1/var)*||X-X_r||^2 =  715383.8 e-6 = 84.2 %; (1+beta)*||Z_e-Z_q||^2 =  134327.4 e-6 = 15.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  809075.2 e-6; = (1/var)*||X-X_r||^2 =  698993.3 e-6 = 86.4 %; (1+beta)*||Z_e-Z_q||^2 =  110081.9 e-6 = 13.6 %)
Min.  Avg. Train Loss across Mini-Batch =  849711.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  809075.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -40636.0 e-6; = (1/var)*||X-X_r||^2 val-train = -16390.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -24245.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.71; perplexity/K = 8.93%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 6.73; perplexity/K = 10.52%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:25:49 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  810389.8 e-6; = (1/var)*||X-X_r||^2 =  687573.5 e-6 = 84.8 %; (1+beta)*||Z_e-Z_q||^2 =  122816.3 e-6 = 15.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  816588.2 e-6; = (1/var)*||X-X_r||^2 =  672822.8 e-6 = 82.4 %; (1+beta)*||Z_e-Z_q||^2 =  143765.4 e-6 = 17.6 %)
Min.  Avg. Train Loss across Mini-Batch =  809179.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767730.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   6198.5 e-6; = (1/var)*||X-X_r||^2 val-train = -14750.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 20949.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 4.51; perplexity/K = 7.05%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 5.94; perplexity/K = 9.28%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  820070.7 e-6; = (1/var)*||X-X_r||^2 =  686932.7 e-6 = 83.8 %; (1+beta)*||Z_e-Z_q||^2 =  133138.0 e-6 = 16.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  794005.8 e-6; = (1/var)*||X-X_r||^2 =  672118.1 e-6 = 84.6 %; (1+beta)*||Z_e-Z_q||^2 =  121887.7 e-6 = 15.4 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -26064.9 e-6; = (1/var)*||X-X_r||^2 val-train = -14814.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -11250.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.51; perplexity/K = 3.92%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.53; perplexity/K = 3.95%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:59 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  858017.4 e-6; = (1/var)*||X-X_r||^2 =  721305.7 e-6 = 84.1 %; (1+beta)*||Z_e-Z_q||^2 =  136711.7 e-6 = 15.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  827484.6 e-6; = (1/var)*||X-X_r||^2 =  704022.1 e-6 = 85.1 %; (1+beta)*||Z_e-Z_q||^2 =  123462.5 e-6 = 14.9 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30532.7 e-6; = (1/var)*||X-X_r||^2 val-train = -17283.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13249.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.01; perplexity/K = 4.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.79; perplexity/K = 4.36%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:34 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  831951.2 e-6; = (1/var)*||X-X_r||^2 =  728350.2 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  103601.1 e-6 = 12.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  814643.7 e-6; = (1/var)*||X-X_r||^2 =  714578.5 e-6 = 87.7 %; (1+beta)*||Z_e-Z_q||^2 =  100065.1 e-6 = 12.3 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17307.5 e-6; = (1/var)*||X-X_r||^2 val-train = -13771.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3535.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.93; perplexity/K = 3.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.26; perplexity/K = 3.53%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  845670.6 e-6; = (1/var)*||X-X_r||^2 =  733448.0 e-6 = 86.7 %; (1+beta)*||Z_e-Z_q||^2 =  112222.7 e-6 = 13.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  815214.8 e-6; = (1/var)*||X-X_r||^2 =  700882.4 e-6 = 86.0 %; (1+beta)*||Z_e-Z_q||^2 =  114332.4 e-6 = 14.0 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -30455.8 e-6; = (1/var)*||X-X_r||^2 val-train = -32565.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2109.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.35; perplexity/K = 5.23%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.05; perplexity/K = 4.76%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:38:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  830253.7 e-6; = (1/var)*||X-X_r||^2 =  724452.6 e-6 = 87.3 %; (1+beta)*||Z_e-Z_q||^2 =  105801.1 e-6 = 12.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  812800.6 e-6; = (1/var)*||X-X_r||^2 =  721471.4 e-6 = 88.8 %; (1+beta)*||Z_e-Z_q||^2 =  91329.2 e-6 = 11.2 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17453.1 e-6; = (1/var)*||X-X_r||^2 val-train = -2981.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -14471.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.00; perplexity/K = 4.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 2.14; perplexity/K = 3.34%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:19 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  872199.4 e-6; = (1/var)*||X-X_r||^2 =  763541.2 e-6 = 87.5 %; (1+beta)*||Z_e-Z_q||^2 =  108658.2 e-6 = 12.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  861357.2 e-6; = (1/var)*||X-X_r||^2 =  764707.9 e-6 = 88.8 %; (1+beta)*||Z_e-Z_q||^2 =  96649.3 e-6 = 11.2 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -10842.3 e-6; = (1/var)*||X-X_r||^2 val-train = 1166.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12008.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.54; perplexity/K = 2.41%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.55; perplexity/K = 2.42%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  849205.9 e-6; = (1/var)*||X-X_r||^2 =  783276.3 e-6 = 92.2 %; (1+beta)*||Z_e-Z_q||^2 =  65929.6 e-6 = 7.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  837761.2 e-6; = (1/var)*||X-X_r||^2 =  760816.7 e-6 = 90.8 %; (1+beta)*||Z_e-Z_q||^2 =  76944.4 e-6 = 9.2 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -11444.7 e-6; = (1/var)*||X-X_r||^2 val-train = -22459.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 11014.8 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.68; perplexity/K = 2.62%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.55; perplexity/K = 2.42%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  945829.9 e-6; = (1/var)*||X-X_r||^2 =  880533.2 e-6 = 93.1 %; (1+beta)*||Z_e-Z_q||^2 =  65296.7 e-6 = 6.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  927249.0 e-6; = (1/var)*||X-X_r||^2 =  874619.8 e-6 = 94.3 %; (1+beta)*||Z_e-Z_q||^2 =  52629.1 e-6 = 5.7 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -18580.9 e-6; = (1/var)*||X-X_r||^2 val-train = -5913.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -12667.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.53; perplexity/K = 2.39%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.15; perplexity/K = 1.79%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:49:13 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  890787.3 e-6; = (1/var)*||X-X_r||^2 =  842278.4 e-6 = 94.6 %; (1+beta)*||Z_e-Z_q||^2 =  48508.9 e-6 = 5.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  885656.8 e-6; = (1/var)*||X-X_r||^2 =  842448.5 e-6 = 95.1 %; (1+beta)*||Z_e-Z_q||^2 =  43208.3 e-6 = 4.9 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5130.4 e-6; = (1/var)*||X-X_r||^2 val-train = 170.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -5300.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.10; perplexity/K = 1.71%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:51:50 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  940223.7 e-6; = (1/var)*||X-X_r||^2 =  921102.7 e-6 = 98.0 %; (1+beta)*||Z_e-Z_q||^2 =  19121.0 e-6 = 2.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  923870.0 e-6; = (1/var)*||X-X_r||^2 =  906030.0 e-6 = 98.1 %; (1+beta)*||Z_e-Z_q||^2 =  17840.0 e-6 = 1.9 %)
Min.  Avg. Train Loss across Mini-Batch =  788405.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  767132.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -16353.7 e-6; = (1/var)*||X-X_r||^2 val-train = -15072.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1281.0 e-6 

----------------------------------------------------------------------------------

Finished [16:49:49 06.01.2023] 293) Finished running for K = 64 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 6) change_channel_size_across_layers = True:
Total training time is = 0:0:26 h/m/s. 

--------------------------------------------------- 

Started [16:49:49 06.01.2023] 294) Finished running for K = 64 & D = 64 & M = 0 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 6) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(8, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(16, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_6): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(64, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
      (conv2d_4): ConvTranspose2d(32, 16, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_4): ReLU(inplace=True)
      (conv2d_5): ConvTranspose2d(16, 8, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_5): ReLU(inplace=True)
      (conv2d_6): ConvTranspose2d(8, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2474 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         0             0.00
1                           encoder.sequential_convs.conv2d_2.weight                         2             0.08
2                           encoder.sequential_convs.conv2d_3.weight                         8             0.32
3                           encoder.sequential_convs.conv2d_4.weight                        32             1.29
4                           encoder.sequential_convs.conv2d_5.weight                       131             5.30
5                           encoder.sequential_convs.conv2d_6.weight                       524            21.18
6                                  encoder.pre_residual_stack.weight                       589            23.81
7   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
8   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
9   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
10  encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
11                             encoder.channel_adjusting_conv.weight                        16             0.65
12                                                       VQ.E.weight                         4             0.16
13                             decoder.channel_adjusting_conv.weight                       147             5.94
14  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.95
15  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
16  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.95
17  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
18                    decoder.sequential_trans_convs.conv2d_1.weight                       524            21.18
19                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.30
20                    decoder.sequential_trans_convs.conv2d_3.weight                        32             1.29
21                    decoder.sequential_trans_convs.conv2d_4.weight                         8             0.32
22                    decoder.sequential_trans_convs.conv2d_5.weight                         2             0.08
23                    decoder.sequential_trans_convs.conv2d_6.weight                         0             0.00

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.79; perplexity/K = 13.74%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.14; perplexity/K = 11.16%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  2039055.1 e-6; = (1/var)*||X-X_r||^2 =  859218.9 e-6 = 42.1 %; (1+beta)*||Z_e-Z_q||^2 =  1179836.2 e-6 = 57.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1248760.7 e-6; = (1/var)*||X-X_r||^2 =  834892.8 e-6 = 66.9 %; (1+beta)*||Z_e-Z_q||^2 =  413868.0 e-6 = 33.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1018885.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  967702.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -790294.4 e-6; = (1/var)*||X-X_r||^2 val-train = -24326.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -765968.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.55; perplexity/K = 5.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 3.66; perplexity/K = 5.71%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:5:14 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1396498.1 e-6; = (1/var)*||X-X_r||^2 =  769954.5 e-6 = 55.1 %; (1+beta)*||Z_e-Z_q||^2 =  626543.6 e-6 = 44.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  1229179.2 e-6; = (1/var)*||X-X_r||^2 =  756794.7 e-6 = 61.6 %; (1+beta)*||Z_e-Z_q||^2 =  472384.5 e-6 = 38.4 %)
Min.  Avg. Train Loss across Mini-Batch =  1018885.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  967702.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -167318.9 e-6; = (1/var)*||X-X_r||^2 val-train = -13159.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -154159.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.55; perplexity/K = 2.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.53; perplexity/K = 2.39%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1133417.4 e-6; = (1/var)*||X-X_r||^2 =  866192.2 e-6 = 76.4 %; (1+beta)*||Z_e-Z_q||^2 =  267225.2 e-6 = 23.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  1073311.4 e-6; = (1/var)*||X-X_r||^2 =  855905.7 e-6 = 79.7 %; (1+beta)*||Z_e-Z_q||^2 =  217405.7 e-6 = 20.3 %)
Min.  Avg. Train Loss across Mini-Batch =  1018885.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  967702.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -60106.0 e-6; = (1/var)*||X-X_r||^2 val-train = -10286.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -49819.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  989106.2 e-6; = (1/var)*||X-X_r||^2 =  978103.4 e-6 = 98.9 %; (1+beta)*||Z_e-Z_q||^2 =  11002.8 e-6 = 1.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  963198.5 e-6; = (1/var)*||X-X_r||^2 =  955513.3 e-6 = 99.2 %; (1+beta)*||Z_e-Z_q||^2 =  7685.2 e-6 = 0.8 %)
Min.  Avg. Train Loss across Mini-Batch =  989106.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -25907.7 e-6; = (1/var)*||X-X_r||^2 val-train = -22590.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3317.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999746.6 e-6; = (1/var)*||X-X_r||^2 =  999605.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  141.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963087.5 e-6; = (1/var)*||X-X_r||^2 =  963002.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  84.8 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36659.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36602.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -56.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:15:37 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999568.0 e-6; = (1/var)*||X-X_r||^2 =  999566.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963081.5 e-6; = (1/var)*||X-X_r||^2 =  963079.5 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  2.0 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36486.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36486.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:18:16 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999387.3 e-6; = (1/var)*||X-X_r||^2 =  999387.0 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963087.5 e-6; = (1/var)*||X-X_r||^2 =  963087.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36299.8 e-6; = (1/var)*||X-X_r||^2 val-train = -36299.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:52 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999642.1 e-6; = (1/var)*||X-X_r||^2 =  999641.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  964214.6 e-6; = (1/var)*||X-X_r||^2 =  964214.1 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -35427.5 e-6; = (1/var)*||X-X_r||^2 val-train = -35427.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:27 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999585.7 e-6; = (1/var)*||X-X_r||^2 =  999585.4 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.2 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963076.1 e-6; = (1/var)*||X-X_r||^2 =  963075.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36509.6 e-6; = (1/var)*||X-X_r||^2 val-train = -36509.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 0.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999497.8 e-6; = (1/var)*||X-X_r||^2 =  999497.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962968.3 e-6; = (1/var)*||X-X_r||^2 =  962966.7 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  1.5 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36529.5 e-6; = (1/var)*||X-X_r||^2 val-train = -36531.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 1.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:28:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999376.0 e-6; = (1/var)*||X-X_r||^2 =  999375.9 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  963284.9 e-6; = (1/var)*||X-X_r||^2 =  963284.8 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.1 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36091.1 e-6; = (1/var)*||X-X_r||^2 val-train = -36091.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 1.00; perplexity/K = 1.56%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:10 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  999331.8 e-6; = (1/var)*||X-X_r||^2 =  999331.3 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.4 e-6 = 0.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  962898.4 e-6; = (1/var)*||X-X_r||^2 =  962898.2 e-6 = 100.0 %; (1+beta)*||Z_e-Z_q||^2 =  0.3 e-6 = 0.0 %)
Min.  Avg. Train Loss across Mini-Batch =  986393.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954918.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -36433.4 e-6; = (1/var)*||X-X_r||^2 val-train = -36433.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -0.2 e-6 

----------------------------------------------------------------------------------

