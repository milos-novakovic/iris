Started [23:21:13 31.12.2022] 201) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 847 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.12
1                           encoder.sequential_convs.conv2d_2.weight                        32             3.78
2                           encoder.sequential_convs.conv2d_3.weight                       131            15.47
3                                  encoder.pre_residual_stack.weight                       147            17.36
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
8                              encoder.channel_adjusting_conv.weight                         8             0.94
9                                                        VQ.E.weight                       131            15.47
10                             decoder.channel_adjusting_conv.weight                        73             8.62
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.47
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.78
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.83; perplexity/K = 0.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.26; perplexity/K = 0.70%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350441.0 e-6; = (1/var)*||X-X_r||^2 =  237200.6 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  113240.5 e-6 = 32.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  348905.7 e-6; = (1/var)*||X-X_r||^2 =  233155.6 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  115750.1 e-6 = 33.2 %)
Min.  Avg. Train Loss across Mini-Batch =  350441.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  342896.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1535.4 e-6; = (1/var)*||X-X_r||^2 val-train = -4045.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2509.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.50; perplexity/K = 0.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.79; perplexity/K = 0.97%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  135017.6 e-6; = (1/var)*||X-X_r||^2 =  109950.4 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  25067.2 e-6 = 18.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  136524.3 e-6; = (1/var)*||X-X_r||^2 =  112356.5 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  24167.8 e-6 = 17.7 %)
Min.  Avg. Train Loss across Mini-Batch =  120576.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  121407.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1506.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2406.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -899.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.50; perplexity/K = 0.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.46; perplexity/K = 0.85%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59314.3 e-6; = (1/var)*||X-X_r||^2 =  45391.8 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  13922.5 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  62023.6 e-6; = (1/var)*||X-X_r||^2 =  48560.7 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  13462.8 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  59314.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62023.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2709.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3168.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -459.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.31; perplexity/K = 0.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.08; perplexity/K = 0.83%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43642.5 e-6; = (1/var)*||X-X_r||^2 =  26818.6 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  16823.9 e-6 = 38.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  37639.1 e-6; = (1/var)*||X-X_r||^2 =  25046.2 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  12592.9 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  28742.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30964.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6003.4 e-6; = (1/var)*||X-X_r||^2 val-train = -1772.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4231.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.98; perplexity/K = 0.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.93; perplexity/K = 0.68%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14168.3 e-6; = (1/var)*||X-X_r||^2 =  7571.5 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  6596.8 e-6 = 46.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  16289.3 e-6; = (1/var)*||X-X_r||^2 =  9575.2 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  6714.1 e-6 = 41.2 %)
Min.  Avg. Train Loss across Mini-Batch =  14168.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16245.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2121.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2003.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 117.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.18; perplexity/K = 0.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.19; perplexity/K = 0.74%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9302.7 e-6; = (1/var)*||X-X_r||^2 =  4680.6 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  4622.1 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  10721.3 e-6; = (1/var)*||X-X_r||^2 =  6072.6 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  4648.6 e-6 = 43.4 %)
Min.  Avg. Train Loss across Mini-Batch =  9302.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10721.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1418.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1392.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.15; perplexity/K = 0.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.88; perplexity/K = 0.63%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7629.4 e-6; = (1/var)*||X-X_r||^2 =  3717.0 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  3912.4 e-6 = 51.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  8761.2 e-6; = (1/var)*||X-X_r||^2 =  4873.9 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  3887.3 e-6 = 44.4 %)
Min.  Avg. Train Loss across Mini-Batch =  6973.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7934.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1131.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1156.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.69; perplexity/K = 0.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.27; perplexity/K = 0.70%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5626.0 e-6; = (1/var)*||X-X_r||^2 =  2495.2 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  3130.8 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  6391.4 e-6; = (1/var)*||X-X_r||^2 =  3306.2 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  3085.2 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  5257.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5935.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   765.3 e-6; = (1/var)*||X-X_r||^2 val-train = 811.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -45.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.69; perplexity/K = 0.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.66; perplexity/K = 0.76%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4716.9 e-6; = (1/var)*||X-X_r||^2 =  2125.3 e-6 = 45.1 %; (1+beta)*||Z_e-Z_q||^2 =  2591.5 e-6 = 54.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5217.6 e-6; = (1/var)*||X-X_r||^2 =  2686.0 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  2531.6 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4382.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4942.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   500.8 e-6; = (1/var)*||X-X_r||^2 val-train = 560.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -59.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.34; perplexity/K = 1.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.50; perplexity/K = 1.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34313.5 e-6; = (1/var)*||X-X_r||^2 =  22549.5 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  11764.0 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  36208.1 e-6; = (1/var)*||X-X_r||^2 =  24421.8 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  11786.3 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1894.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1872.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.65; perplexity/K = 0.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.67; perplexity/K = 0.96%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11221.9 e-6; = (1/var)*||X-X_r||^2 =  8121.2 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  3100.7 e-6 = 27.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  12046.8 e-6; = (1/var)*||X-X_r||^2 =  8959.8 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  3087.0 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   824.9 e-6; = (1/var)*||X-X_r||^2 val-train = 838.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.75; perplexity/K = 0.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.91; perplexity/K = 0.92%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8298.0 e-6; = (1/var)*||X-X_r||^2 =  5968.4 e-6 = 71.9 %; (1+beta)*||Z_e-Z_q||^2 =  2329.6 e-6 = 28.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8857.0 e-6; = (1/var)*||X-X_r||^2 =  6522.5 e-6 = 73.6 %; (1+beta)*||Z_e-Z_q||^2 =  2334.5 e-6 = 26.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   559.0 e-6; = (1/var)*||X-X_r||^2 val-train = 554.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.74; perplexity/K = 0.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.18; perplexity/K = 0.79%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6810.1 e-6; = (1/var)*||X-X_r||^2 =  4836.5 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  1973.6 e-6 = 29.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  7564.0 e-6; = (1/var)*||X-X_r||^2 =  5675.6 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  1888.4 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   753.9 e-6; = (1/var)*||X-X_r||^2 val-train = 839.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -85.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.77; perplexity/K = 0.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.99; perplexity/K = 0.93%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9584.5 e-6; = (1/var)*||X-X_r||^2 =  7363.9 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  2220.6 e-6 = 23.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  9665.5 e-6; = (1/var)*||X-X_r||^2 =  7563.1 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  2102.4 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   81.0 e-6; = (1/var)*||X-X_r||^2 val-train = 199.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -118.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.45; perplexity/K = 0.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.85; perplexity/K = 0.87%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7231.4 e-6; = (1/var)*||X-X_r||^2 =  5248.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  1983.4 e-6 = 27.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  6341.3 e-6; = (1/var)*||X-X_r||^2 =  4695.2 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  1646.1 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -890.1 e-6; = (1/var)*||X-X_r||^2 val-train = -552.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -337.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.71; perplexity/K = 0.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.68; perplexity/K = 0.86%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4940.2 e-6; = (1/var)*||X-X_r||^2 =  3336.3 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  1603.9 e-6 = 32.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5559.2 e-6; = (1/var)*||X-X_r||^2 =  4084.6 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  1474.6 e-6 = 26.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   619.0 e-6; = (1/var)*||X-X_r||^2 val-train = 748.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -129.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.16; perplexity/K = 0.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.64; perplexity/K = 0.81%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5284.1 e-6; = (1/var)*||X-X_r||^2 =  3512.8 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  1771.2 e-6 = 33.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  6026.0 e-6; = (1/var)*||X-X_r||^2 =  4342.2 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  1683.8 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   741.9 e-6; = (1/var)*||X-X_r||^2 val-train = 829.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -87.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.27; perplexity/K = 0.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.24; perplexity/K = 1.04%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6557.5 e-6; = (1/var)*||X-X_r||^2 =  4657.1 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  1900.4 e-6 = 29.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  5554.4 e-6; = (1/var)*||X-X_r||^2 =  4000.6 e-6 = 72.0 %; (1+beta)*||Z_e-Z_q||^2 =  1553.7 e-6 = 28.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1003.2 e-6; = (1/var)*||X-X_r||^2 val-train = -656.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -346.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.20; perplexity/K = 0.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.75; perplexity/K = 0.87%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4267.3 e-6; = (1/var)*||X-X_r||^2 =  2711.0 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  1556.3 e-6 = 36.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5052.2 e-6; = (1/var)*||X-X_r||^2 =  3432.3 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  1619.9 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   784.9 e-6; = (1/var)*||X-X_r||^2 val-train = 721.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.19; perplexity/K = 1.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.41; perplexity/K = 1.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4048.7 e-6; = (1/var)*||X-X_r||^2 =  2509.8 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  1539.0 e-6 = 38.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4718.5 e-6; = (1/var)*||X-X_r||^2 =  3233.0 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  1485.5 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3784.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4522.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   669.8 e-6; = (1/var)*||X-X_r||^2 val-train = 723.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -53.4 e-6 

----------------------------------------------------------------------------------

Finished [00:13:16 01.01.2023] 201) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = True:
Total training time is = 0:7:3 h/m/s. 

--------------------------------------------------- 

Started [00:13:16 01.01.2023] 202) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 256 (i.e. bits = 704) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(256, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 2523 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         3             0.12
1                           encoder.sequential_convs.conv2d_2.weight                       131             5.19
2                           encoder.sequential_convs.conv2d_3.weight                       524            20.77
3                                  encoder.pre_residual_stack.weight                       589            23.35
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.89
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.89
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
8                              encoder.channel_adjusting_conv.weight                        16             0.63
9                                                        VQ.E.weight                       131             5.19
10                             decoder.channel_adjusting_conv.weight                       147             5.83
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        73             2.89
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         8             0.32
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        73             2.89
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         8             0.32
15                    decoder.sequential_trans_convs.conv2d_1.weight                       524            20.77
16                    decoder.sequential_trans_convs.conv2d_2.weight                       131             5.19
17                    decoder.sequential_trans_convs.conv2d_3.weight                         3             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.90; perplexity/K = 1.02%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.67; perplexity/K = 0.91%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:3:20 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  339050.5 e-6; = (1/var)*||X-X_r||^2 =  172381.3 e-6 = 50.8 %; (1+beta)*||Z_e-Z_q||^2 =  166669.2 e-6 = 49.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  351681.7 e-6; = (1/var)*||X-X_r||^2 =  176539.5 e-6 = 50.2 %; (1+beta)*||Z_e-Z_q||^2 =  175142.1 e-6 = 49.8 %)
Min.  Avg. Train Loss across Mini-Batch =  339050.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  345807.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   12631.2 e-6; = (1/var)*||X-X_r||^2 val-train = 4158.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 8472.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.11; perplexity/K = 1.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.68; perplexity/K = 1.06%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:6:40 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  67463.8 e-6; = (1/var)*||X-X_r||^2 =  48327.3 e-6 = 71.6 %; (1+beta)*||Z_e-Z_q||^2 =  19136.5 e-6 = 28.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  71555.1 e-6; = (1/var)*||X-X_r||^2 =  53131.8 e-6 = 74.3 %; (1+beta)*||Z_e-Z_q||^2 =  18423.3 e-6 = 25.7 %)
Min.  Avg. Train Loss across Mini-Batch =  67463.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  71555.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   4091.3 e-6; = (1/var)*||X-X_r||^2 val-train = 4804.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -713.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.14; perplexity/K = 0.69%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.78; perplexity/K = 0.67%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:10:0 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  26663.8 e-6; = (1/var)*||X-X_r||^2 =  17164.6 e-6 = 64.4 %; (1+beta)*||Z_e-Z_q||^2 =  9499.1 e-6 = 35.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  28976.8 e-6; = (1/var)*||X-X_r||^2 =  19691.6 e-6 = 68.0 %; (1+beta)*||Z_e-Z_q||^2 =  9285.2 e-6 = 32.0 %)
Min.  Avg. Train Loss across Mini-Batch =  26663.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  28976.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2313.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2526.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -213.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.75; perplexity/K = 0.67%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.41; perplexity/K = 0.66%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:13:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11388.7 e-6; = (1/var)*||X-X_r||^2 =  6078.3 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  5310.4 e-6 = 46.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  13254.8 e-6; = (1/var)*||X-X_r||^2 =  7812.8 e-6 = 58.9 %; (1+beta)*||Z_e-Z_q||^2 =  5442.0 e-6 = 41.1 %)
Min.  Avg. Train Loss across Mini-Batch =  11388.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  13254.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1866.1 e-6; = (1/var)*||X-X_r||^2 val-train = 1734.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 131.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.71; perplexity/K = 0.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.31; perplexity/K = 0.60%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:16:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6834.8 e-6; = (1/var)*||X-X_r||^2 =  2926.1 e-6 = 42.8 %; (1+beta)*||Z_e-Z_q||^2 =  3908.7 e-6 = 57.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  8170.5 e-6; = (1/var)*||X-X_r||^2 =  4087.6 e-6 = 50.0 %; (1+beta)*||Z_e-Z_q||^2 =  4082.9 e-6 = 50.0 %)
Min.  Avg. Train Loss across Mini-Batch =  6553.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7894.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1335.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1161.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 174.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.40; perplexity/K = 0.51%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 9.86; perplexity/K = 0.48%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:20:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3863.4 e-6; = (1/var)*||X-X_r||^2 =  1709.5 e-6 = 44.2 %; (1+beta)*||Z_e-Z_q||^2 =  2154.0 e-6 = 55.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  4552.9 e-6; = (1/var)*||X-X_r||^2 =  2597.5 e-6 = 57.1 %; (1+beta)*||Z_e-Z_q||^2 =  1955.3 e-6 = 42.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3796.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4552.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   689.4 e-6; = (1/var)*||X-X_r||^2 val-train = 888.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -198.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.23; perplexity/K = 0.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.12; perplexity/K = 0.59%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:23:21 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4237.6 e-6; = (1/var)*||X-X_r||^2 =  1710.7 e-6 = 40.4 %; (1+beta)*||Z_e-Z_q||^2 =  2526.9 e-6 = 59.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  5016.7 e-6; = (1/var)*||X-X_r||^2 =  2569.9 e-6 = 51.2 %; (1+beta)*||Z_e-Z_q||^2 =  2446.8 e-6 = 48.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3602.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4552.9 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   779.1 e-6; = (1/var)*||X-X_r||^2 val-train = 859.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -80.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.30; perplexity/K = 0.65%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.77; perplexity/K = 0.67%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:41 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  3854.3 e-6; = (1/var)*||X-X_r||^2 =  1533.0 e-6 = 39.8 %; (1+beta)*||Z_e-Z_q||^2 =  2321.3 e-6 = 60.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  4449.1 e-6; = (1/var)*||X-X_r||^2 =  2221.7 e-6 = 49.9 %; (1+beta)*||Z_e-Z_q||^2 =  2227.5 e-6 = 50.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1923.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  2443.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   594.8 e-6; = (1/var)*||X-X_r||^2 val-train = 688.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -93.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.47; perplexity/K = 0.56%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.87; perplexity/K = 0.58%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:30:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1787.7 e-6; = (1/var)*||X-X_r||^2 =  612.8 e-6 = 34.3 %; (1+beta)*||Z_e-Z_q||^2 =  1174.9 e-6 = 65.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  2172.7 e-6; = (1/var)*||X-X_r||^2 =  1019.5 e-6 = 46.9 %; (1+beta)*||Z_e-Z_q||^2 =  1153.2 e-6 = 53.1 %)
Min.  Avg. Train Loss across Mini-Batch =  1564.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1954.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   385.0 e-6; = (1/var)*||X-X_r||^2 val-train = 406.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -21.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.09; perplexity/K = 0.54%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.25; perplexity/K = 0.55%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:33:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1041.7 e-6; = (1/var)*||X-X_r||^2 =  395.3 e-6 = 37.9 %; (1+beta)*||Z_e-Z_q||^2 =  646.4 e-6 = 62.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  1363.8 e-6; = (1/var)*||X-X_r||^2 =  725.7 e-6 = 53.2 %; (1+beta)*||Z_e-Z_q||^2 =  638.1 e-6 = 46.8 %)
Min.  Avg. Train Loss across Mini-Batch =  1041.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1363.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   322.1 e-6; = (1/var)*||X-X_r||^2 val-train = 330.4 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -8.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.92; perplexity/K = 0.44%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 7.58; perplexity/K = 0.37%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1191.5 e-6; = (1/var)*||X-X_r||^2 =  375.1 e-6 = 31.5 %; (1+beta)*||Z_e-Z_q||^2 =  816.4 e-6 = 68.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  1460.4 e-6; = (1/var)*||X-X_r||^2 =  665.4 e-6 = 45.6 %; (1+beta)*||Z_e-Z_q||^2 =  794.9 e-6 = 54.4 %)
Min.  Avg. Train Loss across Mini-Batch =  789.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  1102.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   268.9 e-6; = (1/var)*||X-X_r||^2 val-train = 290.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -21.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.42; perplexity/K = 0.61%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.26; perplexity/K = 0.60%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:40:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  963.4 e-6; = (1/var)*||X-X_r||^2 =  323.3 e-6 = 33.6 %; (1+beta)*||Z_e-Z_q||^2 =  640.1 e-6 = 66.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  1496.8 e-6; = (1/var)*||X-X_r||^2 =  965.2 e-6 = 64.5 %; (1+beta)*||Z_e-Z_q||^2 =  531.5 e-6 = 35.5 %)
Min.  Avg. Train Loss across Mini-Batch =  709.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  954.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   533.4 e-6; = (1/var)*||X-X_r||^2 val-train = 642.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -108.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.25; perplexity/K = 0.60%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.88; perplexity/K = 0.63%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  30003.1 e-6; = (1/var)*||X-X_r||^2 =  13854.6 e-6 = 46.2 %; (1+beta)*||Z_e-Z_q||^2 =  16148.4 e-6 = 53.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  12193.3 e-6; = (1/var)*||X-X_r||^2 =  5680.1 e-6 = 46.6 %; (1+beta)*||Z_e-Z_q||^2 =  6513.2 e-6 = 53.4 %)
Min.  Avg. Train Loss across Mini-Batch =  531.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  711.4 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -17809.8 e-6; = (1/var)*||X-X_r||^2 val-train = -8174.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -9635.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.25; perplexity/K = 0.55%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.70; perplexity/K = 0.52%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  682.9 e-6; = (1/var)*||X-X_r||^2 =  174.0 e-6 = 25.5 %; (1+beta)*||Z_e-Z_q||^2 =  508.9 e-6 = 74.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  866.8 e-6; = (1/var)*||X-X_r||^2 =  359.7 e-6 = 41.5 %; (1+beta)*||Z_e-Z_q||^2 =  507.1 e-6 = 58.5 %)
Min.  Avg. Train Loss across Mini-Batch =  388.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  527.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   184.0 e-6; = (1/var)*||X-X_r||^2 val-train = 185.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -1.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.64; perplexity/K = 0.57%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.95; perplexity/K = 0.58%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:50:2 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9210.0 e-6; = (1/var)*||X-X_r||^2 =  3681.8 e-6 = 40.0 %; (1+beta)*||Z_e-Z_q||^2 =  5528.2 e-6 = 60.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  3381.9 e-6; = (1/var)*||X-X_r||^2 =  1292.9 e-6 = 38.2 %; (1+beta)*||Z_e-Z_q||^2 =  2089.0 e-6 = 61.8 %)
Min.  Avg. Train Loss across Mini-Batch =  388.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  527.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -5828.2 e-6; = (1/var)*||X-X_r||^2 val-train = -2388.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -3439.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.98; perplexity/K = 0.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.30; perplexity/K = 0.65%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:53:23 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  769.1 e-6; = (1/var)*||X-X_r||^2 =  205.7 e-6 = 26.7 %; (1+beta)*||Z_e-Z_q||^2 =  563.4 e-6 = 73.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  905.8 e-6; = (1/var)*||X-X_r||^2 =  358.8 e-6 = 39.6 %; (1+beta)*||Z_e-Z_q||^2 =  547.0 e-6 = 60.4 %)
Min.  Avg. Train Loss across Mini-Batch =  312.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  448.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   136.7 e-6; = (1/var)*||X-X_r||^2 val-train = 153.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -16.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.06; perplexity/K = 0.59%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 11.35; perplexity/K = 0.55%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:56:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  532.2 e-6; = (1/var)*||X-X_r||^2 =  134.1 e-6 = 25.2 %; (1+beta)*||Z_e-Z_q||^2 =  398.1 e-6 = 74.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  658.7 e-6; = (1/var)*||X-X_r||^2 =  264.7 e-6 = 40.2 %; (1+beta)*||Z_e-Z_q||^2 =  394.0 e-6 = 59.8 %)
Min.  Avg. Train Loss across Mini-Batch =  240.6 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  375.0 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   126.5 e-6; = (1/var)*||X-X_r||^2 val-train = 130.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.13; perplexity/K = 0.49%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 10.18; perplexity/K = 0.50%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:0:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  745.8 e-6; = (1/var)*||X-X_r||^2 =  199.8 e-6 = 26.8 %; (1+beta)*||Z_e-Z_q||^2 =  546.0 e-6 = 73.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  869.7 e-6; = (1/var)*||X-X_r||^2 =  361.9 e-6 = 41.6 %; (1+beta)*||Z_e-Z_q||^2 =  507.8 e-6 = 58.4 %)
Min.  Avg. Train Loss across Mini-Batch =  200.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  322.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   123.9 e-6; = (1/var)*||X-X_r||^2 val-train = 162.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -38.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.68; perplexity/K = 0.42%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 8.54; perplexity/K = 0.42%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:3:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  324.1 e-6; = (1/var)*||X-X_r||^2 =  66.2 e-6 = 20.4 %; (1+beta)*||Z_e-Z_q||^2 =  257.8 e-6 = 79.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  457.8 e-6; = (1/var)*||X-X_r||^2 =  193.0 e-6 = 42.2 %; (1+beta)*||Z_e-Z_q||^2 =  264.8 e-6 = 57.8 %)
Min.  Avg. Train Loss across Mini-Batch =  182.1 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  283.2 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   133.7 e-6; = (1/var)*||X-X_r||^2 val-train = 126.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 7.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.92; perplexity/K = 0.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.94; perplexity/K = 0.63%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 1:6:44 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  1007.7 e-6; = (1/var)*||X-X_r||^2 =  234.0 e-6 = 23.2 %; (1+beta)*||Z_e-Z_q||^2 =  773.8 e-6 = 76.8 %)
Curr. Avg. Val   Loss across Mini-Batch =  1079.4 e-6; = (1/var)*||X-X_r||^2 =  374.8 e-6 = 34.7 %; (1+beta)*||Z_e-Z_q||^2 =  704.6 e-6 = 65.3 %)
Min.  Avg. Train Loss across Mini-Batch =  171.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  275.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   71.7 e-6; = (1/var)*||X-X_r||^2 val-train = 140.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -69.1 e-6 

----------------------------------------------------------------------------------

