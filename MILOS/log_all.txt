Started [23:21:13 31.12.2022] 201) Finished running for K = 2048 & D = 64 & M = 7 & beta = 0.25 & max_channel_number = 128 (i.e. bits = 704) change_channel_size_across_layers = True:

--------------------------------------------------- 


 PyTorch print of the model:

VQ_VAE(
  (encoder): Encoder(
    (sequential_convs): Sequential(
      (conv2d_1): Conv2d(3, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_3): ReLU(inplace=True)
    )
    (pre_residual_stack): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (channel_adjusting_conv): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1))
  )
  (VQ): VectorQuantizer(
    (E): Embedding(2048, 64)
  )
  (decoder): Decoder(
    (channel_adjusting_conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (residual_stack): ResidualStack(
      (residual_blocks): ModuleList(
        (0): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
        (1): Residual(
          (residual_block): Sequential(
            (0): ReLU(inplace=True)
            (1): Conv2d(128, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
            (2): ReLU(inplace=True)
            (3): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
          )
        )
      )
    )
    (sequential_trans_convs): Sequential(
      (conv2d_1): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_1): ReLU(inplace=True)
      (conv2d_2): ConvTranspose2d(64, 32, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
      (ReLU_2): ReLU(inplace=True)
      (conv2d_3): ConvTranspose2d(32, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))
    )
  )
)


 Total Trainable Params in thousands: 847 

                                                         Module Name  # of params in thousands  # of params [%]
0                           encoder.sequential_convs.conv2d_1.weight                         1             0.12
1                           encoder.sequential_convs.conv2d_2.weight                        32             3.78
2                           encoder.sequential_convs.conv2d_3.weight                       131            15.47
3                                  encoder.pre_residual_stack.weight                       147            17.36
4   encoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
5   encoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
6   encoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
7   encoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
8                              encoder.channel_adjusting_conv.weight                         8             0.94
9                                                        VQ.E.weight                       131            15.47
10                             decoder.channel_adjusting_conv.weight                        73             8.62
11  decoder.residual_stack.residual_blocks.0.residual_block.1.weight                        36             4.25
12  decoder.residual_stack.residual_blocks.0.residual_block.3.weight                         4             0.47
13  decoder.residual_stack.residual_blocks.1.residual_block.1.weight                        36             4.25
14  decoder.residual_stack.residual_blocks.1.residual_block.3.weight                         4             0.47
15                    decoder.sequential_trans_convs.conv2d_1.weight                       131            15.47
16                    decoder.sequential_trans_convs.conv2d_2.weight                        32             3.78
17                    decoder.sequential_trans_convs.conv2d_3.weight                         1             0.12

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.83; perplexity/K = 0.72%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.26; perplexity/K = 0.70%
Epoch 100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:2:26 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  350441.0 e-6; = (1/var)*||X-X_r||^2 =  237200.6 e-6 = 67.7 %; (1+beta)*||Z_e-Z_q||^2 =  113240.5 e-6 = 32.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  348905.7 e-6; = (1/var)*||X-X_r||^2 =  233155.6 e-6 = 66.8 %; (1+beta)*||Z_e-Z_q||^2 =  115750.1 e-6 = 33.2 %)
Min.  Avg. Train Loss across Mini-Batch =  350441.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  342896.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1535.4 e-6; = (1/var)*||X-X_r||^2 val-train = -4045.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 2509.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.50; perplexity/K = 0.95%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.79; perplexity/K = 0.97%
Epoch 200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:4:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  135017.6 e-6; = (1/var)*||X-X_r||^2 =  109950.4 e-6 = 81.4 %; (1+beta)*||Z_e-Z_q||^2 =  25067.2 e-6 = 18.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  136524.3 e-6; = (1/var)*||X-X_r||^2 =  112356.5 e-6 = 82.3 %; (1+beta)*||Z_e-Z_q||^2 =  24167.8 e-6 = 17.7 %)
Min.  Avg. Train Loss across Mini-Batch =  120576.2 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  121407.3 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1506.7 e-6; = (1/var)*||X-X_r||^2 val-train = 2406.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -899.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.50; perplexity/K = 0.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.46; perplexity/K = 0.85%
Epoch 300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:7:18 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  59314.3 e-6; = (1/var)*||X-X_r||^2 =  45391.8 e-6 = 76.5 %; (1+beta)*||Z_e-Z_q||^2 =  13922.5 e-6 = 23.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  62023.6 e-6; = (1/var)*||X-X_r||^2 =  48560.7 e-6 = 78.3 %; (1+beta)*||Z_e-Z_q||^2 =  13462.8 e-6 = 21.7 %)
Min.  Avg. Train Loss across Mini-Batch =  59314.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  62023.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2709.3 e-6; = (1/var)*||X-X_r||^2 val-train = 3168.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -459.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.31; perplexity/K = 0.85%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.08; perplexity/K = 0.83%
Epoch 400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:9:43 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  43642.5 e-6; = (1/var)*||X-X_r||^2 =  26818.6 e-6 = 61.5 %; (1+beta)*||Z_e-Z_q||^2 =  16823.9 e-6 = 38.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  37639.1 e-6; = (1/var)*||X-X_r||^2 =  25046.2 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  12592.9 e-6 = 33.5 %)
Min.  Avg. Train Loss across Mini-Batch =  28742.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  30964.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -6003.4 e-6; = (1/var)*||X-X_r||^2 val-train = -1772.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -4231.0 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.98; perplexity/K = 0.68%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.93; perplexity/K = 0.68%
Epoch 500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:12:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  14168.3 e-6; = (1/var)*||X-X_r||^2 =  7571.5 e-6 = 53.4 %; (1+beta)*||Z_e-Z_q||^2 =  6596.8 e-6 = 46.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  16289.3 e-6; = (1/var)*||X-X_r||^2 =  9575.2 e-6 = 58.8 %; (1+beta)*||Z_e-Z_q||^2 =  6714.1 e-6 = 41.2 %)
Min.  Avg. Train Loss across Mini-Batch =  14168.3 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  16245.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   2121.0 e-6; = (1/var)*||X-X_r||^2 val-train = 2003.8 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 117.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.18; perplexity/K = 0.84%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.19; perplexity/K = 0.74%
Epoch 600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:14:35 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9302.7 e-6; = (1/var)*||X-X_r||^2 =  4680.6 e-6 = 50.3 %; (1+beta)*||Z_e-Z_q||^2 =  4622.1 e-6 = 49.7 %)
Curr. Avg. Val   Loss across Mini-Batch =  10721.3 e-6; = (1/var)*||X-X_r||^2 =  6072.6 e-6 = 56.6 %; (1+beta)*||Z_e-Z_q||^2 =  4648.6 e-6 = 43.4 %)
Min.  Avg. Train Loss across Mini-Batch =  9302.7 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  10721.1 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1418.5 e-6; = (1/var)*||X-X_r||^2 val-train = 1392.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 26.5 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 13.15; perplexity/K = 0.64%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 12.88; perplexity/K = 0.63%
Epoch 700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:17:1 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7629.4 e-6; = (1/var)*||X-X_r||^2 =  3717.0 e-6 = 48.7 %; (1+beta)*||Z_e-Z_q||^2 =  3912.4 e-6 = 51.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  8761.2 e-6; = (1/var)*||X-X_r||^2 =  4873.9 e-6 = 55.6 %; (1+beta)*||Z_e-Z_q||^2 =  3887.3 e-6 = 44.4 %)
Min.  Avg. Train Loss across Mini-Batch =  6973.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  7934.7 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1131.8 e-6; = (1/var)*||X-X_r||^2 val-train = 1156.9 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -25.1 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.69; perplexity/K = 0.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 14.27; perplexity/K = 0.70%
Epoch 800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:19:28 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5626.0 e-6; = (1/var)*||X-X_r||^2 =  2495.2 e-6 = 44.4 %; (1+beta)*||Z_e-Z_q||^2 =  3130.8 e-6 = 55.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  6391.4 e-6; = (1/var)*||X-X_r||^2 =  3306.2 e-6 = 51.7 %; (1+beta)*||Z_e-Z_q||^2 =  3085.2 e-6 = 48.3 %)
Min.  Avg. Train Loss across Mini-Batch =  5257.4 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  5935.8 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   765.3 e-6; = (1/var)*||X-X_r||^2 val-train = 811.0 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -45.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.69; perplexity/K = 0.77%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 15.66; perplexity/K = 0.76%
Epoch 900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:21:55 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4716.9 e-6; = (1/var)*||X-X_r||^2 =  2125.3 e-6 = 45.1 %; (1+beta)*||Z_e-Z_q||^2 =  2591.5 e-6 = 54.9 %)
Curr. Avg. Val   Loss across Mini-Batch =  5217.6 e-6; = (1/var)*||X-X_r||^2 =  2686.0 e-6 = 51.5 %; (1+beta)*||Z_e-Z_q||^2 =  2531.6 e-6 = 48.5 %)
Min.  Avg. Train Loss across Mini-Batch =  4382.5 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4942.5 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   500.8 e-6; = (1/var)*||X-X_r||^2 val-train = 560.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -59.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 23.34; perplexity/K = 1.14%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 22.50; perplexity/K = 1.10%
Epoch 1000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:24:22 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  34313.5 e-6; = (1/var)*||X-X_r||^2 =  22549.5 e-6 = 65.7 %; (1+beta)*||Z_e-Z_q||^2 =  11764.0 e-6 = 34.3 %)
Curr. Avg. Val   Loss across Mini-Batch =  36208.1 e-6; = (1/var)*||X-X_r||^2 =  24421.8 e-6 = 67.4 %; (1+beta)*||Z_e-Z_q||^2 =  11786.3 e-6 = 32.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   1894.6 e-6; = (1/var)*||X-X_r||^2 val-train = 1872.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 22.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.65; perplexity/K = 0.86%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.67; perplexity/K = 0.96%
Epoch 1100/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:26:48 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  11221.9 e-6; = (1/var)*||X-X_r||^2 =  8121.2 e-6 = 72.4 %; (1+beta)*||Z_e-Z_q||^2 =  3100.7 e-6 = 27.6 %)
Curr. Avg. Val   Loss across Mini-Batch =  12046.8 e-6; = (1/var)*||X-X_r||^2 =  8959.8 e-6 = 74.4 %; (1+beta)*||Z_e-Z_q||^2 =  3087.0 e-6 = 25.6 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   824.9 e-6; = (1/var)*||X-X_r||^2 val-train = 838.6 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -13.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.75; perplexity/K = 0.96%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.91; perplexity/K = 0.92%
Epoch 1200/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:29:15 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  8298.0 e-6; = (1/var)*||X-X_r||^2 =  5968.4 e-6 = 71.9 %; (1+beta)*||Z_e-Z_q||^2 =  2329.6 e-6 = 28.1 %)
Curr. Avg. Val   Loss across Mini-Batch =  8857.0 e-6; = (1/var)*||X-X_r||^2 =  6522.5 e-6 = 73.6 %; (1+beta)*||Z_e-Z_q||^2 =  2334.5 e-6 = 26.4 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   559.0 e-6; = (1/var)*||X-X_r||^2 val-train = 554.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 4.9 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.74; perplexity/K = 0.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.18; perplexity/K = 0.79%
Epoch 1300/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:31:42 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6810.1 e-6; = (1/var)*||X-X_r||^2 =  4836.5 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  1973.6 e-6 = 29.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  7564.0 e-6; = (1/var)*||X-X_r||^2 =  5675.6 e-6 = 75.0 %; (1+beta)*||Z_e-Z_q||^2 =  1888.4 e-6 = 25.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   753.9 e-6; = (1/var)*||X-X_r||^2 val-train = 839.1 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -85.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.77; perplexity/K = 0.82%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.99; perplexity/K = 0.93%
Epoch 1400/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:34:9 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  9584.5 e-6; = (1/var)*||X-X_r||^2 =  7363.9 e-6 = 76.8 %; (1+beta)*||Z_e-Z_q||^2 =  2220.6 e-6 = 23.2 %)
Curr. Avg. Val   Loss across Mini-Batch =  9665.5 e-6; = (1/var)*||X-X_r||^2 =  7563.1 e-6 = 78.2 %; (1+beta)*||Z_e-Z_q||^2 =  2102.4 e-6 = 21.8 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   81.0 e-6; = (1/var)*||X-X_r||^2 val-train = 199.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -118.2 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.45; perplexity/K = 0.90%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.85; perplexity/K = 0.87%
Epoch 1500/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:36:36 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  7231.4 e-6; = (1/var)*||X-X_r||^2 =  5248.0 e-6 = 72.6 %; (1+beta)*||Z_e-Z_q||^2 =  1983.4 e-6 = 27.4 %)
Curr. Avg. Val   Loss across Mini-Batch =  6341.3 e-6; = (1/var)*||X-X_r||^2 =  4695.2 e-6 = 74.0 %; (1+beta)*||Z_e-Z_q||^2 =  1646.1 e-6 = 26.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -890.1 e-6; = (1/var)*||X-X_r||^2 val-train = -552.7 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -337.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.71; perplexity/K = 0.91%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.68; perplexity/K = 0.86%
Epoch 1600/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:39:3 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4940.2 e-6; = (1/var)*||X-X_r||^2 =  3336.3 e-6 = 67.5 %; (1+beta)*||Z_e-Z_q||^2 =  1603.9 e-6 = 32.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5559.2 e-6; = (1/var)*||X-X_r||^2 =  4084.6 e-6 = 73.5 %; (1+beta)*||Z_e-Z_q||^2 =  1474.6 e-6 = 26.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   619.0 e-6; = (1/var)*||X-X_r||^2 val-train = 748.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -129.3 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 18.16; perplexity/K = 0.89%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 16.64; perplexity/K = 0.81%
Epoch 1700/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:41:30 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  5284.1 e-6; = (1/var)*||X-X_r||^2 =  3512.8 e-6 = 66.5 %; (1+beta)*||Z_e-Z_q||^2 =  1771.2 e-6 = 33.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  6026.0 e-6; = (1/var)*||X-X_r||^2 =  4342.2 e-6 = 72.1 %; (1+beta)*||Z_e-Z_q||^2 =  1683.8 e-6 = 27.9 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   741.9 e-6; = (1/var)*||X-X_r||^2 val-train = 829.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -87.4 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 19.27; perplexity/K = 0.94%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.24; perplexity/K = 1.04%
Epoch 1800/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:43:57 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  6557.5 e-6; = (1/var)*||X-X_r||^2 =  4657.1 e-6 = 71.0 %; (1+beta)*||Z_e-Z_q||^2 =  1900.4 e-6 = 29.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  5554.4 e-6; = (1/var)*||X-X_r||^2 =  4000.6 e-6 = 72.0 %; (1+beta)*||Z_e-Z_q||^2 =  1553.7 e-6 = 28.0 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =  -1003.2 e-6; = (1/var)*||X-X_r||^2 val-train = -656.5 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -346.7 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.20; perplexity/K = 0.99%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 17.75; perplexity/K = 0.87%
Epoch 1900/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:46:24 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4267.3 e-6; = (1/var)*||X-X_r||^2 =  2711.0 e-6 = 63.5 %; (1+beta)*||Z_e-Z_q||^2 =  1556.3 e-6 = 36.5 %)
Curr. Avg. Val   Loss across Mini-Batch =  5052.2 e-6; = (1/var)*||X-X_r||^2 =  3432.3 e-6 = 67.9 %; (1+beta)*||Z_e-Z_q||^2 =  1619.9 e-6 = 32.1 %)
Min.  Avg. Train Loss across Mini-Batch =  3933.0 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4566.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   784.9 e-6; = (1/var)*||X-X_r||^2 val-train = 721.3 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = 63.6 e-6 

----------------------------------------------------------------------------------

current train      perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 21.19; perplexity/K = 1.03%
current validation perplexity = 2^( H( PMF of codebook words occurance ) bits ) = 20.41; perplexity/K = 1.00%
Epoch 2000/2000;
Training   Samples Mini-Batch size      = 128;
Validation Samples Mini-Batch size      = 128;
Total elapsed time in Training Epoch    = 0:0:1 h/m/s;
Total elapsed time in Validation Epoch  = 0:0:0 h/m/s;
Total elapsed time from begining        = 0:48:51 h/m/s;
Curr. Avg. Train Loss across Mini-Batch =  4048.7 e-6; = (1/var)*||X-X_r||^2 =  2509.8 e-6 = 62.0 %; (1+beta)*||Z_e-Z_q||^2 =  1539.0 e-6 = 38.0 %)
Curr. Avg. Val   Loss across Mini-Batch =  4718.5 e-6; = (1/var)*||X-X_r||^2 =  3233.0 e-6 = 68.5 %; (1+beta)*||Z_e-Z_q||^2 =  1485.5 e-6 = 31.5 %)
Min.  Avg. Train Loss across Mini-Batch =  3784.8 e-6; 
Min.  Avg. Val   Loss across Mini-Batch =  4522.6 e-6; 
Curr. Avg. (Val-Train) overfit gap      =   669.8 e-6; = (1/var)*||X-X_r||^2 val-train = 723.2 e-6 and (1+beta)*||Z_e-Z_q||^2 val-train = -53.4 e-6 

----------------------------------------------------------------------------------

