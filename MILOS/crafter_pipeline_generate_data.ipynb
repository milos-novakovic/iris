{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import autoencoders\n",
    "#/data/alonsoel/workspace/data/crafter\n",
    "#/data/alonsoel/workspace/data/crafter/dataset_1M_steps_heuristic_crafter\n",
    "#from 0 to 5727 with \".pt\" extension\n",
    "\n",
    "#and\n",
    "# /data/alonsoel/workspace/data/crafter/dataset_10M_steps_heuristic_crafter\n",
    "#from 0 to 28467.pt\n",
    "\n",
    "# try 2 shapes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import numpy as np\n",
    "import time\n",
    "# hyperparameters related to images\n",
    "get_hyperparam_value = lambda data_dict, hyperparam_name : \\\n",
    "                            [dict_[hyperparam_name] for dict_ in data_dict\n",
    "                             if hyperparam_name in dict_][0]\n",
    "                            \n",
    "hyperparam_path = \"/home/novakovm/iris/MILOS/crafter_config.yaml\"\n",
    "with open(hyperparam_path) as f:\n",
    "    hyperparam_dict = yaml.load(f, Loader=yaml.SafeLoader)\n",
    "\n",
    "# number of images for training and testing datasets\n",
    "\n",
    "H, W, C = get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'H'), get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'W'), get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'C')\n",
    "\n",
    "NUM_EPOCHS =                            get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'NUM_EPOCHS')\n",
    "NUM_WORKERS =                           get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'NUM_WORKERS') # see what this represents exactly!\n",
    "USE_PRETRAINED_MODEL  =                 get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'USE_PRETRAINED_MODEL')\n",
    "USE_GPU =                               get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'USE_GPU')\n",
    "\n",
    "BATCH_SIZE_TRAIN =                      get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'BATCH_SIZE_TRAIN')\n",
    "BATCH_SIZE_VAL =                        get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'BATCH_SIZE_VAL')\n",
    "BATCH_SIZE_TEST =                       get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'BATCH_SIZE_TEST')\n",
    "\n",
    "LEARNING_RATE =                         get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'LEARNING_RATE')\n",
    "LEARNING_RATE /= 1e6\n",
    "\n",
    "TRAIN_DATA_PATH =                       get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TRAIN_DATA_PATH')\n",
    "VAL_DATA_PATH =                         get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'VAL_DATA_PATH')\n",
    "TEST_DATA_PATH =                        get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TEST_DATA_PATH')\n",
    "DATA_PATH =                             get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'DATA_PATH')\n",
    "ROOT_PATH =                             get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'ROOT_PATH')\n",
    "\n",
    "TRAIN_IMAGES_MEAN_FILE_PATH =           get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TRAIN_IMAGES_MEAN_FILE_PATH')\n",
    "TRAIN_IMAGES_STD_FILE_PATH  =           get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TRAIN_IMAGES_STD_FILE_PATH')\n",
    "\n",
    "TRAIN_IMAGES_TOTAL_MEAN_FILE_PATH =     get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TRAIN_IMAGES_TOTAL_MEAN_FILE_PATH')\n",
    "TRAIN_IMAGES_TOTAL_STD_FILE_PATH  =     get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'TRAIN_IMAGES_TOTAL_STD_FILE_PATH')\n",
    "\n",
    "MAX_TOTAL_IMAGE_NUMBER =                get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'MAX_TOTAL_IMAGE_NUMBER')\n",
    "LOGGER_PATH =                           \"/home/novakovm/iris/MILOS/crafter_logger.txt\"\n",
    "PCA_decomp_in_every_epochs =            get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'PCA_decomp_in_every_epochs')\n",
    "run_id =                                get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'run_id')\n",
    "\n",
    "#GENERATE_DATA_FROM_START =              get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'GENERATE_DATA_FROM_START')\n",
    "#ONLY_GENERATE_DATA   =                  get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'ONLY_GENERATE_DATA')\n",
    "\n",
    "\n",
    "GENERATE_DATA=get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'GENERATE_DATA')\n",
    "PREPROCESS_DATA=get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'PREPROCESS_DATA')\n",
    "INFERENCE_DATA=get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'INFERENCE_DATA')\n",
    "# load train/val/test dataset percentage take adds up to 100 (percent)\n",
    "train_dataset_percentage= get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'train_dataset_percentage')\n",
    "val_dataset_percentage  = get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'val_dataset_percentage')\n",
    "test_dataset_percentage = get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'test_dataset_percentage')\n",
    "assert(100 == train_dataset_percentage + val_dataset_percentage + test_dataset_percentage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['observations', 'actions', 'rewards', 'ends', 'mask_padding'])\n",
      "torch.Size([149, 3, 64, 64])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/novakovm/miniconda3/envs/iris_mn/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "path = \"/data/alonsoel/workspace/data/crafter/dataset_1M_steps_heuristic_crafter/\"\n",
    "crafter_batch_id = 1\n",
    "img_tensor = torch.load(path + f\"{crafter_batch_id}.pt\")\n",
    "print(img_tensor.keys())\n",
    "print(img_tensor['observations'].shape)\n",
    "\n",
    "B,C,H,W = img_tensor['observations'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5e97deef10>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_number_in_batch = 56\n",
    "tensor_image = img_tensor['observations'][sample_number_in_batch,:,:,:]\n",
    "tensor_image_channel_last_column = tensor_image.permute(1, 2, 0)\n",
    "plt.imshow(tensor_image_channel_last_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 174.59734590536058, 440)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "batch_sizes = []\n",
    "MAX_NUMBER_OF_PT_FILES = 5727\n",
    "for pt_file_id in range(MAX_NUMBER_OF_PT_FILES):\n",
    "    img_tensor = torch.load(path + f\"{pt_file_id}.pt\")\n",
    "    batch_sizes.append(img_tensor['observations'].shape[0])\n",
    "\n",
    "batch_sizes = np.array(batch_sizes)\n",
    "batch_sizes.min(),batch_sizes.mean(),batch_sizes.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of images = 999919\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of images = {batch_sizes.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "eloi_data_path = \"/data/alonsoel/workspace/data/crafter/dataset_1M_steps_heuristic_crafter/\"\n",
    "MAX_NUMBER_OF_PT_FILES = 5727\n",
    "root = ROOT_PATH\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "data_paths= {}\n",
    "data_paths['train'] = TRAIN_DATA_PATH\n",
    "data_paths['val'] = VAL_DATA_PATH\n",
    "data_paths['test'] = TEST_DATA_PATH\n",
    "data_paths['data'] = DATA_PATH\n",
    "\n",
    "for dataset_str in data_paths:\n",
    "    data_path = data_paths[dataset_str]\n",
    "    #make folder if it does not exist\n",
    "    if not os.path.exists(data_path):\n",
    "        os.mkdir(data_path)\n",
    "    #clear the content of the folder\n",
    "    files = glob.glob(data_path + '*')\n",
    "    for f in files:\n",
    "       os.remove(f)\n",
    "\n",
    "#unique_img_id = 0\n",
    "unique_image_number = 0\n",
    "\n",
    "unique_image_number_array = []\n",
    "for pt_file_id in range(MAX_NUMBER_OF_PT_FILES):\n",
    "    tensor_img_dsc_path = data_paths['data']\n",
    "    \n",
    "    tensor_img_src_path = eloi_data_path\n",
    "    tensor_img_src_name = f\"{pt_file_id}.pt\"\n",
    "    \n",
    "    loaded_tensor = torch.load(tensor_img_src_path + tensor_img_src_name)\n",
    "    tensor_imgs = loaded_tensor['observations']\n",
    "    \n",
    "    for batch_idx in range(tensor_imgs.size(0)):\n",
    "        \n",
    "        #tensor_img_dsc_name = f\"{str(TOTAL_IMAGE_NUMBER).zfill(7)}_curr_id_{pt_file_id}_past_id.pt\"\n",
    "        tensor_img_dsc_name = 'color_img_' + str(unique_image_number).zfill(len(str(MAX_TOTAL_IMAGE_NUMBER))) + \".png\"\n",
    "        \n",
    "        current_img = tensor_imgs[batch_idx, :, :, :].view(C,H,W).cpu().permute(1, 2, 0).numpy() #HWC\n",
    "        \n",
    "        plt.imsave(tensor_img_dsc_path+tensor_img_dsc_name, current_img)\n",
    "        \n",
    "        #torch.save(tensor_imgs[batch_idx, :, :, :], tensor_img_dsc_path+tensor_img_dsc_name)\n",
    "        unique_image_number_array.append(unique_image_number)\n",
    "        unique_image_number += 1\n",
    "        \n",
    "        if unique_image_number >= MAX_TOTAL_IMAGE_NUMBER:\n",
    "            break\n",
    "        \n",
    "    if unique_image_number >= MAX_TOTAL_IMAGE_NUMBER:\n",
    "            break\n",
    "        \n",
    "unique_image_number_array = np.array(unique_image_number_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED=1\n",
    "np.random.seed(SEED)\n",
    "shuffled_image_ids = unique_image_number_array.copy()\n",
    "np.random.shuffle(shuffled_image_ids)\n",
    "N = len(shuffled_image_ids)\n",
    "# secure no overlap with train, validation and test datasets\n",
    "#75% of the dataset\n",
    "train_shuffled_image_ids = shuffled_image_ids[:int(N* train_dataset_percentage/100)]\n",
    "#12.5% of the dataset\n",
    "val_shuffled_image_ids = shuffled_image_ids[int(N* train_dataset_percentage/100):int(N* (train_dataset_percentage/100 + val_dataset_percentage/100))]\n",
    "#12.5% of the dataset\n",
    "test_shuffled_image_ids = shuffled_image_ids[int(N* (train_dataset_percentage/100 + val_dataset_percentage/100)):]\n",
    "#check that everything adds up\n",
    "assert(N == len(train_shuffled_image_ids) + len(val_shuffled_image_ids) + len(test_shuffled_image_ids))\n",
    "\n",
    "np.save(ROOT_PATH+\"train_shuffled_image_ids.npy\", train_shuffled_image_ids)\n",
    "np.save(ROOT_PATH+\"val_shuffled_image_ids.npy\", val_shuffled_image_ids)\n",
    "np.save(ROOT_PATH+\"test_shuffled_image_ids.npy\", test_shuffled_image_ids)\n",
    "\n",
    "shuffled_image_ids = {'train':train_shuffled_image_ids, 'val':val_shuffled_image_ids, 'test':test_shuffled_image_ids}\n",
    "\n",
    "#cut operation from DATA to either DATA_TEST or DATA_VALIDATE or DATA_TEST\n",
    "for unique_image_number in unique_image_number_array:\n",
    "    img_src_path = data_paths['data']\n",
    "    img_src_name = 'color_img_' + str(unique_image_number).zfill(len(str(MAX_TOTAL_IMAGE_NUMBER))) + \".png\"\n",
    "    \n",
    "    for dataset_str in ['train', 'val', 'test']:\n",
    "        if unique_image_number in shuffled_image_ids[dataset_str]:\n",
    "            img_dsc_path = data_paths[dataset_str]\n",
    "            img_dsc_name = img_src_name # keep the name same\n",
    "            #cut operation from DATA to either DATA_TEST or DATA_VALIDATE or DATA_TEST\n",
    "            os.rename(src=img_src_path+img_src_name,dst=img_dsc_path+img_dsc_name)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load train/val/test image ids and check if their number adds up to total number of images\n",
    "train_shuffled_image_ids= np.load(ROOT_PATH+\"train_shuffled_image_ids.npy\")\n",
    "val_shuffled_image_ids  = np.load(ROOT_PATH+\"val_shuffled_image_ids.npy\")\n",
    "test_shuffled_image_ids = np.load(ROOT_PATH+\"test_shuffled_image_ids.npy\")\n",
    "assert(set(np.concatenate((train_shuffled_image_ids,val_shuffled_image_ids,test_shuffled_image_ids))) == set(np.arange(MAX_TOTAL_IMAGE_NUMBER)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@280.998] global /io/opencv/modules/imgcodecs/src/loadsave.cpp (239) findDecoder imread_('/home/novakovm/DATA_TRAIN/color_img_000003180.png'): can't open/read file: check file path/integrity\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# calculate variance in data\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpreprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m find_mean_std\n\u001b[1;32m      4\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39m# DATA PREPROCESSING - calculating empirical mean & std #\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m#########################################################\u001b[39;00m\n\u001b[1;32m      8\u001b[0m RGB_mean, RGB_std, RGB_mean_np, RGB_std_np, X_mean_bar_all_ch, X_std_bar_all_ch,  Total_mean_np, Total_std_np \u001b[39m=\u001b[39m \\\n\u001b[1;32m      9\u001b[0m find_mean_std(TOTAL_NUMBER_OF_IMAGES \u001b[39m=\u001b[39m MAX_TOTAL_IMAGE_NUMBER, \u001b[39m#for z fill\u001b[39;00m\n\u001b[1;32m     10\u001b[0m               image_ids_numbers \u001b[39m=\u001b[39m train_shuffled_image_ids,\n\u001b[1;32m     11\u001b[0m               train_folder_path \u001b[39m=\u001b[39m TRAIN_DATA_PATH,\n\u001b[1;32m     12\u001b[0m               main_folder_path \u001b[39m=\u001b[39m ROOT_PATH)\n",
      "File \u001b[0;32m~/iris/MILOS/preprocessing.py:150\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39m#TOTAL_NUMBER_OF_IMAGES = [dict_['TOTAL_NUMBER_OF_IMAGES'] for dict_ in data['file_info'] if 'TOTAL_NUMBER_OF_IMAGES' in dict_][0]\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[39m#train_folder_path = [dict_['train_folder_path'] for dict_ in data['file_info'] if 'train_folder_path' in dict_][0]\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[39m#main_folder_path = [dict_['main_folder_path'] for dict_ in data['file_info'] if 'main_folder_path' in dict_][0]\u001b[39;00m\n\u001b[1;32m    148\u001b[0m train_shuffled_image_ids \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mload(main_folder_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/train_shuffled_image_ids.npy\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 150\u001b[0m RGB_mean, RGB_std, RGB_mean_np, RGB_std_np, X_mean_bar_all_ch, X_std_bar_all_ch,  Total_mean_np, Total_std_np \u001b[39m=\u001b[39m find_mean_std(TOTAL_NUMBER_OF_IMAGES, \n\u001b[1;32m    151\u001b[0m                                                                                                                                 image_ids_numbers \u001b[39m=\u001b[39;49m train_shuffled_image_ids,\n\u001b[1;32m    152\u001b[0m                                                                                                                                 train_folder_path\u001b[39m=\u001b[39;49mtrain_folder_path,\n\u001b[1;32m    153\u001b[0m                                                                                                                                 main_folder_path\u001b[39m=\u001b[39;49mmain_folder_path)\n\u001b[1;32m    154\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Images Mean per chanel= \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mround(RGB_mean,\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    155\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTraining Images Std per chanel = \u001b[39m\u001b[39m{\u001b[39;00mnp\u001b[39m.\u001b[39mround(RGB_std,\u001b[39m2\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/iris/MILOS/preprocessing.py:53\u001b[0m, in \u001b[0;36mfind_mean_std\u001b[0;34m(TOTAL_NUMBER_OF_IMAGES, image_ids_numbers, method, NUMPY_DOUBLE_CHECK, train_folder_path, main_folder_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m x \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(image_full_path)\n\u001b[1;32m     52\u001b[0m \u001b[39mif\u001b[39;00m NUMPY_DOUBLE_CHECK:\n\u001b[0;32m---> 53\u001b[0m     R_imgs\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39;49mfloat64(x)[:,:,\u001b[39m2\u001b[39;49m])\n\u001b[1;32m     54\u001b[0m     G_imgs\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mfloat64(x)[:,:,\u001b[39m1\u001b[39m])\n\u001b[1;32m     55\u001b[0m     B_imgs\u001b[39m.\u001b[39mappend(np\u001b[39m.\u001b[39mfloat64(x)[:,:,\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "# calculate variance in data\n",
    "from preprocessing import find_mean_std\n",
    "\n",
    "#########################################################\n",
    "# DATA PREPROCESSING - calculating empirical mean & std #\n",
    "#########################################################\n",
    "\n",
    "RGB_mean, RGB_std, RGB_mean_np, RGB_std_np, X_mean_bar_all_ch, X_std_bar_all_ch,  Total_mean_np, Total_std_np = \\\n",
    "find_mean_std(TOTAL_NUMBER_OF_IMAGES = MAX_TOTAL_IMAGE_NUMBER, #for z fill\n",
    "              image_ids_numbers = train_shuffled_image_ids,\n",
    "              train_folder_path = TRAIN_DATA_PATH,\n",
    "              main_folder_path = ROOT_PATH)\n",
    "\n",
    "\n",
    "#\n",
    "print(f\"Training Images Mean per chanel= {np.round(RGB_mean,2)}\")\n",
    "print(f\"Training Images Std per chanel = {np.round(RGB_std,2)}\")\n",
    "print(f\"Training Images Mean           = {np.round(X_mean_bar_all_ch,2)}\")\n",
    "print(f\"Training Images Std            = {np.round(X_std_bar_all_ch,2)}\\n\")\n",
    "#\n",
    "print(f\"Mean (Empirical - np) diff per chanel= {RGB_mean - RGB_mean_np}\")\n",
    "print(f\"Std (Empirical - np) diff per chanel = {RGB_std - RGB_std_np}\")\n",
    "print(f\"Mean (Empirical - np) diff           = {X_mean_bar_all_ch - Total_mean_np}\")\n",
    "print(f\"Std (Empirical - np) diff            = {X_std_bar_all_ch - Total_std_np}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/novakovm/crafter/'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "###########################################\n",
    "# DATA PREPROCESSING - creating Transforms#\n",
    "###########################################\n",
    "\n",
    "zero_mean_unit_std_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=np.load(TRAIN_IMAGES_MEAN_FILE_PATH).tolist(),\n",
    "                         std=np.load(TRAIN_IMAGES_STD_FILE_PATH).tolist() )\n",
    "    ])\n",
    "zero_min_one_max_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean = [0., 0., 0.],\n",
    "                          std  = [255., 255., 255.])\n",
    "    ]) # OUTPUT SIGMOID of DNN\n",
    "minus_one_min_one_max_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean = [-255./2., -255./2., -255./2.],\n",
    "                          std  = [255./2., 255./2., 255./2.])\n",
    "    ]) # OUTPUT (1/2)*TANH of DNN\n",
    "\n",
    "# Pick one transform that is applied\n",
    "#TRANSFORM_IMG = zero_min_one_max_transform#zero_mean_unit_std_transform # zero_min_one_max_transform\n",
    "TRANSFORM_IMG = minus_one_min_one_max_transform#zero_mean_unit_std_transform # zero_min_one_max_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# DATA LOADING #\n",
    "################\n",
    "\n",
    "class CustomImageDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, args, root = False, transform=None):\n",
    "        self.root = root # './DATA/' = '/home/novakovm/iris/MILOS/DATA/'\n",
    "        self.transform = transform\n",
    "        self.TOTAL_NUMBER_OF_IMAGES = args['TOTAL_NUMBER_OF_IMAGES'] # 2048 for test and val; 12'288 for train\n",
    "        self.image_ids = args['image_ids']# 3314, 2151, 12030, 32, ...\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_ids)#self.TOTAL_NUMBER_OF_IMAGES\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_id = self.image_ids[idx]\n",
    "        img_path = self.root + 'color_img_' + str(image_id).zfill(len(str(self.TOTAL_NUMBER_OF_IMAGES))) + '.png'\n",
    "        image = torchvision.io.read_image(img_path,mode=torchvision.io.image.ImageReadMode.RGB).float() # .double() = torch.float64 and  .float() = torch.float32\n",
    "        if self.transform != None:\n",
    "            image = self.transform(image)\n",
    "        return image, image_id\n",
    "\n",
    "# Train Data & Train data Loader\n",
    "args_train = {'TOTAL_NUMBER_OF_IMAGES' : MAX_TOTAL_IMAGE_NUMBER, 'image_ids' : train_shuffled_image_ids}\n",
    "train_data = CustomImageDataset(args = args_train, root=TRAIN_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "train_data_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size=BATCH_SIZE_TRAIN, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "\n",
    "# Validation Data & Validation data Loader\n",
    "args_val = {'TOTAL_NUMBER_OF_IMAGES' : MAX_TOTAL_IMAGE_NUMBER, 'image_ids' : val_shuffled_image_ids}\n",
    "val_data = CustomImageDataset(args = args_val, root=VAL_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "val_data_loader = torch.utils.data.DataLoader(dataset = val_data, batch_size=BATCH_SIZE_VAL, shuffle=True,  num_workers=NUM_WORKERS)\n",
    "\n",
    "# Test Data & Test data Loader\n",
    "args_test = {'TOTAL_NUMBER_OF_IMAGES' : MAX_TOTAL_IMAGE_NUMBER, 'image_ids' : test_shuffled_image_ids}\n",
    "test_data = CustomImageDataset(args = args_test, root=TEST_DATA_PATH, transform=TRANSFORM_IMG)\n",
    "test_data_loader  = torch.utils.data.DataLoader(dataset = test_data, batch_size=BATCH_SIZE_TEST, shuffle=True, num_workers=NUM_WORKERS) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision.io.read_image(TRAIN_DATA_PATH+ 'color_img_' + str(1).zfill(len(str(MAX_TOTAL_IMAGE_NUMBER))) + '.png',mode=torchvision.io.image.ImageReadMode.RGB).float().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from VQ_VAE import ResidualStack, VectorQuantizer, count_parameters, report_cuda_memory_status\n",
    "\n",
    "class Manual_Encoder(nn.Module):\n",
    "    def __init__(self, args_encoder, res_block_args):\n",
    "        super(Manual_Encoder, self).__init__()\n",
    "        self.args_encoder = args_encoder\n",
    "        nb_layers_in_a_block = 2\n",
    "        C_out_1_init, multiplier_value = 256//8, 2#32 #=256//8  #32#good for K=8 but for higer K (e.g. 128) you can go lower for C_out_1_init to not overfitt the data\n",
    "        C_out_1_init, multiplier_value = args_encoder['C_out_1_init'],args_encoder['multiplier_value']\n",
    "        k,s,p=4,2,1\n",
    "        l=0\n",
    "        self.sequential_convs = torch.nn.Sequential()\n",
    "        if self.args_encoder['M'] <= 31: \n",
    "            self.sequential_convs.add_module(f\"conv2d_{l}\", nn.Conv2d(in_channels=3, out_channels=C_out_1_init, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 32\n",
    "        if self.args_encoder['M'] <= 15:\n",
    "            l+=1\n",
    "            self.sequential_convs.add_module(f\"conv2d_{l}\", nn.Conv2d(in_channels=self.sequential_convs[0].out_channels, out_channels=multiplier_value * self.sequential_convs[0].out_channels, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 64\n",
    "        if self.args_encoder['M'] <= 7:\n",
    "            l+=1\n",
    "            self.sequential_convs.add_module(f\"conv2d_{l}\", nn.Conv2d(in_channels=self.sequential_convs[nb_layers_in_a_block].out_channels, out_channels=multiplier_value * self.sequential_convs[nb_layers_in_a_block].out_channels, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 128\n",
    "        if self.args_encoder['M'] <= 3:\n",
    "            l+=1\n",
    "            self.sequential_convs.add_module(f\"conv2d_{l}\", nn.Conv2d(in_channels=self.sequential_convs[2*nb_layers_in_a_block].out_channels, out_channels=multiplier_value * self.sequential_convs[2*nb_layers_in_a_block].out_channels, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 256\n",
    "            \n",
    "        res_block_args['C_in'] = self.sequential_convs[-nb_layers_in_a_block].out_channels\n",
    "        self.residual_stack = ResidualStack(res_block_args) \n",
    "        \n",
    "        self.channel_adjusting_conv = nn.Conv2d(in_channels=self.sequential_convs[-nb_layers_in_a_block].out_channels, out_channels=args_encoder['D'], kernel_size=1, stride=1, padding=0)\n",
    "        #C_out = D\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sequential_convs(x)\n",
    "        x = self.residual_stack(x)\n",
    "        x = self.channel_adjusting_conv(x)\n",
    "        return x\n",
    "\n",
    "class Manual_Decoder(nn.Module):\n",
    "    def __init__(self, args_decoder, res_block_args):\n",
    "        super(Manual_Decoder, self).__init__()\n",
    "        self.args_decoder = args_decoder\n",
    "        \n",
    "        C_in_1_init, divisor_value = 256, 2 ##256#good for K=8 but for higer K (e.g. 128) you can go lower for C_in_1_init to not overfitt the data\n",
    "        \n",
    "        C_in_1_init, divisor_value = args_decoder['C_in_1_init'], args_decoder['divisor_value']\n",
    "        \n",
    "        res_block_args['C_in'] = C_in_1_init\n",
    "        \n",
    "        self.channel_adjusting_conv = nn.Conv2d(in_channels=args_decoder['D'], out_channels=C_in_1_init, kernel_size=1, stride=1, padding=0)\n",
    "        self.residual_stack = ResidualStack(res_block_args) \n",
    "        self.sequential_trans_convs = torch.nn.Sequential()\n",
    "        \n",
    "        nb_layers_in_a_block = 2\n",
    "        k,s,p=4,2,1\n",
    "        l=0\n",
    "        if self.args_decoder['M'] <= 31: \n",
    "            self.sequential_trans_convs.add_module(f\"trans_conv{l}\", nn.ConvTranspose2d(in_channels=C_in_1_init, out_channels = C_in_1_init // divisor_value, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_trans_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 128\n",
    "        if self.args_decoder['M'] <= 15:\n",
    "            l+=1\n",
    "            self.sequential_trans_convs.add_module(f\"trans_conv{l}\", nn.ConvTranspose2d(in_channels=self.sequential_trans_convs[0].out_channels, out_channels=self.sequential_trans_convs[0].out_channels // divisor_value, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_trans_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 64\n",
    "        if self.args_decoder['M'] <= 7:\n",
    "            l+=1\n",
    "            self.sequential_trans_convs.add_module(f\"trans_conv{l}\", nn.ConvTranspose2d(in_channels=self.sequential_trans_convs[nb_layers_in_a_block].out_channels, out_channels=self.sequential_trans_convs[nb_layers_in_a_block].out_channels // divisor_value, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_trans_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 32\n",
    "        if self.args_decoder['M'] <= 3:\n",
    "            l+=1\n",
    "            self.sequential_trans_convs.add_module(f\"trans_conv{l}\", nn.ConvTranspose2d(in_channels=self.sequential_trans_convs[2*nb_layers_in_a_block].out_channels, out_channels=self.sequential_trans_convs[2*nb_layers_in_a_block].out_channels // divisor_value, kernel_size=k, stride=s, padding=p))\n",
    "            self.sequential_trans_convs.add_module(f\"ReLU_{l}\", nn.ReLU(True))\n",
    "            #C_out = 16\n",
    "            \n",
    "        self.output_conv_layer =  nn.Conv2d(in_channels=self.sequential_trans_convs[-nb_layers_in_a_block].out_channels, out_channels=3, kernel_size=1, stride=1, padding=0)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.channel_adjusting_conv(x)\n",
    "        x = self.residual_stack(x)\n",
    "        x = self.sequential_trans_convs(x)\n",
    "        x = self.output_conv_layer(x)\n",
    "        return x\n",
    "    \n",
    "class VQ_VAE(nn.Module):\n",
    "    def __init__(self, args_encoder, args_VQ, args_decoder, res_block_args_encoder, res_block_args_decoder):\n",
    "        super(VQ_VAE, self).__init__()\n",
    "        \n",
    "        ######################\n",
    "        # Model Constructors #\n",
    "        ######################\n",
    "        self.C_in, self.H_in, self.W_in = args_encoder['C_in'], args_encoder['H_in'], args_encoder['W_in']\n",
    "        self.args_encoder=args_encoder\n",
    "        self.args_VQ=args_VQ\n",
    "        self.args_decoder=args_decoder\n",
    "        self.res_block_args_encoder = res_block_args_encoder\n",
    "        self.res_block_args_decoder = res_block_args_decoder\n",
    "        \n",
    "        self.train_with_quantization = args_VQ['train_with_quantization']\n",
    "        \n",
    "        #self.encoder =  Encoder(self.args_encoder, self.res_block_args_encoder)\n",
    "        self.encoder =  Manual_Encoder(self.args_encoder, self.res_block_args_encoder)\n",
    "        if self.train_with_quantization:\n",
    "            self.VQ      =  VectorQuantizer(args_VQ)\n",
    "        #self.decoder =  Decoder(self.args_decoder, self.res_block_args_decoder)\n",
    "        self.decoder =  Manual_Decoder(self.args_decoder, self.res_block_args_decoder)\n",
    "        \n",
    "\n",
    "    def forward(self, x):                       #torch.Size([128, 3, 64, 64])\n",
    "        Ze = self.encoder(x)                    #torch.Size([128, 64, 16, 16])\n",
    "        \n",
    "        e_and_q_latent_loss, Zq, e_latent_loss, q_latent_loss, estimate_codebook_words_exp_entropy = None, None, None, None, None\n",
    "        if self.train_with_quantization:\n",
    "            e_and_q_latent_loss, Zq, e_latent_loss, q_latent_loss, estimate_codebook_words_exp_entropy = self.VQ(Ze)   #torch.Size([128, 64, 16, 16])\n",
    "        else:\n",
    "            e_and_q_latent_loss, Zq, e_latent_loss, q_latent_loss, estimate_codebook_words_exp_entropy =  0, Ze, 0, 0, 0\n",
    "        x_recon = self.decoder(Zq)              #torch.Size([128, 3, 64, 64])\n",
    "        return e_and_q_latent_loss, x_recon, e_latent_loss, q_latent_loss, estimate_codebook_words_exp_entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# VQ-VAE model creation #\n",
    "#########################\n",
    "# VQ args\n",
    "args_VQ = {}\n",
    "for arg_VQ in ['max_channel_number', 'train_with_quantization', 'D', 'K', 'beta', 'M', 'use_EMA', 'gamma', 'requires_normalization_with_sphere_projection']:\n",
    "    args_VQ[arg_VQ] = get_hyperparam_value(hyperparam_dict['training_hyperparams'], arg_VQ)\n",
    "K,D,M = args_VQ['K'],args_VQ['D'],args_VQ['M']\n",
    "# Encoder Residual Block arguments\n",
    "res_block_args_encoder = {'block_size' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'res_block_size'), 'C_mid' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'res_blocks_channel_number_in_hidden_layers')}\n",
    "# Decoder Residual Block arguments\n",
    "res_block_args_decoder = {'block_size' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'res_block_size'), 'C_mid' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'res_blocks_channel_number_in_hidden_layers')}\n",
    "# Encoder and Decoder args\n",
    "args_encoder = {'M' : args_VQ['M'], 'D' : args_VQ['D'], 'C_in' : C, 'H_in' : H, 'W_in' : W , 'use_BN' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'use_BN')}\n",
    "args_decoder = {'M' : args_VQ['M'], 'D' : args_VQ['D'], 'use_BN' : get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'use_BN')}\n",
    "\n",
    "# channel hyper params\n",
    "nb_of_conv2d_stride_2_layers = {31 : 1, 15: 2, 7: 3, 3: 4, 1:5, 0:6}\n",
    "\n",
    "args_decoder['C_in_1_init'], args_decoder['divisor_value'] = 256, get_hyperparam_value(hyperparam_dict['training_hyperparams'], 'divisor_value')\n",
    "args_encoder['C_out_1_init'], args_encoder['multiplier_value'] = args_decoder['C_in_1_init'] // (args_decoder['divisor_value']**(nb_of_conv2d_stride_2_layers[M] - 1)), args_decoder['divisor_value']\n",
    "\n",
    "# VQ VAE model\n",
    "model = VQ_VAE(args_encoder, args_VQ, args_decoder, res_block_args_encoder, res_block_args_decoder)\n",
    "#print(model)\n",
    "count_parameters(model, path_to_write = LOGGER_PATH)\n",
    "report_cuda_memory_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = vq_vae_implemented_model\n",
    "\n",
    "# create a trainer init arguments\n",
    "training_args = {}\n",
    "training_args['NUM_EPOCHS']         = NUM_EPOCHS\n",
    "training_args['loss_fn']            = nn.MSELoss()\n",
    "training_args['device']             = torch.device(\"cuda:0\" if USE_GPU and torch.cuda.is_available() else \"cpu\") \n",
    "training_args['model']              = model #vq_vae_implemented_model\n",
    "training_args['model_name']         = 'VQ_VAE'\n",
    "training_args['loaders']            = {'train' : train_data_loader, 'val' : val_data_loader, 'test' : test_data_loader}\n",
    "training_args['optimizer_settings'] = {'optimization_algorithm':'Adam','lr':LEARNING_RATE}\n",
    "training_args['logger_path'] = LOGGER_PATH\n",
    "# if PCA_decomp_in_every_epochs True then it considerably (from 50mins to 70 mins, i.e. 40%!) slows down the training loop!!!\n",
    "training_args['PCA_decomp_in_every_epochs'] = True\n",
    "# .item() because it is one element np.array; and we square it because we want variance and not the standard deviation\n",
    "training_args['train_data_variance'] = np.load(TRAIN_IMAGES_TOTAL_STD_FILE_PATH).item() **2\n",
    "# because we divided the chanells with TRANSFORM_IMG.std[0] we have to correct the total training data variance for that in other words training_args['train_data_variance'] was VAR[X] but because we did linear transform TRANSFORM_IMG so that X -> (X - MEAN_TRANSFORM_IMG) / STD_TRANSFORM_IMG we need to adjust the total variance of the data from VAR[X] -> VAR[(X - MEAN_TRANSFORM_IMG) / STD_TRANSFORM_IMG] = VAR[X] / STD_TRANSFORM_IMG**2 and that is precicely what we are doing here\n",
    "training_args['train_data_variance'] /= (TRANSFORM_IMG.transforms[0].std[0]**2)\n",
    "# training_args['train_data_variance']=1.\n",
    "print(f\"Inverse of training data variance term is equal to =  {1. / training_args['train_data_variance']:.1f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import Model_Trainer\n",
    "compressed_number_of_bits_per_image = int(np.ceil(np.log2(model.args_VQ['K']))) * (model.args_VQ['M']+1) ** 2\n",
    "\n",
    "trainer_folder_path = ROOT_PATH + \\\n",
    "                      str(run_id).zfill(3) + \"_\" + training_args['model_name'] + \\\n",
    "                      '_K_' + str(model.args_VQ['K']) + \\\n",
    "                      '_D_' + str(model.args_VQ['D']) + \\\n",
    "                      '_M_' + str(model.args_VQ['M']) + \\\n",
    "                      '_bits_' + str(compressed_number_of_bits_per_image)\n",
    "training_args['main_folder_path']   = trainer_folder_path\n",
    "\n",
    "if not(os.path.exists(trainer_folder_path)):\n",
    "    os.system(f'mkdir {trainer_folder_path}')\n",
    "\n",
    "# create a trainer object\n",
    "trainer = Model_Trainer(args=training_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not USE_PRETRAINED_MODEL:\n",
    "    current_time_str = time.strftime(\"%H:%M:%S %d.%m.%Y\", time.gmtime(time.time()))\n",
    "    log_str = f\"[{current_time_str}] {run_id}) Started running for K = {K} & D = {D} & M = {M} & beta = {model.args_VQ['beta']} & max_channel_number = {args_decoder['C_in_1_init']} (i.e. bits = {compressed_number_of_bits_per_image}) change_channel_size_across_layers by factor = {args_decoder['divisor_value']}\"\n",
    "    \n",
    "    with open(LOGGER_PATH, 'a') as f:\n",
    "        # get current time in the format hh:mm:ss DD.MM.YYYY\n",
    "        f.write(f\"****************************************************************************************************************\\n\\n\")\n",
    "        f.write(f\"----- {current_time_str} BEGIN RUN -----\\n\\n\")\n",
    "        f.write(f\"Started {log_str}:\\n\\n--------------------------------------------------- \\n\\n\")\n",
    "\n",
    "    START_TIME = time.time()\n",
    "    #train\n",
    "    trainer.train()\n",
    "    #train\n",
    "    TOTAL_TRAINING_TIME = int(np.floor(time.time() - START_TIME))\n",
    "    minutes, seconds = divmod(TOTAL_TRAINING_TIME, 60)\n",
    "    hours, minutes = divmod(minutes, 60)\n",
    "    TOTAL_TRAINING_TIME = f\"{hours}:{minutes}:{seconds} h/m/s\"\n",
    "    \n",
    "    # get current time in the format hh:mm:ss DD.MM.YYYY\n",
    "    current_time_str = time.strftime(\"%H:%M:%S %d.%m.%Y\", time.gmtime(time.time()))\n",
    "    with open(LOGGER_PATH, 'a') as f:\n",
    "        f.write(f\"Finished {log_str}:\\nTotal training time is = {TOTAL_TRAINING_TIME}. \\n\\n--------------------------------------------------- \\n\\n\")\n",
    "        f.write(f\"----- {current_time_str} END RUN -----\\n\\n****************************************************************************************************************\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_PRETRAINED_MODEL:\n",
    "    ##########################    \n",
    "    # Use a pretrained model #\n",
    "    ##########################\n",
    "    trainer.epoch_ids_PCA = list(range( int(0.05*trainer.NUM_EPOCHS), trainer.NUM_EPOCHS + 1, int(0.05*trainer.NUM_EPOCHS)))\n",
    "    \n",
    "    if run_id == 600:\n",
    "        current_time_str = \"2023_01_18_01_34_27\" # \"YYYY_MM_DD_HH_MM_SS\" \n",
    "    else:\n",
    "        assert(False)\n",
    "    \n",
    "    # load model that was trained at newly given current_time_str \n",
    "    trainer.load_model(current_time_str = current_time_str, autoencoder_config_params_wrapped_sorted= None)\n",
    "\n",
    "    # load avg. training loss of the training proceedure for a model that was trained at newly given current_time_str \n",
    "    trainer.train_loss_avg = np.load(trainer.main_folder_path + '/' + trainer.model_name + '_train_loss_avg_' + trainer.current_time_str + '.npy')\n",
    "    \n",
    "    # load avg. validation loss of the validating proceedure for a model that was trained at newly given current_time_str \n",
    "    trainer.val_loss_avg = np.load(trainer.main_folder_path + '/' + trainer.model_name + '_val_loss_avg_' + trainer.current_time_str + '.npy')\n",
    "    \n",
    "    # Load individual loss terms for both training and validation datasets\n",
    "    trainer.usage_of_multiple_terms_loss_function = True\n",
    "    # Training Loss data per term\n",
    "    trainer.train_multiple_losses_avg = {}\n",
    "    \n",
    "    # Validation Loss data per term\n",
    "    trainer.val_multiple_losses_avg = {}\n",
    "    \n",
    "    # Training Loss data per term file path\n",
    "    trainer.train_multiple_losses_avg_path = {}\n",
    "    \n",
    "    # Validation Loss data per term file path\n",
    "    trainer.val_multiple_losses_avg_path = {}\n",
    "    \n",
    "    for loss_term in ['reconstruction_loss','commitment_loss', 'VQ_codebook_loss']:\n",
    "        # Training Loss data per term file path\n",
    "        trainer.train_multiple_losses_avg_path[loss_term] = trainer.main_folder_path + '/' + trainer.model_name + '_train_multiple_losses_avg_' + loss_term + '_'  + trainer.current_time_str + '.npy'\n",
    "        \n",
    "        # Validation Loss data per term file path\n",
    "        trainer.val_multiple_losses_avg_path[loss_term]   = trainer.main_folder_path + '/' + trainer.model_name + '_val_multiple_losses_avg_' + loss_term + '_'  + trainer.current_time_str + '.npy'\n",
    "        \n",
    "        # Training Loss data per term\n",
    "        trainer.train_multiple_losses_avg[loss_term] = np.load(trainer.train_multiple_losses_avg_path[loss_term])\n",
    "        \n",
    "        # Validation Loss data per term\n",
    "        trainer.val_multiple_losses_avg[loss_term]   = np.load(trainer.val_multiple_losses_avg_path[loss_term])\n",
    "    \n",
    "    # Load Perplexity over epochs during training\n",
    "    trainer.train_metrics, trainer.val_metrics = {}, {}    \n",
    "    trainer.train_metrics_perplexity_path = trainer.main_folder_path + '/' + trainer.model_name + '_train_perplexity_' + trainer.current_time_str + '.npy'\n",
    "    trainer.val_metrics_perplexity_path = trainer.main_folder_path + '/' + trainer.model_name + '_val_perplexity_' + trainer.current_time_str + '.npy'\n",
    "    trainer.train_metrics['perplexity'] = np.load(trainer.train_metrics_perplexity_path)\n",
    "    trainer.val_metrics['perplexity'] = np.load(trainer.val_metrics_perplexity_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import visualise_output\n",
    "#########################\n",
    "### Testing the model ###\n",
    "#########################\n",
    "loss_fn = trainer.loss_fn\n",
    "loss_fn.to(trainer.device)\n",
    "trainer.test() \n",
    "\n",
    "#############################################################################\n",
    "# Plot train and validation avergae loss across mini-batch across epochs #\n",
    "# Plot Test Loss for every sample in the Test set #\n",
    "#############################################################################\n",
    "trainer.plot()\n",
    "\n",
    "\n",
    "############################################################\n",
    "# Plot top-N worst reconstructed test images\n",
    "# [with their original test images side by side\n",
    "# and rank them from worst (highest reconstruction loss value)\n",
    "# to best reconstructed test image]\n",
    "############################################################\n",
    "TOP_WORST_RECONSTRUCTED_TEST_IMAGES = 50\n",
    "trainer.get_worst_test_samples(TOP_WORST_RECONSTRUCTED_TEST_IMAGES)\n",
    "trainer.model.eval()\n",
    "visualise_output(images             = trainer.worst_top_images, \n",
    "                 model              = trainer.model,\n",
    "                 compose_transforms = TRANSFORM_IMG,\n",
    "                 imgs_ids           = trainer.worst_imgs_ids,\n",
    "                 imgs_losses        = trainer.worst_imgs_losses,\n",
    "                 savefig_path       = trainer.main_folder_path + '/WORST_RECONSTRUCTED_TEST_IMAGES.png',\n",
    "                 device = trainer.device)\n",
    "\n",
    "############################################################\n",
    "# Plot top-N best reconstructed test images\n",
    "# [with their original test images side by side\n",
    "# and rank them from best (lowest reconstruction loss value)\n",
    "# to worst reconstructed test image]\n",
    "############################################################\n",
    "TOP_BEST_RECONSTRUCTED_TEST_IMAGES = 50\n",
    "trainer.get_best_test_samples(TOP_BEST_RECONSTRUCTED_TEST_IMAGES)\n",
    "trainer.model.eval()\n",
    "visualise_output(images             = trainer.best_top_images, \n",
    "                 model              = trainer.model,\n",
    "                 compose_transforms = TRANSFORM_IMG,\n",
    "                 imgs_ids           = trainer.best_imgs_ids,\n",
    "                 imgs_losses        = trainer.best_imgs_losses,\n",
    "                 savefig_path       = trainer.main_folder_path + '/BEST_RECONSTRUCTED_TEST_IMAGES.png',\n",
    "                 device = trainer.device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "################################################\n",
    "### Training & Validation metrics visualized ###\n",
    "################################################\n",
    "if not USE_PRETRAINED_MODEL:\n",
    "    trainer.plot_perlexity()\n",
    "\n",
    "#####################################################################\n",
    "### Codebook (a matrix of codewords) and Tokens Z_Q visualization ###\n",
    "#####################################################################\n",
    "#trainer.codebook_visualization()#nije bitno\n",
    "#trainer.plot_codebook_PCA()\n",
    "trainer.visualize_discrete_codes(compose_transforms = TRANSFORM_IMG, dataset_str = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_PRETRAINED_MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iris_mn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 (main, Nov 24 2022, 14:31:59) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "14524dedb1f240cfa87933405c6db7624e3e5d5d03a9d9b3f38011393b6d44e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
